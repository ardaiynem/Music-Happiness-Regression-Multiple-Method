{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"./data/dataset.csv\"\n",
    "df = pd.read_csv(dataPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['popularity',\n",
       " 'duration_ms',\n",
       " 'explicit',\n",
       " 'danceability',\n",
       " 'energy',\n",
       " 'key',\n",
       " 'loudness',\n",
       " 'mode',\n",
       " 'speechiness',\n",
       " 'acousticness',\n",
       " 'instrumentalness',\n",
       " 'liveness',\n",
       " 'valence',\n",
       " 'tempo',\n",
       " 'time_signature']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(df.columns)\n",
    "columnsToKeep = columns[4: -1]\n",
    "columnsToKeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113995</th>\n",
       "      <td>21</td>\n",
       "      <td>384999</td>\n",
       "      <td>False</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>5</td>\n",
       "      <td>-16.393</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>125.995</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113996</th>\n",
       "      <td>22</td>\n",
       "      <td>385000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.318</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>85.239</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113997</th>\n",
       "      <td>22</td>\n",
       "      <td>271466</td>\n",
       "      <td>False</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0839</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>132.378</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113998</th>\n",
       "      <td>41</td>\n",
       "      <td>283893</td>\n",
       "      <td>False</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>7</td>\n",
       "      <td>-10.889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.3810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>135.960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113999</th>\n",
       "      <td>22</td>\n",
       "      <td>241826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.204</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>79.198</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114000 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        popularity  duration_ms  explicit  danceability  energy  key  \\\n",
       "0               73       230666     False         0.676  0.4610    1   \n",
       "1               55       149610     False         0.420  0.1660    1   \n",
       "2               57       210826     False         0.438  0.3590    0   \n",
       "3               71       201933     False         0.266  0.0596    0   \n",
       "4               82       198853     False         0.618  0.4430    2   \n",
       "...            ...          ...       ...           ...     ...  ...   \n",
       "113995          21       384999     False         0.172  0.2350    5   \n",
       "113996          22       385000     False         0.174  0.1170    0   \n",
       "113997          22       271466     False         0.629  0.3290    0   \n",
       "113998          41       283893     False         0.587  0.5060    7   \n",
       "113999          22       241826     False         0.526  0.4870    1   \n",
       "\n",
       "        loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "0         -6.746     0       0.1430        0.0322          0.000001    0.3580   \n",
       "1        -17.235     1       0.0763        0.9240          0.000006    0.1010   \n",
       "2         -9.734     1       0.0557        0.2100          0.000000    0.1170   \n",
       "3        -18.515     1       0.0363        0.9050          0.000071    0.1320   \n",
       "4         -9.681     1       0.0526        0.4690          0.000000    0.0829   \n",
       "...          ...   ...          ...           ...               ...       ...   \n",
       "113995   -16.393     1       0.0422        0.6400          0.928000    0.0863   \n",
       "113996   -18.318     0       0.0401        0.9940          0.976000    0.1050   \n",
       "113997   -10.895     0       0.0420        0.8670          0.000000    0.0839   \n",
       "113998   -10.889     1       0.0297        0.3810          0.000000    0.2700   \n",
       "113999   -10.204     0       0.0725        0.6810          0.000000    0.0893   \n",
       "\n",
       "        valence    tempo  time_signature  \n",
       "0        0.7150   87.917               4  \n",
       "1        0.2670   77.489               4  \n",
       "2        0.1200   76.332               4  \n",
       "3        0.1430  181.740               3  \n",
       "4        0.1670  119.949               4  \n",
       "...         ...      ...             ...  \n",
       "113995   0.0339  125.995               5  \n",
       "113996   0.0350   85.239               4  \n",
       "113997   0.7430  132.378               4  \n",
       "113998   0.4130  135.960               4  \n",
       "113999   0.7080   79.198               4  \n",
       "\n",
       "[114000 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[columnsToKeep]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bool to numerical data for explicit row\n",
    "df.loc[:, 'explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# One hot encoding for nominal categroies\n",
    "df = pd.get_dummies(df, columns=['key', 'time_signature'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>...</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>key_11</th>\n",
       "      <th>time_signature_0</th>\n",
       "      <th>time_signature_1</th>\n",
       "      <th>time_signature_3</th>\n",
       "      <th>time_signature_4</th>\n",
       "      <th>time_signature_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113995</th>\n",
       "      <td>21</td>\n",
       "      <td>384999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>-16.393</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113996</th>\n",
       "      <td>22</td>\n",
       "      <td>385000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>-18.318</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113997</th>\n",
       "      <td>22</td>\n",
       "      <td>271466</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>-10.895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113998</th>\n",
       "      <td>41</td>\n",
       "      <td>283893</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>-10.889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.3810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113999</th>\n",
       "      <td>22</td>\n",
       "      <td>241826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>-10.204</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114000 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        popularity  duration_ms  explicit  danceability  energy  loudness  \\\n",
       "0               73       230666         0         0.676  0.4610    -6.746   \n",
       "1               55       149610         0         0.420  0.1660   -17.235   \n",
       "2               57       210826         0         0.438  0.3590    -9.734   \n",
       "3               71       201933         0         0.266  0.0596   -18.515   \n",
       "4               82       198853         0         0.618  0.4430    -9.681   \n",
       "...            ...          ...       ...           ...     ...       ...   \n",
       "113995          21       384999         0         0.172  0.2350   -16.393   \n",
       "113996          22       385000         0         0.174  0.1170   -18.318   \n",
       "113997          22       271466         0         0.629  0.3290   -10.895   \n",
       "113998          41       283893         0         0.587  0.5060   -10.889   \n",
       "113999          22       241826         0         0.526  0.4870   -10.204   \n",
       "\n",
       "        mode  speechiness  acousticness  instrumentalness  ...  key_7  key_8  \\\n",
       "0          0       0.1430        0.0322          0.000001  ...      0      0   \n",
       "1          1       0.0763        0.9240          0.000006  ...      0      0   \n",
       "2          1       0.0557        0.2100          0.000000  ...      0      0   \n",
       "3          1       0.0363        0.9050          0.000071  ...      0      0   \n",
       "4          1       0.0526        0.4690          0.000000  ...      0      0   \n",
       "...      ...          ...           ...               ...  ...    ...    ...   \n",
       "113995     1       0.0422        0.6400          0.928000  ...      0      0   \n",
       "113996     0       0.0401        0.9940          0.976000  ...      0      0   \n",
       "113997     0       0.0420        0.8670          0.000000  ...      0      0   \n",
       "113998     1       0.0297        0.3810          0.000000  ...      1      0   \n",
       "113999     0       0.0725        0.6810          0.000000  ...      0      0   \n",
       "\n",
       "        key_9  key_10  key_11  time_signature_0  time_signature_1  \\\n",
       "0           0       0       0                 0                 0   \n",
       "1           0       0       0                 0                 0   \n",
       "2           0       0       0                 0                 0   \n",
       "3           0       0       0                 0                 0   \n",
       "4           0       0       0                 0                 0   \n",
       "...       ...     ...     ...               ...               ...   \n",
       "113995      0       0       0                 0                 0   \n",
       "113996      0       0       0                 0                 0   \n",
       "113997      0       0       0                 0                 0   \n",
       "113998      0       0       0                 0                 0   \n",
       "113999      0       0       0                 0                 0   \n",
       "\n",
       "        time_signature_3  time_signature_4  time_signature_5  \n",
       "0                      0                 1                 0  \n",
       "1                      0                 1                 0  \n",
       "2                      0                 1                 0  \n",
       "3                      1                 0                 0  \n",
       "4                      0                 1                 0  \n",
       "...                  ...               ...               ...  \n",
       "113995                 0                 0                 1  \n",
       "113996                 0                 1                 0  \n",
       "113997                 0                 1                 0  \n",
       "113998                 0                 1                 0  \n",
       "113999                 0                 1                 0  \n",
       "\n",
       "[114000 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(df):\n",
    "    min_vals = df.min()\n",
    "    max_vals = df.max()\n",
    "\n",
    "    feature_range = max_vals - min_vals\n",
    "\n",
    "    # Check if any feature has zero range\n",
    "    zero_range_features = feature_range[feature_range == 0].index\n",
    "\n",
    "    # Remove features with zero range from normalization\n",
    "    valid_features = feature_range[feature_range != 0].index\n",
    "    df_normalized = (df[valid_features] - min_vals[valid_features]) / feature_range[valid_features]\n",
    "\n",
    "    # Concatenate back the zero range features\n",
    "    if not zero_range_features.empty:\n",
    "        df_normalized = pd.concat([df_normalized, df[zero_range_features]], axis=1)\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "def standard_scaling(df):\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    return (df - mean) / std\n",
    "\n",
    "responseFrame = df.pop('valence')\n",
    "predictorFrame = df\n",
    "\n",
    "# Min-Max scaling for predictor variables\n",
    "df_normalized = min_max_scaling(predictorFrame)\n",
    "\n",
    "# Standard scaling for predictor variables\n",
    "df_standardized = standard_scaling(predictorFrame)\n",
    "\n",
    "predictorFrame_scaled = df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LassoRegression:\n",
    "    def __init__(self, lr=0.01, lambdaConstant=0.1, n_iters=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.lambdaConstant = lambdaConstant\n",
    "        self.weightVector = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        biasColumn = np.ones((num_samples, 1))\n",
    "        designMatrix = np.hstack((biasColumn, X))\n",
    "        self.weightVector = np.random.rand(num_features + 1)\n",
    "\n",
    "        self.subgradientDescent(designMatrix, y)\n",
    "        print('Final Train Loss:', self.lossLassoMSE(designMatrix, y))\n",
    "    \n",
    "    def mse_gradient(self, designMatrix, y):\n",
    "        predictions = self.predict(designMatrix)\n",
    "        return  -(2/y.size) * np.dot(designMatrix.T, (y - predictions))\n",
    "    \n",
    "    def subgradientDescent(self, designMatrix, y):\n",
    "        for i in range(self.n_iters):\n",
    "            gradientVector = self.lassoMSE_subgradient(designMatrix, y)\n",
    "\n",
    "            # Update weights using subgradient descent\n",
    "            self.weightVector = self.weightVector - self.lr * gradientVector\n",
    "            loss = self.lossLassoMSE(designMatrix, y)\n",
    "            print(f'Train Loss at iteration {i}:', loss)\n",
    "            self.loss_history.append((i, loss))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def lassoMSE_subgradient(self, designMatrix, y):\n",
    "        mse_gradient = self.mse_gradient(designMatrix, y)\n",
    "        lasso_gradient = np.sign(self.weightVector)\n",
    "        return mse_gradient + self.lambdaConstant * lasso_gradient\n",
    "    \n",
    "    def predict(self, designMatrix):\n",
    "        return np.dot(designMatrix, self.weightVector)\n",
    "    \n",
    "    def inference(self, testData):\n",
    "        num_samples = testData.shape[0]\n",
    "        biasColumn = np.ones((num_samples, 1))\n",
    "        designMatrix = np.hstack((biasColumn, testData))\n",
    "        return np.dot(designMatrix, self.weightVector)\n",
    "    \n",
    "    def lossMSE(self, designMatrix, y):\n",
    "        predictions = self.predict(designMatrix)\n",
    "        error = y - predictions\n",
    "        squaredError = np.dot(error.T, error)\n",
    "        meanSquaredError = 1/(y.size) * squaredError\n",
    "        return meanSquaredError\n",
    "    \n",
    "    def lossLassoMSE(self, designMatrix, y):\n",
    "        mse = self.lossMSE(designMatrix, y)\n",
    "        lasso_mse = mse + self.lambdaConstant * np.sum(np.abs(self.weightVector))\n",
    "        return lasso_mse\n",
    "    \n",
    "    def plot_loss_history(self):\n",
    "        iterations, losses = zip(*self.loss_history)\n",
    "        plt.plot(iterations, losses)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title(f'Training Loss over Iterations - Lasso (lr = {self.lr}, iter = {self.n_iters}, lambda = {self.lambdaConstant})')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseData = responseFrame.to_numpy()\n",
    "predictorData = predictorFrame_scaled.to_numpy()\n",
    "\n",
    "trainSplit = 0.8\n",
    "valSplit = 0.1\n",
    "testSplit = 0.1\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(predictorData))\n",
    "np.random.shuffle(indices)\n",
    "trainIndices = indices[:int(trainSplit * len(indices))]\n",
    "valIndices = indices[int(trainSplit* len(indices)):int((trainSplit + valSplit) * len(indices))]\n",
    "testIndices = indices[int((trainSplit + valSplit) * len(indices)):]\n",
    "\n",
    "trainPredictor, testPredictor, valPredictor = predictorData[trainIndices], predictorData[testIndices], predictorData[valIndices]\n",
    "trainResponse, testResponse, valResponse = responseData[trainIndices], responseData[testIndices], responseData[valIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at iteration 0: 0.5360268742729636\n",
      "Train Loss at iteration 1: 0.22389081105454506\n",
      "Train Loss at iteration 2: 0.2114921068134666\n",
      "Train Loss at iteration 3: 0.20634241151415553\n",
      "Train Loss at iteration 4: 0.20154666725023643\n",
      "Train Loss at iteration 5: 0.19693492350577704\n",
      "Train Loss at iteration 6: 0.1924936376103517\n",
      "Train Loss at iteration 7: 0.18821506478844283\n",
      "Train Loss at iteration 8: 0.1840919619881405\n",
      "Train Loss at iteration 9: 0.1801174837251714\n",
      "Train Loss at iteration 10: 0.17628515390487837\n",
      "Train Loss at iteration 11: 0.17258884143851866\n",
      "Train Loss at iteration 12: 0.16902273778116017\n",
      "Train Loss at iteration 13: 0.16558133619030754\n",
      "Train Loss at iteration 14: 0.16225941255796952\n",
      "Train Loss at iteration 15: 0.15905200768300867\n",
      "Train Loss at iteration 16: 0.15595441086262163\n",
      "Train Loss at iteration 17: 0.15296214469266842\n",
      "Train Loss at iteration 18: 0.15007095097642903\n",
      "Train Loss at iteration 19: 0.14727677765031436\n",
      "Train Loss at iteration 20: 0.14457576664317728\n",
      "Train Loss at iteration 21: 0.14196424259324208\n",
      "Train Loss at iteration 22: 0.13943870235336026\n",
      "Train Loss at iteration 23: 0.13699580522137875\n",
      "Train Loss at iteration 24: 0.13463236383792712\n",
      "Train Loss at iteration 25: 0.1323453356989421\n",
      "Train Loss at iteration 26: 0.13013181523480838\n",
      "Train Loss at iteration 27: 0.127989026412134\n",
      "Train Loss at iteration 28: 0.12591431581794646\n",
      "Train Loss at iteration 29: 0.12390514618952063\n",
      "Train Loss at iteration 30: 0.12195909035616527\n",
      "Train Loss at iteration 31: 0.12007382556213142\n",
      "Train Loss at iteration 32: 0.1182471281423866\n",
      "Train Loss at iteration 33: 0.11647686852535005\n",
      "Train Loss at iteration 34: 0.11476100653882541\n",
      "Train Loss at iteration 35: 0.11309758699731803\n",
      "Train Loss at iteration 36: 0.11148473555070347\n",
      "Train Loss at iteration 37: 0.10992065477583565\n",
      "Train Loss at iteration 38: 0.10840362049416293\n",
      "Train Loss at iteration 39: 0.10693197829977079\n",
      "Train Loss at iteration 40: 0.10550455852861111\n",
      "Train Loss at iteration 41: 0.10412006909022917\n",
      "Train Loss at iteration 42: 0.10277634459839168\n",
      "Train Loss at iteration 43: 0.10147198113635338\n",
      "Train Loss at iteration 44: 0.1002056301373437\n",
      "Train Loss at iteration 45: 0.09897599551160595\n",
      "Train Loss at iteration 46: 0.09778183130408381\n",
      "Train Loss at iteration 47: 0.09662193948301184\n",
      "Train Loss at iteration 48: 0.09549516784264693\n",
      "Train Loss at iteration 49: 0.0944004080129355\n",
      "Train Loss at iteration 50: 0.09333659356962688\n",
      "Train Loss at iteration 51: 0.09230269823881283\n",
      "Train Loss at iteration 52: 0.09129773419029798\n",
      "Train Loss at iteration 53: 0.09032075041459843\n",
      "Train Loss at iteration 54: 0.08937083117872571\n",
      "Train Loss at iteration 55: 0.08844709455624551\n",
      "Train Loss at iteration 56: 0.08754869102740598\n",
      "Train Loss at iteration 57: 0.08667480214541237\n",
      "Train Loss at iteration 58: 0.08582463926518588\n",
      "Train Loss at iteration 59: 0.08499744233118292\n",
      "Train Loss at iteration 60: 0.08419247872107578\n",
      "Train Loss at iteration 61: 0.0834090421422977\n",
      "Train Loss at iteration 62: 0.08264645157864844\n",
      "Train Loss at iteration 63: 0.08190405028432998\n",
      "Train Loss at iteration 64: 0.08118120482294677\n",
      "Train Loss at iteration 65: 0.080477304149155\n",
      "Train Loss at iteration 66: 0.07979175873078634\n",
      "Train Loss at iteration 67: 0.07912399970940223\n",
      "Train Loss at iteration 68: 0.0784734780973551\n",
      "Train Loss at iteration 69: 0.07783966400954666\n",
      "Train Loss at iteration 70: 0.07722204592817826\n",
      "Train Loss at iteration 71: 0.07662012999888539\n",
      "Train Loss at iteration 72: 0.07603343935674081\n",
      "Train Loss at iteration 73: 0.07546151348069491\n",
      "Train Loss at iteration 74: 0.07490390757510233\n",
      "Train Loss at iteration 75: 0.07436019197705743\n",
      "Train Loss at iteration 76: 0.0738299515883312\n",
      "Train Loss at iteration 77: 0.07331278533076747\n",
      "Train Loss at iteration 78: 0.0728083056240563\n",
      "Train Loss at iteration 79: 0.07231613788486092\n",
      "Train Loss at iteration 80: 0.07183592004632722\n",
      "Train Loss at iteration 81: 0.07136730209705537\n",
      "Train Loss at iteration 82: 0.07090994563866083\n",
      "Train Loss at iteration 83: 0.07046352346109563\n",
      "Train Loss at iteration 84: 0.07002771913494381\n",
      "Train Loss at iteration 85: 0.0696022266199429\n",
      "Train Loss at iteration 86: 0.0691867498890219\n",
      "Train Loss at iteration 87: 0.06878100256717952\n",
      "Train Loss at iteration 88: 0.06838470758456104\n",
      "Train Loss at iteration 89: 0.06799759684312198\n",
      "Train Loss at iteration 90: 0.06761941089629703\n",
      "Train Loss at iteration 91: 0.06724992514170318\n",
      "Train Loss at iteration 92: 0.06688891868249329\n",
      "Train Loss at iteration 93: 0.06653610551985878\n",
      "Train Loss at iteration 94: 0.06619125764509688\n",
      "Train Loss at iteration 95: 0.06585415450877223\n",
      "Train Loss at iteration 96: 0.06552458254582129\n",
      "Train Loss at iteration 97: 0.06520233492593183\n",
      "Train Loss at iteration 98: 0.06488721131834913\n",
      "Train Loss at iteration 99: 0.06457901766576016\n",
      "Train Loss at iteration 100: 0.0642775659667657\n",
      "Train Loss at iteration 101: 0.06398267406658129\n",
      "Train Loss at iteration 102: 0.06369416545562696\n",
      "Train Loss at iteration 103: 0.06341186907568248\n",
      "Train Loss at iteration 104: 0.0631356191332975\n",
      "Train Loss at iteration 105: 0.06286525492016197\n",
      "Train Loss at iteration 106: 0.06260062064015443\n",
      "Train Loss at iteration 107: 0.062341565242798394\n",
      "Train Loss at iteration 108: 0.062087942262869504\n",
      "Train Loss at iteration 109: 0.06183960966590675\n",
      "Train Loss at iteration 110: 0.06159642969939265\n",
      "Train Loss at iteration 111: 0.06135826874937706\n",
      "Train Loss at iteration 112: 0.06112499720232883\n",
      "Train Loss at iteration 113: 0.06089648931200987\n",
      "Train Loss at iteration 114: 0.060672623071173194\n",
      "Train Loss at iteration 115: 0.06045328008789738\n",
      "Train Loss at iteration 116: 0.06023834546637534\n",
      "Train Loss at iteration 117: 0.060027707691984906\n",
      "Train Loss at iteration 118: 0.059821258520475214\n",
      "Train Loss at iteration 119: 0.059618892871109656\n",
      "Train Loss at iteration 120: 0.059420508723613284\n",
      "Train Loss at iteration 121: 0.05922600701877854\n",
      "Train Loss at iteration 122: 0.0590352915625894\n",
      "Train Loss at iteration 123: 0.05884826893372978\n",
      "Train Loss at iteration 124: 0.05866484839434744\n",
      "Train Loss at iteration 125: 0.05848494180394984\n",
      "Train Loss at iteration 126: 0.058308463536314005\n",
      "Train Loss at iteration 127: 0.058135330399296144\n",
      "Train Loss at iteration 128: 0.05796546155743273\n",
      "Train Loss at iteration 129: 0.05779877845722794\n",
      "Train Loss at iteration 130: 0.05763520475502723\n",
      "Train Loss at iteration 131: 0.05747466624738083\n",
      "Train Loss at iteration 132: 0.05731709080380429\n",
      "Train Loss at iteration 133: 0.057162408301847635\n",
      "Train Loss at iteration 134: 0.05701055056438747\n",
      "Train Loss at iteration 135: 0.05686145129906035\n",
      "Train Loss at iteration 136: 0.05671504603975847\n",
      "Train Loss at iteration 137: 0.056571358604476846\n",
      "Train Loss at iteration 138: 0.056430531396889465\n",
      "Train Loss at iteration 139: 0.05629221460597789\n",
      "Train Loss at iteration 140: 0.05615635025493349\n",
      "Train Loss at iteration 141: 0.05602288214614094\n",
      "Train Loss at iteration 142: 0.05589175560433438\n",
      "Train Loss at iteration 143: 0.05576291742552163\n",
      "Train Loss at iteration 144: 0.05563631583225543\n",
      "Train Loss at iteration 145: 0.05551190043050092\n",
      "Train Loss at iteration 146: 0.05538962216793533\n",
      "Train Loss at iteration 147: 0.0552694332936267\n",
      "Train Loss at iteration 148: 0.05515128731904226\n",
      "Train Loss at iteration 149: 0.055035138980339356\n",
      "Train Loss at iteration 150: 0.05492094420189351\n",
      "Train Loss at iteration 151: 0.054808660061019886\n",
      "Train Loss at iteration 152: 0.05469839150640326\n",
      "Train Loss at iteration 153: 0.05459019775203609\n",
      "Train Loss at iteration 154: 0.05448378016970384\n",
      "Train Loss at iteration 155: 0.054379100496962436\n",
      "Train Loss at iteration 156: 0.05427612145615652\n",
      "Train Loss at iteration 157: 0.05417480672206865\n",
      "Train Loss at iteration 158: 0.05407512089419247\n",
      "Train Loss at iteration 159: 0.05397702946997797\n",
      "Train Loss at iteration 160: 0.05388049881893312\n",
      "Train Loss at iteration 161: 0.053785496157550634\n",
      "Train Loss at iteration 162: 0.05369198952503096\n",
      "Train Loss at iteration 163: 0.053599947759774315\n",
      "Train Loss at iteration 164: 0.053509340476615555\n",
      "Train Loss at iteration 165: 0.053420138044776154\n",
      "Train Loss at iteration 166: 0.053332311566509076\n",
      "Train Loss at iteration 167: 0.05324583285641283\n",
      "Train Loss at iteration 168: 0.053160674421391996\n",
      "Train Loss at iteration 169: 0.05307680944124231\n",
      "Train Loss at iteration 170: 0.05299421174983926\n",
      "Train Loss at iteration 171: 0.05291285581690979\n",
      "Train Loss at iteration 172: 0.052832716730367435\n",
      "Train Loss at iteration 173: 0.05275377017919205\n",
      "Train Loss at iteration 174: 0.05267599243683605\n",
      "Train Loss at iteration 175: 0.05259936034513918\n",
      "Train Loss at iteration 176: 0.0525238512987353\n",
      "Train Loss at iteration 177: 0.05244944322993452\n",
      "Train Loss at iteration 178: 0.052376114594065194\n",
      "Train Loss at iteration 179: 0.05230384435526022\n",
      "Train Loss at iteration 180: 0.05223261197267333\n",
      "Train Loss at iteration 181: 0.052162397387111\n",
      "Train Loss at iteration 182: 0.052093181008066425\n",
      "Train Loss at iteration 183: 0.05202494370114237\n",
      "Train Loss at iteration 184: 0.05195766677585007\n",
      "Train Loss at iteration 185: 0.051891331973772235\n",
      "Train Loss at iteration 186: 0.051825921457077886\n",
      "Train Loss at iteration 187: 0.051761417797377966\n",
      "Train Loss at iteration 188: 0.05169780396491064\n",
      "Train Loss at iteration 189: 0.0516350633180453\n",
      "Train Loss at iteration 190: 0.051573179593095604\n",
      "Train Loss at iteration 191: 0.051512285728329735\n",
      "Train Loss at iteration 192: 0.05145247575868171\n",
      "Train Loss at iteration 193: 0.05139346797424098\n",
      "Train Loss at iteration 194: 0.05133524762646742\n",
      "Train Loss at iteration 195: 0.05127780035173773\n",
      "Train Loss at iteration 196: 0.051221112107524246\n",
      "Train Loss at iteration 197: 0.05116516916286746\n",
      "Train Loss at iteration 198: 0.051109958090358565\n",
      "Train Loss at iteration 199: 0.05105546575838128\n",
      "Train Loss at iteration 200: 0.051001679323576785\n",
      "Train Loss at iteration 201: 0.05094858622352372\n",
      "Train Loss at iteration 202: 0.05089617416962666\n",
      "Train Loss at iteration 203: 0.05084443114020682\n",
      "Train Loss at iteration 204: 0.050793345373788004\n",
      "Train Loss at iteration 205: 0.05074290536257263\n",
      "Train Loss at iteration 206: 0.05069309984610112\n",
      "Train Loss at iteration 207: 0.050643917805089526\n",
      "Train Loss at iteration 208: 0.05059534845543968\n",
      "Train Loss at iteration 209: 0.050547381242416645\n",
      "Train Loss at iteration 210: 0.050500005834988176\n",
      "Train Loss at iteration 211: 0.050453212120321396\n",
      "Train Loss at iteration 212: 0.05040699019843191\n",
      "Train Loss at iteration 213: 0.05036133037698058\n",
      "Train Loss at iteration 214: 0.0503162231662135\n",
      "Train Loss at iteration 215: 0.05027165927404089\n",
      "Train Loss at iteration 216: 0.05022762960125084\n",
      "Train Loss at iteration 217: 0.05018412523685343\n",
      "Train Loss at iteration 218: 0.05014113745355179\n",
      "Train Loss at iteration 219: 0.050098657703335964\n",
      "Train Loss at iteration 220: 0.05005667761319604\n",
      "Train Loss at iteration 221: 0.050015188980950906\n",
      "Train Loss at iteration 222: 0.049974183771189395\n",
      "Train Loss at iteration 223: 0.04993365411132011\n",
      "Train Loss at iteration 224: 0.049893592287727065\n",
      "Train Loss at iteration 225: 0.04985399074202794\n",
      "Train Loss at iteration 226: 0.049814842067431676\n",
      "Train Loss at iteration 227: 0.0497761390051929\n",
      "Train Loss at iteration 228: 0.04973787444115988\n",
      "Train Loss at iteration 229: 0.0497000414024138\n",
      "Train Loss at iteration 230: 0.049662633053996146\n",
      "Train Loss at iteration 231: 0.04962564269572211\n",
      "Train Loss at iteration 232: 0.04958906375907715\n",
      "Train Loss at iteration 233: 0.04955288980419458\n",
      "Train Loss at iteration 234: 0.04951711451691167\n",
      "Train Loss at iteration 235: 0.049481731705902046\n",
      "Train Loss at iteration 236: 0.04944673529988228\n",
      "Train Loss at iteration 237: 0.0494121193448903\n",
      "Train Loss at iteration 238: 0.04937787800163403\n",
      "Train Loss at iteration 239: 0.049344005542907673\n",
      "Train Loss at iteration 240: 0.04931049635107417\n",
      "Train Loss at iteration 241: 0.049277344915611795\n",
      "Train Loss at iteration 242: 0.04924454583072299\n",
      "Train Loss at iteration 243: 0.0492120937930039\n",
      "Train Loss at iteration 244: 0.0491799835991726\n",
      "Train Loss at iteration 245: 0.049148210143854704\n",
      "Train Loss at iteration 246: 0.04911676841742452\n",
      "Train Loss at iteration 247: 0.049085653503900206\n",
      "Train Loss at iteration 248: 0.04905486057889158\n",
      "Train Loss at iteration 249: 0.04902438490759893\n",
      "Train Loss at iteration 250: 0.04899422184286156\n",
      "Train Loss at iteration 251: 0.048964366823254724\n",
      "Train Loss at iteration 252: 0.04893481537123325\n",
      "Train Loss at iteration 253: 0.04890556309132125\n",
      "Train Loss at iteration 254: 0.04887660566834588\n",
      "Train Loss at iteration 255: 0.04884793886571463\n",
      "Train Loss at iteration 256: 0.048819558523734444\n",
      "Train Loss at iteration 257: 0.04879146055797176\n",
      "Train Loss at iteration 258: 0.04876370765418558\n",
      "Train Loss at iteration 259: 0.04873634925119097\n",
      "Train Loss at iteration 260: 0.04870926053456279\n",
      "Train Loss at iteration 261: 0.04868243765714792\n",
      "Train Loss at iteration 262: 0.04865587688686613\n",
      "Train Loss at iteration 263: 0.04862957455944458\n",
      "Train Loss at iteration 264: 0.048603527075908495\n",
      "Train Loss at iteration 265: 0.04857773090116214\n",
      "Train Loss at iteration 266: 0.04855218256263063\n",
      "Train Loss at iteration 267: 0.048526878648937045\n",
      "Train Loss at iteration 268: 0.04850181580861309\n",
      "Train Loss at iteration 269: 0.04847699074884218\n",
      "Train Loss at iteration 270: 0.04845240023423412\n",
      "Train Loss at iteration 271: 0.04842804108563015\n",
      "Train Loss at iteration 272: 0.04840391017893747\n",
      "Train Loss at iteration 273: 0.048380004443992465\n",
      "Train Loss at iteration 274: 0.04835632086345158\n",
      "Train Loss at iteration 275: 0.048332856471709144\n",
      "Train Loss at iteration 276: 0.04830960835384123\n",
      "Train Loss at iteration 277: 0.04828657364457495\n",
      "Train Loss at iteration 278: 0.048263749527282164\n",
      "Train Loss at iteration 279: 0.04824113323299726\n",
      "Train Loss at iteration 280: 0.04821872203945801\n",
      "Train Loss at iteration 281: 0.04819651327016899\n",
      "Train Loss at iteration 282: 0.04817450429348684\n",
      "Train Loss at iteration 283: 0.04815269252172695\n",
      "Train Loss at iteration 284: 0.04813107541029073\n",
      "Train Loss at iteration 285: 0.04810965045681301\n",
      "Train Loss at iteration 286: 0.04808841520032912\n",
      "Train Loss at iteration 287: 0.04806736722046088\n",
      "Train Loss at iteration 288: 0.04804650413662123\n",
      "Train Loss at iteration 289: 0.048025823607236895\n",
      "Train Loss at iteration 290: 0.048005323328988506\n",
      "Train Loss at iteration 291: 0.04798500103606798\n",
      "Train Loss at iteration 292: 0.047964854499452446\n",
      "Train Loss at iteration 293: 0.04794488152619437\n",
      "Train Loss at iteration 294: 0.047925079958727565\n",
      "Train Loss at iteration 295: 0.04790544767418846\n",
      "Train Loss at iteration 296: 0.04788598258375254\n",
      "Train Loss at iteration 297: 0.04786668263198524\n",
      "Train Loss at iteration 298: 0.0478475457962071\n",
      "Train Loss at iteration 299: 0.047828570085872914\n",
      "Train Loss at iteration 300: 0.047809753541964206\n",
      "Train Loss at iteration 301: 0.04779109423639515\n",
      "Train Loss at iteration 302: 0.04777259027143114\n",
      "Train Loss at iteration 303: 0.04775423977911999\n",
      "Train Loss at iteration 304: 0.04773604092073539\n",
      "Train Loss at iteration 305: 0.04771799188623226\n",
      "Train Loss at iteration 306: 0.0477000908937137\n",
      "Train Loss at iteration 307: 0.04768233618890945\n",
      "Train Loss at iteration 308: 0.04766472604466524\n",
      "Train Loss at iteration 309: 0.04764725876044306\n",
      "Train Loss at iteration 310: 0.047629932661832006\n",
      "Train Loss at iteration 311: 0.047612746100069346\n",
      "Train Loss at iteration 312: 0.047595697451571675\n",
      "Train Loss at iteration 313: 0.04757878511747593\n",
      "Train Loss at iteration 314: 0.04756200752318982\n",
      "Train Loss at iteration 315: 0.04754536311795192\n",
      "Train Loss at iteration 316: 0.047528850374400494\n",
      "Train Loss at iteration 317: 0.04751246778815158\n",
      "Train Loss at iteration 318: 0.047496213877385604\n",
      "Train Loss at iteration 319: 0.047480087182442524\n",
      "Train Loss at iteration 320: 0.047464086265425344\n",
      "Train Loss at iteration 321: 0.04744820970981168\n",
      "Train Loss at iteration 322: 0.04743245612007324\n",
      "Train Loss at iteration 323: 0.047416824121303096\n",
      "Train Loss at iteration 324: 0.04740131235885056\n",
      "Train Loss at iteration 325: 0.04738591949796327\n",
      "Train Loss at iteration 326: 0.047370644223436786\n",
      "Train Loss at iteration 327: 0.04735548523927092\n",
      "Train Loss at iteration 328: 0.04734044126833324\n",
      "Train Loss at iteration 329: 0.047325511052029194\n",
      "Train Loss at iteration 330: 0.047310693349978804\n",
      "Train Loss at iteration 331: 0.04729598693969979\n",
      "Train Loss at iteration 332: 0.04728139061629718\n",
      "Train Loss at iteration 333: 0.04726690319215878\n",
      "Train Loss at iteration 334: 0.04725252349665687\n",
      "Train Loss at iteration 335: 0.04723825037585577\n",
      "Train Loss at iteration 336: 0.0472240826922251\n",
      "Train Loss at iteration 337: 0.04721001932435859\n",
      "Train Loss at iteration 338: 0.04719605916669863\n",
      "Train Loss at iteration 339: 0.04718220112926602\n",
      "Train Loss at iteration 340: 0.04716844413739496\n",
      "Train Loss at iteration 341: 0.04715478713147334\n",
      "Train Loss at iteration 342: 0.04714122906668792\n",
      "Train Loss at iteration 343: 0.04712776891277451\n",
      "Train Loss at iteration 344: 0.047114405653772985\n",
      "Train Loss at iteration 345: 0.04710113828778696\n",
      "Train Loss at iteration 346: 0.047087965826748124\n",
      "Train Loss at iteration 347: 0.047074887296185024\n",
      "Train Loss at iteration 348: 0.04706190173499635\n",
      "Train Loss at iteration 349: 0.0470490081952285\n",
      "Train Loss at iteration 350: 0.04703620574185737\n",
      "Train Loss at iteration 351: 0.04702349345257427\n",
      "Train Loss at iteration 352: 0.047010870417575915\n",
      "Train Loss at iteration 353: 0.04699833573935847\n",
      "Train Loss at iteration 354: 0.04698588853251522\n",
      "Train Loss at iteration 355: 0.04697352792353835\n",
      "Train Loss at iteration 356: 0.0469612530506242\n",
      "Train Loss at iteration 357: 0.046949063063482364\n",
      "Train Loss at iteration 358: 0.046936957123148125\n",
      "Train Loss at iteration 359: 0.04692493440179862\n",
      "Train Loss at iteration 360: 0.04691299408257224\n",
      "Train Loss at iteration 361: 0.0469011353593915\n",
      "Train Loss at iteration 362: 0.04688935743678906\n",
      "Train Loss at iteration 363: 0.04687765952973709\n",
      "Train Loss at iteration 364: 0.04686604086347974\n",
      "Train Loss at iteration 365: 0.04685450067336856\n",
      "Train Loss at iteration 366: 0.04684303820470116\n",
      "Train Loss at iteration 367: 0.046831652712562635\n",
      "Train Loss at iteration 368: 0.04682034346166996\n",
      "Train Loss at iteration 369: 0.046809109726219246\n",
      "Train Loss at iteration 370: 0.04679795078973567\n",
      "Train Loss at iteration 371: 0.0467868659449263\n",
      "Train Loss at iteration 372: 0.04677585449353538\n",
      "Train Loss at iteration 373: 0.04676491574620234\n",
      "Train Loss at iteration 374: 0.04675404902232241\n",
      "Train Loss at iteration 375: 0.046743253649909625\n",
      "Train Loss at iteration 376: 0.0467325289654623\n",
      "Train Loss at iteration 377: 0.046721874313831124\n",
      "Train Loss at iteration 378: 0.0467112890480892\n",
      "Train Loss at iteration 379: 0.04670077252940484\n",
      "Train Loss at iteration 380: 0.04669032412691631\n",
      "Train Loss at iteration 381: 0.04667994321760899\n",
      "Train Loss at iteration 382: 0.04666962918619452\n",
      "Train Loss at iteration 383: 0.046659381424992354\n",
      "Train Loss at iteration 384: 0.04664919933381309\n",
      "Train Loss at iteration 385: 0.04663908231984412\n",
      "Train Loss at iteration 386: 0.0466290297975371\n",
      "Train Loss at iteration 387: 0.04661904118849753\n",
      "Train Loss at iteration 388: 0.04660911592137611\n",
      "Train Loss at iteration 389: 0.04659925343176219\n",
      "Train Loss at iteration 390: 0.046589453162078936\n",
      "Train Loss at iteration 391: 0.04657971456148032\n",
      "Train Loss at iteration 392: 0.046570037085749985\n",
      "Train Loss at iteration 393: 0.04656042019720179\n",
      "Train Loss at iteration 394: 0.04655086336458206\n",
      "Train Loss at iteration 395: 0.046541366062973606\n",
      "Train Loss at iteration 396: 0.04653192777370129\n",
      "Train Loss at iteration 397: 0.046522547984239375\n",
      "Train Loss at iteration 398: 0.04651322618812019\n",
      "Train Loss at iteration 399: 0.04650396188484463\n",
      "Train Loss at iteration 400: 0.04649475457979412\n",
      "Train Loss at iteration 401: 0.04648560378414394\n",
      "Train Loss at iteration 402: 0.04647650901477815\n",
      "Train Loss at iteration 403: 0.04646746979420594\n",
      "Train Loss at iteration 404: 0.04645848565047939\n",
      "Train Loss at iteration 405: 0.04644955611711262\n",
      "Train Loss at iteration 406: 0.046440680733002275\n",
      "Train Loss at iteration 407: 0.04643185904234945\n",
      "Train Loss at iteration 408: 0.046423090594582724\n",
      "Train Loss at iteration 409: 0.04641437494428273\n",
      "Train Loss at iteration 410: 0.04640571165110784\n",
      "Train Loss at iteration 411: 0.0463971002797211\n",
      "Train Loss at iteration 412: 0.046388540399718396\n",
      "Train Loss at iteration 413: 0.04638003158555796\n",
      "Train Loss at iteration 414: 0.046371573416490675\n",
      "Train Loss at iteration 415: 0.046363165476491996\n",
      "Train Loss at iteration 416: 0.0463548073541947\n",
      "Train Loss at iteration 417: 0.04634649864282278\n",
      "Train Loss at iteration 418: 0.04633823894012655\n",
      "Train Loss at iteration 419: 0.04633002784831866\n",
      "Train Loss at iteration 420: 0.046321864974011355\n",
      "Train Loss at iteration 421: 0.04631374992815452\n",
      "Train Loss at iteration 422: 0.046305682325975005\n",
      "Train Loss at iteration 423: 0.046297661786916654\n",
      "Train Loss at iteration 424: 0.046289687934581644\n",
      "Train Loss at iteration 425: 0.04628176039667244\n",
      "Train Loss at iteration 426: 0.04627387880493495\n",
      "Train Loss at iteration 427: 0.04626604279510245\n",
      "Train Loss at iteration 428: 0.0462582520068405\n",
      "Train Loss at iteration 429: 0.046250506083692716\n",
      "Train Loss at iteration 430: 0.0462428046730274\n",
      "Train Loss at iteration 431: 0.04623514742598509\n",
      "Train Loss at iteration 432: 0.04622753399742682\n",
      "Train Loss at iteration 433: 0.04621996404588345\n",
      "Train Loss at iteration 434: 0.046212437233505516\n",
      "Train Loss at iteration 435: 0.04620495322601406\n",
      "Train Loss at iteration 436: 0.04619751169265229\n",
      "Train Loss at iteration 437: 0.04619011230613782\n",
      "Train Loss at iteration 438: 0.0461827547426158\n",
      "Train Loss at iteration 439: 0.04617543868161282\n",
      "Train Loss at iteration 440: 0.04616816380599139\n",
      "Train Loss at iteration 441: 0.04616092980190532\n",
      "Train Loss at iteration 442: 0.04615373635875573\n",
      "Train Loss at iteration 443: 0.04614658316914761\n",
      "Train Loss at iteration 444: 0.04613946992884736\n",
      "Train Loss at iteration 445: 0.04613239633674074\n",
      "Train Loss at iteration 446: 0.04612536209479155\n",
      "Train Loss at iteration 447: 0.046118366908001085\n",
      "Train Loss at iteration 448: 0.04611141048436794\n",
      "Train Loss at iteration 449: 0.04610449253484878\n",
      "Train Loss at iteration 450: 0.04609761277331944\n",
      "Train Loss at iteration 451: 0.04609077091653681\n",
      "Train Loss at iteration 452: 0.04608396668410117\n",
      "Train Loss at iteration 453: 0.04607719979841926\n",
      "Train Loss at iteration 454: 0.046070469984667746\n",
      "Train Loss at iteration 455: 0.046063776970757433\n",
      "Train Loss at iteration 456: 0.04605712048729787\n",
      "Train Loss at iteration 457: 0.04605050026756254\n",
      "Train Loss at iteration 458: 0.04604391604745473\n",
      "Train Loss at iteration 459: 0.04603736756547364\n",
      "Train Loss at iteration 460: 0.0460308545626813\n",
      "Train Loss at iteration 461: 0.04602437678266972\n",
      "Train Loss at iteration 462: 0.04601793397152886\n",
      "Train Loss at iteration 463: 0.046011525877814693\n",
      "Train Loss at iteration 464: 0.046005152252518174\n",
      "Train Loss at iteration 465: 0.04599881284903428\n",
      "Train Loss at iteration 466: 0.04599250742313184\n",
      "Train Loss at iteration 467: 0.045986235732923604\n",
      "Train Loss at iteration 468: 0.045979997538836866\n",
      "Train Loss at iteration 469: 0.04597379260358451\n",
      "Train Loss at iteration 470: 0.0459676206921365\n",
      "Train Loss at iteration 471: 0.045961481571691706\n",
      "Train Loss at iteration 472: 0.04595537501165033\n",
      "Train Loss at iteration 473: 0.04594930078358654\n",
      "Train Loss at iteration 474: 0.04594325866122166\n",
      "Train Loss at iteration 475: 0.045937248420397724\n",
      "Train Loss at iteration 476: 0.045931269839051304\n",
      "Train Loss at iteration 477: 0.04592532269718798\n",
      "Train Loss at iteration 478: 0.045919406776856915\n",
      "Train Loss at iteration 479: 0.04591352186212589\n",
      "Train Loss at iteration 480: 0.04590766773905692\n",
      "Train Loss at iteration 481: 0.04590184419568182\n",
      "Train Loss at iteration 482: 0.0458960510219785\n",
      "Train Loss at iteration 483: 0.04589028800984746\n",
      "Train Loss at iteration 484: 0.04588455495308851\n",
      "Train Loss at iteration 485: 0.04587885164737811\n",
      "Train Loss at iteration 486: 0.045873177890246745\n",
      "Train Loss at iteration 487: 0.04586753348105685\n",
      "Train Loss at iteration 488: 0.045861918220980895\n",
      "Train Loss at iteration 489: 0.04585633191297993\n",
      "Train Loss at iteration 490: 0.04585077436178233\n",
      "Train Loss at iteration 491: 0.04584524537386286\n",
      "Train Loss at iteration 492: 0.04583974475742215\n",
      "Train Loss at iteration 493: 0.045834272322366254\n",
      "Train Loss at iteration 494: 0.0458288278802868\n",
      "Train Loss at iteration 495: 0.04582341124444104\n",
      "Train Loss at iteration 496: 0.04581802222973261\n",
      "Train Loss at iteration 497: 0.045812660652692204\n",
      "Train Loss at iteration 498: 0.04580732633145871\n",
      "Train Loss at iteration 499: 0.04580201908576064\n",
      "Train Loss at iteration 500: 0.045796738736897644\n",
      "Train Loss at iteration 501: 0.045791485107722484\n",
      "Train Loss at iteration 502: 0.045786258022623164\n",
      "Train Loss at iteration 503: 0.04578105730750526\n",
      "Train Loss at iteration 504: 0.045775882789774694\n",
      "Train Loss at iteration 505: 0.04577073429832047\n",
      "Train Loss at iteration 506: 0.04576561166349792\n",
      "Train Loss at iteration 507: 0.04576051471711199\n",
      "Train Loss at iteration 508: 0.045755443292400874\n",
      "Train Loss at iteration 509: 0.04575039722401985\n",
      "Train Loss at iteration 510: 0.04574537634802525\n",
      "Train Loss at iteration 511: 0.04574038050185884\n",
      "Train Loss at iteration 512: 0.04573540952433223\n",
      "Train Loss at iteration 513: 0.04573046325561155\n",
      "Train Loss at iteration 514: 0.0457255415372025\n",
      "Train Loss at iteration 515: 0.045720644211935314\n",
      "Train Loss at iteration 516: 0.04571577112395014\n",
      "Train Loss at iteration 517: 0.04571092211868264\n",
      "Train Loss at iteration 518: 0.04570609704284957\n",
      "Train Loss at iteration 519: 0.04570129574443484\n",
      "Train Loss at iteration 520: 0.04569651807267556\n",
      "Train Loss at iteration 521: 0.04569176387804831\n",
      "Train Loss at iteration 522: 0.045687033012255664\n",
      "Train Loss at iteration 523: 0.04568232532821288\n",
      "Train Loss at iteration 524: 0.045677640680034715\n",
      "Train Loss at iteration 525: 0.045672978923022446\n",
      "Train Loss at iteration 526: 0.04566833991365111\n",
      "Train Loss at iteration 527: 0.04566372350955684\n",
      "Train Loss at iteration 528: 0.04565912956952441\n",
      "Train Loss at iteration 529: 0.04565455795347501\n",
      "Train Loss at iteration 530: 0.04565000852245403\n",
      "Train Loss at iteration 531: 0.04564548113861913\n",
      "Train Loss at iteration 532: 0.04564097566522845\n",
      "Train Loss at iteration 533: 0.04563649196662896\n",
      "Train Loss at iteration 534: 0.045632029908244956\n",
      "Train Loss at iteration 535: 0.045627589356566724\n",
      "Train Loss at iteration 536: 0.04562317017913933\n",
      "Train Loss at iteration 537: 0.04561877224455162\n",
      "Train Loss at iteration 538: 0.04561439542242526\n",
      "Train Loss at iteration 539: 0.04561003958340405\n",
      "Train Loss at iteration 540: 0.04560570459914325\n",
      "Train Loss at iteration 541: 0.04560139034229909\n",
      "Train Loss at iteration 542: 0.045597096686518575\n",
      "Train Loss at iteration 543: 0.045592823506429035\n",
      "Train Loss at iteration 544: 0.045588570677628296\n",
      "Train Loss at iteration 545: 0.04558433807667458\n",
      "Train Loss at iteration 546: 0.04558012558107672\n",
      "Train Loss at iteration 547: 0.045575933069284556\n",
      "Train Loss at iteration 548: 0.04557176042067928\n",
      "Train Loss at iteration 549: 0.045567607515564\n",
      "Train Loss at iteration 550: 0.045563474235154514\n",
      "Train Loss at iteration 551: 0.04555936046156998\n",
      "Train Loss at iteration 552: 0.045555266077823946\n",
      "Train Loss at iteration 553: 0.04555119096781531\n",
      "Train Loss at iteration 554: 0.045547135016319566\n",
      "Train Loss at iteration 555: 0.04554309810897987\n",
      "Train Loss at iteration 556: 0.04553908013229867\n",
      "Train Loss at iteration 557: 0.04553508097362896\n",
      "Train Loss at iteration 558: 0.04553110052116603\n",
      "Train Loss at iteration 559: 0.04552713866393903\n",
      "Train Loss at iteration 560: 0.04552319529180287\n",
      "Train Loss at iteration 561: 0.04551927029543006\n",
      "Train Loss at iteration 562: 0.04551536356630276\n",
      "Train Loss at iteration 563: 0.04551147499670481\n",
      "Train Loss at iteration 564: 0.045507604479714\n",
      "Train Loss at iteration 565: 0.04550375190919433\n",
      "Train Loss at iteration 566: 0.04549991717978842\n",
      "Train Loss at iteration 567: 0.045496100186909985\n",
      "Train Loss at iteration 568: 0.04549230082673639\n",
      "Train Loss at iteration 569: 0.04548851899620138\n",
      "Train Loss at iteration 570: 0.045484754592987786\n",
      "Train Loss at iteration 571: 0.04548100751552042\n",
      "Train Loss at iteration 572: 0.045477277662958955\n",
      "Train Loss at iteration 573: 0.045473564935190995\n",
      "Train Loss at iteration 574: 0.04546986923282515\n",
      "Train Loss at iteration 575: 0.04546619045718426\n",
      "Train Loss at iteration 576: 0.04546252851029864\n",
      "Train Loss at iteration 577: 0.045458883294899444\n",
      "Train Loss at iteration 578: 0.045455254714412134\n",
      "Train Loss at iteration 579: 0.04545164267294991\n",
      "Train Loss at iteration 580: 0.04544804707530737\n",
      "Train Loss at iteration 581: 0.0454444678269542\n",
      "Train Loss at iteration 582: 0.04544090483402884\n",
      "Train Loss at iteration 583: 0.04543735800333236\n",
      "Train Loss at iteration 584: 0.04543382724232235\n",
      "Train Loss at iteration 585: 0.045430312459106836\n",
      "Train Loss at iteration 586: 0.04542681356243838\n",
      "Train Loss at iteration 587: 0.04542333046170816\n",
      "Train Loss at iteration 588: 0.04541986306694015\n",
      "Train Loss at iteration 589: 0.04541641128878535\n",
      "Train Loss at iteration 590: 0.0454129750385161\n",
      "Train Loss at iteration 591: 0.045409554228020496\n",
      "Train Loss at iteration 592: 0.045406148769796775\n",
      "Train Loss at iteration 593: 0.04540275857694791\n",
      "Train Loss at iteration 594: 0.04539938356317608\n",
      "Train Loss at iteration 595: 0.04539602364277738\n",
      "Train Loss at iteration 596: 0.04539267873063649\n",
      "Train Loss at iteration 597: 0.04538934874222146\n",
      "Train Loss at iteration 598: 0.0453860335935785\n",
      "Train Loss at iteration 599: 0.04538273320132687\n",
      "Train Loss at iteration 600: 0.045379447482653795\n",
      "Train Loss at iteration 601: 0.045376176355309564\n",
      "Train Loss at iteration 602: 0.04537291973760242\n",
      "Train Loss at iteration 603: 0.04536967754839379\n",
      "Train Loss at iteration 604: 0.045366449707093436\n",
      "Train Loss at iteration 605: 0.04536323613365462\n",
      "Train Loss at iteration 606: 0.04536003674856942\n",
      "Train Loss at iteration 607: 0.04535685147286407\n",
      "Train Loss at iteration 608: 0.04535368022809431\n",
      "Train Loss at iteration 609: 0.045350522936340794\n",
      "Train Loss at iteration 610: 0.045347379520204664\n",
      "Train Loss at iteration 611: 0.04534424990280299\n",
      "Train Loss at iteration 612: 0.045341134007764405\n",
      "Train Loss at iteration 613: 0.04533803175922467\n",
      "Train Loss at iteration 614: 0.04533494308182248\n",
      "Train Loss at iteration 615: 0.04533186790069507\n",
      "Train Loss at iteration 616: 0.04532880614147407\n",
      "Train Loss at iteration 617: 0.04532575773028124\n",
      "Train Loss at iteration 618: 0.045322722593724454\n",
      "Train Loss at iteration 619: 0.045319700658893514\n",
      "Train Loss at iteration 620: 0.045316691853356156\n",
      "Train Loss at iteration 621: 0.045313696105154046\n",
      "Train Loss at iteration 622: 0.045310713342798806\n",
      "Train Loss at iteration 623: 0.04530774349526814\n",
      "Train Loss at iteration 624: 0.04530478649200191\n",
      "Train Loss at iteration 625: 0.0453018422628984\n",
      "Train Loss at iteration 626: 0.045298910738310376\n",
      "Train Loss at iteration 627: 0.045295991849041525\n",
      "Train Loss at iteration 628: 0.045293085526342584\n",
      "Train Loss at iteration 629: 0.045290191701907796\n",
      "Train Loss at iteration 630: 0.045287310307871226\n",
      "Train Loss at iteration 631: 0.045284441276803165\n",
      "Train Loss at iteration 632: 0.04528158454170662\n",
      "Train Loss at iteration 633: 0.04527874003601373\n",
      "Train Loss at iteration 634: 0.045275907693582404\n",
      "Train Loss at iteration 635: 0.045273087448692774\n",
      "Train Loss at iteration 636: 0.04527027923604384\n",
      "Train Loss at iteration 637: 0.04526748299075009\n",
      "Train Loss at iteration 638: 0.045264698648338195\n",
      "Train Loss at iteration 639: 0.04526192614474367\n",
      "Train Loss at iteration 640: 0.04525916541630764\n",
      "Train Loss at iteration 641: 0.0452564163997736\n",
      "Train Loss at iteration 642: 0.04525367903228422\n",
      "Train Loss at iteration 643: 0.04525095325137821\n",
      "Train Loss at iteration 644: 0.04524823899498712\n",
      "Train Loss at iteration 645: 0.045245536201432325\n",
      "Train Loss at iteration 646: 0.04524284480942188\n",
      "Train Loss at iteration 647: 0.045240164758047575\n",
      "Train Loss at iteration 648: 0.045237495986781825\n",
      "Train Loss at iteration 649: 0.045234838435474795\n",
      "Train Loss at iteration 650: 0.0452321920443514\n",
      "Train Loss at iteration 651: 0.0452295567540084\n",
      "Train Loss at iteration 652: 0.04522693250541153\n",
      "Train Loss at iteration 653: 0.04522431923989261\n",
      "Train Loss at iteration 654: 0.0452217168991468\n",
      "Train Loss at iteration 655: 0.0452191254252297\n",
      "Train Loss at iteration 656: 0.04521654476055461\n",
      "Train Loss at iteration 657: 0.04521397484788984\n",
      "Train Loss at iteration 658: 0.04521141563035599\n",
      "Train Loss at iteration 659: 0.04520886705142314\n",
      "Train Loss at iteration 660: 0.04520632905490837\n",
      "Train Loss at iteration 661: 0.045203801584972975\n",
      "Train Loss at iteration 662: 0.04520128458611992\n",
      "Train Loss at iteration 663: 0.045198778003191276\n",
      "Train Loss at iteration 664: 0.045196281781365595\n",
      "Train Loss at iteration 665: 0.04519379586615546\n",
      "Train Loss at iteration 666: 0.04519132020340492\n",
      "Train Loss at iteration 667: 0.045188854739286986\n",
      "Train Loss at iteration 668: 0.045186399420301226\n",
      "Train Loss at iteration 669: 0.04518395419327133\n",
      "Train Loss at iteration 670: 0.04518151900534262\n",
      "Train Loss at iteration 671: 0.04517909380397975\n",
      "Train Loss at iteration 672: 0.045176678536964276\n",
      "Train Loss at iteration 673: 0.04517427315239237\n",
      "Train Loss at iteration 674: 0.0451718775986724\n",
      "Train Loss at iteration 675: 0.04516949182452276\n",
      "Train Loss at iteration 676: 0.0451671157789695\n",
      "Train Loss at iteration 677: 0.045164749411344116\n",
      "Train Loss at iteration 678: 0.045162392671281255\n",
      "Train Loss at iteration 679: 0.04516004550871655\n",
      "Train Loss at iteration 680: 0.04515770787388444\n",
      "Train Loss at iteration 681: 0.045155379717315966\n",
      "Train Loss at iteration 682: 0.04515306098983663\n",
      "Train Loss at iteration 683: 0.045150751642564234\n",
      "Train Loss at iteration 684: 0.04514845162690684\n",
      "Train Loss at iteration 685: 0.04514616089456059\n",
      "Train Loss at iteration 686: 0.045143879397507684\n",
      "Train Loss at iteration 687: 0.045141607088014346\n",
      "Train Loss at iteration 688: 0.04513934391862872\n",
      "Train Loss at iteration 689: 0.0451370898421789\n",
      "Train Loss at iteration 690: 0.045134844811770956\n",
      "Train Loss at iteration 691: 0.04513260878078687\n",
      "Train Loss at iteration 692: 0.045130381702882666\n",
      "Train Loss at iteration 693: 0.045128163531986426\n",
      "Train Loss at iteration 694: 0.045125954222296316\n",
      "Train Loss at iteration 695: 0.04512375372827881\n",
      "Train Loss at iteration 696: 0.045121562004666646\n",
      "Train Loss at iteration 697: 0.045119379006457044\n",
      "Train Loss at iteration 698: 0.045117204688909826\n",
      "Train Loss at iteration 699: 0.045115039007545564\n",
      "Train Loss at iteration 700: 0.045112881918143805\n",
      "Train Loss at iteration 701: 0.045110733376741145\n",
      "Train Loss at iteration 702: 0.0451085933396296\n",
      "Train Loss at iteration 703: 0.04510646176335467\n",
      "Train Loss at iteration 704: 0.04510433860471374\n",
      "Train Loss at iteration 705: 0.04510222382075417\n",
      "Train Loss at iteration 706: 0.045100117368771675\n",
      "Train Loss at iteration 707: 0.0450980192063086\n",
      "Train Loss at iteration 708: 0.04509592929115217\n",
      "Train Loss at iteration 709: 0.04509384758133285\n",
      "Train Loss at iteration 710: 0.04509177403512266\n",
      "Train Loss at iteration 711: 0.04508970861103353\n",
      "Train Loss at iteration 712: 0.04508765126781566\n",
      "Train Loss at iteration 713: 0.04508560196445585\n",
      "Train Loss at iteration 714: 0.04508356066017596\n",
      "Train Loss at iteration 715: 0.04508152731443127\n",
      "Train Loss at iteration 716: 0.04507950188690885\n",
      "Train Loss at iteration 717: 0.04507748433752615\n",
      "Train Loss at iteration 718: 0.0450754746264292\n",
      "Train Loss at iteration 719: 0.04507347271399127\n",
      "Train Loss at iteration 720: 0.04507147856081127\n",
      "Train Loss at iteration 721: 0.045069492127712205\n",
      "Train Loss at iteration 722: 0.045067513375739714\n",
      "Train Loss at iteration 723: 0.04506554226616049\n",
      "Train Loss at iteration 724: 0.045063578760460965\n",
      "Train Loss at iteration 725: 0.04506162282034569\n",
      "Train Loss at iteration 726: 0.04505967440773591\n",
      "Train Loss at iteration 727: 0.0450577334847682\n",
      "Train Loss at iteration 728: 0.045055800013792925\n",
      "Train Loss at iteration 729: 0.045053873957372925\n",
      "Train Loss at iteration 730: 0.04505195527828201\n",
      "Train Loss at iteration 731: 0.04505004393950365\n",
      "Train Loss at iteration 732: 0.04504813990422953\n",
      "Train Loss at iteration 733: 0.04504624313585821\n",
      "Train Loss at iteration 734: 0.04504435359799374\n",
      "Train Loss at iteration 735: 0.04504247125444435\n",
      "Train Loss at iteration 736: 0.04504059606922107\n",
      "Train Loss at iteration 737: 0.045038728006536397\n",
      "Train Loss at iteration 738: 0.04503686703080304\n",
      "Train Loss at iteration 739: 0.04503501310663254\n",
      "Train Loss at iteration 740: 0.04503316619883402\n",
      "Train Loss at iteration 741: 0.0450313262724129\n",
      "Train Loss at iteration 742: 0.045029493292569577\n",
      "Train Loss at iteration 743: 0.045027667224698205\n",
      "Train Loss at iteration 744: 0.04502584803438544\n",
      "Train Loss at iteration 745: 0.04502403568740917\n",
      "Train Loss at iteration 746: 0.04502223014973729\n",
      "Train Loss at iteration 747: 0.04502043138752651\n",
      "Train Loss at iteration 748: 0.04501863936712106\n",
      "Train Loss at iteration 749: 0.04501685405505158\n",
      "Train Loss at iteration 750: 0.0450150754180339\n",
      "Train Loss at iteration 751: 0.045013303422967756\n",
      "Train Loss at iteration 752: 0.04501153803693577\n",
      "Train Loss at iteration 753: 0.045009779227202194\n",
      "Train Loss at iteration 754: 0.04500802696121171\n",
      "Train Loss at iteration 755: 0.04500628120658841\n",
      "Train Loss at iteration 756: 0.045004541931134526\n",
      "Train Loss at iteration 757: 0.045002809102829396\n",
      "Train Loss at iteration 758: 0.04500108268982826\n",
      "Train Loss at iteration 759: 0.044999362660461215\n",
      "Train Loss at iteration 760: 0.044997648983232066\n",
      "Train Loss at iteration 761: 0.044995941626817255\n",
      "Train Loss at iteration 762: 0.0449942405600648\n",
      "Train Loss at iteration 763: 0.04499254575199308\n",
      "Train Loss at iteration 764: 0.04499085717178996\n",
      "Train Loss at iteration 765: 0.0449891747888116\n",
      "Train Loss at iteration 766: 0.044987498572581434\n",
      "Train Loss at iteration 767: 0.044985828492789114\n",
      "Train Loss at iteration 768: 0.04498416451928949\n",
      "Train Loss at iteration 769: 0.04498250662210158\n",
      "Train Loss at iteration 770: 0.04498085477140749\n",
      "Train Loss at iteration 771: 0.044979208937551494\n",
      "Train Loss at iteration 772: 0.04497756909103896\n",
      "Train Loss at iteration 773: 0.044975935202535386\n",
      "Train Loss at iteration 774: 0.044974307242865365\n",
      "Train Loss at iteration 775: 0.044972685183011625\n",
      "Train Loss at iteration 776: 0.044971068994114086\n",
      "Train Loss at iteration 777: 0.044969458647468805\n",
      "Train Loss at iteration 778: 0.04496785411452712\n",
      "Train Loss at iteration 779: 0.044966255366894595\n",
      "Train Loss at iteration 780: 0.044964662376330164\n",
      "Train Loss at iteration 781: 0.0449630751147451\n",
      "Train Loss at iteration 782: 0.044961493554202106\n",
      "Train Loss at iteration 783: 0.04495991766691446\n",
      "Train Loss at iteration 784: 0.04495834742524502\n",
      "Train Loss at iteration 785: 0.04495678280170532\n",
      "Train Loss at iteration 786: 0.04495522376895466\n",
      "Train Loss at iteration 787: 0.044953670299799235\n",
      "Train Loss at iteration 788: 0.04495212236719122\n",
      "Train Loss at iteration 789: 0.044950579944227895\n",
      "Train Loss at iteration 790: 0.044949043004150736\n",
      "Train Loss at iteration 791: 0.044947511520344575\n",
      "Train Loss at iteration 792: 0.04494598546633672\n",
      "Train Loss at iteration 793: 0.04494446481579609\n",
      "Train Loss at iteration 794: 0.044942949542532355\n",
      "Train Loss at iteration 795: 0.04494143962049509\n",
      "Train Loss at iteration 796: 0.04493993502377292\n",
      "Train Loss at iteration 797: 0.04493843572659277\n",
      "Train Loss at iteration 798: 0.04493694170331884\n",
      "Train Loss at iteration 799: 0.04493545292845203\n",
      "Train Loss at iteration 800: 0.0449339693766289\n",
      "Train Loss at iteration 801: 0.04493249102262098\n",
      "Train Loss at iteration 802: 0.04493101784133395\n",
      "Train Loss at iteration 803: 0.04492954980780678\n",
      "Train Loss at iteration 804: 0.04492808689721102\n",
      "Train Loss at iteration 805: 0.044926629084849935\n",
      "Train Loss at iteration 806: 0.044925176346157746\n",
      "Train Loss at iteration 807: 0.04492372865669888\n",
      "Train Loss at iteration 808: 0.044922285992167126\n",
      "Train Loss at iteration 809: 0.04492084832838497\n",
      "Train Loss at iteration 810: 0.04491941564130272\n",
      "Train Loss at iteration 811: 0.04491798790699784\n",
      "Train Loss at iteration 812: 0.044916565101674126\n",
      "Train Loss at iteration 813: 0.044915147201661024\n",
      "Train Loss at iteration 814: 0.04491373418341284\n",
      "Train Loss at iteration 815: 0.044912326023508024\n",
      "Train Loss at iteration 816: 0.04491092269864844\n",
      "Train Loss at iteration 817: 0.04490952418565863\n",
      "Train Loss at iteration 818: 0.04490813046148508\n",
      "Train Loss at iteration 819: 0.04490674150319556\n",
      "Train Loss at iteration 820: 0.04490535728797835\n",
      "Train Loss at iteration 821: 0.04490397779314156\n",
      "Train Loss at iteration 822: 0.044902602996112415\n",
      "Train Loss at iteration 823: 0.04490123287443661\n",
      "Train Loss at iteration 824: 0.044899867405777553\n",
      "Train Loss at iteration 825: 0.04489850656791567\n",
      "Train Loss at iteration 826: 0.04489715033874779\n",
      "Train Loss at iteration 827: 0.044895798696286446\n",
      "Train Loss at iteration 828: 0.04489445161865917\n",
      "Train Loss at iteration 829: 0.04489310908410782\n",
      "Train Loss at iteration 830: 0.044891771070987976\n",
      "Train Loss at iteration 831: 0.04489043755776823\n",
      "Train Loss at iteration 832: 0.044889108523029526\n",
      "Train Loss at iteration 833: 0.04488778394546455\n",
      "Train Loss at iteration 834: 0.04488646380387705\n",
      "Train Loss at iteration 835: 0.044885148077181206\n",
      "Train Loss at iteration 836: 0.044883836744401\n",
      "Train Loss at iteration 837: 0.04488252978466956\n",
      "Train Loss at iteration 838: 0.04488122717722855\n",
      "Train Loss at iteration 839: 0.04487992890142753\n",
      "Train Loss at iteration 840: 0.044878634936723366\n",
      "Train Loss at iteration 841: 0.04487734526267958\n",
      "Train Loss at iteration 842: 0.04487605985896577\n",
      "Train Loss at iteration 843: 0.044874778705356916\n",
      "Train Loss at iteration 844: 0.044873501781732945\n",
      "Train Loss at iteration 845: 0.04487222906807795\n",
      "Train Loss at iteration 846: 0.04487096054447968\n",
      "Train Loss at iteration 847: 0.044869696191128954\n",
      "Train Loss at iteration 848: 0.04486843598831907\n",
      "Train Loss at iteration 849: 0.04486717991644517\n",
      "Train Loss at iteration 850: 0.04486592795600375\n",
      "Train Loss at iteration 851: 0.044864680087591965\n",
      "Train Loss at iteration 852: 0.04486343629190717\n",
      "Train Loss at iteration 853: 0.04486219654974628\n",
      "Train Loss at iteration 854: 0.04486096084200524\n",
      "Train Loss at iteration 855: 0.04485972914967842\n",
      "Train Loss at iteration 856: 0.04485850145385813\n",
      "Train Loss at iteration 857: 0.044857277735733975\n",
      "Train Loss at iteration 858: 0.04485605797659235\n",
      "Train Loss at iteration 859: 0.04485484215781597\n",
      "Train Loss at iteration 860: 0.04485363026088314\n",
      "Train Loss at iteration 861: 0.04485242226736738\n",
      "Train Loss at iteration 862: 0.044851218158936834\n",
      "Train Loss at iteration 863: 0.04485001791735373\n",
      "Train Loss at iteration 864: 0.04484882152447382\n",
      "Train Loss at iteration 865: 0.04484762896224593\n",
      "Train Loss at iteration 866: 0.04484644021271138\n",
      "Train Loss at iteration 867: 0.04484525525800346\n",
      "Train Loss at iteration 868: 0.04484407408034695\n",
      "Train Loss at iteration 869: 0.044842896662057596\n",
      "Train Loss at iteration 870: 0.044841722985541586\n",
      "Train Loss at iteration 871: 0.044840553033295004\n",
      "Train Loss at iteration 872: 0.044839386787903446\n",
      "Train Loss at iteration 873: 0.0448382242320414\n",
      "Train Loss at iteration 874: 0.04483706534847182\n",
      "Train Loss at iteration 875: 0.044835910120045584\n",
      "Train Loss at iteration 876: 0.04483475852970104\n",
      "Train Loss at iteration 877: 0.04483361056046348\n",
      "Train Loss at iteration 878: 0.04483246619544473\n",
      "Train Loss at iteration 879: 0.04483132541784257\n",
      "Train Loss at iteration 880: 0.04483018821094034\n",
      "Train Loss at iteration 881: 0.0448290545581064\n",
      "Train Loss at iteration 882: 0.04482792444279372\n",
      "Train Loss at iteration 883: 0.04482679784853933\n",
      "Train Loss at iteration 884: 0.04482567475896394\n",
      "Train Loss at iteration 885: 0.044824555157771444\n",
      "Train Loss at iteration 886: 0.044823439028748385\n",
      "Train Loss at iteration 887: 0.04482232635576364\n",
      "Train Loss at iteration 888: 0.04482121712276786\n",
      "Train Loss at iteration 889: 0.04482011131379307\n",
      "Train Loss at iteration 890: 0.044819008912952145\n",
      "Train Loss at iteration 891: 0.04481790990443845\n",
      "Train Loss at iteration 892: 0.04481681427252535\n",
      "Train Loss at iteration 893: 0.044815722001565826\n",
      "Train Loss at iteration 894: 0.04481463307599194\n",
      "Train Loss at iteration 895: 0.04481354748031445\n",
      "Train Loss at iteration 896: 0.0448124651991224\n",
      "Train Loss at iteration 897: 0.04481138621708268\n",
      "Train Loss at iteration 898: 0.04481031051893956\n",
      "Train Loss at iteration 899: 0.04480923808951431\n",
      "Train Loss at iteration 900: 0.04480816891370473\n",
      "Train Loss at iteration 901: 0.044807102976484825\n",
      "Train Loss at iteration 902: 0.04480604026290425\n",
      "Train Loss at iteration 903: 0.04480498075808802\n",
      "Train Loss at iteration 904: 0.044803924447236\n",
      "Train Loss at iteration 905: 0.044802871315622605\n",
      "Train Loss at iteration 906: 0.04480182134859627\n",
      "Train Loss at iteration 907: 0.04480077453157915\n",
      "Train Loss at iteration 908: 0.04479973085006662\n",
      "Train Loss at iteration 909: 0.044798690289627026\n",
      "Train Loss at iteration 910: 0.04479765283590108\n",
      "Train Loss at iteration 911: 0.04479661847460165\n",
      "Train Loss at iteration 912: 0.044795587191513275\n",
      "Train Loss at iteration 913: 0.04479455897249178\n",
      "Train Loss at iteration 914: 0.044793533803463945\n",
      "Train Loss at iteration 915: 0.044792511670427006\n",
      "Train Loss at iteration 916: 0.04479149255944844\n",
      "Train Loss at iteration 917: 0.0447904764566654\n",
      "Train Loss at iteration 918: 0.044789463348284486\n",
      "Train Loss at iteration 919: 0.04478845322058127\n",
      "Train Loss at iteration 920: 0.04478744605989997\n",
      "Train Loss at iteration 921: 0.044786441852653104\n",
      "Train Loss at iteration 922: 0.04478544058532104\n",
      "Train Loss at iteration 923: 0.0447844422444517\n",
      "Train Loss at iteration 924: 0.044783446816660175\n",
      "Train Loss at iteration 925: 0.04478245428862833\n",
      "Train Loss at iteration 926: 0.04478146464710452\n",
      "Train Loss at iteration 927: 0.04478047787890312\n",
      "Train Loss at iteration 928: 0.0447794939709043\n",
      "Train Loss at iteration 929: 0.04477851291005354\n",
      "Train Loss at iteration 930: 0.044777534683361396\n",
      "Train Loss at iteration 931: 0.04477655927790306\n",
      "Train Loss at iteration 932: 0.04477558668081807\n",
      "Train Loss at iteration 933: 0.04477461687930993\n",
      "Train Loss at iteration 934: 0.04477364986064577\n",
      "Train Loss at iteration 935: 0.044772685612156034\n",
      "Train Loss at iteration 936: 0.04477172412123411\n",
      "Train Loss at iteration 937: 0.04477076537533602\n",
      "Train Loss at iteration 938: 0.04476980936198004\n",
      "Train Loss at iteration 939: 0.04476885606874639\n",
      "Train Loss at iteration 940: 0.044767905483276936\n",
      "Train Loss at iteration 941: 0.04476695759327482\n",
      "Train Loss at iteration 942: 0.04476601238650414\n",
      "Train Loss at iteration 943: 0.044765069850789596\n",
      "Train Loss at iteration 944: 0.04476412997401624\n",
      "Train Loss at iteration 945: 0.04476319274412911\n",
      "Train Loss at iteration 946: 0.04476225814913291\n",
      "Train Loss at iteration 947: 0.04476132617709164\n",
      "Train Loss at iteration 948: 0.04476039681612841\n",
      "Train Loss at iteration 949: 0.044759470054425\n",
      "Train Loss at iteration 950: 0.04475854588022161\n",
      "Train Loss at iteration 951: 0.04475762428181655\n",
      "Train Loss at iteration 952: 0.044756705247565885\n",
      "Train Loss at iteration 953: 0.044755788765883195\n",
      "Train Loss at iteration 954: 0.04475487482523918\n",
      "Train Loss at iteration 955: 0.044753963414161484\n",
      "Train Loss at iteration 956: 0.04475305452123427\n",
      "Train Loss at iteration 957: 0.044752148135097966\n",
      "Train Loss at iteration 958: 0.04475124424444901\n",
      "Train Loss at iteration 959: 0.04475034283803948\n",
      "Train Loss at iteration 960: 0.044749443904676826\n",
      "Train Loss at iteration 961: 0.04474854743322362\n",
      "Train Loss at iteration 962: 0.04474765341259723\n",
      "Train Loss at iteration 963: 0.04474676183176946\n",
      "Train Loss at iteration 964: 0.04474587267976643\n",
      "Train Loss at iteration 965: 0.0447449859456681\n",
      "Train Loss at iteration 966: 0.04474410161860814\n",
      "Train Loss at iteration 967: 0.044743219687773554\n",
      "Train Loss at iteration 968: 0.04474234014240443\n",
      "Train Loss at iteration 969: 0.04474146297179366\n",
      "Train Loss at iteration 970: 0.04474058816528667\n",
      "Train Loss at iteration 971: 0.0447397157122811\n",
      "Train Loss at iteration 972: 0.0447388456022266\n",
      "Train Loss at iteration 973: 0.044737977824624506\n",
      "Train Loss at iteration 974: 0.044737112369027575\n",
      "Train Loss at iteration 975: 0.0447362492250397\n",
      "Train Loss at iteration 976: 0.04473538838231571\n",
      "Train Loss at iteration 977: 0.04473452983056101\n",
      "Train Loss at iteration 978: 0.044733673559531395\n",
      "Train Loss at iteration 979: 0.04473281955903273\n",
      "Train Loss at iteration 980: 0.044731967818920686\n",
      "Train Loss at iteration 981: 0.04473111832910053\n",
      "Train Loss at iteration 982: 0.04473027107952685\n",
      "Train Loss at iteration 983: 0.044729426060203255\n",
      "Train Loss at iteration 984: 0.04472858326118214\n",
      "Train Loss at iteration 985: 0.04472774267256448\n",
      "Train Loss at iteration 986: 0.04472690428449946\n",
      "Train Loss at iteration 987: 0.04472606808718436\n",
      "Train Loss at iteration 988: 0.04472523407086423\n",
      "Train Loss at iteration 989: 0.04472440222583163\n",
      "Train Loss at iteration 990: 0.04472357254242639\n",
      "Train Loss at iteration 991: 0.044722745011035424\n",
      "Train Loss at iteration 992: 0.04472191962209239\n",
      "Train Loss at iteration 993: 0.044721096366077503\n",
      "Train Loss at iteration 994: 0.04472027523351732\n",
      "Train Loss at iteration 995: 0.044719456214984396\n",
      "Train Loss at iteration 996: 0.04471863930109716\n",
      "Train Loss at iteration 997: 0.04471782448251959\n",
      "Train Loss at iteration 998: 0.04471701174996103\n",
      "Train Loss at iteration 999: 0.04471620109417595\n",
      "Train Loss at iteration 1000: 0.044715392505963646\n",
      "Train Loss at iteration 1001: 0.044714585976168134\n",
      "Train Loss at iteration 1002: 0.044713781495677764\n",
      "Train Loss at iteration 1003: 0.04471297905542512\n",
      "Train Loss at iteration 1004: 0.04471217864638673\n",
      "Train Loss at iteration 1005: 0.04471138025958281\n",
      "Train Loss at iteration 1006: 0.04471058388607713\n",
      "Train Loss at iteration 1007: 0.04470978951697668\n",
      "Train Loss at iteration 1008: 0.04470899714343154\n",
      "Train Loss at iteration 1009: 0.04470820675663459\n",
      "Train Loss at iteration 1010: 0.04470741834782133\n",
      "Train Loss at iteration 1011: 0.04470663190826963\n",
      "Train Loss at iteration 1012: 0.04470584742929952\n",
      "Train Loss at iteration 1013: 0.044705064902273016\n",
      "Train Loss at iteration 1014: 0.044704284318593815\n",
      "Train Loss at iteration 1015: 0.04470350566970715\n",
      "Train Loss at iteration 1016: 0.04470272894709954\n",
      "Train Loss at iteration 1017: 0.044701954142298626\n",
      "Train Loss at iteration 1018: 0.04470118124687289\n",
      "Train Loss at iteration 1019: 0.04470041025243151\n",
      "Train Loss at iteration 1020: 0.04469964115062406\n",
      "Train Loss at iteration 1021: 0.04469887393314043\n",
      "Train Loss at iteration 1022: 0.0446981085917105\n",
      "Train Loss at iteration 1023: 0.04469734511810403\n",
      "Train Loss at iteration 1024: 0.04469658350413036\n",
      "Train Loss at iteration 1025: 0.04469582374163828\n",
      "Train Loss at iteration 1026: 0.044695065822515805\n",
      "Train Loss at iteration 1027: 0.044694309738689944\n",
      "Train Loss at iteration 1028: 0.04469355548212658\n",
      "Train Loss at iteration 1029: 0.044692803044830136\n",
      "Train Loss at iteration 1030: 0.04469205241884353\n",
      "Train Loss at iteration 1031: 0.044691303596247886\n",
      "Train Loss at iteration 1032: 0.044690556569162335\n",
      "Train Loss at iteration 1033: 0.04468981132974385\n",
      "Train Loss at iteration 1034: 0.04468906787018708\n",
      "Train Loss at iteration 1035: 0.04468832618272406\n",
      "Train Loss at iteration 1036: 0.04468758625962413\n",
      "Train Loss at iteration 1037: 0.0446868480931937\n",
      "Train Loss at iteration 1038: 0.044686111675775984\n",
      "Train Loss at iteration 1039: 0.04468537699975098\n",
      "Train Loss at iteration 1040: 0.04468464405753513\n",
      "Train Loss at iteration 1041: 0.044683912841581186\n",
      "Train Loss at iteration 1042: 0.044683183344378055\n",
      "Train Loss at iteration 1043: 0.04468245555845055\n",
      "Train Loss at iteration 1044: 0.04468172947635931\n",
      "Train Loss at iteration 1045: 0.04468100509070044\n",
      "Train Loss at iteration 1046: 0.04468028239410557\n",
      "Train Loss at iteration 1047: 0.04467956137924145\n",
      "Train Loss at iteration 1048: 0.04467884203880992\n",
      "Train Loss at iteration 1049: 0.044678124365547627\n",
      "Train Loss at iteration 1050: 0.044677408352225924\n",
      "Train Loss at iteration 1051: 0.0446766939916507\n",
      "Train Loss at iteration 1052: 0.04467598127666212\n",
      "Train Loss at iteration 1053: 0.04467527020013454\n",
      "Train Loss at iteration 1054: 0.04467456075497624\n",
      "Train Loss at iteration 1055: 0.04467385293412938\n",
      "Train Loss at iteration 1056: 0.044673146730569706\n",
      "Train Loss at iteration 1057: 0.044672442137306444\n",
      "Train Loss at iteration 1058: 0.04467173914738209\n",
      "Train Loss at iteration 1059: 0.044671037753872316\n",
      "Train Loss at iteration 1060: 0.04467033794988569\n",
      "Train Loss at iteration 1061: 0.044669639728563616\n",
      "Train Loss at iteration 1062: 0.04466894308308011\n",
      "Train Loss at iteration 1063: 0.04466824800664162\n",
      "Train Loss at iteration 1064: 0.04466755449248691\n",
      "Train Loss at iteration 1065: 0.04466686253388689\n",
      "Train Loss at iteration 1066: 0.04466617212414441\n",
      "Train Loss at iteration 1067: 0.04466548325659412\n",
      "Train Loss at iteration 1068: 0.04466479592460233\n",
      "Train Loss at iteration 1069: 0.04466411012156686\n",
      "Train Loss at iteration 1070: 0.04466342584091682\n",
      "Train Loss at iteration 1071: 0.04466274307611247\n",
      "Train Loss at iteration 1072: 0.044662061820645124\n",
      "Train Loss at iteration 1073: 0.044661382068036916\n",
      "Train Loss at iteration 1074: 0.04466070381184071\n",
      "Train Loss at iteration 1075: 0.04466002704563989\n",
      "Train Loss at iteration 1076: 0.04465935176304821\n",
      "Train Loss at iteration 1077: 0.044658677957709704\n",
      "Train Loss at iteration 1078: 0.04465800562329844\n",
      "Train Loss at iteration 1079: 0.04465733475351846\n",
      "Train Loss at iteration 1080: 0.044656665342103545\n",
      "Train Loss at iteration 1081: 0.044655997382817156\n",
      "Train Loss at iteration 1082: 0.044655330869452184\n",
      "Train Loss at iteration 1083: 0.044654665795830877\n",
      "Train Loss at iteration 1084: 0.044654002155804666\n",
      "Train Loss at iteration 1085: 0.04465333994325403\n",
      "Train Loss at iteration 1086: 0.04465267915208831\n",
      "Train Loss at iteration 1087: 0.04465201977624564\n",
      "Train Loss at iteration 1088: 0.04465136180969269\n",
      "Train Loss at iteration 1089: 0.04465070524642466\n",
      "Train Loss at iteration 1090: 0.044650050080465006\n",
      "Train Loss at iteration 1091: 0.0446493963058654\n",
      "Train Loss at iteration 1092: 0.044648743916705536\n",
      "Train Loss at iteration 1093: 0.044648092907093005\n",
      "Train Loss at iteration 1094: 0.04464744327116311\n",
      "Train Loss at iteration 1095: 0.04464679500307881\n",
      "Train Loss at iteration 1096: 0.04464614809703052\n",
      "Train Loss at iteration 1097: 0.044645502547235996\n",
      "Train Loss at iteration 1098: 0.0446448583479402\n",
      "Train Loss at iteration 1099: 0.04464421549341516\n",
      "Train Loss at iteration 1100: 0.044643573977959795\n",
      "Train Loss at iteration 1101: 0.04464293379589986\n",
      "Train Loss at iteration 1102: 0.044642294941587765\n",
      "Train Loss at iteration 1103: 0.04464165740940244\n",
      "Train Loss at iteration 1104: 0.04464102119374917\n",
      "Train Loss at iteration 1105: 0.04464038628905957\n",
      "Train Loss at iteration 1106: 0.044639752689791354\n",
      "Train Loss at iteration 1107: 0.044639120390428215\n",
      "Train Loss at iteration 1108: 0.04463848938547975\n",
      "Train Loss at iteration 1109: 0.04463785966948125\n",
      "Train Loss at iteration 1110: 0.044637231236993716\n",
      "Train Loss at iteration 1111: 0.044636604082603504\n",
      "Train Loss at iteration 1112: 0.044635978200922445\n",
      "Train Loss at iteration 1113: 0.04463535358658751\n",
      "Train Loss at iteration 1114: 0.04463473023426085\n",
      "Train Loss at iteration 1115: 0.04463410813862954\n",
      "Train Loss at iteration 1116: 0.044633487294405555\n",
      "Train Loss at iteration 1117: 0.044632867696325566\n",
      "Train Loss at iteration 1118: 0.04463224933915089\n",
      "Train Loss at iteration 1119: 0.04463163221766731\n",
      "Train Loss at iteration 1120: 0.04463101632668495\n",
      "Train Loss at iteration 1121: 0.044630401661038245\n",
      "Train Loss at iteration 1122: 0.044629788215585685\n",
      "Train Loss at iteration 1123: 0.044629175985209824\n",
      "Train Loss at iteration 1124: 0.04462856496481704\n",
      "Train Loss at iteration 1125: 0.04462795514933756\n",
      "Train Loss at iteration 1126: 0.044627346533725146\n",
      "Train Loss at iteration 1127: 0.044626739112957205\n",
      "Train Loss at iteration 1128: 0.04462613288203448\n",
      "Train Loss at iteration 1129: 0.04462552783598106\n",
      "Train Loss at iteration 1130: 0.044624923969844195\n",
      "Train Loss at iteration 1131: 0.044624321278694194\n",
      "Train Loss at iteration 1132: 0.04462371975762434\n",
      "Train Loss at iteration 1133: 0.04462311940175077\n",
      "Train Loss at iteration 1134: 0.04462252020621229\n",
      "Train Loss at iteration 1135: 0.0446219221661704\n",
      "Train Loss at iteration 1136: 0.04462132527680903\n",
      "Train Loss at iteration 1137: 0.044620729533334534\n",
      "Train Loss at iteration 1138: 0.044620134930975566\n",
      "Train Loss at iteration 1139: 0.04461954146498291\n",
      "Train Loss at iteration 1140: 0.04461894913062946\n",
      "Train Loss at iteration 1141: 0.044618357923210006\n",
      "Train Loss at iteration 1142: 0.044617767838041236\n",
      "Train Loss at iteration 1143: 0.04461717887046153\n",
      "Train Loss at iteration 1144: 0.04461659101583093\n",
      "Train Loss at iteration 1145: 0.044616004269530975\n",
      "Train Loss at iteration 1146: 0.04461541862696464\n",
      "Train Loss at iteration 1147: 0.04461483408355619\n",
      "Train Loss at iteration 1148: 0.04461425063475113\n",
      "Train Loss at iteration 1149: 0.04461366827601602\n",
      "Train Loss at iteration 1150: 0.044613087002838454\n",
      "Train Loss at iteration 1151: 0.044612506810726905\n",
      "Train Loss at iteration 1152: 0.044611927695210635\n",
      "Train Loss at iteration 1153: 0.04461134965183959\n",
      "Train Loss at iteration 1154: 0.04461077267618433\n",
      "Train Loss at iteration 1155: 0.04461019676383587\n",
      "Train Loss at iteration 1156: 0.044609621910405614\n",
      "Train Loss at iteration 1157: 0.044609048111525273\n",
      "Train Loss at iteration 1158: 0.044608475362846726\n",
      "Train Loss at iteration 1159: 0.044607903660041914\n",
      "Train Loss at iteration 1160: 0.04460733299880283\n",
      "Train Loss at iteration 1161: 0.044606763374841284\n",
      "Train Loss at iteration 1162: 0.044606194783888935\n",
      "Train Loss at iteration 1163: 0.0446056272216971\n",
      "Train Loss at iteration 1164: 0.0446050606840367\n",
      "Train Loss at iteration 1165: 0.04460449516669817\n",
      "Train Loss at iteration 1166: 0.04460393066549135\n",
      "Train Loss at iteration 1167: 0.04460336717624536\n",
      "Train Loss at iteration 1168: 0.044602804694808595\n",
      "Train Loss at iteration 1169: 0.04460224321704851\n",
      "Train Loss at iteration 1170: 0.04460168273885165\n",
      "Train Loss at iteration 1171: 0.04460112325612343\n",
      "Train Loss at iteration 1172: 0.044600564764788166\n",
      "Train Loss at iteration 1173: 0.044600007260788874\n",
      "Train Loss at iteration 1174: 0.04459945074008727\n",
      "Train Loss at iteration 1175: 0.04459889519866364\n",
      "Train Loss at iteration 1176: 0.04459834063251671\n",
      "Train Loss at iteration 1177: 0.044597787037663615\n",
      "Train Loss at iteration 1178: 0.04459723441013976\n",
      "Train Loss at iteration 1179: 0.04459668274599884\n",
      "Train Loss at iteration 1180: 0.044596132041312575\n",
      "Train Loss at iteration 1181: 0.04459558229217075\n",
      "Train Loss at iteration 1182: 0.0445950334946811\n",
      "Train Loss at iteration 1183: 0.044594485644969206\n",
      "Train Loss at iteration 1184: 0.04459393873917843\n",
      "Train Loss at iteration 1185: 0.04459339277346981\n",
      "Train Loss at iteration 1186: 0.04459284774402194\n",
      "Train Loss at iteration 1187: 0.044592303647031005\n",
      "Train Loss at iteration 1188: 0.044591760478710524\n",
      "Train Loss at iteration 1189: 0.04459121823529144\n",
      "Train Loss at iteration 1190: 0.04459067691302186\n",
      "Train Loss at iteration 1191: 0.04459013650816716\n",
      "Train Loss at iteration 1192: 0.04458959701700973\n",
      "Train Loss at iteration 1193: 0.044589058435849\n",
      "Train Loss at iteration 1194: 0.04458852076100129\n",
      "Train Loss at iteration 1195: 0.0445879839887998\n",
      "Train Loss at iteration 1196: 0.04458744811559445\n",
      "Train Loss at iteration 1197: 0.044586913137751885\n",
      "Train Loss at iteration 1198: 0.0445863790516553\n",
      "Train Loss at iteration 1199: 0.044585845853704405\n",
      "Train Loss at iteration 1200: 0.044585313540315384\n",
      "Train Loss at iteration 1201: 0.044584782107920724\n",
      "Train Loss at iteration 1202: 0.04458425155296924\n",
      "Train Loss at iteration 1203: 0.0445837218719259\n",
      "Train Loss at iteration 1204: 0.04458319306127182\n",
      "Train Loss at iteration 1205: 0.044582665117504146\n",
      "Train Loss at iteration 1206: 0.04458213803713597\n",
      "Train Loss at iteration 1207: 0.04458161181669628\n",
      "Train Loss at iteration 1208: 0.044581086452729894\n",
      "Train Loss at iteration 1209: 0.04458056194179735\n",
      "Train Loss at iteration 1210: 0.04458003828047481\n",
      "Train Loss at iteration 1211: 0.04457951546535407\n",
      "Train Loss at iteration 1212: 0.044578993493042375\n",
      "Train Loss at iteration 1213: 0.04457847236016243\n",
      "Train Loss at iteration 1214: 0.044577952063352286\n",
      "Train Loss at iteration 1215: 0.04457743259926531\n",
      "Train Loss at iteration 1216: 0.04457691396457\n",
      "Train Loss at iteration 1217: 0.04457639615595008\n",
      "Train Loss at iteration 1218: 0.04457587917010425\n",
      "Train Loss at iteration 1219: 0.044575363003746246\n",
      "Train Loss at iteration 1220: 0.04457484765360471\n",
      "Train Loss at iteration 1221: 0.04457433311642312\n",
      "Train Loss at iteration 1222: 0.04457381938895973\n",
      "Train Loss at iteration 1223: 0.04457330646798752\n",
      "Train Loss at iteration 1224: 0.044572794350294036\n",
      "Train Loss at iteration 1225: 0.04457228303268145\n",
      "Train Loss at iteration 1226: 0.04457177251196638\n",
      "Train Loss at iteration 1227: 0.0445712627849799\n",
      "Train Loss at iteration 1228: 0.044570753848567395\n",
      "Train Loss at iteration 1229: 0.044570245699588555\n",
      "Train Loss at iteration 1230: 0.044569738334917265\n",
      "Train Loss at iteration 1231: 0.04456923175144159\n",
      "Train Loss at iteration 1232: 0.04456872594606362\n",
      "Train Loss at iteration 1233: 0.04456822091569948\n",
      "Train Loss at iteration 1234: 0.04456771665727926\n",
      "Train Loss at iteration 1235: 0.04456721316774688\n",
      "Train Loss at iteration 1236: 0.04456671044406008\n",
      "Train Loss at iteration 1237: 0.04456620848319036\n",
      "Train Loss at iteration 1238: 0.04456570728212289\n",
      "Train Loss at iteration 1239: 0.04456520683785642\n",
      "Train Loss at iteration 1240: 0.04456470714740326\n",
      "Train Loss at iteration 1241: 0.04456420820778921\n",
      "Train Loss at iteration 1242: 0.0445637100160535\n",
      "Train Loss at iteration 1243: 0.044563212569248674\n",
      "Train Loss at iteration 1244: 0.04456271586444056\n",
      "Train Loss at iteration 1245: 0.04456221989870824\n",
      "Train Loss at iteration 1246: 0.044561724669143946\n",
      "Train Loss at iteration 1247: 0.04456123017285299\n",
      "Train Loss at iteration 1248: 0.04456073640695374\n",
      "Train Loss at iteration 1249: 0.0445602433685775\n",
      "Train Loss at iteration 1250: 0.04455975105486852\n",
      "Train Loss at iteration 1251: 0.0445592594629839\n",
      "Train Loss at iteration 1252: 0.04455876859009346\n",
      "Train Loss at iteration 1253: 0.044558278433379825\n",
      "Train Loss at iteration 1254: 0.04455778899003827\n",
      "Train Loss at iteration 1255: 0.044557300257276625\n",
      "Train Loss at iteration 1256: 0.04455681223231529\n",
      "Train Loss at iteration 1257: 0.044556324912387174\n",
      "Train Loss at iteration 1258: 0.044555838294737596\n",
      "Train Loss at iteration 1259: 0.044555352376624176\n",
      "Train Loss at iteration 1260: 0.044554867155316945\n",
      "Train Loss at iteration 1261: 0.04455438262809811\n",
      "Train Loss at iteration 1262: 0.04455389879226209\n",
      "Train Loss at iteration 1263: 0.04455341564511543\n",
      "Train Loss at iteration 1264: 0.04455293318397675\n",
      "Train Loss at iteration 1265: 0.04455245140617668\n",
      "Train Loss at iteration 1266: 0.04455197030905781\n",
      "Train Loss at iteration 1267: 0.044551489889974615\n",
      "Train Loss at iteration 1268: 0.04455101014629345\n",
      "Train Loss at iteration 1269: 0.04455053107539243\n",
      "Train Loss at iteration 1270: 0.04455005267466142\n",
      "Train Loss at iteration 1271: 0.04454957494150192\n",
      "Train Loss at iteration 1272: 0.0445490978733271\n",
      "Train Loss at iteration 1273: 0.04454862146756167\n",
      "Train Loss at iteration 1274: 0.044548145721641855\n",
      "Train Loss at iteration 1275: 0.04454767063301533\n",
      "Train Loss at iteration 1276: 0.044547196199141166\n",
      "Train Loss at iteration 1277: 0.044546722417489824\n",
      "Train Loss at iteration 1278: 0.044546249285543\n",
      "Train Loss at iteration 1279: 0.04454577680079364\n",
      "Train Loss at iteration 1280: 0.044545304960745914\n",
      "Train Loss at iteration 1281: 0.044544833762915084\n",
      "Train Loss at iteration 1282: 0.04454436320482751\n",
      "Train Loss at iteration 1283: 0.04454389328402059\n",
      "Train Loss at iteration 1284: 0.04454342399804267\n",
      "Train Loss at iteration 1285: 0.04454295534445304\n",
      "Train Loss at iteration 1286: 0.04454248732082185\n",
      "Train Loss at iteration 1287: 0.04454201992473007\n",
      "Train Loss at iteration 1288: 0.04454155315376947\n",
      "Train Loss at iteration 1289: 0.044541087005542476\n",
      "Train Loss at iteration 1290: 0.04454062147766224\n",
      "Train Loss at iteration 1291: 0.044540156567752495\n",
      "Train Loss at iteration 1292: 0.044539692273447576\n",
      "Train Loss at iteration 1293: 0.044539228592392274\n",
      "Train Loss at iteration 1294: 0.04453876552224192\n",
      "Train Loss at iteration 1295: 0.044538303060662185\n",
      "Train Loss at iteration 1296: 0.0445378412053292\n",
      "Train Loss at iteration 1297: 0.04453737995392932\n",
      "Train Loss at iteration 1298: 0.04453691930415922\n",
      "Train Loss at iteration 1299: 0.04453645925372581\n",
      "Train Loss at iteration 1300: 0.04453599980034616\n",
      "Train Loss at iteration 1301: 0.04453554094174745\n",
      "Train Loss at iteration 1302: 0.04453508267566692\n",
      "Train Loss at iteration 1303: 0.04453462499985191\n",
      "Train Loss at iteration 1304: 0.04453416791205967\n",
      "Train Loss at iteration 1305: 0.04453371141005745\n",
      "Train Loss at iteration 1306: 0.04453325549162231\n",
      "Train Loss at iteration 1307: 0.044532800154541245\n",
      "Train Loss at iteration 1308: 0.04453234539661098\n",
      "Train Loss at iteration 1309: 0.04453189121563801\n",
      "Train Loss at iteration 1310: 0.04453143760943852\n",
      "Train Loss at iteration 1311: 0.0445309845758384\n",
      "Train Loss at iteration 1312: 0.044530532112673105\n",
      "Train Loss at iteration 1313: 0.04453008021778766\n",
      "Train Loss at iteration 1314: 0.04452962888903665\n",
      "Train Loss at iteration 1315: 0.04452917812428409\n",
      "Train Loss at iteration 1316: 0.04452872792140347\n",
      "Train Loss at iteration 1317: 0.04452827827827767\n",
      "Train Loss at iteration 1318: 0.04452782919279885\n",
      "Train Loss at iteration 1319: 0.04452738066286854\n",
      "Train Loss at iteration 1320: 0.044526932686397534\n",
      "Train Loss at iteration 1321: 0.04452648526130578\n",
      "Train Loss at iteration 1322: 0.044526038385522444\n",
      "Train Loss at iteration 1323: 0.044525592056985815\n",
      "Train Loss at iteration 1324: 0.044525146273643214\n",
      "Train Loss at iteration 1325: 0.04452470103345111\n",
      "Train Loss at iteration 1326: 0.04452425633437485\n",
      "Train Loss at iteration 1327: 0.04452381217438884\n",
      "Train Loss at iteration 1328: 0.04452336855147636\n",
      "Train Loss at iteration 1329: 0.04452292546362954\n",
      "Train Loss at iteration 1330: 0.04452248290884938\n",
      "Train Loss at iteration 1331: 0.04452204088514568\n",
      "Train Loss at iteration 1332: 0.044521599390536924\n",
      "Train Loss at iteration 1333: 0.04452115842305041\n",
      "Train Loss at iteration 1334: 0.044520717980722\n",
      "Train Loss at iteration 1335: 0.04452027806159624\n",
      "Train Loss at iteration 1336: 0.04451983866372627\n",
      "Train Loss at iteration 1337: 0.044519399785173726\n",
      "Train Loss at iteration 1338: 0.04451896142400884\n",
      "Train Loss at iteration 1339: 0.044518523578310196\n",
      "Train Loss at iteration 1340: 0.04451808624616492\n",
      "Train Loss at iteration 1341: 0.04451764942566845\n",
      "Train Loss at iteration 1342: 0.0445172131149246\n",
      "Train Loss at iteration 1343: 0.04451677731204551\n",
      "Train Loss at iteration 1344: 0.04451634201515156\n",
      "Train Loss at iteration 1345: 0.044515907222371386\n",
      "Train Loss at iteration 1346: 0.04451547293184182\n",
      "Train Loss at iteration 1347: 0.044515039141707825\n",
      "Train Loss at iteration 1348: 0.04451460585012254\n",
      "Train Loss at iteration 1349: 0.0445141730552471\n",
      "Train Loss at iteration 1350: 0.04451374075525077\n",
      "Train Loss at iteration 1351: 0.044513308948310765\n",
      "Train Loss at iteration 1352: 0.044512877632612295\n",
      "Train Loss at iteration 1353: 0.04451244680634847\n",
      "Train Loss at iteration 1354: 0.044512016467720336\n",
      "Train Loss at iteration 1355: 0.044511586614936795\n",
      "Train Loss at iteration 1356: 0.04451115724621452\n",
      "Train Loss at iteration 1357: 0.04451072835977801\n",
      "Train Loss at iteration 1358: 0.04451029995385953\n",
      "Train Loss at iteration 1359: 0.04450987202669902\n",
      "Train Loss at iteration 1360: 0.044509444576544085\n",
      "Train Loss at iteration 1361: 0.044509017601650054\n",
      "Train Loss at iteration 1362: 0.04450859110027977\n",
      "Train Loss at iteration 1363: 0.04450816507070369\n",
      "Train Loss at iteration 1364: 0.044507739511199795\n",
      "Train Loss at iteration 1365: 0.04450731442005358\n",
      "Train Loss at iteration 1366: 0.044506889795557995\n",
      "Train Loss at iteration 1367: 0.04450646563601341\n",
      "Train Loss at iteration 1368: 0.04450604193972763\n",
      "Train Loss at iteration 1369: 0.0445056187050158\n",
      "Train Loss at iteration 1370: 0.044505195930200366\n",
      "Train Loss at iteration 1371: 0.044504773613611104\n",
      "Train Loss at iteration 1372: 0.044504351753585046\n",
      "Train Loss at iteration 1373: 0.04450393034846645\n",
      "Train Loss at iteration 1374: 0.04450350939660675\n",
      "Train Loss at iteration 1375: 0.04450308889636456\n",
      "Train Loss at iteration 1376: 0.044502668846105625\n",
      "Train Loss at iteration 1377: 0.04450224924420277\n",
      "Train Loss at iteration 1378: 0.044501830089035875\n",
      "Train Loss at iteration 1379: 0.04450141137899186\n",
      "Train Loss at iteration 1380: 0.044500993112464665\n",
      "Train Loss at iteration 1381: 0.04450057528785517\n",
      "Train Loss at iteration 1382: 0.04450015790357116\n",
      "Train Loss at iteration 1383: 0.04449974095802737\n",
      "Train Loss at iteration 1384: 0.04449932444964537\n",
      "Train Loss at iteration 1385: 0.04449890837685357\n",
      "Train Loss at iteration 1386: 0.04449849273808723\n",
      "Train Loss at iteration 1387: 0.04449807753178831\n",
      "Train Loss at iteration 1388: 0.04449766275640556\n",
      "Train Loss at iteration 1389: 0.04449724841039442\n",
      "Train Loss at iteration 1390: 0.04449683449221704\n",
      "Train Loss at iteration 1391: 0.04449642100034218\n",
      "Train Loss at iteration 1392: 0.04449600793324524\n",
      "Train Loss at iteration 1393: 0.04449559528940822\n",
      "Train Loss at iteration 1394: 0.04449518306731966\n",
      "Train Loss at iteration 1395: 0.04449477126547462\n",
      "Train Loss at iteration 1396: 0.04449435988237468\n",
      "Train Loss at iteration 1397: 0.044493948916527866\n",
      "Train Loss at iteration 1398: 0.044493538366448676\n",
      "Train Loss at iteration 1399: 0.04449312823065798\n",
      "Train Loss at iteration 1400: 0.044492718507683056\n",
      "Train Loss at iteration 1401: 0.044492309196057504\n",
      "Train Loss at iteration 1402: 0.04449190029432125\n",
      "Train Loss at iteration 1403: 0.04449149180102054\n",
      "Train Loss at iteration 1404: 0.044491083714707856\n",
      "Train Loss at iteration 1405: 0.04449067603394191\n",
      "Train Loss at iteration 1406: 0.04449026875728766\n",
      "Train Loss at iteration 1407: 0.04448986188331618\n",
      "Train Loss at iteration 1408: 0.044489455410604735\n",
      "Train Loss at iteration 1409: 0.044489049337736696\n",
      "Train Loss at iteration 1410: 0.04448864366330154\n",
      "Train Loss at iteration 1411: 0.04448823838589479\n",
      "Train Loss at iteration 1412: 0.04448783350411801\n",
      "Train Loss at iteration 1413: 0.04448742901657878\n",
      "Train Loss at iteration 1414: 0.04448702492189066\n",
      "Train Loss at iteration 1415: 0.04448662121867317\n",
      "Train Loss at iteration 1416: 0.04448621790555173\n",
      "Train Loss at iteration 1417: 0.044485814981157695\n",
      "Train Loss at iteration 1418: 0.04448541244412827\n",
      "Train Loss at iteration 1419: 0.044485010293106496\n",
      "Train Loss at iteration 1420: 0.04448460852674128\n",
      "Train Loss at iteration 1421: 0.04448420714368726\n",
      "Train Loss at iteration 1422: 0.044483806142604894\n",
      "Train Loss at iteration 1423: 0.04448340552216035\n",
      "Train Loss at iteration 1424: 0.0444830052810255\n",
      "Train Loss at iteration 1425: 0.04448260541787795\n",
      "Train Loss at iteration 1426: 0.04448220593140091\n",
      "Train Loss at iteration 1427: 0.04448180682028327\n",
      "Train Loss at iteration 1428: 0.044481408083219494\n",
      "Train Loss at iteration 1429: 0.044481009718909656\n",
      "Train Loss at iteration 1430: 0.044480611726059396\n",
      "Train Loss at iteration 1431: 0.044480214103379836\n",
      "Train Loss at iteration 1432: 0.04447981684958769\n",
      "Train Loss at iteration 1433: 0.04447941996340506\n",
      "Train Loss at iteration 1434: 0.04447902344355959\n",
      "Train Loss at iteration 1435: 0.04447862728878431\n",
      "Train Loss at iteration 1436: 0.04447823149781766\n",
      "Train Loss at iteration 1437: 0.04447783606940347\n",
      "Train Loss at iteration 1438: 0.04447744100229097\n",
      "Train Loss at iteration 1439: 0.044477046295234655\n",
      "Train Loss at iteration 1440: 0.04447665194699435\n",
      "Train Loss at iteration 1441: 0.04447625795633522\n",
      "Train Loss at iteration 1442: 0.04447586432202762\n",
      "Train Loss at iteration 1443: 0.04447547104284721\n",
      "Train Loss at iteration 1444: 0.04447507811757479\n",
      "Train Loss at iteration 1445: 0.04447468554499643\n",
      "Train Loss at iteration 1446: 0.0444742933239033\n",
      "Train Loss at iteration 1447: 0.04447390145309177\n",
      "Train Loss at iteration 1448: 0.044473509931363286\n",
      "Train Loss at iteration 1449: 0.04447311875752442\n",
      "Train Loss at iteration 1450: 0.044472727930386806\n",
      "Train Loss at iteration 1451: 0.04447233744876712\n",
      "Train Loss at iteration 1452: 0.0444719473114871\n",
      "Train Loss at iteration 1453: 0.04447155751737346\n",
      "Train Loss at iteration 1454: 0.044471168065257895\n",
      "Train Loss at iteration 1455: 0.04447077895397707\n",
      "Train Loss at iteration 1456: 0.04447039018237261\n",
      "Train Loss at iteration 1457: 0.04447000174929102\n",
      "Train Loss at iteration 1458: 0.0444696136535837\n",
      "Train Loss at iteration 1459: 0.04446922589410695\n",
      "Train Loss at iteration 1460: 0.044468838469721904\n",
      "Train Loss at iteration 1461: 0.0444684513792945\n",
      "Train Loss at iteration 1462: 0.04446806462169553\n",
      "Train Loss at iteration 1463: 0.044467678195800525\n",
      "Train Loss at iteration 1464: 0.04446729210048979\n",
      "Train Loss at iteration 1465: 0.04446690633464839\n",
      "Train Loss at iteration 1466: 0.04446652089716609\n",
      "Train Loss at iteration 1467: 0.044466135786937366\n",
      "Train Loss at iteration 1468: 0.044465751002861334\n",
      "Train Loss at iteration 1469: 0.04446536654384182\n",
      "Train Loss at iteration 1470: 0.04446498240878723\n",
      "Train Loss at iteration 1471: 0.044464598596610644\n",
      "Train Loss at iteration 1472: 0.04446421510622968\n",
      "Train Loss at iteration 1473: 0.04446383193656655\n",
      "Train Loss at iteration 1474: 0.04446344908654803\n",
      "Train Loss at iteration 1475: 0.044463066555105395\n",
      "Train Loss at iteration 1476: 0.04446268434117446\n",
      "Train Loss at iteration 1477: 0.04446230244369552\n",
      "Train Loss at iteration 1478: 0.044461920861613324\n",
      "Train Loss at iteration 1479: 0.04446153959387709\n",
      "Train Loss at iteration 1480: 0.044461158639440466\n",
      "Train Loss at iteration 1481: 0.0444607779972615\n",
      "Train Loss at iteration 1482: 0.04446039766630265\n",
      "Train Loss at iteration 1483: 0.04446001764553069\n",
      "Train Loss at iteration 1484: 0.04445963793391681\n",
      "Train Loss at iteration 1485: 0.0444592585304365\n",
      "Train Loss at iteration 1486: 0.04445887943406956\n",
      "Train Loss at iteration 1487: 0.044458500643800074\n",
      "Train Loss at iteration 1488: 0.04445812215861644\n",
      "Train Loss at iteration 1489: 0.04445774397751127\n",
      "Train Loss at iteration 1490: 0.044457366099481394\n",
      "Train Loss at iteration 1491: 0.04445698852352791\n",
      "Train Loss at iteration 1492: 0.04445661124865608\n",
      "Train Loss at iteration 1493: 0.04445623427387536\n",
      "Train Loss at iteration 1494: 0.04445585759819934\n",
      "Train Loss at iteration 1495: 0.044455481220645764\n",
      "Train Loss at iteration 1496: 0.04445510514023652\n",
      "Train Loss at iteration 1497: 0.04445472935599755\n",
      "Train Loss at iteration 1498: 0.04445435386695892\n",
      "Train Loss at iteration 1499: 0.04445397867215477\n",
      "Train Loss at iteration 1500: 0.04445360377062323\n",
      "Train Loss at iteration 1501: 0.04445322916140654\n",
      "Train Loss at iteration 1502: 0.04445285484355088\n",
      "Train Loss at iteration 1503: 0.04445248081610648\n",
      "Train Loss at iteration 1504: 0.0444521070781275\n",
      "Train Loss at iteration 1505: 0.044451733628672094\n",
      "Train Loss at iteration 1506: 0.044451360466802335\n",
      "Train Loss at iteration 1507: 0.04445098759158423\n",
      "Train Loss at iteration 1508: 0.044450615002087704\n",
      "Train Loss at iteration 1509: 0.044450242697386524\n",
      "Train Loss at iteration 1510: 0.04444987067655838\n",
      "Train Loss at iteration 1511: 0.04444949893868478\n",
      "Train Loss at iteration 1512: 0.0444491274828511\n",
      "Train Loss at iteration 1513: 0.04444875630814649\n",
      "Train Loss at iteration 1514: 0.044448385413663964\n",
      "Train Loss at iteration 1515: 0.04444801479850026\n",
      "Train Loss at iteration 1516: 0.044447644461755934\n",
      "Train Loss at iteration 1517: 0.04444727440253522\n",
      "Train Loss at iteration 1518: 0.0444469046199462\n",
      "Train Loss at iteration 1519: 0.044446535113100566\n",
      "Train Loss at iteration 1520: 0.044446165881113775\n",
      "Train Loss at iteration 1521: 0.04444579692310494\n",
      "Train Loss at iteration 1522: 0.04444542823819686\n",
      "Train Loss at iteration 1523: 0.04444505982551598\n",
      "Train Loss at iteration 1524: 0.04444469168419236\n",
      "Train Loss at iteration 1525: 0.044444323813359714\n",
      "Train Loss at iteration 1526: 0.04444395621215534\n",
      "Train Loss at iteration 1527: 0.044443588879720136\n",
      "Train Loss at iteration 1528: 0.04444322181519856\n",
      "Train Loss at iteration 1529: 0.04444285501773861\n",
      "Train Loss at iteration 1530: 0.04444248848649188\n",
      "Train Loss at iteration 1531: 0.04444212222061342\n",
      "Train Loss at iteration 1532: 0.044441756219261816\n",
      "Train Loss at iteration 1533: 0.044441390481599174\n",
      "Train Loss at iteration 1534: 0.044441025006791056\n",
      "Train Loss at iteration 1535: 0.04444065979400645\n",
      "Train Loss at iteration 1536: 0.04444029484241787\n",
      "Train Loss at iteration 1537: 0.044439930151201165\n",
      "Train Loss at iteration 1538: 0.044439565719535695\n",
      "Train Loss at iteration 1539: 0.04443920154660415\n",
      "Train Loss at iteration 1540: 0.04443883763159265\n",
      "Train Loss at iteration 1541: 0.04443847397369064\n",
      "Train Loss at iteration 1542: 0.04443811057209099\n",
      "Train Loss at iteration 1543: 0.04443774742598984\n",
      "Train Loss at iteration 1544: 0.04443738453458669\n",
      "Train Loss at iteration 1545: 0.04443702189708435\n",
      "Train Loss at iteration 1546: 0.04443665951268892\n",
      "Train Loss at iteration 1547: 0.04443629738060981\n",
      "Train Loss at iteration 1548: 0.044435935500059635\n",
      "Train Loss at iteration 1549: 0.044435573870254326\n",
      "Train Loss at iteration 1550: 0.04443521249041303\n",
      "Train Loss at iteration 1551: 0.04443485135975811\n",
      "Train Loss at iteration 1552: 0.04443449047751514\n",
      "Train Loss at iteration 1553: 0.044434129842912916\n",
      "Train Loss at iteration 1554: 0.04443376945518338\n",
      "Train Loss at iteration 1555: 0.04443340931356167\n",
      "Train Loss at iteration 1556: 0.044433049417286055\n",
      "Train Loss at iteration 1557: 0.044432689765597964\n",
      "Train Loss at iteration 1558: 0.044432330357741925\n",
      "Train Loss at iteration 1559: 0.044431971192965616\n",
      "Train Loss at iteration 1560: 0.04443161227051979\n",
      "Train Loss at iteration 1561: 0.044431253589658284\n",
      "Train Loss at iteration 1562: 0.044430895149637994\n",
      "Train Loss at iteration 1563: 0.04443053694971893\n",
      "Train Loss at iteration 1564: 0.04443017898916408\n",
      "Train Loss at iteration 1565: 0.04442982126723948\n",
      "Train Loss at iteration 1566: 0.044429463783214225\n",
      "Train Loss at iteration 1567: 0.044429106536360334\n",
      "Train Loss at iteration 1568: 0.04442874952595291\n",
      "Train Loss at iteration 1569: 0.04442839275126997\n",
      "Train Loss at iteration 1570: 0.0444280362115925\n",
      "Train Loss at iteration 1571: 0.04442767990620448\n",
      "Train Loss at iteration 1572: 0.04442732383439277\n",
      "Train Loss at iteration 1573: 0.044426967995447215\n",
      "Train Loss at iteration 1574: 0.04442661238866051\n",
      "Train Loss at iteration 1575: 0.044426257013328314\n",
      "Train Loss at iteration 1576: 0.04442590186874912\n",
      "Train Loss at iteration 1577: 0.04442554695422432\n",
      "Train Loss at iteration 1578: 0.04442519226905819\n",
      "Train Loss at iteration 1579: 0.04442483781255781\n",
      "Train Loss at iteration 1580: 0.04442448358403309\n",
      "Train Loss at iteration 1581: 0.04442412958279684\n",
      "Train Loss at iteration 1582: 0.04442377580816462\n",
      "Train Loss at iteration 1583: 0.04442342225945479\n",
      "Train Loss at iteration 1584: 0.04442306893598855\n",
      "Train Loss at iteration 1585: 0.04442271583708977\n",
      "Train Loss at iteration 1586: 0.044422362962085175\n",
      "Train Loss at iteration 1587: 0.044422010310304245\n",
      "Train Loss at iteration 1588: 0.04442165788107913\n",
      "Train Loss at iteration 1589: 0.044421305673744776\n",
      "Train Loss at iteration 1590: 0.044420953687638785\n",
      "Train Loss at iteration 1591: 0.044420601922101496\n",
      "Train Loss at iteration 1592: 0.04442025037647595\n",
      "Train Loss at iteration 1593: 0.04441989905010784\n",
      "Train Loss at iteration 1594: 0.04441954794234552\n",
      "Train Loss at iteration 1595: 0.04441919705254003\n",
      "Train Loss at iteration 1596: 0.044418846380045056\n",
      "Train Loss at iteration 1597: 0.04441849592421687\n",
      "Train Loss at iteration 1598: 0.04441814568441441\n",
      "Train Loss at iteration 1599: 0.044417795659999225\n",
      "Train Loss at iteration 1600: 0.04441744585033542\n",
      "Train Loss at iteration 1601: 0.044417096254789734\n",
      "Train Loss at iteration 1602: 0.044416746872731463\n",
      "Train Loss at iteration 1603: 0.04441639770353245\n",
      "Train Loss at iteration 1604: 0.044416048746567106\n",
      "Train Loss at iteration 1605: 0.044415700001212415\n",
      "Train Loss at iteration 1606: 0.04441535146684783\n",
      "Train Loss at iteration 1607: 0.044415003142855375\n",
      "Train Loss at iteration 1608: 0.04441465502861954\n",
      "Train Loss at iteration 1609: 0.04441430712352738\n",
      "Train Loss at iteration 1610: 0.04441395942696836\n",
      "Train Loss at iteration 1611: 0.04441361193833448\n",
      "Train Loss at iteration 1612: 0.044413264657020154\n",
      "Train Loss at iteration 1613: 0.044412917582422294\n",
      "Train Loss at iteration 1614: 0.04441257071394023\n",
      "Train Loss at iteration 1615: 0.04441222405097576\n",
      "Train Loss at iteration 1616: 0.04441187759293304\n",
      "Train Loss at iteration 1617: 0.04441153133921871\n",
      "Train Loss at iteration 1618: 0.04441118528924176\n",
      "Train Loss at iteration 1619: 0.044410839442413606\n",
      "Train Loss at iteration 1620: 0.04441049379814802\n",
      "Train Loss at iteration 1621: 0.04441014835586118\n",
      "Train Loss at iteration 1622: 0.04440980311497157\n",
      "Train Loss at iteration 1623: 0.04440945807490006\n",
      "Train Loss at iteration 1624: 0.04440911323506984\n",
      "Train Loss at iteration 1625: 0.04440876859490647\n",
      "Train Loss at iteration 1626: 0.04440842415383778\n",
      "Train Loss at iteration 1627: 0.044408079911293946\n",
      "Train Loss at iteration 1628: 0.04440773586670743\n",
      "Train Loss at iteration 1629: 0.044407392019512965\n",
      "Train Loss at iteration 1630: 0.04440704836914759\n",
      "Train Loss at iteration 1631: 0.04440670491505061\n",
      "Train Loss at iteration 1632: 0.04440636165666355\n",
      "Train Loss at iteration 1633: 0.04440601859343028\n",
      "Train Loss at iteration 1634: 0.0444056757247968\n",
      "Train Loss at iteration 1635: 0.04440533305021141\n",
      "Train Loss at iteration 1636: 0.04440499056912459\n",
      "Train Loss at iteration 1637: 0.04440464828098908\n",
      "Train Loss at iteration 1638: 0.044404306185259784\n",
      "Train Loss at iteration 1639: 0.04440396428139378\n",
      "Train Loss at iteration 1640: 0.04440362256885039\n",
      "Train Loss at iteration 1641: 0.04440328104709106\n",
      "Train Loss at iteration 1642: 0.04440293971557941\n",
      "Train Loss at iteration 1643: 0.04440259857378124\n",
      "Train Loss at iteration 1644: 0.044402257621164454\n",
      "Train Loss at iteration 1645: 0.044401916857199124\n",
      "Train Loss at iteration 1646: 0.044401576281357415\n",
      "Train Loss at iteration 1647: 0.044401235893113664\n",
      "Train Loss at iteration 1648: 0.04440089569194425\n",
      "Train Loss at iteration 1649: 0.044400555677327716\n",
      "Train Loss at iteration 1650: 0.04440021584874463\n",
      "Train Loss at iteration 1651: 0.0443998762056777\n",
      "Train Loss at iteration 1652: 0.04439953674761166\n",
      "Train Loss at iteration 1653: 0.04439919747403335\n",
      "Train Loss at iteration 1654: 0.044398858384431625\n",
      "Train Loss at iteration 1655: 0.044398519478297406\n",
      "Train Loss at iteration 1656: 0.044398180755123653\n",
      "Train Loss at iteration 1657: 0.044397842214405354\n",
      "Train Loss at iteration 1658: 0.044397503855639474\n",
      "Train Loss at iteration 1659: 0.044397165678325065\n",
      "Train Loss at iteration 1660: 0.04439682768196312\n",
      "Train Loss at iteration 1661: 0.04439648986605665\n",
      "Train Loss at iteration 1662: 0.044396152230110626\n",
      "Train Loss at iteration 1663: 0.044395814773632025\n",
      "Train Loss at iteration 1664: 0.0443954774961298\n",
      "Train Loss at iteration 1665: 0.04439514039711483\n",
      "Train Loss at iteration 1666: 0.044394803476099934\n",
      "Train Loss at iteration 1667: 0.04439446673259993\n",
      "Train Loss at iteration 1668: 0.044394130166131536\n",
      "Train Loss at iteration 1669: 0.04439379377621337\n",
      "Train Loss at iteration 1670: 0.044393457562366005\n",
      "Train Loss at iteration 1671: 0.04439312152411192\n",
      "Train Loss at iteration 1672: 0.044392785660975466\n",
      "Train Loss at iteration 1673: 0.044392449972482935\n",
      "Train Loss at iteration 1674: 0.044392114458162456\n",
      "Train Loss at iteration 1675: 0.04439177911754405\n",
      "Train Loss at iteration 1676: 0.04439144395015962\n",
      "Train Loss at iteration 1677: 0.04439110895554293\n",
      "Train Loss at iteration 1678: 0.04439077413322955\n",
      "Train Loss at iteration 1679: 0.044390439482756956\n",
      "Train Loss at iteration 1680: 0.04439010500366442\n",
      "Train Loss at iteration 1681: 0.044389770695493096\n",
      "Train Loss at iteration 1682: 0.04438943655778586\n",
      "Train Loss at iteration 1683: 0.044389102590087486\n",
      "Train Loss at iteration 1684: 0.04438876879194454\n",
      "Train Loss at iteration 1685: 0.04438843516290536\n",
      "Train Loss at iteration 1686: 0.04438810170252009\n",
      "Train Loss at iteration 1687: 0.04438776841034065\n",
      "Train Loss at iteration 1688: 0.044387435285920755\n",
      "Train Loss at iteration 1689: 0.04438710232881584\n",
      "Train Loss at iteration 1690: 0.04438676953858314\n",
      "Train Loss at iteration 1691: 0.044386436914781655\n",
      "Train Loss at iteration 1692: 0.04438610445697208\n",
      "Train Loss at iteration 1693: 0.044385772164716855\n",
      "Train Loss at iteration 1694: 0.0443854400375802\n",
      "Train Loss at iteration 1695: 0.04438510807512802\n",
      "Train Loss at iteration 1696: 0.0443847762769279\n",
      "Train Loss at iteration 1697: 0.0443844446425492\n",
      "Train Loss at iteration 1698: 0.044384113171562975\n",
      "Train Loss at iteration 1699: 0.044383781863541916\n",
      "Train Loss at iteration 1700: 0.04438345071806043\n",
      "Train Loss at iteration 1701: 0.044383119734694595\n",
      "Train Loss at iteration 1702: 0.04438278891302221\n",
      "Train Loss at iteration 1703: 0.04438245825262265\n",
      "Train Loss at iteration 1704: 0.04438212775307704\n",
      "Train Loss at iteration 1705: 0.044381797413968076\n",
      "Train Loss at iteration 1706: 0.04438146723488014\n",
      "Train Loss at iteration 1707: 0.04438113721539923\n",
      "Train Loss at iteration 1708: 0.04438080735511302\n",
      "Train Loss at iteration 1709: 0.04438047765361072\n",
      "Train Loss at iteration 1710: 0.04438014811048325\n",
      "Train Loss at iteration 1711: 0.04437981872532306\n",
      "Train Loss at iteration 1712: 0.044379489497724245\n",
      "Train Loss at iteration 1713: 0.044379160427282495\n",
      "Train Loss at iteration 1714: 0.044378831513595056\n",
      "Train Loss at iteration 1715: 0.044378502756260776\n",
      "Train Loss at iteration 1716: 0.044378174154880115\n",
      "Train Loss at iteration 1717: 0.044377845709055015\n",
      "Train Loss at iteration 1718: 0.04437751741838906\n",
      "Train Loss at iteration 1719: 0.04437718928248732\n",
      "Train Loss at iteration 1720: 0.044376861300956486\n",
      "Train Loss at iteration 1721: 0.04437653347340473\n",
      "Train Loss at iteration 1722: 0.044376205799441786\n",
      "Train Loss at iteration 1723: 0.04437587827867891\n",
      "Train Loss at iteration 1724: 0.04437555091072888\n",
      "Train Loss at iteration 1725: 0.04437522369520598\n",
      "Train Loss at iteration 1726: 0.04437489663172604\n",
      "Train Loss at iteration 1727: 0.04437456971990633\n",
      "Train Loss at iteration 1728: 0.04437424295936568\n",
      "Train Loss at iteration 1729: 0.04437391634972436\n",
      "Train Loss at iteration 1730: 0.04437358989060415\n",
      "Train Loss at iteration 1731: 0.044373263581628304\n",
      "Train Loss at iteration 1732: 0.04437293742242155\n",
      "Train Loss at iteration 1733: 0.044372611412610063\n",
      "Train Loss at iteration 1734: 0.04437228555182148\n",
      "Train Loss at iteration 1735: 0.04437195983968491\n",
      "Train Loss at iteration 1736: 0.04437163427583089\n",
      "Train Loss at iteration 1737: 0.044371308859891406\n",
      "Train Loss at iteration 1738: 0.04437098359149988\n",
      "Train Loss at iteration 1739: 0.04437065847029115\n",
      "Train Loss at iteration 1740: 0.04437033349590147\n",
      "Train Loss at iteration 1741: 0.04437000866796854\n",
      "Train Loss at iteration 1742: 0.04436968398613144\n",
      "Train Loss at iteration 1743: 0.04436935945003067\n",
      "Train Loss at iteration 1744: 0.04436903505930813\n",
      "Train Loss at iteration 1745: 0.04436871081360709\n",
      "Train Loss at iteration 1746: 0.04436838671257223\n",
      "Train Loss at iteration 1747: 0.044368062755849605\n",
      "Train Loss at iteration 1748: 0.044367738943086626\n",
      "Train Loss at iteration 1749: 0.04436741527393211\n",
      "Train Loss at iteration 1750: 0.04436709174803619\n",
      "Train Loss at iteration 1751: 0.04436676836505039\n",
      "Train Loss at iteration 1752: 0.04436644512462758\n",
      "Train Loss at iteration 1753: 0.04436612202642197\n",
      "Train Loss at iteration 1754: 0.04436579907008911\n",
      "Train Loss at iteration 1755: 0.044365476255285884\n",
      "Train Loss at iteration 1756: 0.044365153581670494\n",
      "Train Loss at iteration 1757: 0.0443648310489025\n",
      "Train Loss at iteration 1758: 0.044364508656642744\n",
      "Train Loss at iteration 1759: 0.044364186404553405\n",
      "Train Loss at iteration 1760: 0.04436386429229794\n",
      "Train Loss at iteration 1761: 0.04436354231954114\n",
      "Train Loss at iteration 1762: 0.044363220485949054\n",
      "Train Loss at iteration 1763: 0.04436289879118907\n",
      "Train Loss at iteration 1764: 0.044362577234929805\n",
      "Train Loss at iteration 1765: 0.0443622558168412\n",
      "Train Loss at iteration 1766: 0.04436193453659445\n",
      "Train Loss at iteration 1767: 0.04436161339386202\n",
      "Train Loss at iteration 1768: 0.04436129238831763\n",
      "Train Loss at iteration 1769: 0.04436097151963629\n",
      "Train Loss at iteration 1770: 0.04436065078749422\n",
      "Train Loss at iteration 1771: 0.04436033019156892\n",
      "Train Loss at iteration 1772: 0.04436000973153911\n",
      "Train Loss at iteration 1773: 0.044359689407084756\n",
      "Train Loss at iteration 1774: 0.04435936921788708\n",
      "Train Loss at iteration 1775: 0.04435904916362846\n",
      "Train Loss at iteration 1776: 0.04435872924399258\n",
      "Train Loss at iteration 1777: 0.0443584094586643\n",
      "Train Loss at iteration 1778: 0.04435808980732969\n",
      "Train Loss at iteration 1779: 0.04435777028967602\n",
      "Train Loss at iteration 1780: 0.044357450905391775\n",
      "Train Loss at iteration 1781: 0.04435713165416663\n",
      "Train Loss at iteration 1782: 0.04435681253569146\n",
      "Train Loss at iteration 1783: 0.044356493549658316\n",
      "Train Loss at iteration 1784: 0.04435617469576044\n",
      "Train Loss at iteration 1785: 0.04435585597369222\n",
      "Train Loss at iteration 1786: 0.044355537383149284\n",
      "Train Loss at iteration 1787: 0.04435521892382832\n",
      "Train Loss at iteration 1788: 0.04435490059542728\n",
      "Train Loss at iteration 1789: 0.04435458239764522\n",
      "Train Loss at iteration 1790: 0.04435426433018236\n",
      "Train Loss at iteration 1791: 0.04435394639274006\n",
      "Train Loss at iteration 1792: 0.044353628585020836\n",
      "Train Loss at iteration 1793: 0.04435331090672832\n",
      "Train Loss at iteration 1794: 0.044352993357567295\n",
      "Train Loss at iteration 1795: 0.04435267593724368\n",
      "Train Loss at iteration 1796: 0.044352358645464496\n",
      "Train Loss at iteration 1797: 0.04435204148193786\n",
      "Train Loss at iteration 1798: 0.04435172444637309\n",
      "Train Loss at iteration 1799: 0.04435140753848052\n",
      "Train Loss at iteration 1800: 0.04435109075797161\n",
      "Train Loss at iteration 1801: 0.04435077410455897\n",
      "Train Loss at iteration 1802: 0.044350457577956255\n",
      "Train Loss at iteration 1803: 0.04435014117787823\n",
      "Train Loss at iteration 1804: 0.04434982490404072\n",
      "Train Loss at iteration 1805: 0.044349508756160685\n",
      "Train Loss at iteration 1806: 0.04434919273395611\n",
      "Train Loss at iteration 1807: 0.04434887683714609\n",
      "Train Loss at iteration 1808: 0.04434856106545076\n",
      "Train Loss at iteration 1809: 0.04434824541859134\n",
      "Train Loss at iteration 1810: 0.04434792989629007\n",
      "Train Loss at iteration 1811: 0.044347614498270314\n",
      "Train Loss at iteration 1812: 0.04434729922425644\n",
      "Train Loss at iteration 1813: 0.04434698407397386\n",
      "Train Loss at iteration 1814: 0.04434666904714903\n",
      "Train Loss at iteration 1815: 0.04434635414350948\n",
      "Train Loss at iteration 1816: 0.044346039362783723\n",
      "Train Loss at iteration 1817: 0.04434572470470133\n",
      "Train Loss at iteration 1818: 0.04434541016899287\n",
      "Train Loss at iteration 1819: 0.04434509575538998\n",
      "Train Loss at iteration 1820: 0.04434478146362527\n",
      "Train Loss at iteration 1821: 0.044344467293432395\n",
      "Train Loss at iteration 1822: 0.044344153244545964\n",
      "Train Loss at iteration 1823: 0.04434383931670166\n",
      "Train Loss at iteration 1824: 0.04434352550963611\n",
      "Train Loss at iteration 1825: 0.04434321182308697\n",
      "Train Loss at iteration 1826: 0.04434289825679287\n",
      "Train Loss at iteration 1827: 0.04434258481049343\n",
      "Train Loss at iteration 1828: 0.04434227148392925\n",
      "Train Loss at iteration 1829: 0.044341958276841925\n",
      "Train Loss at iteration 1830: 0.04434164518897401\n",
      "Train Loss at iteration 1831: 0.044341332220069016\n",
      "Train Loss at iteration 1832: 0.04434101936987146\n",
      "Train Loss at iteration 1833: 0.044340706638126795\n",
      "Train Loss at iteration 1834: 0.04434039402458143\n",
      "Train Loss at iteration 1835: 0.04434008152898275\n",
      "Train Loss at iteration 1836: 0.04433976915107907\n",
      "Train Loss at iteration 1837: 0.04433945689061965\n",
      "Train Loss at iteration 1838: 0.04433914474735473\n",
      "Train Loss at iteration 1839: 0.044338832721035434\n",
      "Train Loss at iteration 1840: 0.04433852081141386\n",
      "Train Loss at iteration 1841: 0.044338209018243036\n",
      "Train Loss at iteration 1842: 0.04433789734127689\n",
      "Train Loss at iteration 1843: 0.044337585780270306\n",
      "Train Loss at iteration 1844: 0.04433727433497906\n",
      "Train Loss at iteration 1845: 0.04433696300515988\n",
      "Train Loss at iteration 1846: 0.044336651790570364\n",
      "Train Loss at iteration 1847: 0.044336340690969055\n",
      "Train Loss at iteration 1848: 0.04433602970611537\n",
      "Train Loss at iteration 1849: 0.044335718835769616\n",
      "Train Loss at iteration 1850: 0.0443354080796931\n",
      "Train Loss at iteration 1851: 0.04433509743764789\n",
      "Train Loss at iteration 1852: 0.04433478690939698\n",
      "Train Loss at iteration 1853: 0.04433447649470432\n",
      "Train Loss at iteration 1854: 0.04433416619333466\n",
      "Train Loss at iteration 1855: 0.04433385600505368\n",
      "Train Loss at iteration 1856: 0.044333545929627904\n",
      "Train Loss at iteration 1857: 0.04433323596682475\n",
      "Train Loss at iteration 1858: 0.04433292611641248\n",
      "Train Loss at iteration 1859: 0.044332616378160244\n",
      "Train Loss at iteration 1860: 0.044332306751838066\n",
      "Train Loss at iteration 1861: 0.044331997237216765\n",
      "Train Loss at iteration 1862: 0.04433168783406809\n",
      "Train Loss at iteration 1863: 0.04433137854216458\n",
      "Train Loss at iteration 1864: 0.04433106936127966\n",
      "Train Loss at iteration 1865: 0.044330760291187585\n",
      "Train Loss at iteration 1866: 0.044330451331663454\n",
      "Train Loss at iteration 1867: 0.04433014248248318\n",
      "Train Loss at iteration 1868: 0.04432983374342354\n",
      "Train Loss at iteration 1869: 0.04432952511426213\n",
      "Train Loss at iteration 1870: 0.04432921659477738\n",
      "Train Loss at iteration 1871: 0.0443289081847485\n",
      "Train Loss at iteration 1872: 0.04432859988395559\n",
      "Train Loss at iteration 1873: 0.044328291692179535\n",
      "Train Loss at iteration 1874: 0.044327983609201975\n",
      "Train Loss at iteration 1875: 0.04432767563480547\n",
      "Train Loss at iteration 1876: 0.04432736776877328\n",
      "Train Loss at iteration 1877: 0.044327060010889555\n",
      "Train Loss at iteration 1878: 0.044326752360939185\n",
      "Train Loss at iteration 1879: 0.04432644481870787\n",
      "Train Loss at iteration 1880: 0.04432613738398213\n",
      "Train Loss at iteration 1881: 0.04432583005654925\n",
      "Train Loss at iteration 1882: 0.0443255228361973\n",
      "Train Loss at iteration 1883: 0.04432521572271513\n",
      "Train Loss at iteration 1884: 0.04432490871589239\n",
      "Train Loss at iteration 1885: 0.04432460181551951\n",
      "Train Loss at iteration 1886: 0.04432429502138767\n",
      "Train Loss at iteration 1887: 0.04432398833328883\n",
      "Train Loss at iteration 1888: 0.044323681751015705\n",
      "Train Loss at iteration 1889: 0.044323375274361795\n",
      "Train Loss at iteration 1890: 0.04432306890312138\n",
      "Train Loss at iteration 1891: 0.044322762637089445\n",
      "Train Loss at iteration 1892: 0.044322456476061756\n",
      "Train Loss at iteration 1893: 0.044322150419834845\n",
      "Train Loss at iteration 1894: 0.04432184446820598\n",
      "Train Loss at iteration 1895: 0.04432153862097317\n",
      "Train Loss at iteration 1896: 0.04432123287793516\n",
      "Train Loss at iteration 1897: 0.04432092723889146\n",
      "Train Loss at iteration 1898: 0.044320621703642295\n",
      "Train Loss at iteration 1899: 0.044320316271988645\n",
      "Train Loss at iteration 1900: 0.044320010943732197\n",
      "Train Loss at iteration 1901: 0.04431970571867538\n",
      "Train Loss at iteration 1902: 0.04431940059662133\n",
      "Train Loss at iteration 1903: 0.04431909557737394\n",
      "Train Loss at iteration 1904: 0.04431879066073779\n",
      "Train Loss at iteration 1905: 0.04431848584651818\n",
      "Train Loss at iteration 1906: 0.04431818113452113\n",
      "Train Loss at iteration 1907: 0.04431787652455337\n",
      "Train Loss at iteration 1908: 0.044317572016422356\n",
      "Train Loss at iteration 1909: 0.04431726760993619\n",
      "Train Loss at iteration 1910: 0.044316963304903756\n",
      "Train Loss at iteration 1911: 0.04431665910113457\n",
      "Train Loss at iteration 1912: 0.04431635499843887\n",
      "Train Loss at iteration 1913: 0.044316050996627596\n",
      "Train Loss at iteration 1914: 0.044315747095512344\n",
      "Train Loss at iteration 1915: 0.04431544329490546\n",
      "Train Loss at iteration 1916: 0.044315139594619905\n",
      "Train Loss at iteration 1917: 0.04431483599446936\n",
      "Train Loss at iteration 1918: 0.04431453249426817\n",
      "Train Loss at iteration 1919: 0.04431422909383138\n",
      "Train Loss at iteration 1920: 0.044313925792974676\n",
      "Train Loss at iteration 1921: 0.04431362259151444\n",
      "Train Loss at iteration 1922: 0.044313319489267707\n",
      "Train Loss at iteration 1923: 0.044313016486052195\n",
      "Train Loss at iteration 1924: 0.04431271358168625\n",
      "Train Loss at iteration 1925: 0.044312410775988915\n",
      "Train Loss at iteration 1926: 0.04431210806877987\n",
      "Train Loss at iteration 1927: 0.044311805459879464\n",
      "Train Loss at iteration 1928: 0.04431150294910868\n",
      "Train Loss at iteration 1929: 0.04431120053628916\n",
      "Train Loss at iteration 1930: 0.04431089822124319\n",
      "Train Loss at iteration 1931: 0.044310596003793716\n",
      "Train Loss at iteration 1932: 0.04431029388376429\n",
      "Train Loss at iteration 1933: 0.04430999186097915\n",
      "Train Loss at iteration 1934: 0.044309689935263116\n",
      "Train Loss at iteration 1935: 0.04430938810644169\n",
      "Train Loss at iteration 1936: 0.044309086374340965\n",
      "Train Loss at iteration 1937: 0.044308784738787715\n",
      "Train Loss at iteration 1938: 0.04430848319960926\n",
      "Train Loss at iteration 1939: 0.04430818175663364\n",
      "Train Loss at iteration 1940: 0.04430788040968944\n",
      "Train Loss at iteration 1941: 0.044307579158605906\n",
      "Train Loss at iteration 1942: 0.044307278003212866\n",
      "Train Loss at iteration 1943: 0.044306976943340765\n",
      "Train Loss at iteration 1944: 0.04430667597882072\n",
      "Train Loss at iteration 1945: 0.044306375109484396\n",
      "Train Loss at iteration 1946: 0.04430607433516405\n",
      "Train Loss at iteration 1947: 0.0443057736556926\n",
      "Train Loss at iteration 1948: 0.04430547307090351\n",
      "Train Loss at iteration 1949: 0.04430517258063089\n",
      "Train Loss at iteration 1950: 0.04430487218470943\n",
      "Train Loss at iteration 1951: 0.04430457188297438\n",
      "Train Loss at iteration 1952: 0.04430427167526163\n",
      "Train Loss at iteration 1953: 0.04430397156140765\n",
      "Train Loss at iteration 1954: 0.04430367154124946\n",
      "Train Loss at iteration 1955: 0.04430337161462471\n",
      "Train Loss at iteration 1956: 0.044303071781371606\n",
      "Train Loss at iteration 1957: 0.044302772041328924\n",
      "Train Loss at iteration 1958: 0.044302472394336066\n",
      "Train Loss at iteration 1959: 0.044302172840232946\n",
      "Train Loss at iteration 1960: 0.04430187337886011\n",
      "Train Loss at iteration 1961: 0.04430157401005863\n",
      "Train Loss at iteration 1962: 0.044301274733670154\n",
      "Train Loss at iteration 1963: 0.04430097554953691\n",
      "Train Loss at iteration 1964: 0.0443006764575017\n",
      "Train Loss at iteration 1965: 0.04430037745740784\n",
      "Train Loss at iteration 1966: 0.044300078549099245\n",
      "Train Loss at iteration 1967: 0.0442997797324204\n",
      "Train Loss at iteration 1968: 0.0442994810072163\n",
      "Train Loss at iteration 1969: 0.04429918237333251\n",
      "Train Loss at iteration 1970: 0.04429888383061517\n",
      "Train Loss at iteration 1971: 0.044298585378910925\n",
      "Train Loss at iteration 1972: 0.04429828701806699\n",
      "Train Loss at iteration 1973: 0.04429798874793115\n",
      "Train Loss at iteration 1974: 0.04429769056835167\n",
      "Train Loss at iteration 1975: 0.044297392479177404\n",
      "Train Loss at iteration 1976: 0.04429709448025769\n",
      "Train Loss at iteration 1977: 0.044296796571442504\n",
      "Train Loss at iteration 1978: 0.044296498752582215\n",
      "Train Loss at iteration 1979: 0.044296201023527854\n",
      "Train Loss at iteration 1980: 0.044295903384130884\n",
      "Train Loss at iteration 1981: 0.04429560583424333\n",
      "Train Loss at iteration 1982: 0.04429530837371777\n",
      "Train Loss at iteration 1983: 0.04429501100240725\n",
      "Train Loss at iteration 1984: 0.04429471372016538\n",
      "Train Loss at iteration 1985: 0.04429441652684626\n",
      "Train Loss at iteration 1986: 0.04429411942230452\n",
      "Train Loss at iteration 1987: 0.044293822406395296\n",
      "Train Loss at iteration 1988: 0.04429352547897424\n",
      "Train Loss at iteration 1989: 0.04429322863989752\n",
      "Train Loss at iteration 1990: 0.0442929318890218\n",
      "Train Loss at iteration 1991: 0.04429263522620426\n",
      "Train Loss at iteration 1992: 0.04429233865130256\n",
      "Train Loss at iteration 1993: 0.04429204216417491\n",
      "Train Loss at iteration 1994: 0.044291745764679966\n",
      "Train Loss at iteration 1995: 0.0442914494526769\n",
      "Train Loss at iteration 1996: 0.04429115322802541\n",
      "Train Loss at iteration 1997: 0.04429085709058564\n",
      "Train Loss at iteration 1998: 0.044290561040218245\n",
      "Train Loss at iteration 1999: 0.044290265076784394\n",
      "Train Loss at iteration 2000: 0.04428996920014571\n",
      "Train Loss at iteration 2001: 0.04428967341016431\n",
      "Train Loss at iteration 2002: 0.044289377706702805\n",
      "Train Loss at iteration 2003: 0.044289082089624274\n",
      "Train Loss at iteration 2004: 0.04428878655879228\n",
      "Train Loss at iteration 2005: 0.044288491114070884\n",
      "Train Loss at iteration 2006: 0.04428819575532459\n",
      "Train Loss at iteration 2007: 0.04428790048241841\n",
      "Train Loss at iteration 2008: 0.044287605295217795\n",
      "Train Loss at iteration 2009: 0.04428731019358869\n",
      "Train Loss at iteration 2010: 0.04428701517739749\n",
      "Train Loss at iteration 2011: 0.04428672024651108\n",
      "Train Loss at iteration 2012: 0.044286425400796796\n",
      "Train Loss at iteration 2013: 0.044286130640122434\n",
      "Train Loss at iteration 2014: 0.044285835964356264\n",
      "Train Loss at iteration 2015: 0.044285541373366995\n",
      "Train Loss at iteration 2016: 0.044285246867023835\n",
      "Train Loss at iteration 2017: 0.04428495244519637\n",
      "Train Loss at iteration 2018: 0.044284658107754726\n",
      "Train Loss at iteration 2019: 0.044284363854569436\n",
      "Train Loss at iteration 2020: 0.04428406968551148\n",
      "Train Loss at iteration 2021: 0.0442837756004523\n",
      "Train Loss at iteration 2022: 0.0442834815992638\n",
      "Train Loss at iteration 2023: 0.0442831876818183\n",
      "Train Loss at iteration 2024: 0.04428289384798855\n",
      "Train Loss at iteration 2025: 0.044282600097647805\n",
      "Train Loss at iteration 2026: 0.04428230643066969\n",
      "Train Loss at iteration 2027: 0.044282012846928315\n",
      "Train Loss at iteration 2028: 0.044281719346298196\n",
      "Train Loss at iteration 2029: 0.044281425928654326\n",
      "Train Loss at iteration 2030: 0.04428113259387206\n",
      "Train Loss at iteration 2031: 0.04428083934182725\n",
      "Train Loss at iteration 2032: 0.04428054617239615\n",
      "Train Loss at iteration 2033: 0.04428025308545544\n",
      "Train Loss at iteration 2034: 0.044279960080882225\n",
      "Train Loss at iteration 2035: 0.044279667158554054\n",
      "Train Loss at iteration 2036: 0.044279374318348885\n",
      "Train Loss at iteration 2037: 0.04427908156014508\n",
      "Train Loss at iteration 2038: 0.04427878888382143\n",
      "Train Loss at iteration 2039: 0.04427849628925718\n",
      "Train Loss at iteration 2040: 0.04427820377633194\n",
      "Train Loss at iteration 2041: 0.044277911344925744\n",
      "Train Loss at iteration 2042: 0.044277618994919075\n",
      "Train Loss at iteration 2043: 0.04427732672619279\n",
      "Train Loss at iteration 2044: 0.044277034538628156\n",
      "Train Loss at iteration 2045: 0.04427674243210687\n",
      "Train Loss at iteration 2046: 0.04427645040651102\n",
      "Train Loss at iteration 2047: 0.04427615846172311\n",
      "Train Loss at iteration 2048: 0.044275866597626036\n",
      "Train Loss at iteration 2049: 0.044275574814103075\n",
      "Train Loss at iteration 2050: 0.044275283111037955\n",
      "Train Loss at iteration 2051: 0.04427499148831476\n",
      "Train Loss at iteration 2052: 0.04427469994581799\n",
      "Train Loss at iteration 2053: 0.04427440848343253\n",
      "Train Loss at iteration 2054: 0.04427411710104367\n",
      "Train Loss at iteration 2055: 0.04427382579853708\n",
      "Train Loss at iteration 2056: 0.044273534575798815\n",
      "Train Loss at iteration 2057: 0.044273243432715347\n",
      "Train Loss at iteration 2058: 0.044272952369173516\n",
      "Train Loss at iteration 2059: 0.04427266138506054\n",
      "Train Loss at iteration 2060: 0.04427237048026402\n",
      "Train Loss at iteration 2061: 0.044272079654671975\n",
      "Train Loss at iteration 2062: 0.04427178890817277\n",
      "Train Loss at iteration 2063: 0.044271498240655155\n",
      "Train Loss at iteration 2064: 0.04427120765200826\n",
      "Train Loss at iteration 2065: 0.044270917142121606\n",
      "Train Loss at iteration 2066: 0.04427062671088507\n",
      "Train Loss at iteration 2067: 0.04427033635818892\n",
      "Train Loss at iteration 2068: 0.04427004608392378\n",
      "Train Loss at iteration 2069: 0.04426975588798064\n",
      "Train Loss at iteration 2070: 0.044269465770250906\n",
      "Train Loss at iteration 2071: 0.04426917573062627\n",
      "Train Loss at iteration 2072: 0.04426888576899888\n",
      "Train Loss at iteration 2073: 0.0442685958852612\n",
      "Train Loss at iteration 2074: 0.044268306079306034\n",
      "Train Loss at iteration 2075: 0.044268016351026604\n",
      "Train Loss at iteration 2076: 0.04426772670031646\n",
      "Train Loss at iteration 2077: 0.04426743712706952\n",
      "Train Loss at iteration 2078: 0.044267147631180066\n",
      "Train Loss at iteration 2079: 0.044266858212542726\n",
      "Train Loss at iteration 2080: 0.04426656887105247\n",
      "Train Loss at iteration 2081: 0.04426627960660467\n",
      "Train Loss at iteration 2082: 0.04426599041909498\n",
      "Train Loss at iteration 2083: 0.044265701308419475\n",
      "Train Loss at iteration 2084: 0.04426541227447452\n",
      "Train Loss at iteration 2085: 0.044265123317156874\n",
      "Train Loss at iteration 2086: 0.04426483443636362\n",
      "Train Loss at iteration 2087: 0.0442645456319922\n",
      "Train Loss at iteration 2088: 0.04426425690394038\n",
      "Train Loss at iteration 2089: 0.04426396825210629\n",
      "Train Loss at iteration 2090: 0.044263679676388376\n",
      "Train Loss at iteration 2091: 0.04426339117668544\n",
      "Train Loss at iteration 2092: 0.04426310275289665\n",
      "Train Loss at iteration 2093: 0.04426281440492146\n",
      "Train Loss at iteration 2094: 0.04426252613265969\n",
      "Train Loss at iteration 2095: 0.04426223793601148\n",
      "Train Loss at iteration 2096: 0.04426194981487733\n",
      "Train Loss at iteration 2097: 0.044261661769158044\n",
      "Train Loss at iteration 2098: 0.04426137379875476\n",
      "Train Loss at iteration 2099: 0.044261085903568975\n",
      "Train Loss at iteration 2100: 0.04426079808350248\n",
      "Train Loss at iteration 2101: 0.04426051033845738\n",
      "Train Loss at iteration 2102: 0.04426022266833619\n",
      "Train Loss at iteration 2103: 0.04425993507304162\n",
      "Train Loss at iteration 2104: 0.04425964755247684\n",
      "Train Loss at iteration 2105: 0.04425936010654523\n",
      "Train Loss at iteration 2106: 0.04425907273515056\n",
      "Train Loss at iteration 2107: 0.04425878543819688\n",
      "Train Loss at iteration 2108: 0.04425849821558859\n",
      "Train Loss at iteration 2109: 0.044258211067230366\n",
      "Train Loss at iteration 2110: 0.04425792399302726\n",
      "Train Loss at iteration 2111: 0.044257636992884576\n",
      "Train Loss at iteration 2112: 0.04425735006670797\n",
      "Train Loss at iteration 2113: 0.044257063214403404\n",
      "Train Loss at iteration 2114: 0.04425677643587713\n",
      "Train Loss at iteration 2115: 0.044256489731035735\n",
      "Train Loss at iteration 2116: 0.04425620309978609\n",
      "Train Loss at iteration 2117: 0.04425591654203541\n",
      "Train Loss at iteration 2118: 0.04425563005769118\n",
      "Train Loss at iteration 2119: 0.04425534364666122\n",
      "Train Loss at iteration 2120: 0.04425505730885361\n",
      "Train Loss at iteration 2121: 0.044254771044176765\n",
      "Train Loss at iteration 2122: 0.04425448485253942\n",
      "Train Loss at iteration 2123: 0.04425419873385056\n",
      "Train Loss at iteration 2124: 0.044253912688019516\n",
      "Train Loss at iteration 2125: 0.04425362671495589\n",
      "Train Loss at iteration 2126: 0.04425334081456956\n",
      "Train Loss at iteration 2127: 0.04425305498677075\n",
      "Train Loss at iteration 2128: 0.04425276923146996\n",
      "Train Loss at iteration 2129: 0.044252483548577956\n",
      "Train Loss at iteration 2130: 0.04425219793800583\n",
      "Train Loss at iteration 2131: 0.04425191239966495\n",
      "Train Loss at iteration 2132: 0.04425162693346699\n",
      "Train Loss at iteration 2133: 0.04425134153932387\n",
      "Train Loss at iteration 2134: 0.04425105621714785\n",
      "Train Loss at iteration 2135: 0.04425077096685145\n",
      "Train Loss at iteration 2136: 0.04425048578834747\n",
      "Train Loss at iteration 2137: 0.044250200681548994\n",
      "Train Loss at iteration 2138: 0.04424991564636942\n",
      "Train Loss at iteration 2139: 0.04424963068272239\n",
      "Train Loss at iteration 2140: 0.044249345790521824\n",
      "Train Loss at iteration 2141: 0.04424906096968199\n",
      "Train Loss at iteration 2142: 0.04424877622011733\n",
      "Train Loss at iteration 2143: 0.044248491541742645\n",
      "Train Loss at iteration 2144: 0.044248206934473004\n",
      "Train Loss at iteration 2145: 0.044247922398223685\n",
      "Train Loss at iteration 2146: 0.04424763793291034\n",
      "Train Loss at iteration 2147: 0.04424735353844881\n",
      "Train Loss at iteration 2148: 0.04424706921475525\n",
      "Train Loss at iteration 2149: 0.04424678496174607\n",
      "Train Loss at iteration 2150: 0.04424650077933798\n",
      "Train Loss at iteration 2151: 0.04424621666744793\n",
      "Train Loss at iteration 2152: 0.04424593262599312\n",
      "Train Loss at iteration 2153: 0.04424564865489108\n",
      "Train Loss at iteration 2154: 0.04424536475405953\n",
      "Train Loss at iteration 2155: 0.04424508092341653\n",
      "Train Loss at iteration 2156: 0.04424479716288035\n",
      "Train Loss at iteration 2157: 0.044244513472369525\n",
      "Train Loss at iteration 2158: 0.0442442298518029\n",
      "Train Loss at iteration 2159: 0.044243946301099527\n",
      "Train Loss at iteration 2160: 0.04424366282017874\n",
      "Train Loss at iteration 2161: 0.04424337940896014\n",
      "Train Loss at iteration 2162: 0.04424309606736357\n",
      "Train Loss at iteration 2163: 0.044242812795309144\n",
      "Train Loss at iteration 2164: 0.04424252959271721\n",
      "Train Loss at iteration 2165: 0.04424224645950842\n",
      "Train Loss at iteration 2166: 0.0442419633956036\n",
      "Train Loss at iteration 2167: 0.044241680400923925\n",
      "Train Loss at iteration 2168: 0.04424139747539073\n",
      "Train Loss at iteration 2169: 0.04424111461892566\n",
      "Train Loss at iteration 2170: 0.0442408318314506\n",
      "Train Loss at iteration 2171: 0.044240549112887655\n",
      "Train Loss at iteration 2172: 0.04424026646315923\n",
      "Train Loss at iteration 2173: 0.04423998388218794\n",
      "Train Loss at iteration 2174: 0.04423970136989665\n",
      "Train Loss at iteration 2175: 0.04423941892620846\n",
      "Train Loss at iteration 2176: 0.04423913655104674\n",
      "Train Loss at iteration 2177: 0.044238854244335116\n",
      "Train Loss at iteration 2178: 0.0442385720059974\n",
      "Train Loss at iteration 2179: 0.044238289835957706\n",
      "Train Loss at iteration 2180: 0.044238007734140335\n",
      "Train Loss at iteration 2181: 0.04423772570046987\n",
      "Train Loss at iteration 2182: 0.04423744373487111\n",
      "Train Loss at iteration 2183: 0.04423716183726911\n",
      "Train Loss at iteration 2184: 0.044236880007589116\n",
      "Train Loss at iteration 2185: 0.0442365982457567\n",
      "Train Loss at iteration 2186: 0.04423631655169758\n",
      "Train Loss at iteration 2187: 0.044236034925337744\n",
      "Train Loss at iteration 2188: 0.04423575336660342\n",
      "Train Loss at iteration 2189: 0.044235471875421054\n",
      "Train Loss at iteration 2190: 0.04423519045171733\n",
      "Train Loss at iteration 2191: 0.04423490909541916\n",
      "Train Loss at iteration 2192: 0.04423462780645371\n",
      "Train Loss at iteration 2193: 0.04423434658474832\n",
      "Train Loss at iteration 2194: 0.04423406543023061\n",
      "Train Loss at iteration 2195: 0.04423378434282841\n",
      "Train Loss at iteration 2196: 0.04423350332246977\n",
      "Train Loss at iteration 2197: 0.04423322236908296\n",
      "Train Loss at iteration 2198: 0.044232941482596495\n",
      "Train Loss at iteration 2199: 0.04423266066293913\n",
      "Train Loss at iteration 2200: 0.04423237991003976\n",
      "Train Loss at iteration 2201: 0.04423209922382761\n",
      "Train Loss at iteration 2202: 0.04423181860423204\n",
      "Train Loss at iteration 2203: 0.04423153805118268\n",
      "Train Loss at iteration 2204: 0.04423125756460936\n",
      "Train Loss at iteration 2205: 0.04423097714444215\n",
      "Train Loss at iteration 2206: 0.044230696790611305\n",
      "Train Loss at iteration 2207: 0.04423041650304729\n",
      "Train Loss at iteration 2208: 0.044230136281680855\n",
      "Train Loss at iteration 2209: 0.0442298561264429\n",
      "Train Loss at iteration 2210: 0.04422957603726454\n",
      "Train Loss at iteration 2211: 0.04422929601407715\n",
      "Train Loss at iteration 2212: 0.04422901605681227\n",
      "Train Loss at iteration 2213: 0.044228736165401704\n",
      "Train Loss at iteration 2214: 0.0442284563397774\n",
      "Train Loss at iteration 2215: 0.04422817657987157\n",
      "Train Loss at iteration 2216: 0.0442278968856166\n",
      "Train Loss at iteration 2217: 0.04422761725694512\n",
      "Train Loss at iteration 2218: 0.044227337693789955\n",
      "Train Loss at iteration 2219: 0.044227058196084114\n",
      "Train Loss at iteration 2220: 0.044226778763760854\n",
      "Train Loss at iteration 2221: 0.04422649939675359\n",
      "Train Loss at iteration 2222: 0.04422622009499599\n",
      "Train Loss at iteration 2223: 0.044225940858421874\n",
      "Train Loss at iteration 2224: 0.04422566168696531\n",
      "Train Loss at iteration 2225: 0.044225382580560586\n",
      "Train Loss at iteration 2226: 0.0442251035391421\n",
      "Train Loss at iteration 2227: 0.044224824562644566\n",
      "Train Loss at iteration 2228: 0.04422454565100279\n",
      "Train Loss at iteration 2229: 0.04422426680415186\n",
      "Train Loss at iteration 2230: 0.04422398802202703\n",
      "Train Loss at iteration 2231: 0.04422370930456374\n",
      "Train Loss at iteration 2232: 0.04422343065169767\n",
      "Train Loss at iteration 2233: 0.04422315206336464\n",
      "Train Loss at iteration 2234: 0.04422287353950069\n",
      "Train Loss at iteration 2235: 0.04422259508004209\n",
      "Train Loss at iteration 2236: 0.04422231668492524\n",
      "Train Loss at iteration 2237: 0.044222038354086786\n",
      "Train Loss at iteration 2238: 0.04422176008746355\n",
      "Train Loss at iteration 2239: 0.04422148188499253\n",
      "Train Loss at iteration 2240: 0.044221203746610926\n",
      "Train Loss at iteration 2241: 0.04422092567225615\n",
      "Train Loss at iteration 2242: 0.04422064766186577\n",
      "Train Loss at iteration 2243: 0.044220369715377565\n",
      "Train Loss at iteration 2244: 0.044220091832729486\n",
      "Train Loss at iteration 2245: 0.044219814013859694\n",
      "Train Loss at iteration 2246: 0.04421953625870652\n",
      "Train Loss at iteration 2247: 0.044219258567208486\n",
      "Train Loss at iteration 2248: 0.044218980939304285\n",
      "Train Loss at iteration 2249: 0.04421870337493284\n",
      "Train Loss at iteration 2250: 0.04421842587403319\n",
      "Train Loss at iteration 2251: 0.04421814843654464\n",
      "Train Loss at iteration 2252: 0.0442178710624066\n",
      "Train Loss at iteration 2253: 0.04421759375155868\n",
      "Train Loss at iteration 2254: 0.04421731650394073\n",
      "Train Loss at iteration 2255: 0.04421703931949272\n",
      "Train Loss at iteration 2256: 0.044216762198154805\n",
      "Train Loss at iteration 2257: 0.04421648513986734\n",
      "Train Loss at iteration 2258: 0.04421620814457084\n",
      "Train Loss at iteration 2259: 0.04421593121220603\n",
      "Train Loss at iteration 2260: 0.04421565434271377\n",
      "Train Loss at iteration 2261: 0.04421537753603513\n",
      "Train Loss at iteration 2262: 0.04421510079211134\n",
      "Train Loss at iteration 2263: 0.0442148241108838\n",
      "Train Loss at iteration 2264: 0.04421454749229412\n",
      "Train Loss at iteration 2265: 0.044214270936284036\n",
      "Train Loss at iteration 2266: 0.044213994442795466\n",
      "Train Loss at iteration 2267: 0.04421371801177053\n",
      "Train Loss at iteration 2268: 0.04421344164315152\n",
      "Train Loss at iteration 2269: 0.04421316533688085\n",
      "Train Loss at iteration 2270: 0.04421288909290115\n",
      "Train Loss at iteration 2271: 0.044212612911155226\n",
      "Train Loss at iteration 2272: 0.044212336791586\n",
      "Train Loss at iteration 2273: 0.04421206073413663\n",
      "Train Loss at iteration 2274: 0.04421178473875042\n",
      "Train Loss at iteration 2275: 0.044211508805370794\n",
      "Train Loss at iteration 2276: 0.0442112329339414\n",
      "Train Loss at iteration 2277: 0.044210957124406036\n",
      "Train Loss at iteration 2278: 0.04421068137670865\n",
      "Train Loss at iteration 2279: 0.04421040569079339\n",
      "Train Loss at iteration 2280: 0.04421013006660453\n",
      "Train Loss at iteration 2281: 0.04420985450408652\n",
      "Train Loss at iteration 2282: 0.04420957900318401\n",
      "Train Loss at iteration 2283: 0.044209303563841736\n",
      "Train Loss at iteration 2284: 0.04420902818600468\n",
      "Train Loss at iteration 2285: 0.044208752869617916\n",
      "Train Loss at iteration 2286: 0.04420847761462673\n",
      "Train Loss at iteration 2287: 0.04420820242097652\n",
      "Train Loss at iteration 2288: 0.044207927288612905\n",
      "Train Loss at iteration 2289: 0.0442076522174816\n",
      "Train Loss at iteration 2290: 0.044207377207528535\n",
      "Train Loss at iteration 2291: 0.04420710225869974\n",
      "Train Loss at iteration 2292: 0.044206827370941455\n",
      "Train Loss at iteration 2293: 0.04420655254420004\n",
      "Train Loss at iteration 2294: 0.04420627777842202\n",
      "Train Loss at iteration 2295: 0.044206003073554094\n",
      "Train Loss at iteration 2296: 0.04420572842954309\n",
      "Train Loss at iteration 2297: 0.044205453846336014\n",
      "Train Loss at iteration 2298: 0.04420517932388002\n",
      "Train Loss at iteration 2299: 0.044204904862122384\n",
      "Train Loss at iteration 2300: 0.04420463046101056\n",
      "Train Loss at iteration 2301: 0.04420435612049219\n",
      "Train Loss at iteration 2302: 0.04420408184051499\n",
      "Train Loss at iteration 2303: 0.04420380762102689\n",
      "Train Loss at iteration 2304: 0.04420353346197595\n",
      "Train Loss at iteration 2305: 0.04420325936331037\n",
      "Train Loss at iteration 2306: 0.0442029853249785\n",
      "Train Loss at iteration 2307: 0.04420271134692886\n",
      "Train Loss at iteration 2308: 0.04420243742911009\n",
      "Train Loss at iteration 2309: 0.044202163571471005\n",
      "Train Loss at iteration 2310: 0.04420188977396055\n",
      "Train Loss at iteration 2311: 0.044201616036527815\n",
      "Train Loss at iteration 2312: 0.04420134235912205\n",
      "Train Loss at iteration 2313: 0.04420106874169262\n",
      "Train Loss at iteration 2314: 0.04420079518418908\n",
      "Train Loss at iteration 2315: 0.04420052168656109\n",
      "Train Loss at iteration 2316: 0.04420024824875848\n",
      "Train Loss at iteration 2317: 0.0441999748707312\n",
      "Train Loss at iteration 2318: 0.04419970155242936\n",
      "Train Loss at iteration 2319: 0.044199428293803196\n",
      "Train Loss at iteration 2320: 0.044199155094803114\n",
      "Train Loss at iteration 2321: 0.04419888195537964\n",
      "Train Loss at iteration 2322: 0.04419860887548342\n",
      "Train Loss at iteration 2323: 0.0441983358550653\n",
      "Train Loss at iteration 2324: 0.044198062894076205\n",
      "Train Loss at iteration 2325: 0.044197789992467235\n",
      "Train Loss at iteration 2326: 0.0441975171501896\n",
      "Train Loss at iteration 2327: 0.0441972443671947\n",
      "Train Loss at iteration 2328: 0.04419697164343399\n",
      "Train Loss at iteration 2329: 0.04419669897885914\n",
      "Train Loss at iteration 2330: 0.04419642637342191\n",
      "Train Loss at iteration 2331: 0.044196153827074224\n",
      "Train Loss at iteration 2332: 0.04419588133976813\n",
      "Train Loss at iteration 2333: 0.04419560891145578\n",
      "Train Loss at iteration 2334: 0.044195336542089535\n",
      "Train Loss at iteration 2335: 0.044195064231621804\n",
      "Train Loss at iteration 2336: 0.044194791980005206\n",
      "Train Loss at iteration 2337: 0.04419451978719242\n",
      "Train Loss at iteration 2338: 0.04419424765313632\n",
      "Train Loss at iteration 2339: 0.04419397557778989\n",
      "Train Loss at iteration 2340: 0.04419370356110622\n",
      "Train Loss at iteration 2341: 0.044193431603038555\n",
      "Train Loss at iteration 2342: 0.044193159703540305\n",
      "Train Loss at iteration 2343: 0.04419288786256493\n",
      "Train Loss at iteration 2344: 0.04419261608006606\n",
      "Train Loss at iteration 2345: 0.04419234435599749\n",
      "Train Loss at iteration 2346: 0.044192072690313104\n",
      "Train Loss at iteration 2347: 0.04419180108296689\n",
      "Train Loss at iteration 2348: 0.04419152953391303\n",
      "Train Loss at iteration 2349: 0.04419125804310577\n",
      "Train Loss at iteration 2350: 0.044190986610499534\n",
      "Train Loss at iteration 2351: 0.04419071523604883\n",
      "Train Loss at iteration 2352: 0.044190443919708305\n",
      "Train Loss at iteration 2353: 0.04419017266143275\n",
      "Train Loss at iteration 2354: 0.04418990146117707\n",
      "Train Loss at iteration 2355: 0.044189630318896264\n",
      "Train Loss at iteration 2356: 0.04418935923454552\n",
      "Train Loss at iteration 2357: 0.04418908820808008\n",
      "Train Loss at iteration 2358: 0.04418881723945535\n",
      "Train Loss at iteration 2359: 0.044188546328626836\n",
      "Train Loss at iteration 2360: 0.044188275475550216\n",
      "Train Loss at iteration 2361: 0.04418800468018122\n",
      "Train Loss at iteration 2362: 0.044187733942475736\n",
      "Train Loss at iteration 2363: 0.04418746326238977\n",
      "Train Loss at iteration 2364: 0.04418719263987945\n",
      "Train Loss at iteration 2365: 0.04418692207490102\n",
      "Train Loss at iteration 2366: 0.04418665156741082\n",
      "Train Loss at iteration 2367: 0.04418638111736537\n",
      "Train Loss at iteration 2368: 0.044186110724721256\n",
      "Train Loss at iteration 2369: 0.044185840389435196\n",
      "Train Loss at iteration 2370: 0.04418557011146402\n",
      "Train Loss at iteration 2371: 0.04418529989076469\n",
      "Train Loss at iteration 2372: 0.04418502972729427\n",
      "Train Loss at iteration 2373: 0.04418475962100996\n",
      "Train Loss at iteration 2374: 0.044184489571869046\n",
      "Train Loss at iteration 2375: 0.044184219579828946\n",
      "Train Loss at iteration 2376: 0.04418394964484721\n",
      "Train Loss at iteration 2377: 0.04418367976688147\n",
      "Train Loss at iteration 2378: 0.04418340994588949\n",
      "Train Loss at iteration 2379: 0.04418314018182916\n",
      "Train Loss at iteration 2380: 0.04418287047465848\n",
      "Train Loss at iteration 2381: 0.04418260082433549\n",
      "Train Loss at iteration 2382: 0.04418233123081848\n",
      "Train Loss at iteration 2383: 0.04418206169406572\n",
      "Train Loss at iteration 2384: 0.04418179221403569\n",
      "Train Loss at iteration 2385: 0.04418152279068691\n",
      "Train Loss at iteration 2386: 0.04418125342397806\n",
      "Train Loss at iteration 2387: 0.044180984113867915\n",
      "Train Loss at iteration 2388: 0.04418071486031533\n",
      "Train Loss at iteration 2389: 0.044180445663279305\n",
      "Train Loss at iteration 2390: 0.04418017652271896\n",
      "Train Loss at iteration 2391: 0.044179907438593485\n",
      "Train Loss at iteration 2392: 0.04417963841086221\n",
      "Train Loss at iteration 2393: 0.044179369439484556\n",
      "Train Loss at iteration 2394: 0.04417910052442006\n",
      "Train Loss at iteration 2395: 0.04417883166562834\n",
      "Train Loss at iteration 2396: 0.04417856286306918\n",
      "Train Loss at iteration 2397: 0.04417829411670242\n",
      "Train Loss at iteration 2398: 0.04417802542648801\n",
      "Train Loss at iteration 2399: 0.044177756792386016\n",
      "Train Loss at iteration 2400: 0.04417748821435663\n",
      "Train Loss at iteration 2401: 0.044177219692360124\n",
      "Train Loss at iteration 2402: 0.04417695122635684\n",
      "Train Loss at iteration 2403: 0.044176682816307315\n",
      "Train Loss at iteration 2404: 0.04417641446217212\n",
      "Train Loss at iteration 2405: 0.044176146163911946\n",
      "Train Loss at iteration 2406: 0.04417587792148758\n",
      "Train Loss at iteration 2407: 0.04417560973485992\n",
      "Train Loss at iteration 2408: 0.04417534160398999\n",
      "Train Loss at iteration 2409: 0.04417507352883888\n",
      "Train Loss at iteration 2410: 0.04417480550936779\n",
      "Train Loss at iteration 2411: 0.04417453754553802\n",
      "Train Loss at iteration 2412: 0.04417426963731098\n",
      "Train Loss at iteration 2413: 0.04417400178464819\n",
      "Train Loss at iteration 2414: 0.044173733987511246\n",
      "Train Loss at iteration 2415: 0.044173466245861846\n",
      "Train Loss at iteration 2416: 0.04417319855966182\n",
      "Train Loss at iteration 2417: 0.04417293092887306\n",
      "Train Loss at iteration 2418: 0.04417266335345756\n",
      "Train Loss at iteration 2419: 0.04417239583337744\n",
      "Train Loss at iteration 2420: 0.044172128368594875\n",
      "Train Loss at iteration 2421: 0.044171860959072204\n",
      "Train Loss at iteration 2422: 0.044171593604771776\n",
      "Train Loss at iteration 2423: 0.04417132630565613\n",
      "Train Loss at iteration 2424: 0.04417105906168779\n",
      "Train Loss at iteration 2425: 0.044170791872829504\n",
      "Train Loss at iteration 2426: 0.044170524739044016\n",
      "Train Loss at iteration 2427: 0.04417025766029422\n",
      "Train Loss at iteration 2428: 0.044169990636543074\n",
      "Train Loss at iteration 2429: 0.04416972366775365\n",
      "Train Loss at iteration 2430: 0.04416945675388911\n",
      "Train Loss at iteration 2431: 0.0441691898949127\n",
      "Train Loss at iteration 2432: 0.04416892309078777\n",
      "Train Loss at iteration 2433: 0.04416865634147776\n",
      "Train Loss at iteration 2434: 0.04416838964694622\n",
      "Train Loss at iteration 2435: 0.044168123007156765\n",
      "Train Loss at iteration 2436: 0.04416785642207312\n",
      "Train Loss at iteration 2437: 0.044167589891659094\n",
      "Train Loss at iteration 2438: 0.04416732341587859\n",
      "Train Loss at iteration 2439: 0.04416705699469559\n",
      "Train Loss at iteration 2440: 0.044166790628074194\n",
      "Train Loss at iteration 2441: 0.04416652431597858\n",
      "Train Loss at iteration 2442: 0.04416625805837303\n",
      "Train Loss at iteration 2443: 0.04416599185522187\n",
      "Train Loss at iteration 2444: 0.044165725706489564\n",
      "Train Loss at iteration 2445: 0.04416545961214065\n",
      "Train Loss at iteration 2446: 0.044165193572139744\n",
      "Train Loss at iteration 2447: 0.04416492758645157\n",
      "Train Loss at iteration 2448: 0.04416466165504093\n",
      "Train Loss at iteration 2449: 0.044164395777872706\n",
      "Train Loss at iteration 2450: 0.044164129954911896\n",
      "Train Loss at iteration 2451: 0.04416386418612356\n",
      "Train Loss at iteration 2452: 0.04416359847147283\n",
      "Train Loss at iteration 2453: 0.04416333281092497\n",
      "Train Loss at iteration 2454: 0.0441630672044453\n",
      "Train Loss at iteration 2455: 0.04416280165199923\n",
      "Train Loss at iteration 2456: 0.04416253615355228\n",
      "Train Loss at iteration 2457: 0.04416227070907001\n",
      "Train Loss at iteration 2458: 0.0441620053185181\n",
      "Train Loss at iteration 2459: 0.044161739981862305\n",
      "Train Loss at iteration 2460: 0.04416147469906849\n",
      "Train Loss at iteration 2461: 0.044161209470102526\n",
      "Train Loss at iteration 2462: 0.04416094429493047\n",
      "Train Loss at iteration 2463: 0.044160679173518395\n",
      "Train Loss at iteration 2464: 0.0441604141058325\n",
      "Train Loss at iteration 2465: 0.044160149091839\n",
      "Train Loss at iteration 2466: 0.044159884131504276\n",
      "Train Loss at iteration 2467: 0.044159619224794736\n",
      "Train Loss at iteration 2468: 0.04415935437167689\n",
      "Train Loss at iteration 2469: 0.04415908957211734\n",
      "Train Loss at iteration 2470: 0.04415882482608274\n",
      "Train Loss at iteration 2471: 0.04415856013353986\n",
      "Train Loss at iteration 2472: 0.04415829549445552\n",
      "Train Loss at iteration 2473: 0.044158030908796636\n",
      "Train Loss at iteration 2474: 0.04415776637653021\n",
      "Train Loss at iteration 2475: 0.04415750189762333\n",
      "Train Loss at iteration 2476: 0.044157237472043136\n",
      "Train Loss at iteration 2477: 0.04415697309975686\n",
      "Train Loss at iteration 2478: 0.0441567087807318\n",
      "Train Loss at iteration 2479: 0.04415644451493542\n",
      "Train Loss at iteration 2480: 0.044156180302335105\n",
      "Train Loss at iteration 2481: 0.04415591614289847\n",
      "Train Loss at iteration 2482: 0.044155652036593114\n",
      "Train Loss at iteration 2483: 0.04415538798338676\n",
      "Train Loss at iteration 2484: 0.04415512398324719\n",
      "Train Loss at iteration 2485: 0.044154860036142274\n",
      "Train Loss at iteration 2486: 0.044154596142039926\n",
      "Train Loss at iteration 2487: 0.0441543323009082\n",
      "Train Loss at iteration 2488: 0.04415406851271516\n",
      "Train Loss at iteration 2489: 0.044153804777428994\n",
      "Train Loss at iteration 2490: 0.04415354109501794\n",
      "Train Loss at iteration 2491: 0.04415327746545033\n",
      "Train Loss at iteration 2492: 0.04415301388869455\n",
      "Train Loss at iteration 2493: 0.04415275036471909\n",
      "Train Loss at iteration 2494: 0.044152486893492486\n",
      "Train Loss at iteration 2495: 0.04415222347498337\n",
      "Train Loss at iteration 2496: 0.04415196010916043\n",
      "Train Loss at iteration 2497: 0.04415169679599243\n",
      "Train Loss at iteration 2498: 0.044151433535448245\n",
      "Train Loss at iteration 2499: 0.04415117032749677\n",
      "Train Loss at iteration 2500: 0.044150907172107005\n",
      "Train Loss at iteration 2501: 0.04415064406924802\n",
      "Train Loss at iteration 2502: 0.04415038101888895\n",
      "Train Loss at iteration 2503: 0.044150118020999\n",
      "Train Loss at iteration 2504: 0.04414985507554747\n",
      "Train Loss at iteration 2505: 0.044149592182503715\n",
      "Train Loss at iteration 2506: 0.04414932934183715\n",
      "Train Loss at iteration 2507: 0.04414906655351728\n",
      "Train Loss at iteration 2508: 0.04414880381751368\n",
      "Train Loss at iteration 2509: 0.04414854113379601\n",
      "Train Loss at iteration 2510: 0.044148278502333944\n",
      "Train Loss at iteration 2511: 0.04414801592309729\n",
      "Train Loss at iteration 2512: 0.044147753396055894\n",
      "Train Loss at iteration 2513: 0.044147490921179706\n",
      "Train Loss at iteration 2514: 0.04414722849843869\n",
      "Train Loss at iteration 2515: 0.04414696612780292\n",
      "Train Loss at iteration 2516: 0.04414670380924252\n",
      "Train Loss at iteration 2517: 0.04414644154272772\n",
      "Train Loss at iteration 2518: 0.04414617932822877\n",
      "Train Loss at iteration 2519: 0.044145917165716024\n",
      "Train Loss at iteration 2520: 0.044145655055159855\n",
      "Train Loss at iteration 2521: 0.044145392996530794\n",
      "Train Loss at iteration 2522: 0.04414513098979935\n",
      "Train Loss at iteration 2523: 0.04414486903493615\n",
      "Train Loss at iteration 2524: 0.04414460713191185\n",
      "Train Loss at iteration 2525: 0.044144345280697224\n",
      "Train Loss at iteration 2526: 0.044144083481263086\n",
      "Train Loss at iteration 2527: 0.0441438217335803\n",
      "Train Loss at iteration 2528: 0.04414356003761982\n",
      "Train Loss at iteration 2529: 0.044143298393352674\n",
      "Train Loss at iteration 2530: 0.044143036800749914\n",
      "Train Loss at iteration 2531: 0.04414277525978271\n",
      "Train Loss at iteration 2532: 0.04414251377042226\n",
      "Train Loss at iteration 2533: 0.04414225233263984\n",
      "Train Loss at iteration 2534: 0.04414199094640682\n",
      "Train Loss at iteration 2535: 0.044141729611694554\n",
      "Train Loss at iteration 2536: 0.04414146832847456\n",
      "Train Loss at iteration 2537: 0.044141207096718336\n",
      "Train Loss at iteration 2538: 0.04414094591639752\n",
      "Train Loss at iteration 2539: 0.04414068478748375\n",
      "Train Loss at iteration 2540: 0.044140423709948765\n",
      "Train Loss at iteration 2541: 0.04414016268376435\n",
      "Train Loss at iteration 2542: 0.04413990170890236\n",
      "Train Loss at iteration 2543: 0.04413964078533473\n",
      "Train Loss at iteration 2544: 0.0441393799130334\n",
      "Train Loss at iteration 2545: 0.04413911909197045\n",
      "Train Loss at iteration 2546: 0.04413885832211796\n",
      "Train Loss at iteration 2547: 0.04413859760344813\n",
      "Train Loss at iteration 2548: 0.04413833693593316\n",
      "Train Loss at iteration 2549: 0.044138076319545354\n",
      "Train Loss at iteration 2550: 0.04413781575425708\n",
      "Train Loss at iteration 2551: 0.04413755524004071\n",
      "Train Loss at iteration 2552: 0.04413729477686876\n",
      "Train Loss at iteration 2553: 0.044137034364713744\n",
      "Train Loss at iteration 2554: 0.04413677400354826\n",
      "Train Loss at iteration 2555: 0.044136513693344974\n",
      "Train Loss at iteration 2556: 0.04413625343407663\n",
      "Train Loss at iteration 2557: 0.04413599322571596\n",
      "Train Loss at iteration 2558: 0.04413573306823581\n",
      "Train Loss at iteration 2559: 0.044135472961609114\n",
      "Train Loss at iteration 2560: 0.04413521290580878\n",
      "Train Loss at iteration 2561: 0.04413495290080785\n",
      "Train Loss at iteration 2562: 0.044134692946579415\n",
      "Train Loss at iteration 2563: 0.044134433043096576\n",
      "Train Loss at iteration 2564: 0.04413417319033254\n",
      "Train Loss at iteration 2565: 0.04413391338826056\n",
      "Train Loss at iteration 2566: 0.04413365363685395\n",
      "Train Loss at iteration 2567: 0.04413339393608607\n",
      "Train Loss at iteration 2568: 0.044133134285930346\n",
      "Train Loss at iteration 2569: 0.044132874686360256\n",
      "Train Loss at iteration 2570: 0.04413261513734936\n",
      "Train Loss at iteration 2571: 0.04413235563887122\n",
      "Train Loss at iteration 2572: 0.04413209619089953\n",
      "Train Loss at iteration 2573: 0.04413183679340797\n",
      "Train Loss at iteration 2574: 0.044131577446370335\n",
      "Train Loss at iteration 2575: 0.044131318149760436\n",
      "Train Loss at iteration 2576: 0.044131058903552144\n",
      "Train Loss at iteration 2577: 0.04413079970771941\n",
      "Train Loss at iteration 2578: 0.04413054056223622\n",
      "Train Loss at iteration 2579: 0.04413028146707662\n",
      "Train Loss at iteration 2580: 0.04413002242221471\n",
      "Train Loss at iteration 2581: 0.04412976342762467\n",
      "Train Loss at iteration 2582: 0.04412950448328068\n",
      "Train Loss at iteration 2583: 0.044129245589157054\n",
      "Train Loss at iteration 2584: 0.04412898674522807\n",
      "Train Loss at iteration 2585: 0.04412872795146814\n",
      "Train Loss at iteration 2586: 0.04412846920785166\n",
      "Train Loss at iteration 2587: 0.044128210514353165\n",
      "Train Loss at iteration 2588: 0.04412795187094715\n",
      "Train Loss at iteration 2589: 0.044127693277608224\n",
      "Train Loss at iteration 2590: 0.04412743473431105\n",
      "Train Loss at iteration 2591: 0.04412717624103032\n",
      "Train Loss at iteration 2592: 0.044126917797740764\n",
      "Train Loss at iteration 2593: 0.04412665940441723\n",
      "Train Loss at iteration 2594: 0.04412640106103455\n",
      "Train Loss at iteration 2595: 0.04412614276756765\n",
      "Train Loss at iteration 2596: 0.04412588452399148\n",
      "Train Loss at iteration 2597: 0.044125626330281074\n",
      "Train Loss at iteration 2598: 0.04412536818641149\n",
      "Train Loss at iteration 2599: 0.044125110092357867\n",
      "Train Loss at iteration 2600: 0.04412485204809535\n",
      "Train Loss at iteration 2601: 0.04412459405359919\n",
      "Train Loss at iteration 2602: 0.04412433610884465\n",
      "Train Loss at iteration 2603: 0.04412407821380706\n",
      "Train Loss at iteration 2604: 0.0441238203684618\n",
      "Train Loss at iteration 2605: 0.04412356257278428\n",
      "Train Loss at iteration 2606: 0.04412330482675001\n",
      "Train Loss at iteration 2607: 0.04412304713033452\n",
      "Train Loss at iteration 2608: 0.04412278948351337\n",
      "Train Loss at iteration 2609: 0.044122531886262195\n",
      "Train Loss at iteration 2610: 0.04412227433855669\n",
      "Train Loss at iteration 2611: 0.04412201684037256\n",
      "Train Loss at iteration 2612: 0.04412175939168562\n",
      "Train Loss at iteration 2613: 0.04412150199247168\n",
      "Train Loss at iteration 2614: 0.044121244642706614\n",
      "Train Loss at iteration 2615: 0.044120987342366365\n",
      "Train Loss at iteration 2616: 0.044120730091426884\n",
      "Train Loss at iteration 2617: 0.044120472889864246\n",
      "Train Loss at iteration 2618: 0.04412021573765446\n",
      "Train Loss at iteration 2619: 0.04411995863477372\n",
      "Train Loss at iteration 2620: 0.04411970158119814\n",
      "Train Loss at iteration 2621: 0.04411944457690399\n",
      "Train Loss at iteration 2622: 0.044119187621867485\n",
      "Train Loss at iteration 2623: 0.04411893071606497\n",
      "Train Loss at iteration 2624: 0.044118673859472816\n",
      "Train Loss at iteration 2625: 0.04411841705206742\n",
      "Train Loss at iteration 2626: 0.04411816029382524\n",
      "Train Loss at iteration 2627: 0.044117903584722784\n",
      "Train Loss at iteration 2628: 0.0441176469247366\n",
      "Train Loss at iteration 2629: 0.0441173903138433\n",
      "Train Loss at iteration 2630: 0.044117133752019505\n",
      "Train Loss at iteration 2631: 0.04411687723924194\n",
      "Train Loss at iteration 2632: 0.04411662077548731\n",
      "Train Loss at iteration 2633: 0.044116364360732435\n",
      "Train Loss at iteration 2634: 0.04411610799495411\n",
      "Train Loss at iteration 2635: 0.04411585167812925\n",
      "Train Loss at iteration 2636: 0.04411559541023474\n",
      "Train Loss at iteration 2637: 0.04411533919124758\n",
      "Train Loss at iteration 2638: 0.04411508302114477\n",
      "Train Loss at iteration 2639: 0.04411482689990336\n",
      "Train Loss at iteration 2640: 0.04411457082750047\n",
      "Train Loss at iteration 2641: 0.04411431480391323\n",
      "Train Loss at iteration 2642: 0.044114058829118855\n",
      "Train Loss at iteration 2643: 0.04411380290309457\n",
      "Train Loss at iteration 2644: 0.044113547025817657\n",
      "Train Loss at iteration 2645: 0.04411329119726547\n",
      "Train Loss at iteration 2646: 0.04411303541741534\n",
      "Train Loss at iteration 2647: 0.04411277968624472\n",
      "Train Loss at iteration 2648: 0.04411252400373103\n",
      "Train Loss at iteration 2649: 0.044112268369851805\n",
      "Train Loss at iteration 2650: 0.04411201278458458\n",
      "Train Loss at iteration 2651: 0.04411175724790695\n",
      "Train Loss at iteration 2652: 0.044111501759796556\n",
      "Train Loss at iteration 2653: 0.044111246320231064\n",
      "Train Loss at iteration 2654: 0.04411099092918818\n",
      "Train Loss at iteration 2655: 0.04411073558664569\n",
      "Train Loss at iteration 2656: 0.04411048029258141\n",
      "Train Loss at iteration 2657: 0.04411022504697316\n",
      "Train Loss at iteration 2658: 0.04410996984979883\n",
      "Train Loss at iteration 2659: 0.04410971470103639\n",
      "Train Loss at iteration 2660: 0.04410945960066377\n",
      "Train Loss at iteration 2661: 0.04410920454865902\n",
      "Train Loss at iteration 2662: 0.04410894954500017\n",
      "Train Loss at iteration 2663: 0.04410869458966536\n",
      "Train Loss at iteration 2664: 0.04410843968263271\n",
      "Train Loss at iteration 2665: 0.04410818482388039\n",
      "Train Loss at iteration 2666: 0.04410793001338665\n",
      "Train Loss at iteration 2667: 0.04410767525112974\n",
      "Train Loss at iteration 2668: 0.044107420537088\n",
      "Train Loss at iteration 2669: 0.04410716587123972\n",
      "Train Loss at iteration 2670: 0.044106911253563336\n",
      "Train Loss at iteration 2671: 0.044106656684037265\n",
      "Train Loss at iteration 2672: 0.04410640216263999\n",
      "Train Loss at iteration 2673: 0.04410614768935\n",
      "Train Loss at iteration 2674: 0.04410589326414586\n",
      "Train Loss at iteration 2675: 0.04410563888700615\n",
      "Train Loss at iteration 2676: 0.04410538455790951\n",
      "Train Loss at iteration 2677: 0.04410513027683462\n",
      "Train Loss at iteration 2678: 0.04410487604376017\n",
      "Train Loss at iteration 2679: 0.044104621858664914\n",
      "Train Loss at iteration 2680: 0.04410436772152767\n",
      "Train Loss at iteration 2681: 0.04410411363232723\n",
      "Train Loss at iteration 2682: 0.04410385959104249\n",
      "Train Loss at iteration 2683: 0.04410360559765232\n",
      "Train Loss at iteration 2684: 0.04410335165213571\n",
      "Train Loss at iteration 2685: 0.04410309775447163\n",
      "Train Loss at iteration 2686: 0.044102843904639095\n",
      "Train Loss at iteration 2687: 0.04410259010261717\n",
      "Train Loss at iteration 2688: 0.04410233634838497\n",
      "Train Loss at iteration 2689: 0.044102082641921614\n",
      "Train Loss at iteration 2690: 0.04410182898320628\n",
      "Train Loss at iteration 2691: 0.044101575372218205\n",
      "Train Loss at iteration 2692: 0.04410132180893662\n",
      "Train Loss at iteration 2693: 0.044101068293340824\n",
      "Train Loss at iteration 2694: 0.044100814825410145\n",
      "Train Loss at iteration 2695: 0.044100561405123945\n",
      "Train Loss at iteration 2696: 0.04410030803246164\n",
      "Train Loss at iteration 2697: 0.04410005470740265\n",
      "Train Loss at iteration 2698: 0.04409980142992647\n",
      "Train Loss at iteration 2699: 0.044099548200012606\n",
      "Train Loss at iteration 2700: 0.044099295017640615\n",
      "Train Loss at iteration 2701: 0.04409904188279008\n",
      "Train Loss at iteration 2702: 0.04409878879544062\n",
      "Train Loss at iteration 2703: 0.044098535755571926\n",
      "Train Loss at iteration 2704: 0.04409828276316367\n",
      "Train Loss at iteration 2705: 0.04409802981819558\n",
      "Train Loss at iteration 2706: 0.044097776920647445\n",
      "Train Loss at iteration 2707: 0.04409752407049906\n",
      "Train Loss at iteration 2708: 0.044097271267730276\n",
      "Train Loss at iteration 2709: 0.044097018512320965\n",
      "Train Loss at iteration 2710: 0.04409676580425105\n",
      "Train Loss at iteration 2711: 0.04409651314350048\n",
      "Train Loss at iteration 2712: 0.04409626053004922\n",
      "Train Loss at iteration 2713: 0.044096007963877304\n",
      "Train Loss at iteration 2714: 0.04409575544496481\n",
      "Train Loss at iteration 2715: 0.04409550297329178\n",
      "Train Loss at iteration 2716: 0.044095250548838366\n",
      "Train Loss at iteration 2717: 0.04409499817158475\n",
      "Train Loss at iteration 2718: 0.0440947458415111\n",
      "Train Loss at iteration 2719: 0.04409449355859765\n",
      "Train Loss at iteration 2720: 0.044094241322824655\n",
      "Train Loss at iteration 2721: 0.044093989134172444\n",
      "Train Loss at iteration 2722: 0.04409373699262131\n",
      "Train Loss at iteration 2723: 0.04409348489815167\n",
      "Train Loss at iteration 2724: 0.04409323285074388\n",
      "Train Loss at iteration 2725: 0.044092980850378403\n",
      "Train Loss at iteration 2726: 0.044092728897035686\n",
      "Train Loss at iteration 2727: 0.04409247699069626\n",
      "Train Loss at iteration 2728: 0.04409222513134065\n",
      "Train Loss at iteration 2729: 0.044091973318949415\n",
      "Train Loss at iteration 2730: 0.04409172155350318\n",
      "Train Loss at iteration 2731: 0.044091469834982557\n",
      "Train Loss at iteration 2732: 0.04409121816336824\n",
      "Train Loss at iteration 2733: 0.044090966538640905\n",
      "Train Loss at iteration 2734: 0.04409071496078131\n",
      "Train Loss at iteration 2735: 0.04409046342977023\n",
      "Train Loss at iteration 2736: 0.044090211945588445\n",
      "Train Loss at iteration 2737: 0.044089960508216806\n",
      "Train Loss at iteration 2738: 0.04408970911763618\n",
      "Train Loss at iteration 2739: 0.04408945777382745\n",
      "Train Loss at iteration 2740: 0.04408920647677158\n",
      "Train Loss at iteration 2741: 0.044088955226449504\n",
      "Train Loss at iteration 2742: 0.044088704022842234\n",
      "Train Loss at iteration 2743: 0.04408845286593077\n",
      "Train Loss at iteration 2744: 0.04408820175569623\n",
      "Train Loss at iteration 2745: 0.04408795069211967\n",
      "Train Loss at iteration 2746: 0.0440876996751822\n",
      "Train Loss at iteration 2747: 0.04408744870486501\n",
      "Train Loss at iteration 2748: 0.04408719778114925\n",
      "Train Loss at iteration 2749: 0.044086946904016176\n",
      "Train Loss at iteration 2750: 0.04408669607344701\n",
      "Train Loss at iteration 2751: 0.04408644528942304\n",
      "Train Loss at iteration 2752: 0.044086194551925605\n",
      "Train Loss at iteration 2753: 0.044085943860936005\n",
      "Train Loss at iteration 2754: 0.044085693216435635\n",
      "Train Loss at iteration 2755: 0.0440854426184059\n",
      "Train Loss at iteration 2756: 0.044085192066828253\n",
      "Train Loss at iteration 2757: 0.044084941561684125\n",
      "Train Loss at iteration 2758: 0.044084691102955036\n",
      "Train Loss at iteration 2759: 0.044084440690622506\n",
      "Train Loss at iteration 2760: 0.04408419032466809\n",
      "Train Loss at iteration 2761: 0.0440839400050734\n",
      "Train Loss at iteration 2762: 0.044083689731820024\n",
      "Train Loss at iteration 2763: 0.04408343950488961\n",
      "Train Loss at iteration 2764: 0.04408318932426386\n",
      "Train Loss at iteration 2765: 0.044082939189924474\n",
      "Train Loss at iteration 2766: 0.044082689101853166\n",
      "Train Loss at iteration 2767: 0.04408243906003173\n",
      "Train Loss at iteration 2768: 0.04408218906444195\n",
      "Train Loss at iteration 2769: 0.04408193911506566\n",
      "Train Loss at iteration 2770: 0.044081689211884705\n",
      "Train Loss at iteration 2771: 0.044081439354880986\n",
      "Train Loss at iteration 2772: 0.0440811895440364\n",
      "Train Loss at iteration 2773: 0.0440809397793329\n",
      "Train Loss at iteration 2774: 0.04408069006075246\n",
      "Train Loss at iteration 2775: 0.04408044038827707\n",
      "Train Loss at iteration 2776: 0.04408019076188876\n",
      "Train Loss at iteration 2777: 0.044079941181569604\n",
      "Train Loss at iteration 2778: 0.04407969164730169\n",
      "Train Loss at iteration 2779: 0.0440794421590671\n",
      "Train Loss at iteration 2780: 0.044079192716848015\n",
      "Train Loss at iteration 2781: 0.04407894332062659\n",
      "Train Loss at iteration 2782: 0.044078693970385045\n",
      "Train Loss at iteration 2783: 0.04407844466610557\n",
      "Train Loss at iteration 2784: 0.04407819540777045\n",
      "Train Loss at iteration 2785: 0.04407794619536197\n",
      "Train Loss at iteration 2786: 0.04407769702886243\n",
      "Train Loss at iteration 2787: 0.044077447908254186\n",
      "Train Loss at iteration 2788: 0.04407719883351959\n",
      "Train Loss at iteration 2789: 0.04407694980464106\n",
      "Train Loss at iteration 2790: 0.04407670082160099\n",
      "Train Loss at iteration 2791: 0.04407645188438186\n",
      "Train Loss at iteration 2792: 0.044076202992966135\n",
      "Train Loss at iteration 2793: 0.04407595414733631\n",
      "Train Loss at iteration 2794: 0.04407570534747493\n",
      "Train Loss at iteration 2795: 0.04407545659336456\n",
      "Train Loss at iteration 2796: 0.04407520788498776\n",
      "Train Loss at iteration 2797: 0.044074959222327174\n",
      "Train Loss at iteration 2798: 0.04407471060536544\n",
      "Train Loss at iteration 2799: 0.04407446203408522\n",
      "Train Loss at iteration 2800: 0.0440742135084692\n",
      "Train Loss at iteration 2801: 0.04407396502850011\n",
      "Train Loss at iteration 2802: 0.04407371659416067\n",
      "Train Loss at iteration 2803: 0.04407346820543371\n",
      "Train Loss at iteration 2804: 0.04407321986230198\n",
      "Train Loss at iteration 2805: 0.04407297156474833\n",
      "Train Loss at iteration 2806: 0.04407272331275559\n",
      "Train Loss at iteration 2807: 0.04407247510630668\n",
      "Train Loss at iteration 2808: 0.044072226945384464\n",
      "Train Loss at iteration 2809: 0.04407197882997189\n",
      "Train Loss at iteration 2810: 0.04407173076005192\n",
      "Train Loss at iteration 2811: 0.04407148273560751\n",
      "Train Loss at iteration 2812: 0.0440712347566217\n",
      "Train Loss at iteration 2813: 0.044070986823077495\n",
      "Train Loss at iteration 2814: 0.044070738934957984\n",
      "Train Loss at iteration 2815: 0.04407049109224622\n",
      "Train Loss at iteration 2816: 0.044070243294925336\n",
      "Train Loss at iteration 2817: 0.04406999554297846\n",
      "Train Loss at iteration 2818: 0.04406974783638876\n",
      "Train Loss at iteration 2819: 0.04406950017513941\n",
      "Train Loss at iteration 2820: 0.04406925255921361\n",
      "Train Loss at iteration 2821: 0.044069004988594625\n",
      "Train Loss at iteration 2822: 0.04406875746326569\n",
      "Train Loss at iteration 2823: 0.04406850998321008\n",
      "Train Loss at iteration 2824: 0.044068262548411154\n",
      "Train Loss at iteration 2825: 0.04406801515885221\n",
      "Train Loss at iteration 2826: 0.0440677678145166\n",
      "Train Loss at iteration 2827: 0.04406752051538774\n",
      "Train Loss at iteration 2828: 0.04406727326144901\n",
      "Train Loss at iteration 2829: 0.044067026052683846\n",
      "Train Loss at iteration 2830: 0.044066778889075724\n",
      "Train Loss at iteration 2831: 0.04406653177060811\n",
      "Train Loss at iteration 2832: 0.04406628469726449\n",
      "Train Loss at iteration 2833: 0.044066037669028446\n",
      "Train Loss at iteration 2834: 0.044065790685883476\n",
      "Train Loss at iteration 2835: 0.04406554374781318\n",
      "Train Loss at iteration 2836: 0.04406529685480117\n",
      "Train Loss at iteration 2837: 0.04406505000683105\n",
      "Train Loss at iteration 2838: 0.044064803203886484\n",
      "Train Loss at iteration 2839: 0.04406455644595113\n",
      "Train Loss at iteration 2840: 0.044064309733008705\n",
      "Train Loss at iteration 2841: 0.0440640630650429\n",
      "Train Loss at iteration 2842: 0.044063816442037475\n",
      "Train Loss at iteration 2843: 0.044063569863976186\n",
      "Train Loss at iteration 2844: 0.04406332333084285\n",
      "Train Loss at iteration 2845: 0.044063076842621236\n",
      "Train Loss at iteration 2846: 0.044062830399295205\n",
      "Train Loss at iteration 2847: 0.044062584000848615\n",
      "Train Loss at iteration 2848: 0.044062337647265325\n",
      "Train Loss at iteration 2849: 0.04406209133852927\n",
      "Train Loss at iteration 2850: 0.044061845074624385\n",
      "Train Loss at iteration 2851: 0.04406159885553456\n",
      "Train Loss at iteration 2852: 0.044061352681243816\n",
      "Train Loss at iteration 2853: 0.044061106551736146\n",
      "Train Loss at iteration 2854: 0.04406086046699554\n",
      "Train Loss at iteration 2855: 0.04406061442700608\n",
      "Train Loss at iteration 2856: 0.0440603684317518\n",
      "Train Loss at iteration 2857: 0.04406012248121678\n",
      "Train Loss at iteration 2858: 0.044059876575385154\n",
      "Train Loss at iteration 2859: 0.044059630714241\n",
      "Train Loss at iteration 2860: 0.04405938489776853\n",
      "Train Loss at iteration 2861: 0.04405913912595188\n",
      "Train Loss at iteration 2862: 0.044058893398775265\n",
      "Train Loss at iteration 2863: 0.04405864771622289\n",
      "Train Loss at iteration 2864: 0.044058402078279\n",
      "Train Loss at iteration 2865: 0.04405815648492785\n",
      "Train Loss at iteration 2866: 0.04405791093615372\n",
      "Train Loss at iteration 2867: 0.04405766543194092\n",
      "Train Loss at iteration 2868: 0.0440574199722738\n",
      "Train Loss at iteration 2869: 0.04405717455713665\n",
      "Train Loss at iteration 2870: 0.04405692918651389\n",
      "Train Loss at iteration 2871: 0.04405668386038989\n",
      "Train Loss at iteration 2872: 0.044056438578749066\n",
      "Train Loss at iteration 2873: 0.04405619334157585\n",
      "Train Loss at iteration 2874: 0.0440559481488547\n",
      "Train Loss at iteration 2875: 0.04405570300057008\n",
      "Train Loss at iteration 2876: 0.044055457896706486\n",
      "Train Loss at iteration 2877: 0.04405521283724846\n",
      "Train Loss at iteration 2878: 0.04405496782218052\n",
      "Train Loss at iteration 2879: 0.044054722851487225\n",
      "Train Loss at iteration 2880: 0.04405447792515316\n",
      "Train Loss at iteration 2881: 0.044054233043162916\n",
      "Train Loss at iteration 2882: 0.04405398820550115\n",
      "Train Loss at iteration 2883: 0.04405374341215245\n",
      "Train Loss at iteration 2884: 0.044053498663101515\n",
      "Train Loss at iteration 2885: 0.04405325395833301\n",
      "Train Loss at iteration 2886: 0.04405300929783167\n",
      "Train Loss at iteration 2887: 0.04405276468158218\n",
      "Train Loss at iteration 2888: 0.04405252010956931\n",
      "Train Loss at iteration 2889: 0.04405227558177782\n",
      "Train Loss at iteration 2890: 0.04405203109819248\n",
      "Train Loss at iteration 2891: 0.044051786658798116\n",
      "Train Loss at iteration 2892: 0.04405154226357953\n",
      "Train Loss at iteration 2893: 0.04405129791252159\n",
      "Train Loss at iteration 2894: 0.04405105360560915\n",
      "Train Loss at iteration 2895: 0.04405080934282709\n",
      "Train Loss at iteration 2896: 0.04405056512416033\n",
      "Train Loss at iteration 2897: 0.04405032094959379\n",
      "Train Loss at iteration 2898: 0.04405007681911241\n",
      "Train Loss at iteration 2899: 0.04404983273270115\n",
      "Train Loss at iteration 2900: 0.044049588690345004\n",
      "Train Loss at iteration 2901: 0.044049344692028966\n",
      "Train Loss at iteration 2902: 0.04404910073773806\n",
      "Train Loss at iteration 2903: 0.04404885682745734\n",
      "Train Loss at iteration 2904: 0.04404861296117184\n",
      "Train Loss at iteration 2905: 0.04404836913886668\n",
      "Train Loss at iteration 2906: 0.04404812536052693\n",
      "Train Loss at iteration 2907: 0.04404788162613772\n",
      "Train Loss at iteration 2908: 0.044047637935684184\n",
      "Train Loss at iteration 2909: 0.0440473942891515\n",
      "Train Loss at iteration 2910: 0.044047150686524796\n",
      "Train Loss at iteration 2911: 0.04404690712778932\n",
      "Train Loss at iteration 2912: 0.04404666361293026\n",
      "Train Loss at iteration 2913: 0.04404642014193286\n",
      "Train Loss at iteration 2914: 0.04404617671478236\n",
      "Train Loss at iteration 2915: 0.04404593333146403\n",
      "Train Loss at iteration 2916: 0.04404568999196317\n",
      "Train Loss at iteration 2917: 0.0440454466962651\n",
      "Train Loss at iteration 2918: 0.044045203444355134\n",
      "Train Loss at iteration 2919: 0.0440449602362186\n",
      "Train Loss at iteration 2920: 0.04404471707184088\n",
      "Train Loss at iteration 2921: 0.04404447395120736\n",
      "Train Loss at iteration 2922: 0.04404423087430343\n",
      "Train Loss at iteration 2923: 0.04404398784111452\n",
      "Train Loss at iteration 2924: 0.04404374485162605\n",
      "Train Loss at iteration 2925: 0.04404350190582349\n",
      "Train Loss at iteration 2926: 0.04404325900369232\n",
      "Train Loss at iteration 2927: 0.04404301614521801\n",
      "Train Loss at iteration 2928: 0.04404277333038608\n",
      "Train Loss at iteration 2929: 0.044042530559182065\n",
      "Train Loss at iteration 2930: 0.044042287831591496\n",
      "Train Loss at iteration 2931: 0.04404204514759996\n",
      "Train Loss at iteration 2932: 0.044041802507193\n",
      "Train Loss at iteration 2933: 0.04404155991035628\n",
      "Train Loss at iteration 2934: 0.044041317357075324\n",
      "Train Loss at iteration 2935: 0.04404107484733586\n",
      "Train Loss at iteration 2936: 0.04404083238112347\n",
      "Train Loss at iteration 2937: 0.04404058995842386\n",
      "Train Loss at iteration 2938: 0.04404034757922271\n",
      "Train Loss at iteration 2939: 0.04404010524350572\n",
      "Train Loss at iteration 2940: 0.04403986295125863\n",
      "Train Loss at iteration 2941: 0.04403962070246715\n",
      "Train Loss at iteration 2942: 0.04403937849711706\n",
      "Train Loss at iteration 2943: 0.04403913633519413\n",
      "Train Loss at iteration 2944: 0.04403889421668414\n",
      "Train Loss at iteration 2945: 0.044038652141572925\n",
      "Train Loss at iteration 2946: 0.04403841010984627\n",
      "Train Loss at iteration 2947: 0.04403816812149005\n",
      "Train Loss at iteration 2948: 0.04403792617649011\n",
      "Train Loss at iteration 2949: 0.04403768427483234\n",
      "Train Loss at iteration 2950: 0.04403744241650262\n",
      "Train Loss at iteration 2951: 0.04403720060148687\n",
      "Train Loss at iteration 2952: 0.04403695882977102\n",
      "Train Loss at iteration 2953: 0.04403671710134101\n",
      "Train Loss at iteration 2954: 0.04403647541618278\n",
      "Train Loss at iteration 2955: 0.044036233774282334\n",
      "Train Loss at iteration 2956: 0.044035992175625656\n",
      "Train Loss at iteration 2957: 0.044035750620198766\n",
      "Train Loss at iteration 2958: 0.044035509107987675\n",
      "Train Loss at iteration 2959: 0.04403526763897845\n",
      "Train Loss at iteration 2960: 0.04403502621315713\n",
      "Train Loss at iteration 2961: 0.04403478483050979\n",
      "Train Loss at iteration 2962: 0.044034543491022565\n",
      "Train Loss at iteration 2963: 0.044034302194681504\n",
      "Train Loss at iteration 2964: 0.04403406094147276\n",
      "Train Loss at iteration 2965: 0.044033819731382484\n",
      "Train Loss at iteration 2966: 0.04403357856439682\n",
      "Train Loss at iteration 2967: 0.044033337440501946\n",
      "Train Loss at iteration 2968: 0.04403309635968406\n",
      "Train Loss at iteration 2969: 0.04403285532192936\n",
      "Train Loss at iteration 2970: 0.04403261432722408\n",
      "Train Loss at iteration 2971: 0.04403237337555443\n",
      "Train Loss at iteration 2972: 0.04403213246690669\n",
      "Train Loss at iteration 2973: 0.04403189160126712\n",
      "Train Loss at iteration 2974: 0.04403165077862201\n",
      "Train Loss at iteration 2975: 0.04403140999895765\n",
      "Train Loss at iteration 2976: 0.04403116926226039\n",
      "Train Loss at iteration 2977: 0.044030928568516516\n",
      "Train Loss at iteration 2978: 0.044030687917712405\n",
      "Train Loss at iteration 2979: 0.04403044730983442\n",
      "Train Loss at iteration 2980: 0.04403020674486891\n",
      "Train Loss at iteration 2981: 0.04402996622280233\n",
      "Train Loss at iteration 2982: 0.044029725743621045\n",
      "Train Loss at iteration 2983: 0.04402948530731148\n",
      "Train Loss at iteration 2984: 0.04402924491386011\n",
      "Train Loss at iteration 2985: 0.04402900456325335\n",
      "Train Loss at iteration 2986: 0.0440287642554777\n",
      "Train Loss at iteration 2987: 0.04402852399051963\n",
      "Train Loss at iteration 2988: 0.044028283768365664\n",
      "Train Loss at iteration 2989: 0.04402804358900229\n",
      "Train Loss at iteration 2990: 0.044027803452416074\n",
      "Train Loss at iteration 2991: 0.04402756335859353\n",
      "Train Loss at iteration 2992: 0.04402732330752123\n",
      "Train Loss at iteration 2993: 0.044027083299185775\n",
      "Train Loss at iteration 2994: 0.04402684333357374\n",
      "Train Loss at iteration 2995: 0.04402660341067173\n",
      "Train Loss at iteration 2996: 0.044026363530466364\n",
      "Train Loss at iteration 2997: 0.044026123692944286\n",
      "Train Loss at iteration 2998: 0.044025883898092144\n",
      "Train Loss at iteration 2999: 0.04402564414589661\n",
      "Train Loss at iteration 3000: 0.044025404436344354\n",
      "Train Loss at iteration 3001: 0.04402516476942208\n",
      "Train Loss at iteration 3002: 0.0440249251451165\n",
      "Train Loss at iteration 3003: 0.04402468556341434\n",
      "Train Loss at iteration 3004: 0.04402444602430233\n",
      "Train Loss at iteration 3005: 0.044024206527767236\n",
      "Train Loss at iteration 3006: 0.044023967073795814\n",
      "Train Loss at iteration 3007: 0.044023727662374855\n",
      "Train Loss at iteration 3008: 0.044023488293491154\n",
      "Train Loss at iteration 3009: 0.044023248967131536\n",
      "Train Loss at iteration 3010: 0.04402300968328279\n",
      "Train Loss at iteration 3011: 0.044022770441931786\n",
      "Train Loss at iteration 3012: 0.04402253124306537\n",
      "Train Loss at iteration 3013: 0.044022292086670425\n",
      "Train Loss at iteration 3014: 0.04402205297273381\n",
      "Train Loss at iteration 3015: 0.04402181390124243\n",
      "Train Loss at iteration 3016: 0.044021574872183195\n",
      "Train Loss at iteration 3017: 0.04402133588554302\n",
      "Train Loss at iteration 3018: 0.04402109694130888\n",
      "Train Loss at iteration 3019: 0.04402085803946769\n",
      "Train Loss at iteration 3020: 0.04402061918000643\n",
      "Train Loss at iteration 3021: 0.04402038036291209\n",
      "Train Loss at iteration 3022: 0.04402014158817163\n",
      "Train Loss at iteration 3023: 0.0440199028557721\n",
      "Train Loss at iteration 3024: 0.04401966416570049\n",
      "Train Loss at iteration 3025: 0.04401942551794385\n",
      "Train Loss at iteration 3026: 0.04401918691248923\n",
      "Train Loss at iteration 3027: 0.044018948349323676\n",
      "Train Loss at iteration 3028: 0.04401870982843429\n",
      "Train Loss at iteration 3029: 0.04401847134980815\n",
      "Train Loss at iteration 3030: 0.04401823291343235\n",
      "Train Loss at iteration 3031: 0.04401799451929401\n",
      "Train Loss at iteration 3032: 0.04401775616738028\n",
      "Train Loss at iteration 3033: 0.044017517857678275\n",
      "Train Loss at iteration 3034: 0.04401727959017517\n",
      "Train Loss at iteration 3035: 0.044017041364858116\n",
      "Train Loss at iteration 3036: 0.044016803181714324\n",
      "Train Loss at iteration 3037: 0.04401656504073098\n",
      "Train Loss at iteration 3038: 0.044016326941895285\n",
      "Train Loss at iteration 3039: 0.044016088885194454\n",
      "Train Loss at iteration 3040: 0.04401585087061576\n",
      "Train Loss at iteration 3041: 0.04401561289814642\n",
      "Train Loss at iteration 3042: 0.044015374967773704\n",
      "Train Loss at iteration 3043: 0.044015137079484894\n",
      "Train Loss at iteration 3044: 0.04401489923326728\n",
      "Train Loss at iteration 3045: 0.044014661429108155\n",
      "Train Loss at iteration 3046: 0.04401442366699483\n",
      "Train Loss at iteration 3047: 0.04401418594691464\n",
      "Train Loss at iteration 3048: 0.04401394826885492\n",
      "Train Loss at iteration 3049: 0.04401371063280304\n",
      "Train Loss at iteration 3050: 0.04401347303874634\n",
      "Train Loss at iteration 3051: 0.044013235486672214\n",
      "Train Loss at iteration 3052: 0.04401299797656806\n",
      "Train Loss at iteration 3053: 0.04401276050842127\n",
      "Train Loss at iteration 3054: 0.04401252308221926\n",
      "Train Loss at iteration 3055: 0.04401228569794946\n",
      "Train Loss at iteration 3056: 0.04401204835559932\n",
      "Train Loss at iteration 3057: 0.04401181105515628\n",
      "Train Loss at iteration 3058: 0.044011573796607834\n",
      "Train Loss at iteration 3059: 0.044011336579941426\n",
      "Train Loss at iteration 3060: 0.04401109940514459\n",
      "Train Loss at iteration 3061: 0.044010862272204784\n",
      "Train Loss at iteration 3062: 0.044010625181109556\n",
      "Train Loss at iteration 3063: 0.04401038813184644\n",
      "Train Loss at iteration 3064: 0.04401015112440295\n",
      "Train Loss at iteration 3065: 0.04400991415876665\n",
      "Train Loss at iteration 3066: 0.04400967723492512\n",
      "Train Loss at iteration 3067: 0.04400944035286593\n",
      "Train Loss at iteration 3068: 0.04400920351257667\n",
      "Train Loss at iteration 3069: 0.04400896671404494\n",
      "Train Loss at iteration 3070: 0.04400872995725835\n",
      "Train Loss at iteration 3071: 0.04400849324220454\n",
      "Train Loss at iteration 3072: 0.04400825656887115\n",
      "Train Loss at iteration 3073: 0.04400801993724582\n",
      "Train Loss at iteration 3074: 0.044007783347316205\n",
      "Train Loss at iteration 3075: 0.04400754679907001\n",
      "Train Loss at iteration 3076: 0.0440073102924949\n",
      "Train Loss at iteration 3077: 0.044007073827578576\n",
      "Train Loss at iteration 3078: 0.044006837404308746\n",
      "Train Loss at iteration 3079: 0.04400660102267314\n",
      "Train Loss at iteration 3080: 0.0440063646826595\n",
      "Train Loss at iteration 3081: 0.04400612838425556\n",
      "Train Loss at iteration 3082: 0.04400589212744907\n",
      "Train Loss at iteration 3083: 0.044005655912227826\n",
      "Train Loss at iteration 3084: 0.04400541973857959\n",
      "Train Loss at iteration 3085: 0.04400518360649216\n",
      "Train Loss at iteration 3086: 0.04400494751595334\n",
      "Train Loss at iteration 3087: 0.044004711466950945\n",
      "Train Loss at iteration 3088: 0.044004475459472794\n",
      "Train Loss at iteration 3089: 0.04400423949350676\n",
      "Train Loss at iteration 3090: 0.044004003569040655\n",
      "Train Loss at iteration 3091: 0.04400376768606236\n",
      "Train Loss at iteration 3092: 0.044003531844559754\n",
      "Train Loss at iteration 3093: 0.04400329604452072\n",
      "Train Loss at iteration 3094: 0.044003060285933135\n",
      "Train Loss at iteration 3095: 0.04400282456878493\n",
      "Train Loss at iteration 3096: 0.04400258889306403\n",
      "Train Loss at iteration 3097: 0.04400235325875834\n",
      "Train Loss at iteration 3098: 0.04400211766585581\n",
      "Train Loss at iteration 3099: 0.0440018821143444\n",
      "Train Loss at iteration 3100: 0.04400164660421209\n",
      "Train Loss at iteration 3101: 0.04400141113544682\n",
      "Train Loss at iteration 3102: 0.044001175708036605\n",
      "Train Loss at iteration 3103: 0.04400094032196944\n",
      "Train Loss at iteration 3104: 0.04400070497723332\n",
      "Train Loss at iteration 3105: 0.04400046967381627\n",
      "Train Loss at iteration 3106: 0.044000234411706335\n",
      "Train Loss at iteration 3107: 0.043999999190891544\n",
      "Train Loss at iteration 3108: 0.043999764011359944\n",
      "Train Loss at iteration 3109: 0.04399952887309962\n",
      "Train Loss at iteration 3110: 0.04399929377609865\n",
      "Train Loss at iteration 3111: 0.0439990587203451\n",
      "Train Loss at iteration 3112: 0.04399882370582706\n",
      "Train Loss at iteration 3113: 0.04399858873253266\n",
      "Train Loss at iteration 3114: 0.04399835380045001\n",
      "Train Loss at iteration 3115: 0.043998118909567244\n",
      "Train Loss at iteration 3116: 0.0439978840598725\n",
      "Train Loss at iteration 3117: 0.043997649251353924\n",
      "Train Loss at iteration 3118: 0.04399741448399968\n",
      "Train Loss at iteration 3119: 0.04399717975779796\n",
      "Train Loss at iteration 3120: 0.043996945072736916\n",
      "Train Loss at iteration 3121: 0.04399671042880475\n",
      "Train Loss at iteration 3122: 0.04399647582598968\n",
      "Train Loss at iteration 3123: 0.043996241264279916\n",
      "Train Loss at iteration 3124: 0.04399600674366368\n",
      "Train Loss at iteration 3125: 0.04399577226412921\n",
      "Train Loss at iteration 3126: 0.043995537825664746\n",
      "Train Loss at iteration 3127: 0.04399530342825855\n",
      "Train Loss at iteration 3128: 0.043995069071898894\n",
      "Train Loss at iteration 3129: 0.043994834756574064\n",
      "Train Loss at iteration 3130: 0.04399460048227233\n",
      "Train Loss at iteration 3131: 0.04399436624898199\n",
      "Train Loss at iteration 3132: 0.043994132056691365\n",
      "Train Loss at iteration 3133: 0.04399389790538876\n",
      "Train Loss at iteration 3134: 0.04399366379506254\n",
      "Train Loss at iteration 3135: 0.04399342972570101\n",
      "Train Loss at iteration 3136: 0.04399319569729252\n",
      "Train Loss at iteration 3137: 0.043992961709825446\n",
      "Train Loss at iteration 3138: 0.04399272776328816\n",
      "Train Loss at iteration 3139: 0.043992493857669034\n",
      "Train Loss at iteration 3140: 0.04399225999295646\n",
      "Train Loss at iteration 3141: 0.04399202616913885\n",
      "Train Loss at iteration 3142: 0.04399179238620459\n",
      "Train Loss at iteration 3143: 0.04399155864414213\n",
      "Train Loss at iteration 3144: 0.04399132494293988\n",
      "Train Loss at iteration 3145: 0.04399109128258629\n",
      "Train Loss at iteration 3146: 0.04399085766306983\n",
      "Train Loss at iteration 3147: 0.043990624084378926\n",
      "Train Loss at iteration 3148: 0.04399039054650207\n",
      "Train Loss at iteration 3149: 0.04399015704942774\n",
      "Train Loss at iteration 3150: 0.04398992359314442\n",
      "Train Loss at iteration 3151: 0.04398969017764062\n",
      "Train Loss at iteration 3152: 0.04398945680290487\n",
      "Train Loss at iteration 3153: 0.04398922346892565\n",
      "Train Loss at iteration 3154: 0.04398899017569151\n",
      "Train Loss at iteration 3155: 0.043988756923191\n",
      "Train Loss at iteration 3156: 0.04398852371141265\n",
      "Train Loss at iteration 3157: 0.04398829054034503\n",
      "Train Loss at iteration 3158: 0.04398805740997671\n",
      "Train Loss at iteration 3159: 0.04398782432029628\n",
      "Train Loss at iteration 3160: 0.04398759127129231\n",
      "Train Loss at iteration 3161: 0.04398735826295339\n",
      "Train Loss at iteration 3162: 0.043987125295268166\n",
      "Train Loss at iteration 3163: 0.04398689236822523\n",
      "Train Loss at iteration 3164: 0.043986659481813205\n",
      "Train Loss at iteration 3165: 0.043986426636020745\n",
      "Train Loss at iteration 3166: 0.043986193830836476\n",
      "Train Loss at iteration 3167: 0.04398596106624906\n",
      "Train Loss at iteration 3168: 0.04398572834224718\n",
      "Train Loss at iteration 3169: 0.04398549565881949\n",
      "Train Loss at iteration 3170: 0.043985263015954686\n",
      "Train Loss at iteration 3171: 0.043985030413641456\n",
      "Train Loss at iteration 3172: 0.04398479785186849\n",
      "Train Loss at iteration 3173: 0.043984565330624545\n",
      "Train Loss at iteration 3174: 0.04398433284989829\n",
      "Train Loss at iteration 3175: 0.04398410040967848\n",
      "Train Loss at iteration 3176: 0.04398386800995385\n",
      "Train Loss at iteration 3177: 0.04398363565071316\n",
      "Train Loss at iteration 3178: 0.04398340333194517\n",
      "Train Loss at iteration 3179: 0.04398317105363861\n",
      "Train Loss at iteration 3180: 0.04398293881578231\n",
      "Train Loss at iteration 3181: 0.04398270661836503\n",
      "Train Loss at iteration 3182: 0.043982474461375556\n",
      "Train Loss at iteration 3183: 0.04398224234480271\n",
      "Train Loss at iteration 3184: 0.04398201026863531\n",
      "Train Loss at iteration 3185: 0.04398177823286216\n",
      "Train Loss at iteration 3186: 0.043981546237472115\n",
      "Train Loss at iteration 3187: 0.04398131428245399\n",
      "Train Loss at iteration 3188: 0.043981082367796645\n",
      "Train Loss at iteration 3189: 0.04398085049348895\n",
      "Train Loss at iteration 3190: 0.04398061865951976\n",
      "Train Loss at iteration 3191: 0.04398038686587795\n",
      "Train Loss at iteration 3192: 0.043980155112552426\n",
      "Train Loss at iteration 3193: 0.043979923399532064\n",
      "Train Loss at iteration 3194: 0.04397969172680578\n",
      "Train Loss at iteration 3195: 0.04397946009436246\n",
      "Train Loss at iteration 3196: 0.043979228502191055\n",
      "Train Loss at iteration 3197: 0.0439789969502805\n",
      "Train Loss at iteration 3198: 0.04397876543861971\n",
      "Train Loss at iteration 3199: 0.04397853396719765\n",
      "Train Loss at iteration 3200: 0.04397830253600325\n",
      "Train Loss at iteration 3201: 0.04397807114502552\n",
      "Train Loss at iteration 3202: 0.04397783979425339\n",
      "Train Loss at iteration 3203: 0.04397760848367587\n",
      "Train Loss at iteration 3204: 0.04397737721328194\n",
      "Train Loss at iteration 3205: 0.043977145983060614\n",
      "Train Loss at iteration 3206: 0.043976914793000886\n",
      "Train Loss at iteration 3207: 0.04397668364309177\n",
      "Train Loss at iteration 3208: 0.04397645253332231\n",
      "Train Loss at iteration 3209: 0.04397622146368154\n",
      "Train Loss at iteration 3210: 0.04397599043415849\n",
      "Train Loss at iteration 3211: 0.04397575944474221\n",
      "Train Loss at iteration 3212: 0.04397552849542178\n",
      "Train Loss at iteration 3213: 0.04397529758618625\n",
      "Train Loss at iteration 3214: 0.04397506671702472\n",
      "Train Loss at iteration 3215: 0.04397483588792625\n",
      "Train Loss at iteration 3216: 0.04397460509887994\n",
      "Train Loss at iteration 3217: 0.0439743743498749\n",
      "Train Loss at iteration 3218: 0.04397414364090026\n",
      "Train Loss at iteration 3219: 0.0439739129719451\n",
      "Train Loss at iteration 3220: 0.04397368234299857\n",
      "Train Loss at iteration 3221: 0.043973451754049805\n",
      "Train Loss at iteration 3222: 0.04397322120508795\n",
      "Train Loss at iteration 3223: 0.04397299069610216\n",
      "Train Loss at iteration 3224: 0.043972760227081596\n",
      "Train Loss at iteration 3225: 0.04397252979801543\n",
      "Train Loss at iteration 3226: 0.04397229940889283\n",
      "Train Loss at iteration 3227: 0.04397206905970299\n",
      "Train Loss at iteration 3228: 0.04397183875043511\n",
      "Train Loss at iteration 3229: 0.04397160848107835\n",
      "Train Loss at iteration 3230: 0.043971378251622006\n",
      "Train Loss at iteration 3231: 0.04397114806205523\n",
      "Train Loss at iteration 3232: 0.04397091791236726\n",
      "Train Loss at iteration 3233: 0.04397068780254734\n",
      "Train Loss at iteration 3234: 0.04397045773258473\n",
      "Train Loss at iteration 3235: 0.04397022770246865\n",
      "Train Loss at iteration 3236: 0.043969997712188374\n",
      "Train Loss at iteration 3237: 0.043969767761733176\n",
      "Train Loss at iteration 3238: 0.04396953785109232\n",
      "Train Loss at iteration 3239: 0.043969307980255085\n",
      "Train Loss at iteration 3240: 0.04396907814921079\n",
      "Train Loss at iteration 3241: 0.04396884835794871\n",
      "Train Loss at iteration 3242: 0.04396861860645817\n",
      "Train Loss at iteration 3243: 0.04396838889472847\n",
      "Train Loss at iteration 3244: 0.043968159222748944\n",
      "Train Loss at iteration 3245: 0.04396792959050891\n",
      "Train Loss at iteration 3246: 0.04396769999799773\n",
      "Train Loss at iteration 3247: 0.043967470445204736\n",
      "Train Loss at iteration 3248: 0.0439672409321193\n",
      "Train Loss at iteration 3249: 0.04396701145873074\n",
      "Train Loss at iteration 3250: 0.04396678202502849\n",
      "Train Loss at iteration 3251: 0.04396655263100188\n",
      "Train Loss at iteration 3252: 0.04396632327664033\n",
      "Train Loss at iteration 3253: 0.04396609396193322\n",
      "Train Loss at iteration 3254: 0.04396586468686994\n",
      "Train Loss at iteration 3255: 0.04396563545143991\n",
      "Train Loss at iteration 3256: 0.043965406255632555\n",
      "Train Loss at iteration 3257: 0.04396517709943729\n",
      "Train Loss at iteration 3258: 0.04396494798284357\n",
      "Train Loss at iteration 3259: 0.0439647189058408\n",
      "Train Loss at iteration 3260: 0.043964489868418465\n",
      "Train Loss at iteration 3261: 0.043964260870566\n",
      "Train Loss at iteration 3262: 0.043964031912272855\n",
      "Train Loss at iteration 3263: 0.043963802993528533\n",
      "Train Loss at iteration 3264: 0.043963574114322494\n",
      "Train Loss at iteration 3265: 0.043963345274644225\n",
      "Train Loss at iteration 3266: 0.04396311647448323\n",
      "Train Loss at iteration 3267: 0.043962887713829016\n",
      "Train Loss at iteration 3268: 0.04396265899267107\n",
      "Train Loss at iteration 3269: 0.04396243031099892\n",
      "Train Loss at iteration 3270: 0.043962201668802096\n",
      "Train Loss at iteration 3271: 0.04396197306607012\n",
      "Train Loss at iteration 3272: 0.043961744502792534\n",
      "Train Loss at iteration 3273: 0.04396151597895888\n",
      "Train Loss at iteration 3274: 0.04396128749455873\n",
      "Train Loss at iteration 3275: 0.04396105904958162\n",
      "Train Loss at iteration 3276: 0.043960830644017145\n",
      "Train Loss at iteration 3277: 0.043960602277854866\n",
      "Train Loss at iteration 3278: 0.04396037395108436\n",
      "Train Loss at iteration 3279: 0.04396014566369524\n",
      "Train Loss at iteration 3280: 0.04395991741567709\n",
      "Train Loss at iteration 3281: 0.04395968920701951\n",
      "Train Loss at iteration 3282: 0.04395946103771213\n",
      "Train Loss at iteration 3283: 0.04395923290774455\n",
      "Train Loss at iteration 3284: 0.043959004817106406\n",
      "Train Loss at iteration 3285: 0.04395877676578735\n",
      "Train Loss at iteration 3286: 0.043958548753777\n",
      "Train Loss at iteration 3287: 0.04395832078106502\n",
      "Train Loss at iteration 3288: 0.04395809284764105\n",
      "Train Loss at iteration 3289: 0.043957864953494775\n",
      "Train Loss at iteration 3290: 0.04395763709861585\n",
      "Train Loss at iteration 3291: 0.04395740928299395\n",
      "Train Loss at iteration 3292: 0.04395718150661879\n",
      "Train Loss at iteration 3293: 0.04395695376948003\n",
      "Train Loss at iteration 3294: 0.04395672607156737\n",
      "Train Loss at iteration 3295: 0.043956498412870554\n",
      "Train Loss at iteration 3296: 0.04395627079337925\n",
      "Train Loss at iteration 3297: 0.0439560432130832\n",
      "Train Loss at iteration 3298: 0.04395581567197212\n",
      "Train Loss at iteration 3299: 0.043955588170035774\n",
      "Train Loss at iteration 3300: 0.04395536070726388\n",
      "Train Loss at iteration 3301: 0.043955133283646176\n",
      "Train Loss at iteration 3302: 0.04395490589917245\n",
      "Train Loss at iteration 3303: 0.043954678553832435\n",
      "Train Loss at iteration 3304: 0.04395445124761593\n",
      "Train Loss at iteration 3305: 0.043954223980512686\n",
      "Train Loss at iteration 3306: 0.0439539967525125\n",
      "Train Loss at iteration 3307: 0.04395376956360516\n",
      "Train Loss at iteration 3308: 0.04395354241378046\n",
      "Train Loss at iteration 3309: 0.04395331530302821\n",
      "Train Loss at iteration 3310: 0.04395308823133823\n",
      "Train Loss at iteration 3311: 0.043952861198700316\n",
      "Train Loss at iteration 3312: 0.0439526342051043\n",
      "Train Loss at iteration 3313: 0.04395240725054003\n",
      "Train Loss at iteration 3314: 0.04395218033499734\n",
      "Train Loss at iteration 3315: 0.04395195345846606\n",
      "Train Loss at iteration 3316: 0.04395172662093605\n",
      "Train Loss at iteration 3317: 0.043951499822397176\n",
      "Train Loss at iteration 3318: 0.0439512730628393\n",
      "Train Loss at iteration 3319: 0.043951046342252297\n",
      "Train Loss at iteration 3320: 0.04395081966062604\n",
      "Train Loss at iteration 3321: 0.04395059301795043\n",
      "Train Loss at iteration 3322: 0.04395036641421534\n",
      "Train Loss at iteration 3323: 0.04395013984941068\n",
      "Train Loss at iteration 3324: 0.04394991332352635\n",
      "Train Loss at iteration 3325: 0.043949686836552294\n",
      "Train Loss at iteration 3326: 0.043949460388478384\n",
      "Train Loss at iteration 3327: 0.04394923397929458\n",
      "Train Loss at iteration 3328: 0.0439490076089908\n",
      "Train Loss at iteration 3329: 0.04394878127755699\n",
      "Train Loss at iteration 3330: 0.043948554984983096\n",
      "Train Loss at iteration 3331: 0.043948328731259076\n",
      "Train Loss at iteration 3332: 0.043948102516374876\n",
      "Train Loss at iteration 3333: 0.04394787634032047\n",
      "Train Loss at iteration 3334: 0.04394765020308583\n",
      "Train Loss at iteration 3335: 0.04394742410466094\n",
      "Train Loss at iteration 3336: 0.043947198045035794\n",
      "Train Loss at iteration 3337: 0.04394697202420035\n",
      "Train Loss at iteration 3338: 0.043946746042144644\n",
      "Train Loss at iteration 3339: 0.04394652009885866\n",
      "Train Loss at iteration 3340: 0.04394629419433242\n",
      "Train Loss at iteration 3341: 0.04394606832855593\n",
      "Train Loss at iteration 3342: 0.04394584250151923\n",
      "Train Loss at iteration 3343: 0.04394561671321235\n",
      "Train Loss at iteration 3344: 0.043945390963625305\n",
      "Train Loss at iteration 3345: 0.04394516525274817\n",
      "Train Loss at iteration 3346: 0.04394493958057097\n",
      "Train Loss at iteration 3347: 0.043944713947083776\n",
      "Train Loss at iteration 3348: 0.04394448835227666\n",
      "Train Loss at iteration 3349: 0.043944262796139666\n",
      "Train Loss at iteration 3350: 0.04394403727866289\n",
      "Train Loss at iteration 3351: 0.04394381179983642\n",
      "Train Loss at iteration 3352: 0.043943586359650316\n",
      "Train Loss at iteration 3353: 0.043943360958094695\n",
      "Train Loss at iteration 3354: 0.04394313559515966\n",
      "Train Loss at iteration 3355: 0.043942910270835314\n",
      "Train Loss at iteration 3356: 0.043942684985111745\n",
      "Train Loss at iteration 3357: 0.04394245973797911\n",
      "Train Loss at iteration 3358: 0.04394223452942753\n",
      "Train Loss at iteration 3359: 0.04394200935944713\n",
      "Train Loss at iteration 3360: 0.043941784228028045\n",
      "Train Loss at iteration 3361: 0.04394155913516041\n",
      "Train Loss at iteration 3362: 0.04394133408083441\n",
      "Train Loss at iteration 3363: 0.043941109065040174\n",
      "Train Loss at iteration 3364: 0.04394088408776786\n",
      "Train Loss at iteration 3365: 0.04394065914900767\n",
      "Train Loss at iteration 3366: 0.043940434248749764\n",
      "Train Loss at iteration 3367: 0.043940209386984314\n",
      "Train Loss at iteration 3368: 0.04393998456370151\n",
      "Train Loss at iteration 3369: 0.04393975977889156\n",
      "Train Loss at iteration 3370: 0.043939535032544656\n",
      "Train Loss at iteration 3371: 0.04393931032465101\n",
      "Train Loss at iteration 3372: 0.04393908565520082\n",
      "Train Loss at iteration 3373: 0.04393886102418433\n",
      "Train Loss at iteration 3374: 0.04393863643159174\n",
      "Train Loss at iteration 3375: 0.043938411877413285\n",
      "Train Loss at iteration 3376: 0.04393818736163922\n",
      "Train Loss at iteration 3377: 0.04393796288425977\n",
      "Train Loss at iteration 3378: 0.04393773844526518\n",
      "Train Loss at iteration 3379: 0.04393751404464573\n",
      "Train Loss at iteration 3380: 0.04393728968239167\n",
      "Train Loss at iteration 3381: 0.043937065358493246\n",
      "Train Loss at iteration 3382: 0.043936841072940755\n",
      "Train Loss at iteration 3383: 0.04393661682572447\n",
      "Train Loss at iteration 3384: 0.04393639261683467\n",
      "Train Loss at iteration 3385: 0.04393616844626166\n",
      "Train Loss at iteration 3386: 0.04393594431399571\n",
      "Train Loss at iteration 3387: 0.043935720220027154\n",
      "Train Loss at iteration 3388: 0.04393549616434629\n",
      "Train Loss at iteration 3389: 0.04393527214694342\n",
      "Train Loss at iteration 3390: 0.043935048167808875\n",
      "Train Loss at iteration 3391: 0.04393482422693298\n",
      "Train Loss at iteration 3392: 0.043934600324306065\n",
      "Train Loss at iteration 3393: 0.043934376459918476\n",
      "Train Loss at iteration 3394: 0.043934152633760555\n",
      "Train Loss at iteration 3395: 0.043933928845822624\n",
      "Train Loss at iteration 3396: 0.043933705096095066\n",
      "Train Loss at iteration 3397: 0.043933481384568256\n",
      "Train Loss at iteration 3398: 0.04393325771123253\n",
      "Train Loss at iteration 3399: 0.04393303407607827\n",
      "Train Loss at iteration 3400: 0.04393281047909586\n",
      "Train Loss at iteration 3401: 0.043932586920275685\n",
      "Train Loss at iteration 3402: 0.04393236339960815\n",
      "Train Loss at iteration 3403: 0.04393213991708361\n",
      "Train Loss at iteration 3404: 0.0439319164726925\n",
      "Train Loss at iteration 3405: 0.04393169306642521\n",
      "Train Loss at iteration 3406: 0.04393146969827217\n",
      "Train Loss at iteration 3407: 0.04393124636822378\n",
      "Train Loss at iteration 3408: 0.04393102307627049\n",
      "Train Loss at iteration 3409: 0.043930799822402694\n",
      "Train Loss at iteration 3410: 0.043930576606610865\n",
      "Train Loss at iteration 3411: 0.04393035342888542\n",
      "Train Loss at iteration 3412: 0.0439301302892168\n",
      "Train Loss at iteration 3413: 0.04392990718759549\n",
      "Train Loss at iteration 3414: 0.043929684124011915\n",
      "Train Loss at iteration 3415: 0.043929461098456565\n",
      "Train Loss at iteration 3416: 0.043929238110919894\n",
      "Train Loss at iteration 3417: 0.043929015161392394\n",
      "Train Loss at iteration 3418: 0.04392879224986452\n",
      "Train Loss at iteration 3419: 0.04392856937632677\n",
      "Train Loss at iteration 3420: 0.04392834654076964\n",
      "Train Loss at iteration 3421: 0.04392812374318363\n",
      "Train Loss at iteration 3422: 0.04392790098355923\n",
      "Train Loss at iteration 3423: 0.043927678261886954\n",
      "Train Loss at iteration 3424: 0.04392745557815733\n",
      "Train Loss at iteration 3425: 0.04392723293236086\n",
      "Train Loss at iteration 3426: 0.04392701032448808\n",
      "Train Loss at iteration 3427: 0.0439267877545295\n",
      "Train Loss at iteration 3428: 0.04392656522247569\n",
      "Train Loss at iteration 3429: 0.04392634272831718\n",
      "Train Loss at iteration 3430: 0.043926120272044504\n",
      "Train Loss at iteration 3431: 0.04392589785364823\n",
      "Train Loss at iteration 3432: 0.04392567547311889\n",
      "Train Loss at iteration 3433: 0.04392545313044707\n",
      "Train Loss at iteration 3434: 0.043925230825623336\n",
      "Train Loss at iteration 3435: 0.043925008558638276\n",
      "Train Loss at iteration 3436: 0.04392478632948243\n",
      "Train Loss at iteration 3437: 0.04392456413814641\n",
      "Train Loss at iteration 3438: 0.04392434198462082\n",
      "Train Loss at iteration 3439: 0.04392411986889622\n",
      "Train Loss at iteration 3440: 0.04392389779096326\n",
      "Train Loss at iteration 3441: 0.04392367575081249\n",
      "Train Loss at iteration 3442: 0.04392345374843456\n",
      "Train Loss at iteration 3443: 0.04392323178382008\n",
      "Train Loss at iteration 3444: 0.04392300985695967\n",
      "Train Loss at iteration 3445: 0.043922787967843956\n",
      "Train Loss at iteration 3446: 0.04392256611646357\n",
      "Train Loss at iteration 3447: 0.043922344302809155\n",
      "Train Loss at iteration 3448: 0.04392212252687137\n",
      "Train Loss at iteration 3449: 0.043921900788640826\n",
      "Train Loss at iteration 3450: 0.043921679088108206\n",
      "Train Loss at iteration 3451: 0.04392145742526416\n",
      "Train Loss at iteration 3452: 0.043921235800099365\n",
      "Train Loss at iteration 3453: 0.043921014212604484\n",
      "Train Loss at iteration 3454: 0.043920792662770186\n",
      "Train Loss at iteration 3455: 0.04392057115058716\n",
      "Train Loss at iteration 3456: 0.043920349676046085\n",
      "Train Loss at iteration 3457: 0.04392012823913765\n",
      "Train Loss at iteration 3458: 0.043919906839852585\n",
      "Train Loss at iteration 3459: 0.04391968547818154\n",
      "Train Loss at iteration 3460: 0.04391946415411527\n",
      "Train Loss at iteration 3461: 0.04391924286764443\n",
      "Train Loss at iteration 3462: 0.04391902161875979\n",
      "Train Loss at iteration 3463: 0.043918800407452045\n",
      "Train Loss at iteration 3464: 0.04391857923371195\n",
      "Train Loss at iteration 3465: 0.0439183580975302\n",
      "Train Loss at iteration 3466: 0.043918136998897545\n",
      "Train Loss at iteration 3467: 0.043917915937804745\n",
      "Train Loss at iteration 3468: 0.043917694914242536\n",
      "Train Loss at iteration 3469: 0.043917473928201674\n",
      "Train Loss at iteration 3470: 0.04391725297967291\n",
      "Train Loss at iteration 3471: 0.04391703206864701\n",
      "Train Loss at iteration 3472: 0.043916811195114765\n",
      "Train Loss at iteration 3473: 0.04391659035906692\n",
      "Train Loss at iteration 3474: 0.04391636956049426\n",
      "Train Loss at iteration 3475: 0.043916148799387565\n",
      "Train Loss at iteration 3476: 0.04391592807573764\n",
      "Train Loss at iteration 3477: 0.04391570738953526\n",
      "Train Loss at iteration 3478: 0.04391548674077124\n",
      "Train Loss at iteration 3479: 0.043915266129436366\n",
      "Train Loss at iteration 3480: 0.04391504555552147\n",
      "Train Loss at iteration 3481: 0.04391482501901734\n",
      "Train Loss at iteration 3482: 0.04391460451991481\n",
      "Train Loss at iteration 3483: 0.043914384058204706\n",
      "Train Loss at iteration 3484: 0.043914163633877866\n",
      "Train Loss at iteration 3485: 0.04391394324692508\n",
      "Train Loss at iteration 3486: 0.04391372289733725\n",
      "Train Loss at iteration 3487: 0.04391350258510516\n",
      "Train Loss at iteration 3488: 0.043913282310219695\n",
      "Train Loss at iteration 3489: 0.043913062072671705\n",
      "Train Loss at iteration 3490: 0.04391284187245203\n",
      "Train Loss at iteration 3491: 0.04391262170955157\n",
      "Train Loss at iteration 3492: 0.04391240158396115\n",
      "Train Loss at iteration 3493: 0.04391218149567166\n",
      "Train Loss at iteration 3494: 0.04391196144467399\n",
      "Train Loss at iteration 3495: 0.04391174143095901\n",
      "Train Loss at iteration 3496: 0.04391152145451761\n",
      "Train Loss at iteration 3497: 0.04391130151534071\n",
      "Train Loss at iteration 3498: 0.04391108161341916\n",
      "Train Loss at iteration 3499: 0.043910861748743896\n",
      "Train Loss at iteration 3500: 0.043910641921305796\n",
      "Train Loss at iteration 3501: 0.04391042213109581\n",
      "Train Loss at iteration 3502: 0.04391020237810482\n",
      "Train Loss at iteration 3503: 0.04390998266232379\n",
      "Train Loss at iteration 3504: 0.043909762983743605\n",
      "Train Loss at iteration 3505: 0.04390954334235522\n",
      "Train Loss at iteration 3506: 0.04390932373814957\n",
      "Train Loss at iteration 3507: 0.04390910417111758\n",
      "Train Loss at iteration 3508: 0.043908884641250215\n",
      "Train Loss at iteration 3509: 0.043908665148538424\n",
      "Train Loss at iteration 3510: 0.04390844569297314\n",
      "Train Loss at iteration 3511: 0.04390822627454535\n",
      "Train Loss at iteration 3512: 0.04390800689324601\n",
      "Train Loss at iteration 3513: 0.043907787549066095\n",
      "Train Loss at iteration 3514: 0.04390756824199656\n",
      "Train Loss at iteration 3515: 0.0439073489720284\n",
      "Train Loss at iteration 3516: 0.04390712973915261\n",
      "Train Loss at iteration 3517: 0.043906910543360156\n",
      "Train Loss at iteration 3518: 0.04390669138464204\n",
      "Train Loss at iteration 3519: 0.04390647226298928\n",
      "Train Loss at iteration 3520: 0.04390625317839285\n",
      "Train Loss at iteration 3521: 0.04390603413084376\n",
      "Train Loss at iteration 3522: 0.04390581512033304\n",
      "Train Loss at iteration 3523: 0.04390559614685171\n",
      "Train Loss at iteration 3524: 0.04390537721039076\n",
      "Train Loss at iteration 3525: 0.04390515831094125\n",
      "Train Loss at iteration 3526: 0.043904939448494204\n",
      "Train Loss at iteration 3527: 0.043904720623040634\n",
      "Train Loss at iteration 3528: 0.04390450183457161\n",
      "Train Loss at iteration 3529: 0.04390428308307817\n",
      "Train Loss at iteration 3530: 0.04390406436855135\n",
      "Train Loss at iteration 3531: 0.04390384569098221\n",
      "Train Loss at iteration 3532: 0.04390362705036182\n",
      "Train Loss at iteration 3533: 0.04390340844668124\n",
      "Train Loss at iteration 3534: 0.04390318987993152\n",
      "Train Loss at iteration 3535: 0.043902971350103764\n",
      "Train Loss at iteration 3536: 0.04390275285718902\n",
      "Train Loss at iteration 3537: 0.043902534401178375\n",
      "Train Loss at iteration 3538: 0.04390231598206293\n",
      "Train Loss at iteration 3539: 0.04390209759983378\n",
      "Train Loss at iteration 3540: 0.043901879254481975\n",
      "Train Loss at iteration 3541: 0.04390166094599868\n",
      "Train Loss at iteration 3542: 0.043901442674374964\n",
      "Train Loss at iteration 3543: 0.043901224439601934\n",
      "Train Loss at iteration 3544: 0.04390100624167071\n",
      "Train Loss at iteration 3545: 0.04390078808057241\n",
      "Train Loss at iteration 3546: 0.04390056995629816\n",
      "Train Loss at iteration 3547: 0.04390035186883908\n",
      "Train Loss at iteration 3548: 0.04390013381818632\n",
      "Train Loss at iteration 3549: 0.04389991580433099\n",
      "Train Loss at iteration 3550: 0.04389969782726425\n",
      "Train Loss at iteration 3551: 0.04389947988697722\n",
      "Train Loss at iteration 3552: 0.04389926198346109\n",
      "Train Loss at iteration 3553: 0.043899044116706996\n",
      "Train Loss at iteration 3554: 0.04389882628670609\n",
      "Train Loss at iteration 3555: 0.043898608493449516\n",
      "Train Loss at iteration 3556: 0.04389839073692847\n",
      "Train Loss at iteration 3557: 0.04389817301713415\n",
      "Train Loss at iteration 3558: 0.04389795533405767\n",
      "Train Loss at iteration 3559: 0.04389773768769025\n",
      "Train Loss at iteration 3560: 0.043897520078023076\n",
      "Train Loss at iteration 3561: 0.043897302505047316\n",
      "Train Loss at iteration 3562: 0.04389708496875419\n",
      "Train Loss at iteration 3563: 0.043896867469134876\n",
      "Train Loss at iteration 3564: 0.043896650006180586\n",
      "Train Loss at iteration 3565: 0.043896432579882516\n",
      "Train Loss at iteration 3566: 0.043896215190231895\n",
      "Train Loss at iteration 3567: 0.04389599783721993\n",
      "Train Loss at iteration 3568: 0.04389578052083785\n",
      "Train Loss at iteration 3569: 0.04389556324107685\n",
      "Train Loss at iteration 3570: 0.043895345997928215\n",
      "Train Loss at iteration 3571: 0.043895128791383124\n",
      "Train Loss at iteration 3572: 0.04389491162143284\n",
      "Train Loss at iteration 3573: 0.04389469448806861\n",
      "Train Loss at iteration 3574: 0.043894477391281654\n",
      "Train Loss at iteration 3575: 0.043894260331063265\n",
      "Train Loss at iteration 3576: 0.04389404330740466\n",
      "Train Loss at iteration 3577: 0.04389382632029712\n",
      "Train Loss at iteration 3578: 0.043893609369731884\n",
      "Train Loss at iteration 3579: 0.04389339245570025\n",
      "Train Loss at iteration 3580: 0.043893175578193486\n",
      "Train Loss at iteration 3581: 0.04389295873720285\n",
      "Train Loss at iteration 3582: 0.04389274193271964\n",
      "Train Loss at iteration 3583: 0.04389252516473512\n",
      "Train Loss at iteration 3584: 0.043892308433240615\n",
      "Train Loss at iteration 3585: 0.04389209173822739\n",
      "Train Loss at iteration 3586: 0.043891875079686754\n",
      "Train Loss at iteration 3587: 0.04389165845761\n",
      "Train Loss at iteration 3588: 0.04389144187198844\n",
      "Train Loss at iteration 3589: 0.04389122532281341\n",
      "Train Loss at iteration 3590: 0.04389100881007618\n",
      "Train Loss at iteration 3591: 0.043890792333768094\n",
      "Train Loss at iteration 3592: 0.04389057589388048\n",
      "Train Loss at iteration 3593: 0.043890359490404644\n",
      "Train Loss at iteration 3594: 0.043890143123331934\n",
      "Train Loss at iteration 3595: 0.043889926792653695\n",
      "Train Loss at iteration 3596: 0.043889710498361253\n",
      "Train Loss at iteration 3597: 0.04388949424044595\n",
      "Train Loss at iteration 3598: 0.04388927801889914\n",
      "Train Loss at iteration 3599: 0.043889061833712176\n",
      "Train Loss at iteration 3600: 0.04388884568487643\n",
      "Train Loss at iteration 3601: 0.04388862957238323\n",
      "Train Loss at iteration 3602: 0.043888413496223966\n",
      "Train Loss at iteration 3603: 0.04388819745639\n",
      "Train Loss at iteration 3604: 0.04388798145287271\n",
      "Train Loss at iteration 3605: 0.04388776548566347\n",
      "Train Loss at iteration 3606: 0.04388754955475366\n",
      "Train Loss at iteration 3607: 0.04388733366013469\n",
      "Train Loss at iteration 3608: 0.0438871178017979\n",
      "Train Loss at iteration 3609: 0.04388690197973472\n",
      "Train Loss at iteration 3610: 0.04388668619393654\n",
      "Train Loss at iteration 3611: 0.04388647044439478\n",
      "Train Loss at iteration 3612: 0.04388625473110083\n",
      "Train Loss at iteration 3613: 0.04388603905404609\n",
      "Train Loss at iteration 3614: 0.04388582341322199\n",
      "Train Loss at iteration 3615: 0.04388560780861995\n",
      "Train Loss at iteration 3616: 0.04388539224023138\n",
      "Train Loss at iteration 3617: 0.04388517670804773\n",
      "Train Loss at iteration 3618: 0.043884961212060404\n",
      "Train Loss at iteration 3619: 0.043884745752260854\n",
      "Train Loss at iteration 3620: 0.04388453032864052\n",
      "Train Loss at iteration 3621: 0.043884314941190834\n",
      "Train Loss at iteration 3622: 0.04388409958990326\n",
      "Train Loss at iteration 3623: 0.04388388427476924\n",
      "Train Loss at iteration 3624: 0.04388366899578021\n",
      "Train Loss at iteration 3625: 0.043883453752927674\n",
      "Train Loss at iteration 3626: 0.04388323854620306\n",
      "Train Loss at iteration 3627: 0.043883023375597845\n",
      "Train Loss at iteration 3628: 0.0438828082411035\n",
      "Train Loss at iteration 3629: 0.04388259314271151\n",
      "Train Loss at iteration 3630: 0.043882378080413335\n",
      "Train Loss at iteration 3631: 0.04388216305420048\n",
      "Train Loss at iteration 3632: 0.04388194806406441\n",
      "Train Loss at iteration 3633: 0.04388173310999665\n",
      "Train Loss at iteration 3634: 0.043881518191988655\n",
      "Train Loss at iteration 3635: 0.043881303310031955\n",
      "Train Loss at iteration 3636: 0.04388108846411805\n",
      "Train Loss at iteration 3637: 0.04388087365423842\n",
      "Train Loss at iteration 3638: 0.043880658880384615\n",
      "Train Loss at iteration 3639: 0.043880444142548124\n",
      "Train Loss at iteration 3640: 0.04388022944072048\n",
      "Train Loss at iteration 3641: 0.04388001477489319\n",
      "Train Loss at iteration 3642: 0.04387980014505781\n",
      "Train Loss at iteration 3643: 0.04387958555120585\n",
      "Train Loss at iteration 3644: 0.04387937099332884\n",
      "Train Loss at iteration 3645: 0.04387915647141832\n",
      "Train Loss at iteration 3646: 0.04387894198546585\n",
      "Train Loss at iteration 3647: 0.04387872753546297\n",
      "Train Loss at iteration 3648: 0.04387851312140123\n",
      "Train Loss at iteration 3649: 0.04387829874327217\n",
      "Train Loss at iteration 3650: 0.04387808440106737\n",
      "Train Loss at iteration 3651: 0.04387787009477838\n",
      "Train Loss at iteration 3652: 0.04387765582439678\n",
      "Train Loss at iteration 3653: 0.04387744158991412\n",
      "Train Loss at iteration 3654: 0.04387722739132199\n",
      "Train Loss at iteration 3655: 0.04387701322861196\n",
      "Train Loss at iteration 3656: 0.04387679910177562\n",
      "Train Loss at iteration 3657: 0.043876585010804534\n",
      "Train Loss at iteration 3658: 0.043876370955690316\n",
      "Train Loss at iteration 3659: 0.04387615693642455\n",
      "Train Loss at iteration 3660: 0.043875942952998834\n",
      "Train Loss at iteration 3661: 0.04387572900540477\n",
      "Train Loss at iteration 3662: 0.04387551509363396\n",
      "Train Loss at iteration 3663: 0.04387530121767801\n",
      "Train Loss at iteration 3664: 0.04387508737752854\n",
      "Train Loss at iteration 3665: 0.04387487357317718\n",
      "Train Loss at iteration 3666: 0.04387465980461552\n",
      "Train Loss at iteration 3667: 0.043874446071835195\n",
      "Train Loss at iteration 3668: 0.04387423237482784\n",
      "Train Loss at iteration 3669: 0.043874018713585095\n",
      "Train Loss at iteration 3670: 0.043873805088098565\n",
      "Train Loss at iteration 3671: 0.04387359149835991\n",
      "Train Loss at iteration 3672: 0.043873377944360775\n",
      "Train Loss at iteration 3673: 0.0438731644260928\n",
      "Train Loss at iteration 3674: 0.043872950943547644\n",
      "Train Loss at iteration 3675: 0.04387273749671694\n",
      "Train Loss at iteration 3676: 0.043872524085592376\n",
      "Train Loss at iteration 3677: 0.043872310710165584\n",
      "Train Loss at iteration 3678: 0.04387209737042826\n",
      "Train Loss at iteration 3679: 0.04387188406637205\n",
      "Train Loss at iteration 3680: 0.04387167079798864\n",
      "Train Loss at iteration 3681: 0.04387145756526971\n",
      "Train Loss at iteration 3682: 0.04387124436820691\n",
      "Train Loss at iteration 3683: 0.04387103120679196\n",
      "Train Loss at iteration 3684: 0.04387081808101655\n",
      "Train Loss at iteration 3685: 0.043870604990872336\n",
      "Train Loss at iteration 3686: 0.04387039193635105\n",
      "Train Loss at iteration 3687: 0.043870178917444366\n",
      "Train Loss at iteration 3688: 0.043869965934144\n",
      "Train Loss at iteration 3689: 0.043869752986441655\n",
      "Train Loss at iteration 3690: 0.04386954007432905\n",
      "Train Loss at iteration 3691: 0.04386932719779788\n",
      "Train Loss at iteration 3692: 0.04386911435683988\n",
      "Train Loss at iteration 3693: 0.043868901551446765\n",
      "Train Loss at iteration 3694: 0.043868688781610266\n",
      "Train Loss at iteration 3695: 0.043868476047322094\n",
      "Train Loss at iteration 3696: 0.04386826334857401\n",
      "Train Loss at iteration 3697: 0.04386805068535772\n",
      "Train Loss at iteration 3698: 0.04386783805766499\n",
      "Train Loss at iteration 3699: 0.04386762546548755\n",
      "Train Loss at iteration 3700: 0.04386741290881715\n",
      "Train Loss at iteration 3701: 0.043867200387645536\n",
      "Train Loss at iteration 3702: 0.04386698790196447\n",
      "Train Loss at iteration 3703: 0.043866775451765705\n",
      "Train Loss at iteration 3704: 0.043866563037041\n",
      "Train Loss at iteration 3705: 0.04386635065778213\n",
      "Train Loss at iteration 3706: 0.04386613831398087\n",
      "Train Loss at iteration 3707: 0.04386592600562896\n",
      "Train Loss at iteration 3708: 0.043865713732718206\n",
      "Train Loss at iteration 3709: 0.04386550149524039\n",
      "Train Loss at iteration 3710: 0.04386528929318728\n",
      "Train Loss at iteration 3711: 0.043865077126550675\n",
      "Train Loss at iteration 3712: 0.04386486499532234\n",
      "Train Loss at iteration 3713: 0.043864652899494094\n",
      "Train Loss at iteration 3714: 0.043864440839057744\n",
      "Train Loss at iteration 3715: 0.04386422881400508\n",
      "Train Loss at iteration 3716: 0.04386401682432788\n",
      "Train Loss at iteration 3717: 0.043863804870017994\n",
      "Train Loss at iteration 3718: 0.04386359295106722\n",
      "Train Loss at iteration 3719: 0.043863381067467365\n",
      "Train Loss at iteration 3720: 0.043863169219210255\n",
      "Train Loss at iteration 3721: 0.043862957406287706\n",
      "Train Loss at iteration 3722: 0.04386274562869156\n",
      "Train Loss at iteration 3723: 0.04386253388641363\n",
      "Train Loss at iteration 3724: 0.04386232217944577\n",
      "Train Loss at iteration 3725: 0.04386211050777981\n",
      "Train Loss at iteration 3726: 0.043861898871407574\n",
      "Train Loss at iteration 3727: 0.04386168727032093\n",
      "Train Loss at iteration 3728: 0.0438614757045117\n",
      "Train Loss at iteration 3729: 0.04386126417397176\n",
      "Train Loss at iteration 3730: 0.04386105267869293\n",
      "Train Loss at iteration 3731: 0.043860841218667124\n",
      "Train Loss at iteration 3732: 0.04386062979388616\n",
      "Train Loss at iteration 3733: 0.04386041840434191\n",
      "Train Loss at iteration 3734: 0.04386020705002626\n",
      "Train Loss at iteration 3735: 0.04385999573093108\n",
      "Train Loss at iteration 3736: 0.04385978444704822\n",
      "Train Loss at iteration 3737: 0.043859573198369584\n",
      "Train Loss at iteration 3738: 0.04385936198488705\n",
      "Train Loss at iteration 3739: 0.04385915080659251\n",
      "Train Loss at iteration 3740: 0.04385893966347783\n",
      "Train Loss at iteration 3741: 0.043858728555534916\n",
      "Train Loss at iteration 3742: 0.04385851748275569\n",
      "Train Loss at iteration 3743: 0.04385830644513203\n",
      "Train Loss at iteration 3744: 0.043858095442655835\n",
      "Train Loss at iteration 3745: 0.043857884475319006\n",
      "Train Loss at iteration 3746: 0.04385767354311347\n",
      "Train Loss at iteration 3747: 0.04385746264603115\n",
      "Train Loss at iteration 3748: 0.04385725178406395\n",
      "Train Loss at iteration 3749: 0.04385704095720379\n",
      "Train Loss at iteration 3750: 0.043856830165442603\n",
      "Train Loss at iteration 3751: 0.04385661940877229\n",
      "Train Loss at iteration 3752: 0.043856408687184815\n",
      "Train Loss at iteration 3753: 0.043856198000672114\n",
      "Train Loss at iteration 3754: 0.043855987349226105\n",
      "Train Loss at iteration 3755: 0.04385577673283875\n",
      "Train Loss at iteration 3756: 0.04385556615150195\n",
      "Train Loss at iteration 3757: 0.043855355605207695\n",
      "Train Loss at iteration 3758: 0.04385514509394795\n",
      "Train Loss at iteration 3759: 0.043854934617714635\n",
      "Train Loss at iteration 3760: 0.04385472417649973\n",
      "Train Loss at iteration 3761: 0.04385451377029517\n",
      "Train Loss at iteration 3762: 0.04385430339909295\n",
      "Train Loss at iteration 3763: 0.04385409306288504\n",
      "Train Loss at iteration 3764: 0.04385388276166338\n",
      "Train Loss at iteration 3765: 0.043853672495419986\n",
      "Train Loss at iteration 3766: 0.04385346226414681\n",
      "Train Loss at iteration 3767: 0.04385325206783585\n",
      "Train Loss at iteration 3768: 0.04385304190647907\n",
      "Train Loss at iteration 3769: 0.04385283178006849\n",
      "Train Loss at iteration 3770: 0.04385262168859606\n",
      "Train Loss at iteration 3771: 0.04385241163205384\n",
      "Train Loss at iteration 3772: 0.04385220161043378\n",
      "Train Loss at iteration 3773: 0.043851991623727896\n",
      "Train Loss at iteration 3774: 0.043851781671928186\n",
      "Train Loss at iteration 3775: 0.04385157175502667\n",
      "Train Loss at iteration 3776: 0.04385136187301536\n",
      "Train Loss at iteration 3777: 0.04385115202588628\n",
      "Train Loss at iteration 3778: 0.04385094221363142\n",
      "Train Loss at iteration 3779: 0.04385073243624283\n",
      "Train Loss at iteration 3780: 0.04385052269371254\n",
      "Train Loss at iteration 3781: 0.04385031298603257\n",
      "Train Loss at iteration 3782: 0.04385010331319493\n",
      "Train Loss at iteration 3783: 0.043849893675191695\n",
      "Train Loss at iteration 3784: 0.043849684072014866\n",
      "Train Loss at iteration 3785: 0.04384947450365653\n",
      "Train Loss at iteration 3786: 0.0438492649701087\n",
      "Train Loss at iteration 3787: 0.04384905547136343\n",
      "Train Loss at iteration 3788: 0.0438488460074128\n",
      "Train Loss at iteration 3789: 0.04384863657824882\n",
      "Train Loss at iteration 3790: 0.04384842718386359\n",
      "Train Loss at iteration 3791: 0.04384821782424915\n",
      "Train Loss at iteration 3792: 0.04384800849939758\n",
      "Train Loss at iteration 3793: 0.04384779920930093\n",
      "Train Loss at iteration 3794: 0.04384758995395129\n",
      "Train Loss at iteration 3795: 0.04384738073334073\n",
      "Train Loss at iteration 3796: 0.043847171547461324\n",
      "Train Loss at iteration 3797: 0.04384696239630517\n",
      "Train Loss at iteration 3798: 0.043846753279864335\n",
      "Train Loss at iteration 3799: 0.043846544198130916\n",
      "Train Loss at iteration 3800: 0.04384633515109699\n",
      "Train Loss at iteration 3801: 0.04384612613875469\n",
      "Train Loss at iteration 3802: 0.04384591716109608\n",
      "Train Loss at iteration 3803: 0.043845708218113275\n",
      "Train Loss at iteration 3804: 0.043845499309798384\n",
      "Train Loss at iteration 3805: 0.043845290436143505\n",
      "Train Loss at iteration 3806: 0.04384508159714076\n",
      "Train Loss at iteration 3807: 0.04384487279278225\n",
      "Train Loss at iteration 3808: 0.04384466402306011\n",
      "Train Loss at iteration 3809: 0.04384445528796644\n",
      "Train Loss at iteration 3810: 0.04384424658749338\n",
      "Train Loss at iteration 3811: 0.043844037921633064\n",
      "Train Loss at iteration 3812: 0.04384382929037762\n",
      "Train Loss at iteration 3813: 0.043843620693719165\n",
      "Train Loss at iteration 3814: 0.043843412131649856\n",
      "Train Loss at iteration 3815: 0.043843203604161815\n",
      "Train Loss at iteration 3816: 0.043842995111247195\n",
      "Train Loss at iteration 3817: 0.043842786652898154\n",
      "Train Loss at iteration 3818: 0.04384257822910683\n",
      "Train Loss at iteration 3819: 0.043842369839865385\n",
      "Train Loss at iteration 3820: 0.04384216148516595\n",
      "Train Loss at iteration 3821: 0.04384195316500071\n",
      "Train Loss at iteration 3822: 0.04384174487936184\n",
      "Train Loss at iteration 3823: 0.04384153662824148\n",
      "Train Loss at iteration 3824: 0.04384132841163181\n",
      "Train Loss at iteration 3825: 0.04384112022952499\n",
      "Train Loss at iteration 3826: 0.0438409120819132\n",
      "Train Loss at iteration 3827: 0.04384070396878864\n",
      "Train Loss at iteration 3828: 0.043840495890143466\n",
      "Train Loss at iteration 3829: 0.04384028784596989\n",
      "Train Loss at iteration 3830: 0.04384007983626007\n",
      "Train Loss at iteration 3831: 0.043839871861006206\n",
      "Train Loss at iteration 3832: 0.04383966392020052\n",
      "Train Loss at iteration 3833: 0.043839456013835175\n",
      "Train Loss at iteration 3834: 0.04383924814190239\n",
      "Train Loss at iteration 3835: 0.04383904030439436\n",
      "Train Loss at iteration 3836: 0.0438388325013033\n",
      "Train Loss at iteration 3837: 0.043838624732621415\n",
      "Train Loss at iteration 3838: 0.043838416998340915\n",
      "Train Loss at iteration 3839: 0.043838209298454035\n",
      "Train Loss at iteration 3840: 0.04383800163295297\n",
      "Train Loss at iteration 3841: 0.04383779400182996\n",
      "Train Loss at iteration 3842: 0.04383758640507723\n",
      "Train Loss at iteration 3843: 0.043837378842687\n",
      "Train Loss at iteration 3844: 0.04383717131465151\n",
      "Train Loss at iteration 3845: 0.04383696382096299\n",
      "Train Loss at iteration 3846: 0.043836756361613674\n",
      "Train Loss at iteration 3847: 0.043836548936595825\n",
      "Train Loss at iteration 3848: 0.04383634154590165\n",
      "Train Loss at iteration 3849: 0.04383613418952344\n",
      "Train Loss at iteration 3850: 0.04383592686745343\n",
      "Train Loss at iteration 3851: 0.04383571957968385\n",
      "Train Loss at iteration 3852: 0.043835512326206985\n",
      "Train Loss at iteration 3853: 0.0438353051070151\n",
      "Train Loss at iteration 3854: 0.04383509792210044\n",
      "Train Loss at iteration 3855: 0.04383489077145526\n",
      "Train Loss at iteration 3856: 0.04383468365507187\n",
      "Train Loss at iteration 3857: 0.04383447657294251\n",
      "Train Loss at iteration 3858: 0.04383426952505945\n",
      "Train Loss at iteration 3859: 0.043834062511414994\n",
      "Train Loss at iteration 3860: 0.043833855532001406\n",
      "Train Loss at iteration 3861: 0.043833648586811\n",
      "Train Loss at iteration 3862: 0.04383344167583601\n",
      "Train Loss at iteration 3863: 0.043833234799068775\n",
      "Train Loss at iteration 3864: 0.04383302795650157\n",
      "Train Loss at iteration 3865: 0.04383282114812668\n",
      "Train Loss at iteration 3866: 0.04383261437393642\n",
      "Train Loss at iteration 3867: 0.04383240763392311\n",
      "Train Loss at iteration 3868: 0.043832200928079025\n",
      "Train Loss at iteration 3869: 0.043831994256396486\n",
      "Train Loss at iteration 3870: 0.04383178761886781\n",
      "Train Loss at iteration 3871: 0.043831581015485316\n",
      "Train Loss at iteration 3872: 0.04383137444624129\n",
      "Train Loss at iteration 3873: 0.0438311679111281\n",
      "Train Loss at iteration 3874: 0.04383096141013803\n",
      "Train Loss at iteration 3875: 0.043830754943263425\n",
      "Train Loss at iteration 3876: 0.04383054851049663\n",
      "Train Loss at iteration 3877: 0.04383034211182995\n",
      "Train Loss at iteration 3878: 0.04383013574725575\n",
      "Train Loss at iteration 3879: 0.04382992941676633\n",
      "Train Loss at iteration 3880: 0.04382972312035408\n",
      "Train Loss at iteration 3881: 0.043829516858011314\n",
      "Train Loss at iteration 3882: 0.04382931062973038\n",
      "Train Loss at iteration 3883: 0.04382910443550364\n",
      "Train Loss at iteration 3884: 0.043828898275323463\n",
      "Train Loss at iteration 3885: 0.04382869214918217\n",
      "Train Loss at iteration 3886: 0.04382848605707213\n",
      "Train Loss at iteration 3887: 0.04382827999898574\n",
      "Train Loss at iteration 3888: 0.04382807397491533\n",
      "Train Loss at iteration 3889: 0.04382786798485329\n",
      "Train Loss at iteration 3890: 0.04382766202879198\n",
      "Train Loss at iteration 3891: 0.04382745610672378\n",
      "Train Loss at iteration 3892: 0.04382725021864107\n",
      "Train Loss at iteration 3893: 0.04382704436453623\n",
      "Train Loss at iteration 3894: 0.04382683854440162\n",
      "Train Loss at iteration 3895: 0.04382663275822968\n",
      "Train Loss at iteration 3896: 0.043826427006012754\n",
      "Train Loss at iteration 3897: 0.04382622128774326\n",
      "Train Loss at iteration 3898: 0.043826015603413565\n",
      "Train Loss at iteration 3899: 0.043825809953016104\n",
      "Train Loss at iteration 3900: 0.04382560433654327\n",
      "Train Loss at iteration 3901: 0.04382539875398745\n",
      "Train Loss at iteration 3902: 0.043825193205341055\n",
      "Train Loss at iteration 3903: 0.04382498769059652\n",
      "Train Loss at iteration 3904: 0.04382478220974623\n",
      "Train Loss at iteration 3905: 0.04382457676278261\n",
      "Train Loss at iteration 3906: 0.04382437134969809\n",
      "Train Loss at iteration 3907: 0.04382416597048508\n",
      "Train Loss at iteration 3908: 0.043823960625136014\n",
      "Train Loss at iteration 3909: 0.04382375531364331\n",
      "Train Loss at iteration 3910: 0.043823550035999415\n",
      "Train Loss at iteration 3911: 0.04382334479219674\n",
      "Train Loss at iteration 3912: 0.04382313958222774\n",
      "Train Loss at iteration 3913: 0.043822934406084854\n",
      "Train Loss at iteration 3914: 0.043822729263760525\n",
      "Train Loss at iteration 3915: 0.04382252415524718\n",
      "Train Loss at iteration 3916: 0.0438223190805373\n",
      "Train Loss at iteration 3917: 0.043822114039623314\n",
      "Train Loss at iteration 3918: 0.04382190903249767\n",
      "Train Loss at iteration 3919: 0.04382170405915286\n",
      "Train Loss at iteration 3920: 0.043821499119581296\n",
      "Train Loss at iteration 3921: 0.04382129421377549\n",
      "Train Loss at iteration 3922: 0.04382108934172786\n",
      "Train Loss at iteration 3923: 0.04382088450343091\n",
      "Train Loss at iteration 3924: 0.0438206796988771\n",
      "Train Loss at iteration 3925: 0.043820474928058906\n",
      "Train Loss at iteration 3926: 0.04382027019096881\n",
      "Train Loss at iteration 3927: 0.043820065487599286\n",
      "Train Loss at iteration 3928: 0.04381986081794281\n",
      "Train Loss at iteration 3929: 0.04381965618199189\n",
      "Train Loss at iteration 3930: 0.04381945157973899\n",
      "Train Loss at iteration 3931: 0.04381924701117663\n",
      "Train Loss at iteration 3932: 0.043819042476297275\n",
      "Train Loss at iteration 3933: 0.04381883797509344\n",
      "Train Loss at iteration 3934: 0.043818633507557624\n",
      "Train Loss at iteration 3935: 0.04381842907368232\n",
      "Train Loss at iteration 3936: 0.043818224673460054\n",
      "Train Loss at iteration 3937: 0.04381802030688332\n",
      "Train Loss at iteration 3938: 0.04381781597394462\n",
      "Train Loss at iteration 3939: 0.0438176116746365\n",
      "Train Loss at iteration 3940: 0.04381740740895144\n",
      "Train Loss at iteration 3941: 0.043817203176881994\n",
      "Train Loss at iteration 3942: 0.04381699897842068\n",
      "Train Loss at iteration 3943: 0.04381679481356\n",
      "Train Loss at iteration 3944: 0.043816590682292496\n",
      "Train Loss at iteration 3945: 0.043816386584610706\n",
      "Train Loss at iteration 3946: 0.04381618252050715\n",
      "Train Loss at iteration 3947: 0.0438159784899744\n",
      "Train Loss at iteration 3948: 0.04381577449300496\n",
      "Train Loss at iteration 3949: 0.04381557052959138\n",
      "Train Loss at iteration 3950: 0.043815366599726215\n",
      "Train Loss at iteration 3951: 0.04381516270340201\n",
      "Train Loss at iteration 3952: 0.043814958840611325\n",
      "Train Loss at iteration 3953: 0.04381475501134668\n",
      "Train Loss at iteration 3954: 0.04381455121560068\n",
      "Train Loss at iteration 3955: 0.043814347453365855\n",
      "Train Loss at iteration 3956: 0.04381414372463477\n",
      "Train Loss at iteration 3957: 0.0438139400294\n",
      "Train Loss at iteration 3958: 0.0438137363676541\n",
      "Train Loss at iteration 3959: 0.04381353273938967\n",
      "Train Loss at iteration 3960: 0.043813329144599254\n",
      "Train Loss at iteration 3961: 0.04381312558327543\n",
      "Train Loss at iteration 3962: 0.04381292205541079\n",
      "Train Loss at iteration 3963: 0.04381271856099792\n",
      "Train Loss at iteration 3964: 0.043812515100029384\n",
      "Train Loss at iteration 3965: 0.043812311672497786\n",
      "Train Loss at iteration 3966: 0.04381210827839571\n",
      "Train Loss at iteration 3967: 0.04381190491771577\n",
      "Train Loss at iteration 3968: 0.04381170159045053\n",
      "Train Loss at iteration 3969: 0.043811498296592616\n",
      "Train Loss at iteration 3970: 0.04381129503613461\n",
      "Train Loss at iteration 3971: 0.04381109180906912\n",
      "Train Loss at iteration 3972: 0.04381088861538875\n",
      "Train Loss at iteration 3973: 0.04381068545508613\n",
      "Train Loss at iteration 3974: 0.043810482328153856\n",
      "Train Loss at iteration 3975: 0.04381027923458454\n",
      "Train Loss at iteration 3976: 0.04381007617437082\n",
      "Train Loss at iteration 3977: 0.04380987314750529\n",
      "Train Loss at iteration 3978: 0.043809670153980594\n",
      "Train Loss at iteration 3979: 0.043809467193789364\n",
      "Train Loss at iteration 3980: 0.0438092642669242\n",
      "Train Loss at iteration 3981: 0.043809061373377756\n",
      "Train Loss at iteration 3982: 0.043808858513142666\n",
      "Train Loss at iteration 3983: 0.043808655686211555\n",
      "Train Loss at iteration 3984: 0.043808452892577086\n",
      "Train Loss at iteration 3985: 0.04380825013223188\n",
      "Train Loss at iteration 3986: 0.0438080474051686\n",
      "Train Loss at iteration 3987: 0.04380784471137986\n",
      "Train Loss at iteration 3988: 0.04380764205085834\n",
      "Train Loss at iteration 3989: 0.04380743942359672\n",
      "Train Loss at iteration 3990: 0.0438072368295876\n",
      "Train Loss at iteration 3991: 0.04380703426882367\n",
      "Train Loss at iteration 3992: 0.04380683174129758\n",
      "Train Loss at iteration 3993: 0.043806629247002016\n",
      "Train Loss at iteration 3994: 0.04380642678592962\n",
      "Train Loss at iteration 3995: 0.04380622435807308\n",
      "Train Loss at iteration 3996: 0.043806021963425076\n",
      "Train Loss at iteration 3997: 0.04380581960197825\n",
      "Train Loss at iteration 3998: 0.043805617273725284\n",
      "Train Loss at iteration 3999: 0.043805414978658905\n",
      "Train Loss at iteration 4000: 0.04380521271677175\n",
      "Train Loss at iteration 4001: 0.043805010488056524\n",
      "Train Loss at iteration 4002: 0.04380480829250591\n",
      "Train Loss at iteration 4003: 0.04380460613011262\n",
      "Train Loss at iteration 4004: 0.04380440400086932\n",
      "Train Loss at iteration 4005: 0.043804201904768705\n",
      "Train Loss at iteration 4006: 0.0438039998418035\n",
      "Train Loss at iteration 4007: 0.04380379781196641\n",
      "Train Loss at iteration 4008: 0.043803595815250104\n",
      "Train Loss at iteration 4009: 0.043803393851647325\n",
      "Train Loss at iteration 4010: 0.04380319192115077\n",
      "Train Loss at iteration 4011: 0.04380299002375313\n",
      "Train Loss at iteration 4012: 0.04380278815944717\n",
      "Train Loss at iteration 4013: 0.04380258632822556\n",
      "Train Loss at iteration 4014: 0.04380238453008106\n",
      "Train Loss at iteration 4015: 0.04380218276500637\n",
      "Train Loss at iteration 4016: 0.04380198103299422\n",
      "Train Loss at iteration 4017: 0.043801779334037345\n",
      "Train Loss at iteration 4018: 0.043801577668128464\n",
      "Train Loss at iteration 4019: 0.04380137603526034\n",
      "Train Loss at iteration 4020: 0.04380117443542567\n",
      "Train Loss at iteration 4021: 0.04380097286861723\n",
      "Train Loss at iteration 4022: 0.04380077133482775\n",
      "Train Loss at iteration 4023: 0.04380056983404995\n",
      "Train Loss at iteration 4024: 0.04380036836627662\n",
      "Train Loss at iteration 4025: 0.043800166931500475\n",
      "Train Loss at iteration 4026: 0.043799965529714296\n",
      "Train Loss at iteration 4027: 0.04379976416091082\n",
      "Train Loss at iteration 4028: 0.04379956282508281\n",
      "Train Loss at iteration 4029: 0.04379936152222303\n",
      "Train Loss at iteration 4030: 0.04379916025232424\n",
      "Train Loss at iteration 4031: 0.0437989590153792\n",
      "Train Loss at iteration 4032: 0.0437987578113807\n",
      "Train Loss at iteration 4033: 0.043798556640321486\n",
      "Train Loss at iteration 4034: 0.04379835550219435\n",
      "Train Loss at iteration 4035: 0.043798154396992056\n",
      "Train Loss at iteration 4036: 0.043797953324707393\n",
      "Train Loss at iteration 4037: 0.04379775228533314\n",
      "Train Loss at iteration 4038: 0.04379755127886208\n",
      "Train Loss at iteration 4039: 0.043797350305286996\n",
      "Train Loss at iteration 4040: 0.04379714936460069\n",
      "Train Loss at iteration 4041: 0.043796948456795944\n",
      "Train Loss at iteration 4042: 0.04379674758186555\n",
      "Train Loss at iteration 4043: 0.043796546739802315\n",
      "Train Loss at iteration 4044: 0.043796345930599036\n",
      "Train Loss at iteration 4045: 0.04379614515424851\n",
      "Train Loss at iteration 4046: 0.04379594441074355\n",
      "Train Loss at iteration 4047: 0.04379574370007696\n",
      "Train Loss at iteration 4048: 0.04379554302224155\n",
      "Train Loss at iteration 4049: 0.04379534237723014\n",
      "Train Loss at iteration 4050: 0.04379514176503554\n",
      "Train Loss at iteration 4051: 0.04379494118565057\n",
      "Train Loss at iteration 4052: 0.04379474063906805\n",
      "Train Loss at iteration 4053: 0.04379454012528079\n",
      "Train Loss at iteration 4054: 0.04379433964428163\n",
      "Train Loss at iteration 4055: 0.043794139196063404\n",
      "Train Loss at iteration 4056: 0.04379393878061895\n",
      "Train Loss at iteration 4057: 0.04379373839794107\n",
      "Train Loss at iteration 4058: 0.04379353804802262\n",
      "Train Loss at iteration 4059: 0.043793337730856446\n",
      "Train Loss at iteration 4060: 0.043793137446435364\n",
      "Train Loss at iteration 4061: 0.04379293719475226\n",
      "Train Loss at iteration 4062: 0.04379273697579994\n",
      "Train Loss at iteration 4063: 0.04379253678957127\n",
      "Train Loss at iteration 4064: 0.04379233663605911\n",
      "Train Loss at iteration 4065: 0.04379213651525629\n",
      "Train Loss at iteration 4066: 0.04379193642715568\n",
      "Train Loss at iteration 4067: 0.04379173637175016\n",
      "Train Loss at iteration 4068: 0.04379153634903256\n",
      "Train Loss at iteration 4069: 0.043791336358995764\n",
      "Train Loss at iteration 4070: 0.04379113640163262\n",
      "Train Loss at iteration 4071: 0.04379093647693602\n",
      "Train Loss at iteration 4072: 0.04379073658489883\n",
      "Train Loss at iteration 4073: 0.04379053672551392\n",
      "Train Loss at iteration 4074: 0.043790336898774165\n",
      "Train Loss at iteration 4075: 0.04379013710467244\n",
      "Train Loss at iteration 4076: 0.04378993734320164\n",
      "Train Loss at iteration 4077: 0.04378973761435464\n",
      "Train Loss at iteration 4078: 0.043789537918124344\n",
      "Train Loss at iteration 4079: 0.043789338254503624\n",
      "Train Loss at iteration 4080: 0.04378913862348539\n",
      "Train Loss at iteration 4081: 0.04378893902506251\n",
      "Train Loss at iteration 4082: 0.0437887394592279\n",
      "Train Loss at iteration 4083: 0.04378853992597445\n",
      "Train Loss at iteration 4084: 0.0437883404252951\n",
      "Train Loss at iteration 4085: 0.0437881409571827\n",
      "Train Loss at iteration 4086: 0.04378794152163018\n",
      "Train Loss at iteration 4087: 0.04378774211863046\n",
      "Train Loss at iteration 4088: 0.04378754274817644\n",
      "Train Loss at iteration 4089: 0.043787343410261054\n",
      "Train Loss at iteration 4090: 0.04378714410487718\n",
      "Train Loss at iteration 4091: 0.04378694483201778\n",
      "Train Loss at iteration 4092: 0.04378674559167578\n",
      "Train Loss at iteration 4093: 0.04378654638384405\n",
      "Train Loss at iteration 4094: 0.043786347208515576\n",
      "Train Loss at iteration 4095: 0.043786148065683254\n",
      "Train Loss at iteration 4096: 0.04378594895534003\n",
      "Train Loss at iteration 4097: 0.04378574987747884\n",
      "Train Loss at iteration 4098: 0.04378555083209263\n",
      "Train Loss at iteration 4099: 0.04378535181917431\n",
      "Train Loss at iteration 4100: 0.04378515283871684\n",
      "Train Loss at iteration 4101: 0.043784953890713185\n",
      "Train Loss at iteration 4102: 0.04378475497515626\n",
      "Train Loss at iteration 4103: 0.043784556092039045\n",
      "Train Loss at iteration 4104: 0.04378435724135447\n",
      "Train Loss at iteration 4105: 0.04378415842309549\n",
      "Train Loss at iteration 4106: 0.04378395963725508\n",
      "Train Loss at iteration 4107: 0.04378376088382617\n",
      "Train Loss at iteration 4108: 0.04378356216280175\n",
      "Train Loss at iteration 4109: 0.04378336347417479\n",
      "Train Loss at iteration 4110: 0.04378316481793823\n",
      "Train Loss at iteration 4111: 0.04378296619408506\n",
      "Train Loss at iteration 4112: 0.043782767602608255\n",
      "Train Loss at iteration 4113: 0.043782569043500766\n",
      "Train Loss at iteration 4114: 0.0437823705167556\n",
      "Train Loss at iteration 4115: 0.043782172022365715\n",
      "Train Loss at iteration 4116: 0.043781973560324094\n",
      "Train Loss at iteration 4117: 0.04378177513062374\n",
      "Train Loss at iteration 4118: 0.04378157673325762\n",
      "Train Loss at iteration 4119: 0.04378137836821876\n",
      "Train Loss at iteration 4120: 0.04378118003550009\n",
      "Train Loss at iteration 4121: 0.043780981735094655\n",
      "Train Loss at iteration 4122: 0.043780783466995425\n",
      "Train Loss at iteration 4123: 0.04378058523119542\n",
      "Train Loss at iteration 4124: 0.04378038702768762\n",
      "Train Loss at iteration 4125: 0.04378018885646507\n",
      "Train Loss at iteration 4126: 0.04377999071752072\n",
      "Train Loss at iteration 4127: 0.043779792610847613\n",
      "Train Loss at iteration 4128: 0.04377959453643876\n",
      "Train Loss at iteration 4129: 0.04377939649428717\n",
      "Train Loss at iteration 4130: 0.04377919848438586\n",
      "Train Loss at iteration 4131: 0.04377900050672784\n",
      "Train Loss at iteration 4132: 0.043778802561306146\n",
      "Train Loss at iteration 4133: 0.0437786046481138\n",
      "Train Loss at iteration 4134: 0.04377840676714382\n",
      "Train Loss at iteration 4135: 0.04377820891838924\n",
      "Train Loss at iteration 4136: 0.04377801110184309\n",
      "Train Loss at iteration 4137: 0.043777813317498394\n",
      "Train Loss at iteration 4138: 0.043777615565348205\n",
      "Train Loss at iteration 4139: 0.04377741784538555\n",
      "Train Loss at iteration 4140: 0.04377722015760348\n",
      "Train Loss at iteration 4141: 0.04377702250199503\n",
      "Train Loss at iteration 4142: 0.04377682487855325\n",
      "Train Loss at iteration 4143: 0.04377662728727118\n",
      "Train Loss at iteration 4144: 0.04377642972814187\n",
      "Train Loss at iteration 4145: 0.04377623220115838\n",
      "Train Loss at iteration 4146: 0.04377603470631378\n",
      "Train Loss at iteration 4147: 0.043775837243601075\n",
      "Train Loss at iteration 4148: 0.043775639813013405\n",
      "Train Loss at iteration 4149: 0.043775442414543765\n",
      "Train Loss at iteration 4150: 0.04377524504818523\n",
      "Train Loss at iteration 4151: 0.04377504771393091\n",
      "Train Loss at iteration 4152: 0.04377485041177383\n",
      "Train Loss at iteration 4153: 0.04377465314170709\n",
      "Train Loss at iteration 4154: 0.04377445590372373\n",
      "Train Loss at iteration 4155: 0.04377425869781687\n",
      "Train Loss at iteration 4156: 0.043774061523979564\n",
      "Train Loss at iteration 4157: 0.043773864382204895\n",
      "Train Loss at iteration 4158: 0.043773667272485946\n",
      "Train Loss at iteration 4159: 0.04377347019481581\n",
      "Train Loss at iteration 4160: 0.04377327314918757\n",
      "Train Loss at iteration 4161: 0.04377307613559432\n",
      "Train Loss at iteration 4162: 0.04377287915402916\n",
      "Train Loss at iteration 4163: 0.043772682204485186\n",
      "Train Loss at iteration 4164: 0.04377248528695546\n",
      "Train Loss at iteration 4165: 0.04377228840143314\n",
      "Train Loss at iteration 4166: 0.0437720915479113\n",
      "Train Loss at iteration 4167: 0.04377189472638304\n",
      "Train Loss at iteration 4168: 0.04377169793684148\n",
      "Train Loss at iteration 4169: 0.04377150117927971\n",
      "Train Loss at iteration 4170: 0.04377130445369087\n",
      "Train Loss at iteration 4171: 0.04377110776006807\n",
      "Train Loss at iteration 4172: 0.043770911098404405\n",
      "Train Loss at iteration 4173: 0.04377071446869302\n",
      "Train Loss at iteration 4174: 0.04377051787092703\n",
      "Train Loss at iteration 4175: 0.04377032130509953\n",
      "Train Loss at iteration 4176: 0.0437701247712037\n",
      "Train Loss at iteration 4177: 0.04376992826923263\n",
      "Train Loss at iteration 4178: 0.04376973179917945\n",
      "Train Loss at iteration 4179: 0.04376953536103732\n",
      "Train Loss at iteration 4180: 0.043769338954799376\n",
      "Train Loss at iteration 4181: 0.043769142580458734\n",
      "Train Loss at iteration 4182: 0.043768946238008526\n",
      "Train Loss at iteration 4183: 0.04376874992744194\n",
      "Train Loss at iteration 4184: 0.043768553648752075\n",
      "Train Loss at iteration 4185: 0.043768357401932106\n",
      "Train Loss at iteration 4186: 0.04376816118697517\n",
      "Train Loss at iteration 4187: 0.043767965003874434\n",
      "Train Loss at iteration 4188: 0.04376776885262305\n",
      "Train Loss at iteration 4189: 0.04376757273321415\n",
      "Train Loss at iteration 4190: 0.04376737664564092\n",
      "Train Loss at iteration 4191: 0.043767180589896516\n",
      "Train Loss at iteration 4192: 0.0437669845659741\n",
      "Train Loss at iteration 4193: 0.043766788573866844\n",
      "Train Loss at iteration 4194: 0.04376659261356791\n",
      "Train Loss at iteration 4195: 0.043766396685070465\n",
      "Train Loss at iteration 4196: 0.04376620078836771\n",
      "Train Loss at iteration 4197: 0.04376600492345278\n",
      "Train Loss at iteration 4198: 0.04376580909031887\n",
      "Train Loss at iteration 4199: 0.04376561328895918\n",
      "Train Loss at iteration 4200: 0.043765417519366856\n",
      "Train Loss at iteration 4201: 0.04376522178153512\n",
      "Train Loss at iteration 4202: 0.04376502607545714\n",
      "Train Loss at iteration 4203: 0.0437648304011261\n",
      "Train Loss at iteration 4204: 0.04376463475853521\n",
      "Train Loss at iteration 4205: 0.043764439147677636\n",
      "Train Loss at iteration 4206: 0.04376424356854662\n",
      "Train Loss at iteration 4207: 0.04376404802113532\n",
      "Train Loss at iteration 4208: 0.043763852505436955\n",
      "Train Loss at iteration 4209: 0.043763657021444725\n",
      "Train Loss at iteration 4210: 0.04376346156915183\n",
      "Train Loss at iteration 4211: 0.04376326614855149\n",
      "Train Loss at iteration 4212: 0.04376307075963692\n",
      "Train Loss at iteration 4213: 0.0437628754024013\n",
      "Train Loss at iteration 4214: 0.04376268007683788\n",
      "Train Loss at iteration 4215: 0.04376248478293987\n",
      "Train Loss at iteration 4216: 0.04376228952070048\n",
      "Train Loss at iteration 4217: 0.04376209429011293\n",
      "Train Loss at iteration 4218: 0.04376189909117046\n",
      "Train Loss at iteration 4219: 0.04376170392386628\n",
      "Train Loss at iteration 4220: 0.04376150878819364\n",
      "Train Loss at iteration 4221: 0.04376131368414575\n",
      "Train Loss at iteration 4222: 0.043761118611715846\n",
      "Train Loss at iteration 4223: 0.04376092357089717\n",
      "Train Loss at iteration 4224: 0.043760728561682956\n",
      "Train Loss at iteration 4225: 0.043760533584066445\n",
      "Train Loss at iteration 4226: 0.0437603386380409\n",
      "Train Loss at iteration 4227: 0.04376014372359953\n",
      "Train Loss at iteration 4228: 0.0437599488407356\n",
      "Train Loss at iteration 4229: 0.043759753989442374\n",
      "Train Loss at iteration 4230: 0.043759559169713066\n",
      "Train Loss at iteration 4231: 0.04375936438154097\n",
      "Train Loss at iteration 4232: 0.04375916962491931\n",
      "Train Loss at iteration 4233: 0.04375897489984137\n",
      "Train Loss at iteration 4234: 0.04375878020630039\n",
      "Train Loss at iteration 4235: 0.04375858554428964\n",
      "Train Loss at iteration 4236: 0.04375839091380238\n",
      "Train Loss at iteration 4237: 0.04375819631483192\n",
      "Train Loss at iteration 4238: 0.04375800174737145\n",
      "Train Loss at iteration 4239: 0.04375780721141432\n",
      "Train Loss at iteration 4240: 0.04375761270695376\n",
      "Train Loss at iteration 4241: 0.04375741823398305\n",
      "Train Loss at iteration 4242: 0.043757223792495484\n",
      "Train Loss at iteration 4243: 0.04375702938248433\n",
      "Train Loss at iteration 4244: 0.04375683500394287\n",
      "Train Loss at iteration 4245: 0.0437566406568644\n",
      "Train Loss at iteration 4246: 0.04375644634124221\n",
      "Train Loss at iteration 4247: 0.04375625205706959\n",
      "Train Loss at iteration 4248: 0.04375605780433982\n",
      "Train Loss at iteration 4249: 0.043755863583046194\n",
      "Train Loss at iteration 4250: 0.04375566939318201\n",
      "Train Loss at iteration 4251: 0.043755475234740576\n",
      "Train Loss at iteration 4252: 0.04375528110771519\n",
      "Train Loss at iteration 4253: 0.04375508701209916\n",
      "Train Loss at iteration 4254: 0.04375489294788577\n",
      "Train Loss at iteration 4255: 0.04375469891506837\n",
      "Train Loss at iteration 4256: 0.043754504913640226\n",
      "Train Loss at iteration 4257: 0.04375431094359468\n",
      "Train Loss at iteration 4258: 0.043754117004925014\n",
      "Train Loss at iteration 4259: 0.04375392309762459\n",
      "Train Loss at iteration 4260: 0.0437537292216867\n",
      "Train Loss at iteration 4261: 0.043753535377104656\n",
      "Train Loss at iteration 4262: 0.043753341563871805\n",
      "Train Loss at iteration 4263: 0.043753147781981444\n",
      "Train Loss at iteration 4264: 0.043752954031426934\n",
      "Train Loss at iteration 4265: 0.043752760312201586\n",
      "Train Loss at iteration 4266: 0.04375256662429874\n",
      "Train Loss at iteration 4267: 0.04375237296771173\n",
      "Train Loss at iteration 4268: 0.043752179342433875\n",
      "Train Loss at iteration 4269: 0.04375198574845854\n",
      "Train Loss at iteration 4270: 0.043751792185779036\n",
      "Train Loss at iteration 4271: 0.043751598654388754\n",
      "Train Loss at iteration 4272: 0.04375140515428099\n",
      "Train Loss at iteration 4273: 0.04375121168544912\n",
      "Train Loss at iteration 4274: 0.04375101824788649\n",
      "Train Loss at iteration 4275: 0.04375082484158643\n",
      "Train Loss at iteration 4276: 0.04375063146654233\n",
      "Train Loss at iteration 4277: 0.04375043812274753\n",
      "Train Loss at iteration 4278: 0.04375024481019539\n",
      "Train Loss at iteration 4279: 0.043750051528879265\n",
      "Train Loss at iteration 4280: 0.043749858278792526\n",
      "Train Loss at iteration 4281: 0.04374966505992854\n",
      "Train Loss at iteration 4282: 0.04374947187228066\n",
      "Train Loss at iteration 4283: 0.043749278715842284\n",
      "Train Loss at iteration 4284: 0.04374908559060676\n",
      "Train Loss at iteration 4285: 0.04374889249656746\n",
      "Train Loss at iteration 4286: 0.043748699433717765\n",
      "Train Loss at iteration 4287: 0.04374850640205108\n",
      "Train Loss at iteration 4288: 0.04374831340156074\n",
      "Train Loss at iteration 4289: 0.043748120432240176\n",
      "Train Loss at iteration 4290: 0.043747927494082725\n",
      "Train Loss at iteration 4291: 0.043747734587081824\n",
      "Train Loss at iteration 4292: 0.04374754171123081\n",
      "Train Loss at iteration 4293: 0.04374734886652312\n",
      "Train Loss at iteration 4294: 0.043747156052952124\n",
      "Train Loss at iteration 4295: 0.04374696327051122\n",
      "Train Loss at iteration 4296: 0.04374677051919382\n",
      "Train Loss at iteration 4297: 0.043746577798993304\n",
      "Train Loss at iteration 4298: 0.04374638510990309\n",
      "Train Loss at iteration 4299: 0.04374619245191657\n",
      "Train Loss at iteration 4300: 0.04374599982502718\n",
      "Train Loss at iteration 4301: 0.04374580722922829\n",
      "Train Loss at iteration 4302: 0.043745614664513335\n",
      "Train Loss at iteration 4303: 0.04374542213087572\n",
      "Train Loss at iteration 4304: 0.04374522962830886\n",
      "Train Loss at iteration 4305: 0.04374503715680619\n",
      "Train Loss at iteration 4306: 0.0437448447163611\n",
      "Train Loss at iteration 4307: 0.04374465230696703\n",
      "Train Loss at iteration 4308: 0.04374445992861741\n",
      "Train Loss at iteration 4309: 0.04374426758130566\n",
      "Train Loss at iteration 4310: 0.043744075265025203\n",
      "Train Loss at iteration 4311: 0.04374388297976947\n",
      "Train Loss at iteration 4312: 0.04374369072553191\n",
      "Train Loss at iteration 4313: 0.043743498502305946\n",
      "Train Loss at iteration 4314: 0.043743306310085\n",
      "Train Loss at iteration 4315: 0.04374311414886253\n",
      "Train Loss at iteration 4316: 0.04374292201863198\n",
      "Train Loss at iteration 4317: 0.04374272991938679\n",
      "Train Loss at iteration 4318: 0.0437425378511204\n",
      "Train Loss at iteration 4319: 0.043742345813826265\n",
      "Train Loss at iteration 4320: 0.04374215380749782\n",
      "Train Loss at iteration 4321: 0.043741961832128536\n",
      "Train Loss at iteration 4322: 0.04374176988771187\n",
      "Train Loss at iteration 4323: 0.04374157797424126\n",
      "Train Loss at iteration 4324: 0.043741386091710174\n",
      "Train Loss at iteration 4325: 0.04374119424011207\n",
      "Train Loss at iteration 4326: 0.04374100241944042\n",
      "Train Loss at iteration 4327: 0.043740810629688674\n",
      "Train Loss at iteration 4328: 0.043740618870850304\n",
      "Train Loss at iteration 4329: 0.04374042714291878\n",
      "Train Loss at iteration 4330: 0.04374023544588759\n",
      "Train Loss at iteration 4331: 0.043740043779750194\n",
      "Train Loss at iteration 4332: 0.04373985214450006\n",
      "Train Loss at iteration 4333: 0.04373966054013067\n",
      "Train Loss at iteration 4334: 0.04373946896663551\n",
      "Train Loss at iteration 4335: 0.04373927742400806\n",
      "Train Loss at iteration 4336: 0.0437390859122418\n",
      "Train Loss at iteration 4337: 0.04373889443133022\n",
      "Train Loss at iteration 4338: 0.04373870298126681\n",
      "Train Loss at iteration 4339: 0.04373851156204506\n",
      "Train Loss at iteration 4340: 0.04373832017365846\n",
      "Train Loss at iteration 4341: 0.04373812881610051\n",
      "Train Loss at iteration 4342: 0.04373793748936469\n",
      "Train Loss at iteration 4343: 0.043737746193444525\n",
      "Train Loss at iteration 4344: 0.04373755492833349\n",
      "Train Loss at iteration 4345: 0.043737363694025116\n",
      "Train Loss at iteration 4346: 0.04373717249051288\n",
      "Train Loss at iteration 4347: 0.0437369813177903\n",
      "Train Loss at iteration 4348: 0.043736790175850894\n",
      "Train Loss at iteration 4349: 0.043736599064688186\n",
      "Train Loss at iteration 4350: 0.04373640798429564\n",
      "Train Loss at iteration 4351: 0.043736216934666824\n",
      "Train Loss at iteration 4352: 0.043736025915795236\n",
      "Train Loss at iteration 4353: 0.043735834927674376\n",
      "Train Loss at iteration 4354: 0.04373564397029781\n",
      "Train Loss at iteration 4355: 0.04373545304365903\n",
      "Train Loss at iteration 4356: 0.04373526214775158\n",
      "Train Loss at iteration 4357: 0.043735071282568955\n",
      "Train Loss at iteration 4358: 0.043734880448104724\n",
      "Train Loss at iteration 4359: 0.043734689644352405\n",
      "Train Loss at iteration 4360: 0.043734498871305524\n",
      "Train Loss at iteration 4361: 0.04373430812895763\n",
      "Train Loss at iteration 4362: 0.04373411741730226\n",
      "Train Loss at iteration 4363: 0.04373392673633296\n",
      "Train Loss at iteration 4364: 0.043733736086043244\n",
      "Train Loss at iteration 4365: 0.04373354546642671\n",
      "Train Loss at iteration 4366: 0.04373335487747686\n",
      "Train Loss at iteration 4367: 0.04373316431918726\n",
      "Train Loss at iteration 4368: 0.04373297379155145\n",
      "Train Loss at iteration 4369: 0.043732783294562996\n",
      "Train Loss at iteration 4370: 0.043732592828215455\n",
      "Train Loss at iteration 4371: 0.04373240239250238\n",
      "Train Loss at iteration 4372: 0.04373221198741734\n",
      "Train Loss at iteration 4373: 0.043732021612953864\n",
      "Train Loss at iteration 4374: 0.04373183126910556\n",
      "Train Loss at iteration 4375: 0.04373164095586597\n",
      "Train Loss at iteration 4376: 0.04373145067322866\n",
      "Train Loss at iteration 4377: 0.0437312604211872\n",
      "Train Loss at iteration 4378: 0.04373107019973518\n",
      "Train Loss at iteration 4379: 0.043730880008866144\n",
      "Train Loss at iteration 4380: 0.04373068984857371\n",
      "Train Loss at iteration 4381: 0.043730499718851415\n",
      "Train Loss at iteration 4382: 0.04373030961969286\n",
      "Train Loss at iteration 4383: 0.043730119551091635\n",
      "Train Loss at iteration 4384: 0.04372992951304129\n",
      "Train Loss at iteration 4385: 0.04372973950553546\n",
      "Train Loss at iteration 4386: 0.04372954952856769\n",
      "Train Loss at iteration 4387: 0.04372935958213161\n",
      "Train Loss at iteration 4388: 0.043729169666220764\n",
      "Train Loss at iteration 4389: 0.04372897978082879\n",
      "Train Loss at iteration 4390: 0.04372878992594927\n",
      "Train Loss at iteration 4391: 0.0437286001015758\n",
      "Train Loss at iteration 4392: 0.043728410307702\n",
      "Train Loss at iteration 4393: 0.04372822054432143\n",
      "Train Loss at iteration 4394: 0.043728030811427746\n",
      "Train Loss at iteration 4395: 0.043727841109014516\n",
      "Train Loss at iteration 4396: 0.04372765143707536\n",
      "Train Loss at iteration 4397: 0.04372746179560391\n",
      "Train Loss at iteration 4398: 0.043727272184593755\n",
      "Train Loss at iteration 4399: 0.04372708260403849\n",
      "Train Loss at iteration 4400: 0.0437268930539318\n",
      "Train Loss at iteration 4401: 0.043726703534267254\n",
      "Train Loss at iteration 4402: 0.04372651404503848\n",
      "Train Loss at iteration 4403: 0.0437263245862391\n",
      "Train Loss at iteration 4404: 0.04372613515786275\n",
      "Train Loss at iteration 4405: 0.043725945759903045\n",
      "Train Loss at iteration 4406: 0.04372575639235363\n",
      "Train Loss at iteration 4407: 0.04372556705520812\n",
      "Train Loss at iteration 4408: 0.043725377748460165\n",
      "Train Loss at iteration 4409: 0.04372518847210339\n",
      "Train Loss at iteration 4410: 0.04372499922613144\n",
      "Train Loss at iteration 4411: 0.043724810010537934\n",
      "Train Loss at iteration 4412: 0.04372462082531654\n",
      "Train Loss at iteration 4413: 0.0437244316704609\n",
      "Train Loss at iteration 4414: 0.04372424254596464\n",
      "Train Loss at iteration 4415: 0.043724053451821426\n",
      "Train Loss at iteration 4416: 0.043723864388024884\n",
      "Train Loss at iteration 4417: 0.0437236753545687\n",
      "Train Loss at iteration 4418: 0.043723486351446496\n",
      "Train Loss at iteration 4419: 0.043723297378651944\n",
      "Train Loss at iteration 4420: 0.043723108436178715\n",
      "Train Loss at iteration 4421: 0.04372291952402045\n",
      "Train Loss at iteration 4422: 0.043722730642170804\n",
      "Train Loss at iteration 4423: 0.04372254179062345\n",
      "Train Loss at iteration 4424: 0.04372235296937207\n",
      "Train Loss at iteration 4425: 0.043722164178410294\n",
      "Train Loss at iteration 4426: 0.04372197541773184\n",
      "Train Loss at iteration 4427: 0.043721786687330344\n",
      "Train Loss at iteration 4428: 0.0437215979871995\n",
      "Train Loss at iteration 4429: 0.043721409317332954\n",
      "Train Loss at iteration 4430: 0.04372122067772443\n",
      "Train Loss at iteration 4431: 0.043721032068367575\n",
      "Train Loss at iteration 4432: 0.04372084348925608\n",
      "Train Loss at iteration 4433: 0.04372065494038363\n",
      "Train Loss at iteration 4434: 0.0437204664217439\n",
      "Train Loss at iteration 4435: 0.04372027793333059\n",
      "Train Loss at iteration 4436: 0.04372008947513739\n",
      "Train Loss at iteration 4437: 0.043719901047157984\n",
      "Train Loss at iteration 4438: 0.04371971264938608\n",
      "Train Loss at iteration 4439: 0.043719524281815365\n",
      "Train Loss at iteration 4440: 0.043719335944439525\n",
      "Train Loss at iteration 4441: 0.04371914763725228\n",
      "Train Loss at iteration 4442: 0.04371895936024733\n",
      "Train Loss at iteration 4443: 0.04371877111341836\n",
      "Train Loss at iteration 4444: 0.043718582896759096\n",
      "Train Loss at iteration 4445: 0.04371839471026325\n",
      "Train Loss at iteration 4446: 0.043718206553924506\n",
      "Train Loss at iteration 4447: 0.0437180184277366\n",
      "Train Loss at iteration 4448: 0.04371783033169323\n",
      "Train Loss at iteration 4449: 0.04371764226578812\n",
      "Train Loss at iteration 4450: 0.04371745423001498\n",
      "Train Loss at iteration 4451: 0.04371726622436754\n",
      "Train Loss at iteration 4452: 0.043717078248839525\n",
      "Train Loss at iteration 4453: 0.04371689030342466\n",
      "Train Loss at iteration 4454: 0.043716702388116646\n",
      "Train Loss at iteration 4455: 0.04371651450290923\n",
      "Train Loss at iteration 4456: 0.04371632664779614\n",
      "Train Loss at iteration 4457: 0.0437161388227711\n",
      "Train Loss at iteration 4458: 0.04371595102782786\n",
      "Train Loss at iteration 4459: 0.043715763262960144\n",
      "Train Loss at iteration 4460: 0.043715575528161695\n",
      "Train Loss at iteration 4461: 0.04371538782342625\n",
      "Train Loss at iteration 4462: 0.04371520014874753\n",
      "Train Loss at iteration 4463: 0.04371501250411932\n",
      "Train Loss at iteration 4464: 0.04371482488953534\n",
      "Train Loss at iteration 4465: 0.04371463730498934\n",
      "Train Loss at iteration 4466: 0.04371444975047506\n",
      "Train Loss at iteration 4467: 0.04371426222598627\n",
      "Train Loss at iteration 4468: 0.043714074731516714\n",
      "Train Loss at iteration 4469: 0.04371388726706015\n",
      "Train Loss at iteration 4470: 0.04371369983261034\n",
      "Train Loss at iteration 4471: 0.043713512428161024\n",
      "Train Loss at iteration 4472: 0.04371332505370598\n",
      "Train Loss at iteration 4473: 0.04371313770923898\n",
      "Train Loss at iteration 4474: 0.04371295039475376\n",
      "Train Loss at iteration 4475: 0.043712763110244114\n",
      "Train Loss at iteration 4476: 0.04371257585570379\n",
      "Train Loss at iteration 4477: 0.04371238863112657\n",
      "Train Loss at iteration 4478: 0.04371220143650624\n",
      "Train Loss at iteration 4479: 0.043712014271836554\n",
      "Train Loss at iteration 4480: 0.043711827137111296\n",
      "Train Loss at iteration 4481: 0.04371164003232424\n",
      "Train Loss at iteration 4482: 0.043711452957469164\n",
      "Train Loss at iteration 4483: 0.04371126591253988\n",
      "Train Loss at iteration 4484: 0.04371107889753013\n",
      "Train Loss at iteration 4485: 0.043710891912433734\n",
      "Train Loss at iteration 4486: 0.04371070495724447\n",
      "Train Loss at iteration 4487: 0.0437105180319561\n",
      "Train Loss at iteration 4488: 0.04371033113656247\n",
      "Train Loss at iteration 4489: 0.043710144271057336\n",
      "Train Loss at iteration 4490: 0.0437099574354345\n",
      "Train Loss at iteration 4491: 0.04370977062968777\n",
      "Train Loss at iteration 4492: 0.043709583853810935\n",
      "Train Loss at iteration 4493: 0.04370939710779782\n",
      "Train Loss at iteration 4494: 0.043709210391642196\n",
      "Train Loss at iteration 4495: 0.04370902370533789\n",
      "Train Loss at iteration 4496: 0.0437088370488787\n",
      "Train Loss at iteration 4497: 0.04370865042225845\n",
      "Train Loss at iteration 4498: 0.04370846382547094\n",
      "Train Loss at iteration 4499: 0.043708277258509974\n",
      "Train Loss at iteration 4500: 0.043708090721369386\n",
      "Train Loss at iteration 4501: 0.043707904214042984\n",
      "Train Loss at iteration 4502: 0.0437077177365246\n",
      "Train Loss at iteration 4503: 0.043707531288808034\n",
      "Train Loss at iteration 4504: 0.043707344870887115\n",
      "Train Loss at iteration 4505: 0.04370715848275569\n",
      "Train Loss at iteration 4506: 0.043706972124407564\n",
      "Train Loss at iteration 4507: 0.043706785795836575\n",
      "Train Loss at iteration 4508: 0.04370659949703654\n",
      "Train Loss at iteration 4509: 0.04370641322800131\n",
      "Train Loss at iteration 4510: 0.04370622698872471\n",
      "Train Loss at iteration 4511: 0.043706040779200574\n",
      "Train Loss at iteration 4512: 0.04370585459942274\n",
      "Train Loss at iteration 4513: 0.04370566844938507\n",
      "Train Loss at iteration 4514: 0.04370548232908139\n",
      "Train Loss at iteration 4515: 0.04370529623850554\n",
      "Train Loss at iteration 4516: 0.043705110177651364\n",
      "Train Loss at iteration 4517: 0.04370492414651272\n",
      "Train Loss at iteration 4518: 0.04370473814508346\n",
      "Train Loss at iteration 4519: 0.04370455217335742\n",
      "Train Loss at iteration 4520: 0.04370436623132847\n",
      "Train Loss at iteration 4521: 0.04370418031899047\n",
      "Train Loss at iteration 4522: 0.043703994436337254\n",
      "Train Loss at iteration 4523: 0.0437038085833627\n",
      "Train Loss at iteration 4524: 0.04370362276006067\n",
      "Train Loss at iteration 4525: 0.04370343696642501\n",
      "Train Loss at iteration 4526: 0.04370325120244961\n",
      "Train Loss at iteration 4527: 0.04370306546812832\n",
      "Train Loss at iteration 4528: 0.04370287976345502\n",
      "Train Loss at iteration 4529: 0.04370269408842356\n",
      "Train Loss at iteration 4530: 0.04370250844302784\n",
      "Train Loss at iteration 4531: 0.04370232282726172\n",
      "Train Loss at iteration 4532: 0.043702137241119064\n",
      "Train Loss at iteration 4533: 0.04370195168459378\n",
      "Train Loss at iteration 4534: 0.04370176615767973\n",
      "Train Loss at iteration 4535: 0.0437015806603708\n",
      "Train Loss at iteration 4536: 0.04370139519266088\n",
      "Train Loss at iteration 4537: 0.04370120975454384\n",
      "Train Loss at iteration 4538: 0.04370102434601359\n",
      "Train Loss at iteration 4539: 0.043700838967064\n",
      "Train Loss at iteration 4540: 0.04370065361768897\n",
      "Train Loss at iteration 4541: 0.04370046829788239\n",
      "Train Loss at iteration 4542: 0.043700283007638174\n",
      "Train Loss at iteration 4543: 0.04370009774695019\n",
      "Train Loss at iteration 4544: 0.04369991251581235\n",
      "Train Loss at iteration 4545: 0.043699727314218555\n",
      "Train Loss at iteration 4546: 0.043699542142162734\n",
      "Train Loss at iteration 4547: 0.04369935699963875\n",
      "Train Loss at iteration 4548: 0.04369917188664052\n",
      "Train Loss at iteration 4549: 0.043698986803161956\n",
      "Train Loss at iteration 4550: 0.043698801749197\n",
      "Train Loss at iteration 4551: 0.04369861672473951\n",
      "Train Loss at iteration 4552: 0.043698431729783445\n",
      "Train Loss at iteration 4553: 0.0436982467643227\n",
      "Train Loss at iteration 4554: 0.043698061828351176\n",
      "Train Loss at iteration 4555: 0.04369787692186284\n",
      "Train Loss at iteration 4556: 0.04369769204485156\n",
      "Train Loss at iteration 4557: 0.04369750719731131\n",
      "Train Loss at iteration 4558: 0.04369732237923598\n",
      "Train Loss at iteration 4559: 0.04369713759061951\n",
      "Train Loss at iteration 4560: 0.04369695283145584\n",
      "Train Loss at iteration 4561: 0.04369676810173887\n",
      "Train Loss at iteration 4562: 0.04369658340146258\n",
      "Train Loss at iteration 4563: 0.04369639873062084\n",
      "Train Loss at iteration 4564: 0.043696214089207645\n",
      "Train Loss at iteration 4565: 0.0436960294772169\n",
      "Train Loss at iteration 4566: 0.04369584489464257\n",
      "Train Loss at iteration 4567: 0.04369566034147858\n",
      "Train Loss at iteration 4568: 0.04369547581771889\n",
      "Train Loss at iteration 4569: 0.04369529132335742\n",
      "Train Loss at iteration 4570: 0.04369510685838814\n",
      "Train Loss at iteration 4571: 0.043694922422804996\n",
      "Train Loss at iteration 4572: 0.04369473801660192\n",
      "Train Loss at iteration 4573: 0.04369455363977289\n",
      "Train Loss at iteration 4574: 0.04369436929231186\n",
      "Train Loss at iteration 4575: 0.043694184974212756\n",
      "Train Loss at iteration 4576: 0.04369400068546959\n",
      "Train Loss at iteration 4577: 0.043693816426076275\n",
      "Train Loss at iteration 4578: 0.043693632196026795\n",
      "Train Loss at iteration 4579: 0.043693447995315104\n",
      "Train Loss at iteration 4580: 0.04369326382393519\n",
      "Train Loss at iteration 4581: 0.04369307968188099\n",
      "Train Loss at iteration 4582: 0.0436928955691465\n",
      "Train Loss at iteration 4583: 0.04369271148572568\n",
      "Train Loss at iteration 4584: 0.043692527431612505\n",
      "Train Loss at iteration 4585: 0.04369234340680096\n",
      "Train Loss at iteration 4586: 0.043692159411285\n",
      "Train Loss at iteration 4587: 0.043691975445058616\n",
      "Train Loss at iteration 4588: 0.04369179150811579\n",
      "Train Loss at iteration 4589: 0.04369160760045051\n",
      "Train Loss at iteration 4590: 0.04369142372205676\n",
      "Train Loss at iteration 4591: 0.04369123987292852\n",
      "Train Loss at iteration 4592: 0.043691056053059785\n",
      "Train Loss at iteration 4593: 0.04369087226244454\n",
      "Train Loss at iteration 4594: 0.04369068850107676\n",
      "Train Loss at iteration 4595: 0.04369050476895048\n",
      "Train Loss at iteration 4596: 0.04369032106605966\n",
      "Train Loss at iteration 4597: 0.04369013739239831\n",
      "Train Loss at iteration 4598: 0.043689953747960426\n",
      "Train Loss at iteration 4599: 0.043689770132740016\n",
      "Train Loss at iteration 4600: 0.04368958654673108\n",
      "Train Loss at iteration 4601: 0.04368940298992762\n",
      "Train Loss at iteration 4602: 0.04368921946232364\n",
      "Train Loss at iteration 4603: 0.04368903596391315\n",
      "Train Loss at iteration 4604: 0.043688852494690185\n",
      "Train Loss at iteration 4605: 0.043688669054648716\n",
      "Train Loss at iteration 4606: 0.043688485643782776\n",
      "Train Loss at iteration 4607: 0.043688302262086386\n",
      "Train Loss at iteration 4608: 0.04368811890955356\n",
      "Train Loss at iteration 4609: 0.04368793558617831\n",
      "Train Loss at iteration 4610: 0.043687752291954665\n",
      "Train Loss at iteration 4611: 0.04368756902687665\n",
      "Train Loss at iteration 4612: 0.043687385790938275\n",
      "Train Loss at iteration 4613: 0.043687202584133576\n",
      "Train Loss at iteration 4614: 0.043687019406456584\n",
      "Train Loss at iteration 4615: 0.043686836257901325\n",
      "Train Loss at iteration 4616: 0.043686653138461845\n",
      "Train Loss at iteration 4617: 0.04368647004813216\n",
      "Train Loss at iteration 4618: 0.04368628698690631\n",
      "Train Loss at iteration 4619: 0.04368610395477833\n",
      "Train Loss at iteration 4620: 0.04368592095174227\n",
      "Train Loss at iteration 4621: 0.043685737977792155\n",
      "Train Loss at iteration 4622: 0.04368555503292204\n",
      "Train Loss at iteration 4623: 0.04368537211712598\n",
      "Train Loss at iteration 4624: 0.043685189230398005\n",
      "Train Loss at iteration 4625: 0.04368500637273215\n",
      "Train Loss at iteration 4626: 0.043684823544122474\n",
      "Train Loss at iteration 4627: 0.043684640744563075\n",
      "Train Loss at iteration 4628: 0.043684457974047936\n",
      "Train Loss at iteration 4629: 0.04368427523257116\n",
      "Train Loss at iteration 4630: 0.043684092520126776\n",
      "Train Loss at iteration 4631: 0.043683909836708855\n",
      "Train Loss at iteration 4632: 0.04368372718231147\n",
      "Train Loss at iteration 4633: 0.04368354455692866\n",
      "Train Loss at iteration 4634: 0.04368336196055451\n",
      "Train Loss at iteration 4635: 0.04368317939318307\n",
      "Train Loss at iteration 4636: 0.04368299685480841\n",
      "Train Loss at iteration 4637: 0.04368281434542463\n",
      "Train Loss at iteration 4638: 0.04368263186502576\n",
      "Train Loss at iteration 4639: 0.04368244941360588\n",
      "Train Loss at iteration 4640: 0.04368226699115908\n",
      "Train Loss at iteration 4641: 0.04368208459767943\n",
      "Train Loss at iteration 4642: 0.04368190223316103\n",
      "Train Loss at iteration 4643: 0.04368171989759792\n",
      "Train Loss at iteration 4644: 0.04368153759098421\n",
      "Train Loss at iteration 4645: 0.04368135531331399\n",
      "Train Loss at iteration 4646: 0.04368117306458131\n",
      "Train Loss at iteration 4647: 0.0436809908447803\n",
      "Train Loss at iteration 4648: 0.04368080865390502\n",
      "Train Loss at iteration 4649: 0.043680626491949565\n",
      "Train Loss at iteration 4650: 0.04368044435890805\n",
      "Train Loss at iteration 4651: 0.043680262254774546\n",
      "Train Loss at iteration 4652: 0.043680080179543154\n",
      "Train Loss at iteration 4653: 0.043679898133207974\n",
      "Train Loss at iteration 4654: 0.04367971611576311\n",
      "Train Loss at iteration 4655: 0.04367953412720267\n",
      "Train Loss at iteration 4656: 0.04367935216752074\n",
      "Train Loss at iteration 4657: 0.04367917023671143\n",
      "Train Loss at iteration 4658: 0.04367898833476886\n",
      "Train Loss at iteration 4659: 0.04367880646168712\n",
      "Train Loss at iteration 4660: 0.04367862461746034\n",
      "Train Loss at iteration 4661: 0.04367844280208262\n",
      "Train Loss at iteration 4662: 0.04367826101554807\n",
      "Train Loss at iteration 4663: 0.043678079257850826\n",
      "Train Loss at iteration 4664: 0.04367789752898499\n",
      "Train Loss at iteration 4665: 0.043677715828944676\n",
      "Train Loss at iteration 4666: 0.04367753415772403\n",
      "Train Loss at iteration 4667: 0.043677352515317146\n",
      "Train Loss at iteration 4668: 0.04367717090171815\n",
      "Train Loss at iteration 4669: 0.0436769893169212\n",
      "Train Loss at iteration 4670: 0.043676807760920394\n",
      "Train Loss at iteration 4671: 0.043676626233709874\n",
      "Train Loss at iteration 4672: 0.04367644473528378\n",
      "Train Loss at iteration 4673: 0.04367626326563622\n",
      "Train Loss at iteration 4674: 0.04367608182476135\n",
      "Train Loss at iteration 4675: 0.0436759004126533\n",
      "Train Loss at iteration 4676: 0.04367571902930621\n",
      "Train Loss at iteration 4677: 0.043675537674714236\n",
      "Train Loss at iteration 4678: 0.04367535634887149\n",
      "Train Loss at iteration 4679: 0.04367517505177213\n",
      "Train Loss at iteration 4680: 0.04367499378341031\n",
      "Train Loss at iteration 4681: 0.043674812543780174\n",
      "Train Loss at iteration 4682: 0.043674631332875864\n",
      "Train Loss at iteration 4683: 0.04367445015069153\n",
      "Train Loss at iteration 4684: 0.04367426899722133\n",
      "Train Loss at iteration 4685: 0.04367408787245943\n",
      "Train Loss at iteration 4686: 0.043673906776399964\n",
      "Train Loss at iteration 4687: 0.043673725709037096\n",
      "Train Loss at iteration 4688: 0.043673544670364994\n",
      "Train Loss at iteration 4689: 0.04367336366037782\n",
      "Train Loss at iteration 4690: 0.04367318267906974\n",
      "Train Loss at iteration 4691: 0.04367300172643491\n",
      "Train Loss at iteration 4692: 0.04367282080246748\n",
      "Train Loss at iteration 4693: 0.04367263990716165\n",
      "Train Loss at iteration 4694: 0.043672459040511584\n",
      "Train Loss at iteration 4695: 0.04367227820251143\n",
      "Train Loss at iteration 4696: 0.043672097393155404\n",
      "Train Loss at iteration 4697: 0.043671916612437627\n",
      "Train Loss at iteration 4698: 0.043671735860352334\n",
      "Train Loss at iteration 4699: 0.04367155513689366\n",
      "Train Loss at iteration 4700: 0.043671374442055794\n",
      "Train Loss at iteration 4701: 0.043671193775832944\n",
      "Train Loss at iteration 4702: 0.043671013138219264\n",
      "Train Loss at iteration 4703: 0.043670832529208954\n",
      "Train Loss at iteration 4704: 0.043670651948796185\n",
      "Train Loss at iteration 4705: 0.04367047139697519\n",
      "Train Loss at iteration 4706: 0.0436702908737401\n",
      "Train Loss at iteration 4707: 0.04367011037908514\n",
      "Train Loss at iteration 4708: 0.04366992991300451\n",
      "Train Loss at iteration 4709: 0.0436697494754924\n",
      "Train Loss at iteration 4710: 0.043669569066543006\n",
      "Train Loss at iteration 4711: 0.04366938868615052\n",
      "Train Loss at iteration 4712: 0.043669208334309165\n",
      "Train Loss at iteration 4713: 0.04366902801101313\n",
      "Train Loss at iteration 4714: 0.0436688477162566\n",
      "Train Loss at iteration 4715: 0.04366866745003382\n",
      "Train Loss at iteration 4716: 0.043668487212338965\n",
      "Train Loss at iteration 4717: 0.04366830700316627\n",
      "Train Loss at iteration 4718: 0.04366812682250991\n",
      "Train Loss at iteration 4719: 0.04366794667036416\n",
      "Train Loss at iteration 4720: 0.04366776654672318\n",
      "Train Loss at iteration 4721: 0.043667586451581204\n",
      "Train Loss at iteration 4722: 0.04366740638493246\n",
      "Train Loss at iteration 4723: 0.04366722634677115\n",
      "Train Loss at iteration 4724: 0.043667046337091515\n",
      "Train Loss at iteration 4725: 0.04366686635588777\n",
      "Train Loss at iteration 4726: 0.043666686403154145\n",
      "Train Loss at iteration 4727: 0.04366650647888485\n",
      "Train Loss at iteration 4728: 0.04366632658307412\n",
      "Train Loss at iteration 4729: 0.04366614671571621\n",
      "Train Loss at iteration 4730: 0.043665966876805316\n",
      "Train Loss at iteration 4731: 0.04366578706633569\n",
      "Train Loss at iteration 4732: 0.04366560728430158\n",
      "Train Loss at iteration 4733: 0.0436654275306972\n",
      "Train Loss at iteration 4734: 0.043665247805516796\n",
      "Train Loss at iteration 4735: 0.043665068108754625\n",
      "Train Loss at iteration 4736: 0.043664888440404906\n",
      "Train Loss at iteration 4737: 0.0436647088004619\n",
      "Train Loss at iteration 4738: 0.043664529188919846\n",
      "Train Loss at iteration 4739: 0.04366434960577298\n",
      "Train Loss at iteration 4740: 0.04366417005101556\n",
      "Train Loss at iteration 4741: 0.04366399052464186\n",
      "Train Loss at iteration 4742: 0.043663811026646085\n",
      "Train Loss at iteration 4743: 0.04366363155702253\n",
      "Train Loss at iteration 4744: 0.04366345211576542\n",
      "Train Loss at iteration 4745: 0.043663272702869065\n",
      "Train Loss at iteration 4746: 0.043663093318327656\n",
      "Train Loss at iteration 4747: 0.0436629139621355\n",
      "Train Loss at iteration 4748: 0.04366273463428685\n",
      "Train Loss at iteration 4749: 0.043662555334775956\n",
      "Train Loss at iteration 4750: 0.04366237606359709\n",
      "Train Loss at iteration 4751: 0.043662196820744535\n",
      "Train Loss at iteration 4752: 0.04366201760621255\n",
      "Train Loss at iteration 4753: 0.0436618384199954\n",
      "Train Loss at iteration 4754: 0.043661659262087364\n",
      "Train Loss at iteration 4755: 0.04366148013248271\n",
      "Train Loss at iteration 4756: 0.04366130103117572\n",
      "Train Loss at iteration 4757: 0.04366112195816069\n",
      "Train Loss at iteration 4758: 0.043660942913431866\n",
      "Train Loss at iteration 4759: 0.04366076389698354\n",
      "Train Loss at iteration 4760: 0.043660584908810016\n",
      "Train Loss at iteration 4761: 0.043660405948905544\n",
      "Train Loss at iteration 4762: 0.04366022701726444\n",
      "Train Loss at iteration 4763: 0.04366004811388098\n",
      "Train Loss at iteration 4764: 0.04365986923874945\n",
      "Train Loss at iteration 4765: 0.043659690391864146\n",
      "Train Loss at iteration 4766: 0.04365951157321936\n",
      "Train Loss at iteration 4767: 0.04365933278280938\n",
      "Train Loss at iteration 4768: 0.043659154020628525\n",
      "Train Loss at iteration 4769: 0.043658975286671065\n",
      "Train Loss at iteration 4770: 0.04365879658093131\n",
      "Train Loss at iteration 4771: 0.04365861790340356\n",
      "Train Loss at iteration 4772: 0.04365843925408213\n",
      "Train Loss at iteration 4773: 0.04365826063296132\n",
      "Train Loss at iteration 4774: 0.04365808204003542\n",
      "Train Loss at iteration 4775: 0.04365790347529874\n",
      "Train Loss at iteration 4776: 0.04365772493874562\n",
      "Train Loss at iteration 4777: 0.043657546430370335\n",
      "Train Loss at iteration 4778: 0.043657367950167225\n",
      "Train Loss at iteration 4779: 0.043657189498130584\n",
      "Train Loss at iteration 4780: 0.043657011074254744\n",
      "Train Loss at iteration 4781: 0.043656832678534\n",
      "Train Loss at iteration 4782: 0.0436566543109627\n",
      "Train Loss at iteration 4783: 0.04365647597153517\n",
      "Train Loss at iteration 4784: 0.043656297660245695\n",
      "Train Loss at iteration 4785: 0.04365611937708863\n",
      "Train Loss at iteration 4786: 0.04365594112205828\n",
      "Train Loss at iteration 4787: 0.04365576289514899\n",
      "Train Loss at iteration 4788: 0.043655584696355096\n",
      "Train Loss at iteration 4789: 0.0436554065256709\n",
      "Train Loss at iteration 4790: 0.04365522838309076\n",
      "Train Loss at iteration 4791: 0.043655050268609\n",
      "Train Loss at iteration 4792: 0.04365487218221996\n",
      "Train Loss at iteration 4793: 0.04365469412391798\n",
      "Train Loss at iteration 4794: 0.0436545160936974\n",
      "Train Loss at iteration 4795: 0.04365433809155255\n",
      "Train Loss at iteration 4796: 0.04365416011747778\n",
      "Train Loss at iteration 4797: 0.04365398217146744\n",
      "Train Loss at iteration 4798: 0.043653804253515865\n",
      "Train Loss at iteration 4799: 0.043653626363617415\n",
      "Train Loss at iteration 4800: 0.04365344850176643\n",
      "Train Loss at iteration 4801: 0.04365327066795726\n",
      "Train Loss at iteration 4802: 0.043653092862184266\n",
      "Train Loss at iteration 4803: 0.043652915084441794\n",
      "Train Loss at iteration 4804: 0.04365273733472421\n",
      "Train Loss at iteration 4805: 0.04365255961302587\n",
      "Train Loss at iteration 4806: 0.043652381919341114\n",
      "Train Loss at iteration 4807: 0.04365220425366432\n",
      "Train Loss at iteration 4808: 0.04365202661598987\n",
      "Train Loss at iteration 4809: 0.04365184900631207\n",
      "Train Loss at iteration 4810: 0.043651671424625345\n",
      "Train Loss at iteration 4811: 0.043651493870924034\n",
      "Train Loss at iteration 4812: 0.04365131634520251\n",
      "Train Loss at iteration 4813: 0.04365113884745515\n",
      "Train Loss at iteration 4814: 0.04365096137767632\n",
      "Train Loss at iteration 4815: 0.043650783935860395\n",
      "Train Loss at iteration 4816: 0.04365060652200175\n",
      "Train Loss at iteration 4817: 0.043650429136094765\n",
      "Train Loss at iteration 4818: 0.043650251778133815\n",
      "Train Loss at iteration 4819: 0.04365007444811328\n",
      "Train Loss at iteration 4820: 0.04364989714602756\n",
      "Train Loss at iteration 4821: 0.043649719871871\n",
      "Train Loss at iteration 4822: 0.04364954262563803\n",
      "Train Loss at iteration 4823: 0.043649365407323004\n",
      "Train Loss at iteration 4824: 0.04364918821692033\n",
      "Train Loss at iteration 4825: 0.0436490110544244\n",
      "Train Loss at iteration 4826: 0.043648833919829565\n",
      "Train Loss at iteration 4827: 0.043648656813130265\n",
      "Train Loss at iteration 4828: 0.04364847973432089\n",
      "Train Loss at iteration 4829: 0.043648302683395826\n",
      "Train Loss at iteration 4830: 0.043648125660349466\n",
      "Train Loss at iteration 4831: 0.04364794866517622\n",
      "Train Loss at iteration 4832: 0.043647771697870485\n",
      "Train Loss at iteration 4833: 0.04364759475842667\n",
      "Train Loss at iteration 4834: 0.04364741784683916\n",
      "Train Loss at iteration 4835: 0.0436472409631024\n",
      "Train Loss at iteration 4836: 0.04364706410721076\n",
      "Train Loss at iteration 4837: 0.04364688727915867\n",
      "Train Loss at iteration 4838: 0.04364671047894052\n",
      "Train Loss at iteration 4839: 0.04364653370655076\n",
      "Train Loss at iteration 4840: 0.043646356961983775\n",
      "Train Loss at iteration 4841: 0.04364618024523399\n",
      "Train Loss at iteration 4842: 0.04364600355629582\n",
      "Train Loss at iteration 4843: 0.04364582689516369\n",
      "Train Loss at iteration 4844: 0.04364565026183202\n",
      "Train Loss at iteration 4845: 0.04364547365629524\n",
      "Train Loss at iteration 4846: 0.043645297078547744\n",
      "Train Loss at iteration 4847: 0.04364512052858399\n",
      "Train Loss at iteration 4848: 0.04364494400639839\n",
      "Train Loss at iteration 4849: 0.043644767511985386\n",
      "Train Loss at iteration 4850: 0.04364459104533939\n",
      "Train Loss at iteration 4851: 0.04364441460645485\n",
      "Train Loss at iteration 4852: 0.04364423819532619\n",
      "Train Loss at iteration 4853: 0.043644061811947854\n",
      "Train Loss at iteration 4854: 0.043643885456314264\n",
      "Train Loss at iteration 4855: 0.043643709128419875\n",
      "Train Loss at iteration 4856: 0.04364353282825913\n",
      "Train Loss at iteration 4857: 0.04364335655582646\n",
      "Train Loss at iteration 4858: 0.0436431803111163\n",
      "Train Loss at iteration 4859: 0.04364300409412312\n",
      "Train Loss at iteration 4860: 0.04364282790484134\n",
      "Train Loss at iteration 4861: 0.04364265174326544\n",
      "Train Loss at iteration 4862: 0.04364247560938982\n",
      "Train Loss at iteration 4863: 0.043642299503209\n",
      "Train Loss at iteration 4864: 0.04364212342471737\n",
      "Train Loss at iteration 4865: 0.04364194737390941\n",
      "Train Loss at iteration 4866: 0.043641771350779596\n",
      "Train Loss at iteration 4867: 0.043641595355322346\n",
      "Train Loss at iteration 4868: 0.043641419387532146\n",
      "Train Loss at iteration 4869: 0.04364124344740345\n",
      "Train Loss at iteration 4870: 0.04364106753493073\n",
      "Train Loss at iteration 4871: 0.04364089165010843\n",
      "Train Loss at iteration 4872: 0.04364071579293103\n",
      "Train Loss at iteration 4873: 0.043640539963393\n",
      "Train Loss at iteration 4874: 0.043640364161488795\n",
      "Train Loss at iteration 4875: 0.043640188387212904\n",
      "Train Loss at iteration 4876: 0.043640012640559785\n",
      "Train Loss at iteration 4877: 0.043639836921523914\n",
      "Train Loss at iteration 4878: 0.04363966123009978\n",
      "Train Loss at iteration 4879: 0.04363948556628184\n",
      "Train Loss at iteration 4880: 0.04363930993006459\n",
      "Train Loss at iteration 4881: 0.043639134321442494\n",
      "Train Loss at iteration 4882: 0.043638958740410044\n",
      "Train Loss at iteration 4883: 0.04363878318696171\n",
      "Train Loss at iteration 4884: 0.043638607661091997\n",
      "Train Loss at iteration 4885: 0.04363843216279538\n",
      "Train Loss at iteration 4886: 0.04363825669206635\n",
      "Train Loss at iteration 4887: 0.04363808124889938\n",
      "Train Loss at iteration 4888: 0.04363790583328899\n",
      "Train Loss at iteration 4889: 0.04363773044522966\n",
      "Train Loss at iteration 4890: 0.04363755508471588\n",
      "Train Loss at iteration 4891: 0.04363737975174215\n",
      "Train Loss at iteration 4892: 0.04363720444630296\n",
      "Train Loss at iteration 4893: 0.04363702916839282\n",
      "Train Loss at iteration 4894: 0.04363685391800624\n",
      "Train Loss at iteration 4895: 0.04363667869513769\n",
      "Train Loss at iteration 4896: 0.043636503499781695\n",
      "Train Loss at iteration 4897: 0.043636328331932754\n",
      "Train Loss at iteration 4898: 0.04363615319158539\n",
      "Train Loss at iteration 4899: 0.043635978078734095\n",
      "Train Loss at iteration 4900: 0.04363580299337338\n",
      "Train Loss at iteration 4901: 0.04363562793549775\n",
      "Train Loss at iteration 4902: 0.043635452905101735\n",
      "Train Loss at iteration 4903: 0.04363527790217985\n",
      "Train Loss at iteration 4904: 0.0436351029267266\n",
      "Train Loss at iteration 4905: 0.043634927978736505\n",
      "Train Loss at iteration 4906: 0.04363475305820408\n",
      "Train Loss at iteration 4907: 0.04363457816512386\n",
      "Train Loss at iteration 4908: 0.04363440329949035\n",
      "Train Loss at iteration 4909: 0.04363422846129809\n",
      "Train Loss at iteration 4910: 0.0436340536505416\n",
      "Train Loss at iteration 4911: 0.043633878867215405\n",
      "Train Loss at iteration 4912: 0.043633704111314044\n",
      "Train Loss at iteration 4913: 0.04363352938283201\n",
      "Train Loss at iteration 4914: 0.04363335468176389\n",
      "Train Loss at iteration 4915: 0.04363318000810418\n",
      "Train Loss at iteration 4916: 0.04363300536184743\n",
      "Train Loss at iteration 4917: 0.04363283074298817\n",
      "Train Loss at iteration 4918: 0.043632656151520934\n",
      "Train Loss at iteration 4919: 0.04363248158744027\n",
      "Train Loss at iteration 4920: 0.04363230705074073\n",
      "Train Loss at iteration 4921: 0.04363213254141684\n",
      "Train Loss at iteration 4922: 0.043631958059463134\n",
      "Train Loss at iteration 4923: 0.043631783604874186\n",
      "Train Loss at iteration 4924: 0.043631609177644526\n",
      "Train Loss at iteration 4925: 0.04363143477776871\n",
      "Train Loss at iteration 4926: 0.043631260405241276\n",
      "Train Loss at iteration 4927: 0.04363108606005679\n",
      "Train Loss at iteration 4928: 0.0436309117422098\n",
      "Train Loss at iteration 4929: 0.043630737451694865\n",
      "Train Loss at iteration 4930: 0.04363056318850654\n",
      "Train Loss at iteration 4931: 0.043630388952639376\n",
      "Train Loss at iteration 4932: 0.04363021474408796\n",
      "Train Loss at iteration 4933: 0.043630040562846806\n",
      "Train Loss at iteration 4934: 0.043629866408910506\n",
      "Train Loss at iteration 4935: 0.04362969228227363\n",
      "Train Loss at iteration 4936: 0.04362951818293073\n",
      "Train Loss at iteration 4937: 0.04362934411087639\n",
      "Train Loss at iteration 4938: 0.04362917006610515\n",
      "Train Loss at iteration 4939: 0.04362899604861162\n",
      "Train Loss at iteration 4940: 0.043628822058390325\n",
      "Train Loss at iteration 4941: 0.043628648095435894\n",
      "Train Loss at iteration 4942: 0.04362847415974286\n",
      "Train Loss at iteration 4943: 0.0436283002513058\n",
      "Train Loss at iteration 4944: 0.04362812637011931\n",
      "Train Loss at iteration 4945: 0.04362795251617799\n",
      "Train Loss at iteration 4946: 0.04362777868947636\n",
      "Train Loss at iteration 4947: 0.04362760489000907\n",
      "Train Loss at iteration 4948: 0.04362743111777064\n",
      "Train Loss at iteration 4949: 0.04362725737275571\n",
      "Train Loss at iteration 4950: 0.043627083654958834\n",
      "Train Loss at iteration 4951: 0.043626909964374634\n",
      "Train Loss at iteration 4952: 0.043626736300997664\n",
      "Train Loss at iteration 4953: 0.04362656266482255\n",
      "Train Loss at iteration 4954: 0.04362638905584385\n",
      "Train Loss at iteration 4955: 0.043626215474056186\n",
      "Train Loss at iteration 4956: 0.04362604191945415\n",
      "Train Loss at iteration 4957: 0.04362586839203232\n",
      "Train Loss at iteration 4958: 0.043625694891785326\n",
      "Train Loss at iteration 4959: 0.04362552141870775\n",
      "Train Loss at iteration 4960: 0.0436253479727942\n",
      "Train Loss at iteration 4961: 0.043625174554039275\n",
      "Train Loss at iteration 4962: 0.04362500116243759\n",
      "Train Loss at iteration 4963: 0.043624827797983746\n",
      "Train Loss at iteration 4964: 0.04362465446067237\n",
      "Train Loss at iteration 4965: 0.04362448115049803\n",
      "Train Loss at iteration 4966: 0.04362430786745538\n",
      "Train Loss at iteration 4967: 0.04362413461153901\n",
      "Train Loss at iteration 4968: 0.04362396138274355\n",
      "Train Loss at iteration 4969: 0.04362378818106361\n",
      "Train Loss at iteration 4970: 0.0436236150064938\n",
      "Train Loss at iteration 4971: 0.04362344185902875\n",
      "Train Loss at iteration 4972: 0.04362326873866307\n",
      "Train Loss at iteration 4973: 0.0436230956453914\n",
      "Train Loss at iteration 4974: 0.043622922579208356\n",
      "Train Loss at iteration 4975: 0.04362274954010856\n",
      "Train Loss at iteration 4976: 0.04362257652808663\n",
      "Train Loss at iteration 4977: 0.04362240354313722\n",
      "Train Loss at iteration 4978: 0.04362223058525494\n",
      "Train Loss at iteration 4979: 0.04362205765443442\n",
      "Train Loss at iteration 4980: 0.043621884750670296\n",
      "Train Loss at iteration 4981: 0.04362171187395723\n",
      "Train Loss at iteration 4982: 0.043621539024289814\n",
      "Train Loss at iteration 4983: 0.04362136620166271\n",
      "Train Loss at iteration 4984: 0.04362119340607055\n",
      "Train Loss at iteration 4985: 0.04362102063750799\n",
      "Train Loss at iteration 4986: 0.04362084789596966\n",
      "Train Loss at iteration 4987: 0.04362067518145018\n",
      "Train Loss at iteration 4988: 0.043620502493944245\n",
      "Train Loss at iteration 4989: 0.04362032983344646\n",
      "Train Loss at iteration 4990: 0.0436201571999515\n",
      "Train Loss at iteration 4991: 0.043619984593454\n",
      "Train Loss at iteration 4992: 0.04361981201394862\n",
      "Train Loss at iteration 4993: 0.04361963946142999\n",
      "Train Loss at iteration 4994: 0.04361946693589281\n",
      "Train Loss at iteration 4995: 0.04361929443733168\n",
      "Train Loss at iteration 4996: 0.04361912196574129\n",
      "Train Loss at iteration 4997: 0.04361894952111631\n",
      "Train Loss at iteration 4998: 0.04361877710345137\n",
      "Train Loss at iteration 4999: 0.043618604712741135\n",
      "Train Loss at iteration 5000: 0.04361843234898029\n",
      "Train Loss at iteration 5001: 0.04361826001216349\n",
      "Train Loss at iteration 5002: 0.04361808770228539\n",
      "Train Loss at iteration 5003: 0.04361791541934067\n",
      "Train Loss at iteration 5004: 0.043617743163323994\n",
      "Train Loss at iteration 5005: 0.043617570934230035\n",
      "Train Loss at iteration 5006: 0.043617398732053475\n",
      "Train Loss at iteration 5007: 0.04361722655678896\n",
      "Train Loss at iteration 5008: 0.043617054408431194\n",
      "Train Loss at iteration 5009: 0.04361688228697483\n",
      "Train Loss at iteration 5010: 0.04361671019241457\n",
      "Train Loss at iteration 5011: 0.043616538124745065\n",
      "Train Loss at iteration 5012: 0.043616366083961036\n",
      "Train Loss at iteration 5013: 0.043616194070057124\n",
      "Train Loss at iteration 5014: 0.04361602208302804\n",
      "Train Loss at iteration 5015: 0.04361585012286845\n",
      "Train Loss at iteration 5016: 0.04361567818957305\n",
      "Train Loss at iteration 5017: 0.04361550628313655\n",
      "Train Loss at iteration 5018: 0.04361533440355359\n",
      "Train Loss at iteration 5019: 0.04361516255081891\n",
      "Train Loss at iteration 5020: 0.043614990724927194\n",
      "Train Loss at iteration 5021: 0.0436148189258731\n",
      "Train Loss at iteration 5022: 0.04361464715365136\n",
      "Train Loss at iteration 5023: 0.04361447540825667\n",
      "Train Loss at iteration 5024: 0.04361430368968371\n",
      "Train Loss at iteration 5025: 0.0436141319979272\n",
      "Train Loss at iteration 5026: 0.043613960332981844\n",
      "Train Loss at iteration 5027: 0.04361378869484231\n",
      "Train Loss at iteration 5028: 0.04361361708350334\n",
      "Train Loss at iteration 5029: 0.043613445498959624\n",
      "Train Loss at iteration 5030: 0.04361327394120587\n",
      "Train Loss at iteration 5031: 0.043613102410236804\n",
      "Train Loss at iteration 5032: 0.04361293090604711\n",
      "Train Loss at iteration 5033: 0.0436127594286315\n",
      "Train Loss at iteration 5034: 0.0436125879779847\n",
      "Train Loss at iteration 5035: 0.04361241655410145\n",
      "Train Loss at iteration 5036: 0.04361224515697643\n",
      "Train Loss at iteration 5037: 0.04361207378660437\n",
      "Train Loss at iteration 5038: 0.043611902442979994\n",
      "Train Loss at iteration 5039: 0.043611731126098015\n",
      "Train Loss at iteration 5040: 0.043611559835953155\n",
      "Train Loss at iteration 5041: 0.04361138857254014\n",
      "Train Loss at iteration 5042: 0.04361121733585371\n",
      "Train Loss at iteration 5043: 0.043611046125888564\n",
      "Train Loss at iteration 5044: 0.043610874942639447\n",
      "Train Loss at iteration 5045: 0.043610703786101095\n",
      "Train Loss at iteration 5046: 0.04361053265626823\n",
      "Train Loss at iteration 5047: 0.04361036155313559\n",
      "Train Loss at iteration 5048: 0.04361019047669789\n",
      "Train Loss at iteration 5049: 0.04361001942694989\n",
      "Train Loss at iteration 5050: 0.04360984840388632\n",
      "Train Loss at iteration 5051: 0.0436096774075019\n",
      "Train Loss at iteration 5052: 0.04360950643779141\n",
      "Train Loss at iteration 5053: 0.043609335494749546\n",
      "Train Loss at iteration 5054: 0.04360916457837109\n",
      "Train Loss at iteration 5055: 0.04360899368865076\n",
      "Train Loss at iteration 5056: 0.043608822825583315\n",
      "Train Loss at iteration 5057: 0.04360865198916349\n",
      "Train Loss at iteration 5058: 0.043608481179386045\n",
      "Train Loss at iteration 5059: 0.043608310396245735\n",
      "Train Loss at iteration 5060: 0.04360813963973731\n",
      "Train Loss at iteration 5061: 0.0436079689098555\n",
      "Train Loss at iteration 5062: 0.04360779820659507\n",
      "Train Loss at iteration 5063: 0.0436076275299508\n",
      "Train Loss at iteration 5064: 0.04360745687991742\n",
      "Train Loss at iteration 5065: 0.043607286256489707\n",
      "Train Loss at iteration 5066: 0.0436071156596624\n",
      "Train Loss at iteration 5067: 0.04360694508943029\n",
      "Train Loss at iteration 5068: 0.04360677454578812\n",
      "Train Loss at iteration 5069: 0.04360660402873064\n",
      "Train Loss at iteration 5070: 0.04360643353825266\n",
      "Train Loss at iteration 5071: 0.0436062630743489\n",
      "Train Loss at iteration 5072: 0.04360609263701417\n",
      "Train Loss at iteration 5073: 0.04360592222624322\n",
      "Train Loss at iteration 5074: 0.04360575184203081\n",
      "Train Loss at iteration 5075: 0.04360558148437175\n",
      "Train Loss at iteration 5076: 0.043605411153260776\n",
      "Train Loss at iteration 5077: 0.043605240848692675\n",
      "Train Loss at iteration 5078: 0.04360507057066225\n",
      "Train Loss at iteration 5079: 0.04360490031916425\n",
      "Train Loss at iteration 5080: 0.04360473009419349\n",
      "Train Loss at iteration 5081: 0.043604559895744695\n",
      "Train Loss at iteration 5082: 0.04360438972381269\n",
      "Train Loss at iteration 5083: 0.04360421957839225\n",
      "Train Loss at iteration 5084: 0.04360404945947818\n",
      "Train Loss at iteration 5085: 0.043603879367065224\n",
      "Train Loss at iteration 5086: 0.04360370930114822\n",
      "Train Loss at iteration 5087: 0.04360353926172193\n",
      "Train Loss at iteration 5088: 0.043603369248781144\n",
      "Train Loss at iteration 5089: 0.043603199262320674\n",
      "Train Loss at iteration 5090: 0.04360302930233532\n",
      "Train Loss at iteration 5091: 0.043602859368819843\n",
      "Train Loss at iteration 5092: 0.04360268946176906\n",
      "Train Loss at iteration 5093: 0.04360251958117779\n",
      "Train Loss at iteration 5094: 0.043602349727040794\n",
      "Train Loss at iteration 5095: 0.04360217989935293\n",
      "Train Loss at iteration 5096: 0.043602010098108934\n",
      "Train Loss at iteration 5097: 0.04360184032330366\n",
      "Train Loss at iteration 5098: 0.0436016705749319\n",
      "Train Loss at iteration 5099: 0.04360150085298845\n",
      "Train Loss at iteration 5100: 0.04360133115746814\n",
      "Train Loss at iteration 5101: 0.04360116148836577\n",
      "Train Loss at iteration 5102: 0.04360099184567615\n",
      "Train Loss at iteration 5103: 0.0436008222293941\n",
      "Train Loss at iteration 5104: 0.04360065263951443\n",
      "Train Loss at iteration 5105: 0.04360048307603196\n",
      "Train Loss at iteration 5106: 0.04360031353894151\n",
      "Train Loss at iteration 5107: 0.0436001440282379\n",
      "Train Loss at iteration 5108: 0.04359997454391593\n",
      "Train Loss at iteration 5109: 0.04359980508597046\n",
      "Train Loss at iteration 5110: 0.043599635654396264\n",
      "Train Loss at iteration 5111: 0.04359946624918821\n",
      "Train Loss at iteration 5112: 0.043599296870341114\n",
      "Train Loss at iteration 5113: 0.043599127517849785\n",
      "Train Loss at iteration 5114: 0.04359895819170908\n",
      "Train Loss at iteration 5115: 0.043598788891913806\n",
      "Train Loss at iteration 5116: 0.043598619618458824\n",
      "Train Loss at iteration 5117: 0.04359845037133892\n",
      "Train Loss at iteration 5118: 0.043598281150548986\n",
      "Train Loss at iteration 5119: 0.04359811195608382\n",
      "Train Loss at iteration 5120: 0.043597942787938264\n",
      "Train Loss at iteration 5121: 0.04359777364610716\n",
      "Train Loss at iteration 5122: 0.043597604530585365\n",
      "Train Loss at iteration 5123: 0.0435974354413677\n",
      "Train Loss at iteration 5124: 0.04359726637844902\n",
      "Train Loss at iteration 5125: 0.04359709734182415\n",
      "Train Loss at iteration 5126: 0.043596928331487976\n",
      "Train Loss at iteration 5127: 0.0435967593474353\n",
      "Train Loss at iteration 5128: 0.04359659038966101\n",
      "Train Loss at iteration 5129: 0.04359642145815993\n",
      "Train Loss at iteration 5130: 0.043596252552926935\n",
      "Train Loss at iteration 5131: 0.04359608367395684\n",
      "Train Loss at iteration 5132: 0.043595914821244544\n",
      "Train Loss at iteration 5133: 0.04359574599478487\n",
      "Train Loss at iteration 5134: 0.0435955771945727\n",
      "Train Loss at iteration 5135: 0.043595408420602874\n",
      "Train Loss at iteration 5136: 0.043595239672870253\n",
      "Train Loss at iteration 5137: 0.04359507095136972\n",
      "Train Loss at iteration 5138: 0.04359490225609612\n",
      "Train Loss at iteration 5139: 0.04359473358704432\n",
      "Train Loss at iteration 5140: 0.04359456494420919\n",
      "Train Loss at iteration 5141: 0.04359439632758559\n",
      "Train Loss at iteration 5142: 0.043594227737168395\n",
      "Train Loss at iteration 5143: 0.043594059172952476\n",
      "Train Loss at iteration 5144: 0.0435938906349327\n",
      "Train Loss at iteration 5145: 0.043593722123103945\n",
      "Train Loss at iteration 5146: 0.04359355363746108\n",
      "Train Loss at iteration 5147: 0.04359338517799898\n",
      "Train Loss at iteration 5148: 0.043593216744712525\n",
      "Train Loss at iteration 5149: 0.043593048337596606\n",
      "Train Loss at iteration 5150: 0.04359287995664608\n",
      "Train Loss at iteration 5151: 0.043592711601855835\n",
      "Train Loss at iteration 5152: 0.04359254327322076\n",
      "Train Loss at iteration 5153: 0.04359237497073575\n",
      "Train Loss at iteration 5154: 0.043592206694395674\n",
      "Train Loss at iteration 5155: 0.04359203844419541\n",
      "Train Loss at iteration 5156: 0.04359187022012988\n",
      "Train Loss at iteration 5157: 0.04359170202219392\n",
      "Train Loss at iteration 5158: 0.043591533850382475\n",
      "Train Loss at iteration 5159: 0.04359136570469042\n",
      "Train Loss at iteration 5160: 0.04359119758511264\n",
      "Train Loss at iteration 5161: 0.04359102949164402\n",
      "Train Loss at iteration 5162: 0.04359086142427949\n",
      "Train Loss at iteration 5163: 0.04359069338301392\n",
      "Train Loss at iteration 5164: 0.04359052536784223\n",
      "Train Loss at iteration 5165: 0.043590357378759305\n",
      "Train Loss at iteration 5166: 0.043590189415760046\n",
      "Train Loss at iteration 5167: 0.04359002147883937\n",
      "Train Loss at iteration 5168: 0.04358985356799217\n",
      "Train Loss at iteration 5169: 0.04358968568321336\n",
      "Train Loss at iteration 5170: 0.04358951782449784\n",
      "Train Loss at iteration 5171: 0.043589349991840526\n",
      "Train Loss at iteration 5172: 0.04358918218523633\n",
      "Train Loss at iteration 5173: 0.04358901440468016\n",
      "Train Loss at iteration 5174: 0.043588846650166935\n",
      "Train Loss at iteration 5175: 0.04358867892169156\n",
      "Train Loss at iteration 5176: 0.043588511219248956\n",
      "Train Loss at iteration 5177: 0.043588343542834036\n",
      "Train Loss at iteration 5178: 0.043588175892441725\n",
      "Train Loss at iteration 5179: 0.043588008268066934\n",
      "Train Loss at iteration 5180: 0.04358784066970461\n",
      "Train Loss at iteration 5181: 0.04358767309734964\n",
      "Train Loss at iteration 5182: 0.04358750555099697\n",
      "Train Loss at iteration 5183: 0.04358733803064152\n",
      "Train Loss at iteration 5184: 0.04358717053627823\n",
      "Train Loss at iteration 5185: 0.043587003067902\n",
      "Train Loss at iteration 5186: 0.04358683562550779\n",
      "Train Loss at iteration 5187: 0.04358666820909052\n",
      "Train Loss at iteration 5188: 0.04358650081864511\n",
      "Train Loss at iteration 5189: 0.0435863334541665\n",
      "Train Loss at iteration 5190: 0.043586166115649645\n",
      "Train Loss at iteration 5191: 0.04358599880308946\n",
      "Train Loss at iteration 5192: 0.043585831516480884\n",
      "Train Loss at iteration 5193: 0.04358566425581886\n",
      "Train Loss at iteration 5194: 0.04358549702109834\n",
      "Train Loss at iteration 5195: 0.04358532981231425\n",
      "Train Loss at iteration 5196: 0.04358516262946154\n",
      "Train Loss at iteration 5197: 0.04358499547253517\n",
      "Train Loss at iteration 5198: 0.04358482834153006\n",
      "Train Loss at iteration 5199: 0.04358466123644117\n",
      "Train Loss at iteration 5200: 0.04358449415726344\n",
      "Train Loss at iteration 5201: 0.04358432710399184\n",
      "Train Loss at iteration 5202: 0.0435841600766213\n",
      "Train Loss at iteration 5203: 0.0435839930751468\n",
      "Train Loss at iteration 5204: 0.043583826099563266\n",
      "Train Loss at iteration 5205: 0.043583659149865665\n",
      "Train Loss at iteration 5206: 0.04358349222604895\n",
      "Train Loss at iteration 5207: 0.04358332532810809\n",
      "Train Loss at iteration 5208: 0.043583158456038044\n",
      "Train Loss at iteration 5209: 0.04358299160983377\n",
      "Train Loss at iteration 5210: 0.04358282478949022\n",
      "Train Loss at iteration 5211: 0.04358265799500236\n",
      "Train Loss at iteration 5212: 0.043582491226365185\n",
      "Train Loss at iteration 5213: 0.043582324483573624\n",
      "Train Loss at iteration 5214: 0.04358215776662266\n",
      "Train Loss at iteration 5215: 0.04358199107550727\n",
      "Train Loss at iteration 5216: 0.04358182441022242\n",
      "Train Loss at iteration 5217: 0.04358165777076305\n",
      "Train Loss at iteration 5218: 0.043581491157124196\n",
      "Train Loss at iteration 5219: 0.04358132456930079\n",
      "Train Loss at iteration 5220: 0.04358115800728782\n",
      "Train Loss at iteration 5221: 0.04358099147108025\n",
      "Train Loss at iteration 5222: 0.043580824960673094\n",
      "Train Loss at iteration 5223: 0.04358065847606128\n",
      "Train Loss at iteration 5224: 0.04358049201723983\n",
      "Train Loss at iteration 5225: 0.04358032558420373\n",
      "Train Loss at iteration 5226: 0.04358015917694792\n",
      "Train Loss at iteration 5227: 0.04357999279546744\n",
      "Train Loss at iteration 5228: 0.04357982643975724\n",
      "Train Loss at iteration 5229: 0.043579660109812325\n",
      "Train Loss at iteration 5230: 0.043579493805627686\n",
      "Train Loss at iteration 5231: 0.04357932752719829\n",
      "Train Loss at iteration 5232: 0.04357916127451916\n",
      "Train Loss at iteration 5233: 0.04357899504758528\n",
      "Train Loss at iteration 5234: 0.04357882884639164\n",
      "Train Loss at iteration 5235: 0.04357866267093324\n",
      "Train Loss at iteration 5236: 0.043578496521205067\n",
      "Train Loss at iteration 5237: 0.04357833039720214\n",
      "Train Loss at iteration 5238: 0.043578164298919454\n",
      "Train Loss at iteration 5239: 0.04357799822635199\n",
      "Train Loss at iteration 5240: 0.043577832179494784\n",
      "Train Loss at iteration 5241: 0.043577666158342805\n",
      "Train Loss at iteration 5242: 0.0435775001628911\n",
      "Train Loss at iteration 5243: 0.04357733419313463\n",
      "Train Loss at iteration 5244: 0.04357716824906844\n",
      "Train Loss at iteration 5245: 0.04357700233068752\n",
      "Train Loss at iteration 5246: 0.043576836437986904\n",
      "Train Loss at iteration 5247: 0.04357667057096158\n",
      "Train Loss at iteration 5248: 0.04357650472960657\n",
      "Train Loss at iteration 5249: 0.04357633891391688\n",
      "Train Loss at iteration 5250: 0.04357617312388755\n",
      "Train Loss at iteration 5251: 0.04357600735951358\n",
      "Train Loss at iteration 5252: 0.04357584162078999\n",
      "Train Loss at iteration 5253: 0.04357567590771182\n",
      "Train Loss at iteration 5254: 0.043575510220274054\n",
      "Train Loss at iteration 5255: 0.043575344558471736\n",
      "Train Loss at iteration 5256: 0.043575178922299916\n",
      "Train Loss at iteration 5257: 0.04357501331175357\n",
      "Train Loss at iteration 5258: 0.04357484772682777\n",
      "Train Loss at iteration 5259: 0.04357468216751751\n",
      "Train Loss at iteration 5260: 0.04357451663381784\n",
      "Train Loss at iteration 5261: 0.04357435112572379\n",
      "Train Loss at iteration 5262: 0.043574185643230394\n",
      "Train Loss at iteration 5263: 0.043574020186332676\n",
      "Train Loss at iteration 5264: 0.043573854755025676\n",
      "Train Loss at iteration 5265: 0.04357368934930442\n",
      "Train Loss at iteration 5266: 0.04357352396916396\n",
      "Train Loss at iteration 5267: 0.043573358614599336\n",
      "Train Loss at iteration 5268: 0.043573193285605584\n",
      "Train Loss at iteration 5269: 0.043573027982177756\n",
      "Train Loss at iteration 5270: 0.04357286270431088\n",
      "Train Loss at iteration 5271: 0.04357269745199999\n",
      "Train Loss at iteration 5272: 0.04357253222524015\n",
      "Train Loss at iteration 5273: 0.043572367024026414\n",
      "Train Loss at iteration 5274: 0.043572201848353816\n",
      "Train Loss at iteration 5275: 0.043572036698217416\n",
      "Train Loss at iteration 5276: 0.043571871573612246\n",
      "Train Loss at iteration 5277: 0.043571706474533387\n",
      "Train Loss at iteration 5278: 0.043571541400975876\n",
      "Train Loss at iteration 5279: 0.04357137635293476\n",
      "Train Loss at iteration 5280: 0.043571211330405105\n",
      "Train Loss at iteration 5281: 0.04357104633338198\n",
      "Train Loss at iteration 5282: 0.04357088136186042\n",
      "Train Loss at iteration 5283: 0.0435707164158355\n",
      "Train Loss at iteration 5284: 0.043570551495302284\n",
      "Train Loss at iteration 5285: 0.043570386600255834\n",
      "Train Loss at iteration 5286: 0.04357022173069121\n",
      "Train Loss at iteration 5287: 0.04357005688660348\n",
      "Train Loss at iteration 5288: 0.04356989206798769\n",
      "Train Loss at iteration 5289: 0.04356972727483894\n",
      "Train Loss at iteration 5290: 0.0435695625071523\n",
      "Train Loss at iteration 5291: 0.04356939776492281\n",
      "Train Loss at iteration 5292: 0.04356923304814557\n",
      "Train Loss at iteration 5293: 0.04356906835681564\n",
      "Train Loss at iteration 5294: 0.04356890369092809\n",
      "Train Loss at iteration 5295: 0.04356873905047802\n",
      "Train Loss at iteration 5296: 0.043568574435460486\n",
      "Train Loss at iteration 5297: 0.04356840984587057\n",
      "Train Loss at iteration 5298: 0.04356824528170335\n",
      "Train Loss at iteration 5299: 0.04356808074295392\n",
      "Train Loss at iteration 5300: 0.043567916229617344\n",
      "Train Loss at iteration 5301: 0.04356775174168872\n",
      "Train Loss at iteration 5302: 0.04356758727916312\n",
      "Train Loss at iteration 5303: 0.04356742284203566\n",
      "Train Loss at iteration 5304: 0.043567258430301396\n",
      "Train Loss at iteration 5305: 0.04356709404395543\n",
      "Train Loss at iteration 5306: 0.04356692968299285\n",
      "Train Loss at iteration 5307: 0.043566765347408755\n",
      "Train Loss at iteration 5308: 0.04356660103719822\n",
      "Train Loss at iteration 5309: 0.043566436752356365\n",
      "Train Loss at iteration 5310: 0.043566272492878255\n",
      "Train Loss at iteration 5311: 0.04356610825875902\n",
      "Train Loss at iteration 5312: 0.04356594404999375\n",
      "Train Loss at iteration 5313: 0.04356577986657752\n",
      "Train Loss at iteration 5314: 0.04356561570850544\n",
      "Train Loss at iteration 5315: 0.043565451575772636\n",
      "Train Loss at iteration 5316: 0.0435652874683742\n",
      "Train Loss at iteration 5317: 0.04356512338630523\n",
      "Train Loss at iteration 5318: 0.0435649593295608\n",
      "Train Loss at iteration 5319: 0.043564795298136096\n",
      "Train Loss at iteration 5320: 0.04356463129202616\n",
      "Train Loss at iteration 5321: 0.04356446731122613\n",
      "Train Loss at iteration 5322: 0.04356430335573111\n",
      "Train Loss at iteration 5323: 0.04356413942553622\n",
      "Train Loss at iteration 5324: 0.04356397552063655\n",
      "Train Loss at iteration 5325: 0.043563811641027246\n",
      "Train Loss at iteration 5326: 0.043563647786703415\n",
      "Train Loss at iteration 5327: 0.04356348395766015\n",
      "Train Loss at iteration 5328: 0.043563320153892617\n",
      "Train Loss at iteration 5329: 0.043563156375395894\n",
      "Train Loss at iteration 5330: 0.04356299262216513\n",
      "Train Loss at iteration 5331: 0.043562828894195424\n",
      "Train Loss at iteration 5332: 0.04356266519148192\n",
      "Train Loss at iteration 5333: 0.043562501514019726\n",
      "Train Loss at iteration 5334: 0.043562337861803996\n",
      "Train Loss at iteration 5335: 0.04356217423482983\n",
      "Train Loss at iteration 5336: 0.043562010633092375\n",
      "Train Loss at iteration 5337: 0.04356184705658676\n",
      "Train Loss at iteration 5338: 0.04356168350530809\n",
      "Train Loss at iteration 5339: 0.04356151997925154\n",
      "Train Loss at iteration 5340: 0.043561356478412215\n",
      "Train Loss at iteration 5341: 0.04356119300278526\n",
      "Train Loss at iteration 5342: 0.043561029552365814\n",
      "Train Loss at iteration 5343: 0.04356086612714901\n",
      "Train Loss at iteration 5344: 0.04356070272713\n",
      "Train Loss at iteration 5345: 0.04356053935230392\n",
      "Train Loss at iteration 5346: 0.043560376002665886\n",
      "Train Loss at iteration 5347: 0.04356021267821108\n",
      "Train Loss at iteration 5348: 0.043560049378934616\n",
      "Train Loss at iteration 5349: 0.04355988610483166\n",
      "Train Loss at iteration 5350: 0.04355972285589736\n",
      "Train Loss at iteration 5351: 0.043559559632126855\n",
      "Train Loss at iteration 5352: 0.0435593964335153\n",
      "Train Loss at iteration 5353: 0.04355923326005783\n",
      "Train Loss at iteration 5354: 0.04355907011174963\n",
      "Train Loss at iteration 5355: 0.04355890698858582\n",
      "Train Loss at iteration 5356: 0.04355874389056159\n",
      "Train Loss at iteration 5357: 0.04355858081767206\n",
      "Train Loss at iteration 5358: 0.04355841776991241\n",
      "Train Loss at iteration 5359: 0.04355825474727778\n",
      "Train Loss at iteration 5360: 0.043558091749763356\n",
      "Train Loss at iteration 5361: 0.04355792877736428\n",
      "Train Loss at iteration 5362: 0.043557765830075715\n",
      "Train Loss at iteration 5363: 0.04355760290789283\n",
      "Train Loss at iteration 5364: 0.043557440010810806\n",
      "Train Loss at iteration 5365: 0.04355727713882478\n",
      "Train Loss at iteration 5366: 0.043557114291929946\n",
      "Train Loss at iteration 5367: 0.04355695147012144\n",
      "Train Loss at iteration 5368: 0.04355678867339447\n",
      "Train Loss at iteration 5369: 0.04355662590174418\n",
      "Train Loss at iteration 5370: 0.04355646315516575\n",
      "Train Loss at iteration 5371: 0.04355630043365437\n",
      "Train Loss at iteration 5372: 0.04355613773720519\n",
      "Train Loss at iteration 5373: 0.04355597506581341\n",
      "Train Loss at iteration 5374: 0.043555812419474184\n",
      "Train Loss at iteration 5375: 0.0435556497981827\n",
      "Train Loss at iteration 5376: 0.04355548720193417\n",
      "Train Loss at iteration 5377: 0.043555324630723716\n",
      "Train Loss at iteration 5378: 0.043555162084546566\n",
      "Train Loss at iteration 5379: 0.04355499956339789\n",
      "Train Loss at iteration 5380: 0.043554837067272874\n",
      "Train Loss at iteration 5381: 0.043554674596166694\n",
      "Train Loss at iteration 5382: 0.04355451215007456\n",
      "Train Loss at iteration 5383: 0.04355434972899166\n",
      "Train Loss at iteration 5384: 0.04355418733291314\n",
      "Train Loss at iteration 5385: 0.04355402496183426\n",
      "Train Loss at iteration 5386: 0.04355386261575016\n",
      "Train Loss at iteration 5387: 0.04355370029465606\n",
      "Train Loss at iteration 5388: 0.043553537998547144\n",
      "Train Loss at iteration 5389: 0.04355337572741861\n",
      "Train Loss at iteration 5390: 0.043553213481265673\n",
      "Train Loss at iteration 5391: 0.04355305126008349\n",
      "Train Loss at iteration 5392: 0.04355288906386733\n",
      "Train Loss at iteration 5393: 0.04355272689261233\n",
      "Train Loss at iteration 5394: 0.04355256474631372\n",
      "Train Loss at iteration 5395: 0.04355240262496671\n",
      "Train Loss at iteration 5396: 0.04355224052856648\n",
      "Train Loss at iteration 5397: 0.043552078457108276\n",
      "Train Loss at iteration 5398: 0.04355191641058726\n",
      "Train Loss at iteration 5399: 0.043551754388998686\n",
      "Train Loss at iteration 5400: 0.04355159239233776\n",
      "Train Loss at iteration 5401: 0.04355143042059964\n",
      "Train Loss at iteration 5402: 0.043551268473779596\n",
      "Train Loss at iteration 5403: 0.04355110655187281\n",
      "Train Loss at iteration 5404: 0.04355094465487453\n",
      "Train Loss at iteration 5405: 0.04355078278277994\n",
      "Train Loss at iteration 5406: 0.04355062093558429\n",
      "Train Loss at iteration 5407: 0.04355045911328276\n",
      "Train Loss at iteration 5408: 0.043550297315870595\n",
      "Train Loss at iteration 5409: 0.04355013554334303\n",
      "Train Loss at iteration 5410: 0.043549973795695265\n",
      "Train Loss at iteration 5411: 0.04354981207292252\n",
      "Train Loss at iteration 5412: 0.04354965037502004\n",
      "Train Loss at iteration 5413: 0.043549488701983045\n",
      "Train Loss at iteration 5414: 0.04354932705380675\n",
      "Train Loss at iteration 5415: 0.0435491654304864\n",
      "Train Loss at iteration 5416: 0.04354900383201723\n",
      "Train Loss at iteration 5417: 0.04354884225839445\n",
      "Train Loss at iteration 5418: 0.043548680709613326\n",
      "Train Loss at iteration 5419: 0.04354851918566904\n",
      "Train Loss at iteration 5420: 0.0435483576865569\n",
      "Train Loss at iteration 5421: 0.04354819621227207\n",
      "Train Loss at iteration 5422: 0.04354803476280984\n",
      "Train Loss at iteration 5423: 0.04354787333816542\n",
      "Train Loss at iteration 5424: 0.04354771193833406\n",
      "Train Loss at iteration 5425: 0.043547550563311\n",
      "Train Loss at iteration 5426: 0.04354738921309149\n",
      "Train Loss at iteration 5427: 0.043547227887670764\n",
      "Train Loss at iteration 5428: 0.04354706658704407\n",
      "Train Loss at iteration 5429: 0.04354690531120666\n",
      "Train Loss at iteration 5430: 0.04354674406015378\n",
      "Train Loss at iteration 5431: 0.04354658283388068\n",
      "Train Loss at iteration 5432: 0.04354642163238261\n",
      "Train Loss at iteration 5433: 0.043546260455654816\n",
      "Train Loss at iteration 5434: 0.04354609930369255\n",
      "Train Loss at iteration 5435: 0.043545938176491085\n",
      "Train Loss at iteration 5436: 0.043545777074045645\n",
      "Train Loss at iteration 5437: 0.04354561599635151\n",
      "Train Loss at iteration 5438: 0.04354545494340393\n",
      "Train Loss at iteration 5439: 0.043545293915198166\n",
      "Train Loss at iteration 5440: 0.043545132911729484\n",
      "Train Loss at iteration 5441: 0.043544971932993136\n",
      "Train Loss at iteration 5442: 0.043544810978984376\n",
      "Train Loss at iteration 5443: 0.0435446500496985\n",
      "Train Loss at iteration 5444: 0.04354448914513073\n",
      "Train Loss at iteration 5445: 0.04354432826527637\n",
      "Train Loss at iteration 5446: 0.043544167410130674\n",
      "Train Loss at iteration 5447: 0.04354400657968891\n",
      "Train Loss at iteration 5448: 0.043543845773946335\n",
      "Train Loss at iteration 5449: 0.04354368499289824\n",
      "Train Loss at iteration 5450: 0.04354352423653989\n",
      "Train Loss at iteration 5451: 0.04354336350486657\n",
      "Train Loss at iteration 5452: 0.04354320279787353\n",
      "Train Loss at iteration 5453: 0.043543042115556055\n",
      "Train Loss at iteration 5454: 0.043542881457909445\n",
      "Train Loss at iteration 5455: 0.043542720824928945\n",
      "Train Loss at iteration 5456: 0.04354256021660986\n",
      "Train Loss at iteration 5457: 0.04354239963294747\n",
      "Train Loss at iteration 5458: 0.04354223907393704\n",
      "Train Loss at iteration 5459: 0.04354207853957387\n",
      "Train Loss at iteration 5460: 0.04354191802985322\n",
      "Train Loss at iteration 5461: 0.04354175754477041\n",
      "Train Loss at iteration 5462: 0.043541597084320705\n",
      "Train Loss at iteration 5463: 0.0435414366484994\n",
      "Train Loss at iteration 5464: 0.04354127623730178\n",
      "Train Loss at iteration 5465: 0.04354111585072316\n",
      "Train Loss at iteration 5466: 0.04354095548875879\n",
      "Train Loss at iteration 5467: 0.04354079515140398\n",
      "Train Loss at iteration 5468: 0.043540634838654044\n",
      "Train Loss at iteration 5469: 0.043540474550504256\n",
      "Train Loss at iteration 5470: 0.043540314286949926\n",
      "Train Loss at iteration 5471: 0.043540154047986356\n",
      "Train Loss at iteration 5472: 0.04353999383360882\n",
      "Train Loss at iteration 5473: 0.04353983364381263\n",
      "Train Loss at iteration 5474: 0.04353967347859311\n",
      "Train Loss at iteration 5475: 0.04353951333794554\n",
      "Train Loss at iteration 5476: 0.043539353221865223\n",
      "Train Loss at iteration 5477: 0.04353919313034747\n",
      "Train Loss at iteration 5478: 0.0435390330633876\n",
      "Train Loss at iteration 5479: 0.043538873020980896\n",
      "Train Loss at iteration 5480: 0.04353871300312268\n",
      "Train Loss at iteration 5481: 0.04353855300980827\n",
      "Train Loss at iteration 5482: 0.04353839304103298\n",
      "Train Loss at iteration 5483: 0.04353823309679209\n",
      "Train Loss at iteration 5484: 0.04353807317708096\n",
      "Train Loss at iteration 5485: 0.043537913281894866\n",
      "Train Loss at iteration 5486: 0.04353775341122914\n",
      "Train Loss at iteration 5487: 0.043537593565079115\n",
      "Train Loss at iteration 5488: 0.04353743374344008\n",
      "Train Loss at iteration 5489: 0.04353727394630737\n",
      "Train Loss at iteration 5490: 0.043537114173676304\n",
      "Train Loss at iteration 5491: 0.04353695442554222\n",
      "Train Loss at iteration 5492: 0.04353679470190041\n",
      "Train Loss at iteration 5493: 0.043536635002746224\n",
      "Train Loss at iteration 5494: 0.04353647532807498\n",
      "Train Loss at iteration 5495: 0.043536315677882\n",
      "Train Loss at iteration 5496: 0.04353615605216262\n",
      "Train Loss at iteration 5497: 0.043535996450912176\n",
      "Train Loss at iteration 5498: 0.04353583687412598\n",
      "Train Loss at iteration 5499: 0.043535677321799374\n",
      "Train Loss at iteration 5500: 0.04353551779392769\n",
      "Train Loss at iteration 5501: 0.04353535829050628\n",
      "Train Loss at iteration 5502: 0.043535198811530444\n",
      "Train Loss at iteration 5503: 0.04353503935699555\n",
      "Train Loss at iteration 5504: 0.04353487992689691\n",
      "Train Loss at iteration 5505: 0.04353472052122989\n",
      "Train Loss at iteration 5506: 0.04353456113998981\n",
      "Train Loss at iteration 5507: 0.04353440178317203\n",
      "Train Loss at iteration 5508: 0.04353424245077186\n",
      "Train Loss at iteration 5509: 0.043534083142784674\n",
      "Train Loss at iteration 5510: 0.04353392385920583\n",
      "Train Loss at iteration 5511: 0.043533764600030625\n",
      "Train Loss at iteration 5512: 0.043533605365254455\n",
      "Train Loss at iteration 5513: 0.04353344615487263\n",
      "Train Loss at iteration 5514: 0.043533286968880536\n",
      "Train Loss at iteration 5515: 0.04353312780727349\n",
      "Train Loss at iteration 5516: 0.04353296867004687\n",
      "Train Loss at iteration 5517: 0.04353280955719602\n",
      "Train Loss at iteration 5518: 0.0435326504687163\n",
      "Train Loss at iteration 5519: 0.04353249140460306\n",
      "Train Loss at iteration 5520: 0.04353233236485165\n",
      "Train Loss at iteration 5521: 0.043532173349457436\n",
      "Train Loss at iteration 5522: 0.04353201435841578\n",
      "Train Loss at iteration 5523: 0.04353185539172205\n",
      "Train Loss at iteration 5524: 0.04353169644937159\n",
      "Train Loss at iteration 5525: 0.043531537531359764\n",
      "Train Loss at iteration 5526: 0.04353137863768195\n",
      "Train Loss at iteration 5527: 0.043531219768333504\n",
      "Train Loss at iteration 5528: 0.0435310609233098\n",
      "Train Loss at iteration 5529: 0.04353090210260622\n",
      "Train Loss at iteration 5530: 0.04353074330621808\n",
      "Train Loss at iteration 5531: 0.0435305845341408\n",
      "Train Loss at iteration 5532: 0.043530425786369745\n",
      "Train Loss at iteration 5533: 0.043530267062900266\n",
      "Train Loss at iteration 5534: 0.04353010836372775\n",
      "Train Loss at iteration 5535: 0.04352994968884756\n",
      "Train Loss at iteration 5536: 0.0435297910382551\n",
      "Train Loss at iteration 5537: 0.04352963241194573\n",
      "Train Loss at iteration 5538: 0.04352947380991482\n",
      "Train Loss at iteration 5539: 0.04352931523215777\n",
      "Train Loss at iteration 5540: 0.04352915667866994\n",
      "Train Loss at iteration 5541: 0.043528998149446725\n",
      "Train Loss at iteration 5542: 0.04352883964448351\n",
      "Train Loss at iteration 5543: 0.04352868116377565\n",
      "Train Loss at iteration 5544: 0.04352852270731858\n",
      "Train Loss at iteration 5545: 0.04352836427510765\n",
      "Train Loss at iteration 5546: 0.04352820586713827\n",
      "Train Loss at iteration 5547: 0.04352804748340581\n",
      "Train Loss at iteration 5548: 0.04352788912390565\n",
      "Train Loss at iteration 5549: 0.04352773078863322\n",
      "Train Loss at iteration 5550: 0.04352757247758389\n",
      "Train Loss at iteration 5551: 0.04352741419075305\n",
      "Train Loss at iteration 5552: 0.0435272559281361\n",
      "Train Loss at iteration 5553: 0.043527097689728435\n",
      "Train Loss at iteration 5554: 0.04352693947552547\n",
      "Train Loss at iteration 5555: 0.04352678128552257\n",
      "Train Loss at iteration 5556: 0.04352662311971516\n",
      "Train Loss at iteration 5557: 0.04352646497809864\n",
      "Train Loss at iteration 5558: 0.0435263068606684\n",
      "Train Loss at iteration 5559: 0.04352614876741983\n",
      "Train Loss at iteration 5560: 0.04352599069834837\n",
      "Train Loss at iteration 5561: 0.04352583265344942\n",
      "Train Loss at iteration 5562: 0.04352567463271836\n",
      "Train Loss at iteration 5563: 0.04352551663615062\n",
      "Train Loss at iteration 5564: 0.04352535866374159\n",
      "Train Loss at iteration 5565: 0.043525200715486684\n",
      "Train Loss at iteration 5566: 0.04352504279138134\n",
      "Train Loss at iteration 5567: 0.043524884891420944\n",
      "Train Loss at iteration 5568: 0.04352472701560093\n",
      "Train Loss at iteration 5569: 0.04352456916391668\n",
      "Train Loss at iteration 5570: 0.04352441133636365\n",
      "Train Loss at iteration 5571: 0.043524253532937214\n",
      "Train Loss at iteration 5572: 0.04352409575363282\n",
      "Train Loss at iteration 5573: 0.04352393799844589\n",
      "Train Loss at iteration 5574: 0.04352378026737183\n",
      "Train Loss at iteration 5575: 0.04352362256040608\n",
      "Train Loss at iteration 5576: 0.043523464877544035\n",
      "Train Loss at iteration 5577: 0.043523307218781136\n",
      "Train Loss at iteration 5578: 0.043523149584112804\n",
      "Train Loss at iteration 5579: 0.043522991973534486\n",
      "Train Loss at iteration 5580: 0.04352283438704158\n",
      "Train Loss at iteration 5581: 0.04352267682462954\n",
      "Train Loss at iteration 5582: 0.04352251928629376\n",
      "Train Loss at iteration 5583: 0.043522361772029707\n",
      "Train Loss at iteration 5584: 0.0435222042818328\n",
      "Train Loss at iteration 5585: 0.043522046815698476\n",
      "Train Loss at iteration 5586: 0.04352188937362216\n",
      "Train Loss at iteration 5587: 0.0435217319555993\n",
      "Train Loss at iteration 5588: 0.04352157456162533\n",
      "Train Loss at iteration 5589: 0.04352141719169567\n",
      "Train Loss at iteration 5590: 0.0435212598458058\n",
      "Train Loss at iteration 5591: 0.04352110252395112\n",
      "Train Loss at iteration 5592: 0.043520945226127095\n",
      "Train Loss at iteration 5593: 0.04352078795232914\n",
      "Train Loss at iteration 5594: 0.04352063070255273\n",
      "Train Loss at iteration 5595: 0.0435204734767933\n",
      "Train Loss at iteration 5596: 0.043520316275046295\n",
      "Train Loss at iteration 5597: 0.04352015909730716\n",
      "Train Loss at iteration 5598: 0.043520001943571335\n",
      "Train Loss at iteration 5599: 0.0435198448138343\n",
      "Train Loss at iteration 5600: 0.04351968770809147\n",
      "Train Loss at iteration 5601: 0.043519530626338315\n",
      "Train Loss at iteration 5602: 0.04351937356857028\n",
      "Train Loss at iteration 5603: 0.04351921653478283\n",
      "Train Loss at iteration 5604: 0.04351905952497142\n",
      "Train Loss at iteration 5605: 0.04351890253913151\n",
      "Train Loss at iteration 5606: 0.04351874557725854\n",
      "Train Loss at iteration 5607: 0.04351858863934798\n",
      "Train Loss at iteration 5608: 0.04351843172539529\n",
      "Train Loss at iteration 5609: 0.04351827483539592\n",
      "Train Loss at iteration 5610: 0.04351811796934535\n",
      "Train Loss at iteration 5611: 0.04351796112723902\n",
      "Train Loss at iteration 5612: 0.04351780430907244\n",
      "Train Loss at iteration 5613: 0.04351764751484102\n",
      "Train Loss at iteration 5614: 0.043517490744540255\n",
      "Train Loss at iteration 5615: 0.04351733399816562\n",
      "Train Loss at iteration 5616: 0.043517177275712575\n",
      "Train Loss at iteration 5617: 0.043517020577176574\n",
      "Train Loss at iteration 5618: 0.04351686390255311\n",
      "Train Loss at iteration 5619: 0.043516707251837664\n",
      "Train Loss at iteration 5620: 0.043516550625025685\n",
      "Train Loss at iteration 5621: 0.04351639402211267\n",
      "Train Loss at iteration 5622: 0.04351623744309407\n",
      "Train Loss at iteration 5623: 0.04351608088796538\n",
      "Train Loss at iteration 5624: 0.04351592435672207\n",
      "Train Loss at iteration 5625: 0.04351576784935963\n",
      "Train Loss at iteration 5626: 0.04351561136587353\n",
      "Train Loss at iteration 5627: 0.04351545490625926\n",
      "Train Loss at iteration 5628: 0.04351529847051229\n",
      "Train Loss at iteration 5629: 0.04351514205862812\n",
      "Train Loss at iteration 5630: 0.04351498567060224\n",
      "Train Loss at iteration 5631: 0.04351482930643011\n",
      "Train Loss at iteration 5632: 0.043514672966107226\n",
      "Train Loss at iteration 5633: 0.04351451664962909\n",
      "Train Loss at iteration 5634: 0.043514360356991175\n",
      "Train Loss at iteration 5635: 0.04351420408818899\n",
      "Train Loss at iteration 5636: 0.04351404784321802\n",
      "Train Loss at iteration 5637: 0.04351389162207376\n",
      "Train Loss at iteration 5638: 0.04351373542475169\n",
      "Train Loss at iteration 5639: 0.04351357925124732\n",
      "Train Loss at iteration 5640: 0.043513423101556134\n",
      "Train Loss at iteration 5641: 0.04351326697567364\n",
      "Train Loss at iteration 5642: 0.043513110873595336\n",
      "Train Loss at iteration 5643: 0.043512954795316715\n",
      "Train Loss at iteration 5644: 0.043512798740833296\n",
      "Train Loss at iteration 5645: 0.04351264271014056\n",
      "Train Loss at iteration 5646: 0.04351248670323402\n",
      "Train Loss at iteration 5647: 0.04351233072010918\n",
      "Train Loss at iteration 5648: 0.04351217476076154\n",
      "Train Loss at iteration 5649: 0.04351201882518662\n",
      "Train Loss at iteration 5650: 0.04351186291337991\n",
      "Train Loss at iteration 5651: 0.04351170702533692\n",
      "Train Loss at iteration 5652: 0.043511551161053194\n",
      "Train Loss at iteration 5653: 0.0435113953205242\n",
      "Train Loss at iteration 5654: 0.04351123950374547\n",
      "Train Loss at iteration 5655: 0.04351108371071252\n",
      "Train Loss at iteration 5656: 0.043510927941420856\n",
      "Train Loss at iteration 5657: 0.043510772195866\n",
      "Train Loss at iteration 5658: 0.043510616474043445\n",
      "Train Loss at iteration 5659: 0.04351046077594876\n",
      "Train Loss at iteration 5660: 0.04351030510157743\n",
      "Train Loss at iteration 5661: 0.043510149450924976\n",
      "Train Loss at iteration 5662: 0.04350999382398692\n",
      "Train Loss at iteration 5663: 0.04350983822075878\n",
      "Train Loss at iteration 5664: 0.0435096826412361\n",
      "Train Loss at iteration 5665: 0.04350952708541438\n",
      "Train Loss at iteration 5666: 0.043509371553289156\n",
      "Train Loss at iteration 5667: 0.04350921604485596\n",
      "Train Loss at iteration 5668: 0.04350906056011033\n",
      "Train Loss at iteration 5669: 0.04350890509904777\n",
      "Train Loss at iteration 5670: 0.04350874966166381\n",
      "Train Loss at iteration 5671: 0.043508594247954016\n",
      "Train Loss at iteration 5672: 0.04350843885791388\n",
      "Train Loss at iteration 5673: 0.04350828349153897\n",
      "Train Loss at iteration 5674: 0.04350812814882479\n",
      "Train Loss at iteration 5675: 0.0435079728297669\n",
      "Train Loss at iteration 5676: 0.043507817534360824\n",
      "Train Loss at iteration 5677: 0.0435076622626021\n",
      "Train Loss at iteration 5678: 0.04350750701448627\n",
      "Train Loss at iteration 5679: 0.04350735179000888\n",
      "Train Loss at iteration 5680: 0.043507196589165464\n",
      "Train Loss at iteration 5681: 0.043507041411951566\n",
      "Train Loss at iteration 5682: 0.04350688625836274\n",
      "Train Loss at iteration 5683: 0.04350673112839451\n",
      "Train Loss at iteration 5684: 0.04350657602204245\n",
      "Train Loss at iteration 5685: 0.043506420939302064\n",
      "Train Loss at iteration 5686: 0.04350626588016894\n",
      "Train Loss at iteration 5687: 0.04350611084463862\n",
      "Train Loss at iteration 5688: 0.04350595583270664\n",
      "Train Loss at iteration 5689: 0.04350580084436856\n",
      "Train Loss at iteration 5690: 0.04350564587961994\n",
      "Train Loss at iteration 5691: 0.043505490938456313\n",
      "Train Loss at iteration 5692: 0.04350533602087326\n",
      "Train Loss at iteration 5693: 0.04350518112686633\n",
      "Train Loss at iteration 5694: 0.043505026256431045\n",
      "Train Loss at iteration 5695: 0.043504871409563016\n",
      "Train Loss at iteration 5696: 0.04350471658625777\n",
      "Train Loss at iteration 5697: 0.04350456178651088\n",
      "Train Loss at iteration 5698: 0.04350440701031791\n",
      "Train Loss at iteration 5699: 0.04350425225767441\n",
      "Train Loss at iteration 5700: 0.04350409752857594\n",
      "Train Loss at iteration 5701: 0.043503942823018074\n",
      "Train Loss at iteration 5702: 0.0435037881409964\n",
      "Train Loss at iteration 5703: 0.04350363348250644\n",
      "Train Loss at iteration 5704: 0.043503478847543804\n",
      "Train Loss at iteration 5705: 0.04350332423610402\n",
      "Train Loss at iteration 5706: 0.0435031696481827\n",
      "Train Loss at iteration 5707: 0.043503015083775395\n",
      "Train Loss at iteration 5708: 0.043502860542877685\n",
      "Train Loss at iteration 5709: 0.04350270602548512\n",
      "Train Loss at iteration 5710: 0.043502551531593305\n",
      "Train Loss at iteration 5711: 0.043502397061197795\n",
      "Train Loss at iteration 5712: 0.043502242614294175\n",
      "Train Loss at iteration 5713: 0.04350208819087804\n",
      "Train Loss at iteration 5714: 0.043501933790944934\n",
      "Train Loss at iteration 5715: 0.04350177941449045\n",
      "Train Loss at iteration 5716: 0.04350162506151019\n",
      "Train Loss at iteration 5717: 0.04350147073199972\n",
      "Train Loss at iteration 5718: 0.043501316425954624\n",
      "Train Loss at iteration 5719: 0.04350116214337049\n",
      "Train Loss at iteration 5720: 0.043501007884242895\n",
      "Train Loss at iteration 5721: 0.04350085364856744\n",
      "Train Loss at iteration 5722: 0.04350069943633969\n",
      "Train Loss at iteration 5723: 0.04350054524755526\n",
      "Train Loss at iteration 5724: 0.043500391082209725\n",
      "Train Loss at iteration 5725: 0.043500236940298684\n",
      "Train Loss at iteration 5726: 0.04350008282181773\n",
      "Train Loss at iteration 5727: 0.04349992872676244\n",
      "Train Loss at iteration 5728: 0.04349977465512842\n",
      "Train Loss at iteration 5729: 0.043499620606911295\n",
      "Train Loss at iteration 5730: 0.0434994665821066\n",
      "Train Loss at iteration 5731: 0.043499312580709985\n",
      "Train Loss at iteration 5732: 0.043499158602717025\n",
      "Train Loss at iteration 5733: 0.04349900464812332\n",
      "Train Loss at iteration 5734: 0.043498850716924484\n",
      "Train Loss at iteration 5735: 0.043498696809116104\n",
      "Train Loss at iteration 5736: 0.04349854292469379\n",
      "Train Loss at iteration 5737: 0.04349838906365315\n",
      "Train Loss at iteration 5738: 0.04349823522598978\n",
      "Train Loss at iteration 5739: 0.0434980814116993\n",
      "Train Loss at iteration 5740: 0.04349792762077731\n",
      "Train Loss at iteration 5741: 0.04349777385321942\n",
      "Train Loss at iteration 5742: 0.04349762010902122\n",
      "Train Loss at iteration 5743: 0.04349746638817836\n",
      "Train Loss at iteration 5744: 0.043497312690686414\n",
      "Train Loss at iteration 5745: 0.04349715901654103\n",
      "Train Loss at iteration 5746: 0.04349700536573779\n",
      "Train Loss at iteration 5747: 0.043496851738272335\n",
      "Train Loss at iteration 5748: 0.04349669813414027\n",
      "Train Loss at iteration 5749: 0.04349654455333721\n",
      "Train Loss at iteration 5750: 0.04349639099585877\n",
      "Train Loss at iteration 5751: 0.043496237461700574\n",
      "Train Loss at iteration 5752: 0.043496083950858265\n",
      "Train Loss at iteration 5753: 0.04349593046332743\n",
      "Train Loss at iteration 5754: 0.0434957769991037\n",
      "Train Loss at iteration 5755: 0.0434956235581827\n",
      "Train Loss at iteration 5756: 0.04349547014056007\n",
      "Train Loss at iteration 5757: 0.04349531674623143\n",
      "Train Loss at iteration 5758: 0.043495163375192394\n",
      "Train Loss at iteration 5759: 0.04349501002743861\n",
      "Train Loss at iteration 5760: 0.04349485670296568\n",
      "Train Loss at iteration 5761: 0.04349470340176925\n",
      "Train Loss at iteration 5762: 0.04349455012384497\n",
      "Train Loss at iteration 5763: 0.04349439686918844\n",
      "Train Loss at iteration 5764: 0.04349424363779532\n",
      "Train Loss at iteration 5765: 0.043494090429661215\n",
      "Train Loss at iteration 5766: 0.0434939372447818\n",
      "Train Loss at iteration 5767: 0.04349378408315268\n",
      "Train Loss at iteration 5768: 0.0434936309447695\n",
      "Train Loss at iteration 5769: 0.043493477829627904\n",
      "Train Loss at iteration 5770: 0.043493324737723535\n",
      "Train Loss at iteration 5771: 0.04349317166905202\n",
      "Train Loss at iteration 5772: 0.04349301862360903\n",
      "Train Loss at iteration 5773: 0.04349286560139017\n",
      "Train Loss at iteration 5774: 0.04349271260239111\n",
      "Train Loss at iteration 5775: 0.04349255962660749\n",
      "Train Loss at iteration 5776: 0.043492406674034964\n",
      "Train Loss at iteration 5777: 0.04349225374466915\n",
      "Train Loss at iteration 5778: 0.043492100838505736\n",
      "Train Loss at iteration 5779: 0.043491947955540344\n",
      "Train Loss at iteration 5780: 0.04349179509576864\n",
      "Train Loss at iteration 5781: 0.04349164225918627\n",
      "Train Loss at iteration 5782: 0.04349148944578889\n",
      "Train Loss at iteration 5783: 0.04349133665557214\n",
      "Train Loss at iteration 5784: 0.043491183888531695\n",
      "Train Loss at iteration 5785: 0.0434910311446632\n",
      "Train Loss at iteration 5786: 0.04349087842396231\n",
      "Train Loss at iteration 5787: 0.0434907257264247\n",
      "Train Loss at iteration 5788: 0.04349057305204602\n",
      "Train Loss at iteration 5789: 0.04349042040082192\n",
      "Train Loss at iteration 5790: 0.04349026777274807\n",
      "Train Loss at iteration 5791: 0.04349011516782012\n",
      "Train Loss at iteration 5792: 0.04348996258603377\n",
      "Train Loss at iteration 5793: 0.04348981002738465\n",
      "Train Loss at iteration 5794: 0.04348965749186842\n",
      "Train Loss at iteration 5795: 0.043489504979480786\n",
      "Train Loss at iteration 5796: 0.043489352490217376\n",
      "Train Loss at iteration 5797: 0.04348920002407389\n",
      "Train Loss at iteration 5798: 0.04348904758104598\n",
      "Train Loss at iteration 5799: 0.04348889516112931\n",
      "Train Loss at iteration 5800: 0.043488742764319584\n",
      "Train Loss at iteration 5801: 0.04348859039061243\n",
      "Train Loss at iteration 5802: 0.043488438040003574\n",
      "Train Loss at iteration 5803: 0.043488285712488646\n",
      "Train Loss at iteration 5804: 0.04348813340806335\n",
      "Train Loss at iteration 5805: 0.04348798112672334\n",
      "Train Loss at iteration 5806: 0.04348782886846432\n",
      "Train Loss at iteration 5807: 0.043487676633281964\n",
      "Train Loss at iteration 5808: 0.04348752442117193\n",
      "Train Loss at iteration 5809: 0.04348737223212992\n",
      "Train Loss at iteration 5810: 0.04348722006615162\n",
      "Train Loss at iteration 5811: 0.04348706792323271\n",
      "Train Loss at iteration 5812: 0.043486915803368865\n",
      "Train Loss at iteration 5813: 0.043486763706555784\n",
      "Train Loss at iteration 5814: 0.04348661163278915\n",
      "Train Loss at iteration 5815: 0.04348645958206464\n",
      "Train Loss at iteration 5816: 0.04348630755437796\n",
      "Train Loss at iteration 5817: 0.043486155549724785\n",
      "Train Loss at iteration 5818: 0.043486003568100826\n",
      "Train Loss at iteration 5819: 0.043485851609501754\n",
      "Train Loss at iteration 5820: 0.043485699673923274\n",
      "Train Loss at iteration 5821: 0.04348554776136107\n",
      "Train Loss at iteration 5822: 0.04348539587181086\n",
      "Train Loss at iteration 5823: 0.04348524400526831\n",
      "Train Loss at iteration 5824: 0.04348509216172915\n",
      "Train Loss at iteration 5825: 0.043484940341189056\n",
      "Train Loss at iteration 5826: 0.04348478854364372\n",
      "Train Loss at iteration 5827: 0.04348463676908887\n",
      "Train Loss at iteration 5828: 0.0434844850175202\n",
      "Train Loss at iteration 5829: 0.04348433328893339\n",
      "Train Loss at iteration 5830: 0.04348418158332417\n",
      "Train Loss at iteration 5831: 0.04348402990068824\n",
      "Train Loss at iteration 5832: 0.0434838782410213\n",
      "Train Loss at iteration 5833: 0.04348372660431906\n",
      "Train Loss at iteration 5834: 0.043483574990577215\n",
      "Train Loss at iteration 5835: 0.0434834233997915\n",
      "Train Loss at iteration 5836: 0.043483271831957604\n",
      "Train Loss at iteration 5837: 0.043483120287071256\n",
      "Train Loss at iteration 5838: 0.04348296876512813\n",
      "Train Loss at iteration 5839: 0.04348281726612399\n",
      "Train Loss at iteration 5840: 0.043482665790054525\n",
      "Train Loss at iteration 5841: 0.043482514336915454\n",
      "Train Loss at iteration 5842: 0.0434823629067025\n",
      "Train Loss at iteration 5843: 0.043482211499411366\n",
      "Train Loss at iteration 5844: 0.043482060115037774\n",
      "Train Loss at iteration 5845: 0.04348190875357745\n",
      "Train Loss at iteration 5846: 0.043481757415026114\n",
      "Train Loss at iteration 5847: 0.04348160609937947\n",
      "Train Loss at iteration 5848: 0.04348145480663327\n",
      "Train Loss at iteration 5849: 0.04348130353678323\n",
      "Train Loss at iteration 5850: 0.04348115228982506\n",
      "Train Loss at iteration 5851: 0.04348100106575449\n",
      "Train Loss at iteration 5852: 0.04348084986456727\n",
      "Train Loss at iteration 5853: 0.043480698686259096\n",
      "Train Loss at iteration 5854: 0.043480547530825704\n",
      "Train Loss at iteration 5855: 0.043480396398262844\n",
      "Train Loss at iteration 5856: 0.04348024528856623\n",
      "Train Loss at iteration 5857: 0.04348009420173159\n",
      "Train Loss at iteration 5858: 0.04347994313775467\n",
      "Train Loss at iteration 5859: 0.0434797920966312\n",
      "Train Loss at iteration 5860: 0.04347964107835691\n",
      "Train Loss at iteration 5861: 0.04347949008292755\n",
      "Train Loss at iteration 5862: 0.043479339110338834\n",
      "Train Loss at iteration 5863: 0.043479188160586525\n",
      "Train Loss at iteration 5864: 0.04347903723366636\n",
      "Train Loss at iteration 5865: 0.04347888632957406\n",
      "Train Loss at iteration 5866: 0.043478735448305376\n",
      "Train Loss at iteration 5867: 0.043478584589856054\n",
      "Train Loss at iteration 5868: 0.04347843375422184\n",
      "Train Loss at iteration 5869: 0.04347828294139847\n",
      "Train Loss at iteration 5870: 0.043478132151381695\n",
      "Train Loss at iteration 5871: 0.04347798138416726\n",
      "Train Loss at iteration 5872: 0.0434778306397509\n",
      "Train Loss at iteration 5873: 0.043477679918128405\n",
      "Train Loss at iteration 5874: 0.04347752921929547\n",
      "Train Loss at iteration 5875: 0.04347737854324788\n",
      "Train Loss at iteration 5876: 0.04347722788998139\n",
      "Train Loss at iteration 5877: 0.04347707725949172\n",
      "Train Loss at iteration 5878: 0.04347692665177468\n",
      "Train Loss at iteration 5879: 0.04347677606682596\n",
      "Train Loss at iteration 5880: 0.04347662550464136\n",
      "Train Loss at iteration 5881: 0.04347647496521662\n",
      "Train Loss at iteration 5882: 0.0434763244485475\n",
      "Train Loss at iteration 5883: 0.04347617395462977\n",
      "Train Loss at iteration 5884: 0.04347602348345917\n",
      "Train Loss at iteration 5885: 0.04347587303503147\n",
      "Train Loss at iteration 5886: 0.043475722609342446\n",
      "Train Loss at iteration 5887: 0.04347557220638784\n",
      "Train Loss at iteration 5888: 0.043475421826163436\n",
      "Train Loss at iteration 5889: 0.043475271468664964\n",
      "Train Loss at iteration 5890: 0.04347512113388824\n",
      "Train Loss at iteration 5891: 0.04347497082182899\n",
      "Train Loss at iteration 5892: 0.04347482053248299\n",
      "Train Loss at iteration 5893: 0.04347467026584604\n",
      "Train Loss at iteration 5894: 0.04347452002191388\n",
      "Train Loss at iteration 5895: 0.04347436980068228\n",
      "Train Loss at iteration 5896: 0.043474219602147035\n",
      "Train Loss at iteration 5897: 0.0434740694263039\n",
      "Train Loss at iteration 5898: 0.043473919273148635\n",
      "Train Loss at iteration 5899: 0.04347376914267707\n",
      "Train Loss at iteration 5900: 0.04347361903488493\n",
      "Train Loss at iteration 5901: 0.043473468949768\n",
      "Train Loss at iteration 5902: 0.043473318887322074\n",
      "Train Loss at iteration 5903: 0.04347316884754292\n",
      "Train Loss at iteration 5904: 0.04347301883042633\n",
      "Train Loss at iteration 5905: 0.04347286883596808\n",
      "Train Loss at iteration 5906: 0.04347271886416395\n",
      "Train Loss at iteration 5907: 0.04347256891500972\n",
      "Train Loss at iteration 5908: 0.04347241898850117\n",
      "Train Loss at iteration 5909: 0.04347226908463411\n",
      "Train Loss at iteration 5910: 0.04347211920340431\n",
      "Train Loss at iteration 5911: 0.04347196934480755\n",
      "Train Loss at iteration 5912: 0.04347181950883964\n",
      "Train Loss at iteration 5913: 0.043471669695496366\n",
      "Train Loss at iteration 5914: 0.043471519904773485\n",
      "Train Loss at iteration 5915: 0.04347137013666683\n",
      "Train Loss at iteration 5916: 0.043471220391172176\n",
      "Train Loss at iteration 5917: 0.04347107066828532\n",
      "Train Loss at iteration 5918: 0.04347092096800205\n",
      "Train Loss at iteration 5919: 0.04347077129031817\n",
      "Train Loss at iteration 5920: 0.043470621635229474\n",
      "Train Loss at iteration 5921: 0.04347047200273177\n",
      "Train Loss at iteration 5922: 0.043470322392820844\n",
      "Train Loss at iteration 5923: 0.0434701728054925\n",
      "Train Loss at iteration 5924: 0.04347002324074252\n",
      "Train Loss at iteration 5925: 0.043469873698566745\n",
      "Train Loss at iteration 5926: 0.04346972417896095\n",
      "Train Loss at iteration 5927: 0.04346957468192095\n",
      "Train Loss at iteration 5928: 0.04346942520744254\n",
      "Train Loss at iteration 5929: 0.043469275755521515\n",
      "Train Loss at iteration 5930: 0.043469126326153726\n",
      "Train Loss at iteration 5931: 0.04346897691933494\n",
      "Train Loss at iteration 5932: 0.043468827535061\n",
      "Train Loss at iteration 5933: 0.04346867817332766\n",
      "Train Loss at iteration 5934: 0.04346852883413079\n",
      "Train Loss at iteration 5935: 0.04346837951746617\n",
      "Train Loss at iteration 5936: 0.04346823022332963\n",
      "Train Loss at iteration 5937: 0.04346808095171697\n",
      "Train Loss at iteration 5938: 0.04346793170262401\n",
      "Train Loss at iteration 5939: 0.04346778247604657\n",
      "Train Loss at iteration 5940: 0.04346763327198046\n",
      "Train Loss at iteration 5941: 0.0434674840904215\n",
      "Train Loss at iteration 5942: 0.04346733493136552\n",
      "Train Loss at iteration 5943: 0.04346718579480833\n",
      "Train Loss at iteration 5944: 0.04346703668074575\n",
      "Train Loss at iteration 5945: 0.0434668875891736\n",
      "Train Loss at iteration 5946: 0.04346673852008771\n",
      "Train Loss at iteration 5947: 0.0434665894734839\n",
      "Train Loss at iteration 5948: 0.043466440449358004\n",
      "Train Loss at iteration 5949: 0.04346629144770583\n",
      "Train Loss at iteration 5950: 0.04346614246852321\n",
      "Train Loss at iteration 5951: 0.043465993511806\n",
      "Train Loss at iteration 5952: 0.04346584457755\n",
      "Train Loss at iteration 5953: 0.04346569566575104\n",
      "Train Loss at iteration 5954: 0.04346554677640495\n",
      "Train Loss at iteration 5955: 0.0434653979095076\n",
      "Train Loss at iteration 5956: 0.043465249065054765\n",
      "Train Loss at iteration 5957: 0.043465100243042315\n",
      "Train Loss at iteration 5958: 0.04346495144346608\n",
      "Train Loss at iteration 5959: 0.04346480266632189\n",
      "Train Loss at iteration 5960: 0.04346465391160559\n",
      "Train Loss at iteration 5961: 0.04346450517931302\n",
      "Train Loss at iteration 5962: 0.04346435646944\n",
      "Train Loss at iteration 5963: 0.0434642077819824\n",
      "Train Loss at iteration 5964: 0.04346405911693604\n",
      "Train Loss at iteration 5965: 0.04346391047429675\n",
      "Train Loss at iteration 5966: 0.04346376185406041\n",
      "Train Loss at iteration 5967: 0.043463613256222826\n",
      "Train Loss at iteration 5968: 0.04346346468077988\n",
      "Train Loss at iteration 5969: 0.04346331612772738\n",
      "Train Loss at iteration 5970: 0.04346316759706121\n",
      "Train Loss at iteration 5971: 0.04346301908877719\n",
      "Train Loss at iteration 5972: 0.04346287060287117\n",
      "Train Loss at iteration 5973: 0.04346272213933903\n",
      "Train Loss at iteration 5974: 0.04346257369817658\n",
      "Train Loss at iteration 5975: 0.04346242527937971\n",
      "Train Loss at iteration 5976: 0.043462276882944254\n",
      "Train Loss at iteration 5977: 0.04346212850886606\n",
      "Train Loss at iteration 5978: 0.04346198015714099\n",
      "Train Loss at iteration 5979: 0.04346183182776491\n",
      "Train Loss at iteration 5980: 0.043461683520733665\n",
      "Train Loss at iteration 5981: 0.043461535236043104\n",
      "Train Loss at iteration 5982: 0.04346138697368911\n",
      "Train Loss at iteration 5983: 0.043461238733667536\n",
      "Train Loss at iteration 5984: 0.04346109051597422\n",
      "Train Loss at iteration 5985: 0.04346094232060505\n",
      "Train Loss at iteration 5986: 0.04346079414755589\n",
      "Train Loss at iteration 5987: 0.043460645996822596\n",
      "Train Loss at iteration 5988: 0.04346049786840102\n",
      "Train Loss at iteration 5989: 0.04346034976228705\n",
      "Train Loss at iteration 5990: 0.04346020167847652\n",
      "Train Loss at iteration 5991: 0.043460053616965345\n",
      "Train Loss at iteration 5992: 0.04345990557774936\n",
      "Train Loss at iteration 5993: 0.04345975756082445\n",
      "Train Loss at iteration 5994: 0.04345960956618647\n",
      "Train Loss at iteration 5995: 0.043459461593831315\n",
      "Train Loss at iteration 5996: 0.04345931364375484\n",
      "Train Loss at iteration 5997: 0.043459165715952916\n",
      "Train Loss at iteration 5998: 0.043459017810421434\n",
      "Train Loss at iteration 5999: 0.04345886992715626\n",
      "Train Loss at iteration 6000: 0.04345872206615328\n",
      "Train Loss at iteration 6001: 0.04345857422740835\n",
      "Train Loss at iteration 6002: 0.04345842641091737\n",
      "Train Loss at iteration 6003: 0.04345827861667622\n",
      "Train Loss at iteration 6004: 0.04345813084468077\n",
      "Train Loss at iteration 6005: 0.0434579830949269\n",
      "Train Loss at iteration 6006: 0.0434578353674105\n",
      "Train Loss at iteration 6007: 0.043457687662127445\n",
      "Train Loss at iteration 6008: 0.04345753997907364\n",
      "Train Loss at iteration 6009: 0.043457392318244956\n",
      "Train Loss at iteration 6010: 0.04345724467963726\n",
      "Train Loss at iteration 6011: 0.04345709706324648\n",
      "Train Loss at iteration 6012: 0.04345694946906848\n",
      "Train Loss at iteration 6013: 0.04345680189709914\n",
      "Train Loss at iteration 6014: 0.04345665434733439\n",
      "Train Loss at iteration 6015: 0.04345650681977007\n",
      "Train Loss at iteration 6016: 0.04345635931440213\n",
      "Train Loss at iteration 6017: 0.043456211831226416\n",
      "Train Loss at iteration 6018: 0.04345606437023884\n",
      "Train Loss at iteration 6019: 0.043455916931435294\n",
      "Train Loss at iteration 6020: 0.043455769514811664\n",
      "Train Loss at iteration 6021: 0.043455622120363885\n",
      "Train Loss at iteration 6022: 0.04345547474808782\n",
      "Train Loss at iteration 6023: 0.04345532739797939\n",
      "Train Loss at iteration 6024: 0.04345518007003447\n",
      "Train Loss at iteration 6025: 0.04345503276424898\n",
      "Train Loss at iteration 6026: 0.04345488548061881\n",
      "Train Loss at iteration 6027: 0.04345473821913988\n",
      "Train Loss at iteration 6028: 0.04345459097980808\n",
      "Train Loss at iteration 6029: 0.043454443762619324\n",
      "Train Loss at iteration 6030: 0.04345429656756951\n",
      "Train Loss at iteration 6031: 0.04345414939465455\n",
      "Train Loss at iteration 6032: 0.04345400224387037\n",
      "Train Loss at iteration 6033: 0.043453855115212825\n",
      "Train Loss at iteration 6034: 0.04345370800867789\n",
      "Train Loss at iteration 6035: 0.04345356092426144\n",
      "Train Loss at iteration 6036: 0.04345341386195939\n",
      "Train Loss at iteration 6037: 0.04345326682176766\n",
      "Train Loss at iteration 6038: 0.04345311980368216\n",
      "Train Loss at iteration 6039: 0.04345297280769881\n",
      "Train Loss at iteration 6040: 0.04345282583381352\n",
      "Train Loss at iteration 6041: 0.04345267888202221\n",
      "Train Loss at iteration 6042: 0.04345253195232079\n",
      "Train Loss at iteration 6043: 0.04345238504470519\n",
      "Train Loss at iteration 6044: 0.04345223815917132\n",
      "Train Loss at iteration 6045: 0.04345209129571511\n",
      "Train Loss at iteration 6046: 0.043451944454332474\n",
      "Train Loss at iteration 6047: 0.04345179763501934\n",
      "Train Loss at iteration 6048: 0.04345165083777163\n",
      "Train Loss at iteration 6049: 0.043451504062585265\n",
      "Train Loss at iteration 6050: 0.043451357309456186\n",
      "Train Loss at iteration 6051: 0.0434512105783803\n",
      "Train Loss at iteration 6052: 0.04345106386935354\n",
      "Train Loss at iteration 6053: 0.04345091718237184\n",
      "Train Loss at iteration 6054: 0.04345077051743111\n",
      "Train Loss at iteration 6055: 0.04345062387452731\n",
      "Train Loss at iteration 6056: 0.04345047725365637\n",
      "Train Loss at iteration 6057: 0.04345033065481419\n",
      "Train Loss at iteration 6058: 0.04345018407799674\n",
      "Train Loss at iteration 6059: 0.04345003752319991\n",
      "Train Loss at iteration 6060: 0.04344989099041968\n",
      "Train Loss at iteration 6061: 0.04344974447965197\n",
      "Train Loss at iteration 6062: 0.043449597990892715\n",
      "Train Loss at iteration 6063: 0.04344945152413786\n",
      "Train Loss at iteration 6064: 0.04344930507938331\n",
      "Train Loss at iteration 6065: 0.043449158656625075\n",
      "Train Loss at iteration 6066: 0.04344901225585903\n",
      "Train Loss at iteration 6067: 0.04344886587708114\n",
      "Train Loss at iteration 6068: 0.04344871952028735\n",
      "Train Loss at iteration 6069: 0.04344857318547362\n",
      "Train Loss at iteration 6070: 0.04344842687263586\n",
      "Train Loss at iteration 6071: 0.04344828058177004\n",
      "Train Loss at iteration 6072: 0.04344813431287209\n",
      "Train Loss at iteration 6073: 0.04344798806593797\n",
      "Train Loss at iteration 6074: 0.043447841840963626\n",
      "Train Loss at iteration 6075: 0.04344769563794501\n",
      "Train Loss at iteration 6076: 0.043447549456878065\n",
      "Train Loss at iteration 6077: 0.043447403297758765\n",
      "Train Loss at iteration 6078: 0.043447257160583036\n",
      "Train Loss at iteration 6079: 0.04344711104534683\n",
      "Train Loss at iteration 6080: 0.04344696495204613\n",
      "Train Loss at iteration 6081: 0.04344681888067686\n",
      "Train Loss at iteration 6082: 0.043446672831234996\n",
      "Train Loss at iteration 6083: 0.04344652680371649\n",
      "Train Loss at iteration 6084: 0.0434463807981173\n",
      "Train Loss at iteration 6085: 0.04344623481443338\n",
      "Train Loss at iteration 6086: 0.0434460888526607\n",
      "Train Loss at iteration 6087: 0.043445942912795216\n",
      "Train Loss at iteration 6088: 0.043445796994832885\n",
      "Train Loss at iteration 6089: 0.04344565109876968\n",
      "Train Loss at iteration 6090: 0.04344550522460155\n",
      "Train Loss at iteration 6091: 0.04344535937232447\n",
      "Train Loss at iteration 6092: 0.04344521354193442\n",
      "Train Loss at iteration 6093: 0.043445067733427344\n",
      "Train Loss at iteration 6094: 0.04344492194679921\n",
      "Train Loss at iteration 6095: 0.043444776182046015\n",
      "Train Loss at iteration 6096: 0.04344463043916368\n",
      "Train Loss at iteration 6097: 0.04344448471814821\n",
      "Train Loss at iteration 6098: 0.04344433901899559\n",
      "Train Loss at iteration 6099: 0.04344419334170175\n",
      "Train Loss at iteration 6100: 0.0434440476862627\n",
      "Train Loss at iteration 6101: 0.043443902052674406\n",
      "Train Loss at iteration 6102: 0.04344375644093282\n",
      "Train Loss at iteration 6103: 0.043443610851033945\n",
      "Train Loss at iteration 6104: 0.04344346528297374\n",
      "Train Loss at iteration 6105: 0.043443319736748205\n",
      "Train Loss at iteration 6106: 0.04344317421235331\n",
      "Train Loss at iteration 6107: 0.04344302870978502\n",
      "Train Loss at iteration 6108: 0.04344288322903933\n",
      "Train Loss at iteration 6109: 0.04344273777011221\n",
      "Train Loss at iteration 6110: 0.04344259233299967\n",
      "Train Loss at iteration 6111: 0.04344244691769768\n",
      "Train Loss at iteration 6112: 0.04344230152420219\n",
      "Train Loss at iteration 6113: 0.043442156152509244\n",
      "Train Loss at iteration 6114: 0.04344201080261478\n",
      "Train Loss at iteration 6115: 0.043441865474514826\n",
      "Train Loss at iteration 6116: 0.04344172016820534\n",
      "Train Loss at iteration 6117: 0.04344157488368233\n",
      "Train Loss at iteration 6118: 0.043441429620941774\n",
      "Train Loss at iteration 6119: 0.04344128437997967\n",
      "Train Loss at iteration 6120: 0.04344113916079202\n",
      "Train Loss at iteration 6121: 0.043440993963374794\n",
      "Train Loss at iteration 6122: 0.04344084878772401\n",
      "Train Loss at iteration 6123: 0.043440703633835634\n",
      "Train Loss at iteration 6124: 0.04344055850170569\n",
      "Train Loss at iteration 6125: 0.04344041339133016\n",
      "Train Loss at iteration 6126: 0.04344026830270504\n",
      "Train Loss at iteration 6127: 0.04344012323582635\n",
      "Train Loss at iteration 6128: 0.043439978190690065\n",
      "Train Loss at iteration 6129: 0.0434398331672922\n",
      "Train Loss at iteration 6130: 0.043439688165628765\n",
      "Train Loss at iteration 6131: 0.043439543185695734\n",
      "Train Loss at iteration 6132: 0.04343939822748913\n",
      "Train Loss at iteration 6133: 0.04343925329100495\n",
      "Train Loss at iteration 6134: 0.043439108376239224\n",
      "Train Loss at iteration 6135: 0.04343896348318791\n",
      "Train Loss at iteration 6136: 0.043438818611847056\n",
      "Train Loss at iteration 6137: 0.043438673762212665\n",
      "Train Loss at iteration 6138: 0.043438528934280726\n",
      "Train Loss at iteration 6139: 0.04343838412804727\n",
      "Train Loss at iteration 6140: 0.0434382393435083\n",
      "Train Loss at iteration 6141: 0.043438094580659836\n",
      "Train Loss at iteration 6142: 0.04343794983949788\n",
      "Train Loss at iteration 6143: 0.04343780512001844\n",
      "Train Loss at iteration 6144: 0.04343766042221753\n",
      "Train Loss at iteration 6145: 0.0434375157460912\n",
      "Train Loss at iteration 6146: 0.04343737109163543\n",
      "Train Loss at iteration 6147: 0.04343722645884624\n",
      "Train Loss at iteration 6148: 0.04343708184771968\n",
      "Train Loss at iteration 6149: 0.043436937258251734\n",
      "Train Loss at iteration 6150: 0.043436792690438444\n",
      "Train Loss at iteration 6151: 0.043436648144275815\n",
      "Train Loss at iteration 6152: 0.043436503619759884\n",
      "Train Loss at iteration 6153: 0.04343635911688667\n",
      "Train Loss at iteration 6154: 0.043436214635652184\n",
      "Train Loss at iteration 6155: 0.04343607017605248\n",
      "Train Loss at iteration 6156: 0.043435925738083546\n",
      "Train Loss at iteration 6157: 0.04343578132174143\n",
      "Train Loss at iteration 6158: 0.043435636927022174\n",
      "Train Loss at iteration 6159: 0.04343549255392178\n",
      "Train Loss at iteration 6160: 0.043435348202436296\n",
      "Train Loss at iteration 6161: 0.043435203872561744\n",
      "Train Loss at iteration 6162: 0.04343505956429416\n",
      "Train Loss at iteration 6163: 0.04343491527762957\n",
      "Train Loss at iteration 6164: 0.043434771012564\n",
      "Train Loss at iteration 6165: 0.04343462676909351\n",
      "Train Loss at iteration 6166: 0.043434482547214125\n",
      "Train Loss at iteration 6167: 0.04343433834692186\n",
      "Train Loss at iteration 6168: 0.04343419416821278\n",
      "Train Loss at iteration 6169: 0.04343405001108292\n",
      "Train Loss at iteration 6170: 0.04343390587552829\n",
      "Train Loss at iteration 6171: 0.04343376176154497\n",
      "Train Loss at iteration 6172: 0.04343361766912897\n",
      "Train Loss at iteration 6173: 0.04343347359827633\n",
      "Train Loss at iteration 6174: 0.04343332954898314\n",
      "Train Loss at iteration 6175: 0.04343318552124538\n",
      "Train Loss at iteration 6176: 0.04343304151505913\n",
      "Train Loss at iteration 6177: 0.043432897530420456\n",
      "Train Loss at iteration 6178: 0.04343275356732535\n",
      "Train Loss at iteration 6179: 0.043432609625769884\n",
      "Train Loss at iteration 6180: 0.04343246995324018\n",
      "Train Loss at iteration 6181: 0.043432366774917554\n",
      "Train Loss at iteration 6182: 0.04343226361248037\n",
      "Train Loss at iteration 6183: 0.043432160464952074\n",
      "Train Loss at iteration 6184: 0.04343205733230413\n",
      "Train Loss at iteration 6185: 0.04343195421453054\n",
      "Train Loss at iteration 6186: 0.04343185111162591\n",
      "Train Loss at iteration 6187: 0.043431748023585084\n",
      "Train Loss at iteration 6188: 0.043431644950403\n",
      "Train Loss at iteration 6189: 0.04343154189207471\n",
      "Train Loss at iteration 6190: 0.04343143884859542\n",
      "Train Loss at iteration 6191: 0.04343133581996043\n",
      "Train Loss at iteration 6192: 0.043431232806165114\n",
      "Train Loss at iteration 6193: 0.04343112980720499\n",
      "Train Loss at iteration 6194: 0.04343102682307565\n",
      "Train Loss at iteration 6195: 0.04343092385377274\n",
      "Train Loss at iteration 6196: 0.04343082089929203\n",
      "Train Loss at iteration 6197: 0.04343071795962933\n",
      "Train Loss at iteration 6198: 0.04343061503478055\n",
      "Train Loss at iteration 6199: 0.04343051212474163\n",
      "Train Loss at iteration 6200: 0.043430409229508596\n",
      "Train Loss at iteration 6201: 0.04343030634907753\n",
      "Train Loss at iteration 6202: 0.043430203483444556\n",
      "Train Loss at iteration 6203: 0.04343010063260586\n",
      "Train Loss at iteration 6204: 0.04342999779655767\n",
      "Train Loss at iteration 6205: 0.04342989497529627\n",
      "Train Loss at iteration 6206: 0.043429792168817954\n",
      "Train Loss at iteration 6207: 0.04342968937711911\n",
      "Train Loss at iteration 6208: 0.04342958660019611\n",
      "Train Loss at iteration 6209: 0.04342948383804539\n",
      "Train Loss at iteration 6210: 0.04342938109066343\n",
      "Train Loss at iteration 6211: 0.043429278358046705\n",
      "Train Loss at iteration 6212: 0.04342917564019175\n",
      "Train Loss at iteration 6213: 0.04342907293709512\n",
      "Train Loss at iteration 6214: 0.04342897024875342\n",
      "Train Loss at iteration 6215: 0.043428867575163226\n",
      "Train Loss at iteration 6216: 0.043428764916321166\n",
      "Train Loss at iteration 6217: 0.04342866227222392\n",
      "Train Loss at iteration 6218: 0.04342855964286816\n",
      "Train Loss at iteration 6219: 0.04342845702825057\n",
      "Train Loss at iteration 6220: 0.043428354428367885\n",
      "Train Loss at iteration 6221: 0.04342825184321682\n",
      "Train Loss at iteration 6222: 0.04342814927279415\n",
      "Train Loss at iteration 6223: 0.04342804671709662\n",
      "Train Loss at iteration 6224: 0.04342794417612104\n",
      "Train Loss at iteration 6225: 0.043427841649864185\n",
      "Train Loss at iteration 6226: 0.043427739138322895\n",
      "Train Loss at iteration 6227: 0.043427636641493975\n",
      "Train Loss at iteration 6228: 0.043427534159374295\n",
      "Train Loss at iteration 6229: 0.043427431691960686\n",
      "Train Loss at iteration 6230: 0.04342732923925002\n",
      "Train Loss at iteration 6231: 0.043427226801239185\n",
      "Train Loss at iteration 6232: 0.04342712437792506\n",
      "Train Loss at iteration 6233: 0.04342702196930455\n",
      "Train Loss at iteration 6234: 0.04342691957537456\n",
      "Train Loss at iteration 6235: 0.043426817196131985\n",
      "Train Loss at iteration 6236: 0.04342671483157382\n",
      "Train Loss at iteration 6237: 0.04342661248169694\n",
      "Train Loss at iteration 6238: 0.04342651014649831\n",
      "Train Loss at iteration 6239: 0.043426407825974886\n",
      "Train Loss at iteration 6240: 0.043426305520123616\n",
      "Train Loss at iteration 6241: 0.043426203228941467\n",
      "Train Loss at iteration 6242: 0.04342610095242543\n",
      "Train Loss at iteration 6243: 0.04342599869057248\n",
      "Train Loss at iteration 6244: 0.04342589644337962\n",
      "Train Loss at iteration 6245: 0.043425794210843804\n",
      "Train Loss at iteration 6246: 0.04342569199296207\n",
      "Train Loss at iteration 6247: 0.04342558978973141\n",
      "Train Loss at iteration 6248: 0.04342548760114883\n",
      "Train Loss at iteration 6249: 0.04342538542721136\n",
      "Train Loss at iteration 6250: 0.04342528326791601\n",
      "Train Loss at iteration 6251: 0.04342518112325982\n",
      "Train Loss at iteration 6252: 0.0434250789932398\n",
      "Train Loss at iteration 6253: 0.043424976877853017\n",
      "Train Loss at iteration 6254: 0.04342487477709649\n",
      "Train Loss at iteration 6255: 0.04342477269096728\n",
      "Train Loss at iteration 6256: 0.04342467061946241\n",
      "Train Loss at iteration 6257: 0.043424568562578966\n",
      "Train Loss at iteration 6258: 0.04342446652031399\n",
      "Train Loss at iteration 6259: 0.043424364492664534\n",
      "Train Loss at iteration 6260: 0.043424262479627676\n",
      "Train Loss at iteration 6261: 0.04342416048120049\n",
      "Train Loss at iteration 6262: 0.04342405849738004\n",
      "Train Loss at iteration 6263: 0.043423956528163395\n",
      "Train Loss at iteration 6264: 0.04342385457354764\n",
      "Train Loss at iteration 6265: 0.043423752633529863\n",
      "Train Loss at iteration 6266: 0.04342365070810713\n",
      "Train Loss at iteration 6267: 0.04342354879727656\n",
      "Train Loss at iteration 6268: 0.0434234469010352\n",
      "Train Loss at iteration 6269: 0.04342334501938018\n",
      "Train Loss at iteration 6270: 0.04342324315230859\n",
      "Train Loss at iteration 6271: 0.043423141299817516\n",
      "Train Loss at iteration 6272: 0.04342303946190407\n",
      "Train Loss at iteration 6273: 0.04342293763856534\n",
      "Train Loss at iteration 6274: 0.04342283582979846\n",
      "Train Loss at iteration 6275: 0.043422734035600534\n",
      "Train Loss at iteration 6276: 0.04342263225596864\n",
      "Train Loss at iteration 6277: 0.04342253049089993\n",
      "Train Loss at iteration 6278: 0.0434224287403915\n",
      "Train Loss at iteration 6279: 0.043422327004440466\n",
      "Train Loss at iteration 6280: 0.043422225283043965\n",
      "Train Loss at iteration 6281: 0.04342212357619911\n",
      "Train Loss at iteration 6282: 0.04342202188390301\n",
      "Train Loss at iteration 6283: 0.04342192020615283\n",
      "Train Loss at iteration 6284: 0.04342181854294566\n",
      "Train Loss at iteration 6285: 0.04342171689427863\n",
      "Train Loss at iteration 6286: 0.043421615260148906\n",
      "Train Loss at iteration 6287: 0.0434215136405536\n",
      "Train Loss at iteration 6288: 0.04342141203548984\n",
      "Train Loss at iteration 6289: 0.04342131044495478\n",
      "Train Loss at iteration 6290: 0.04342120886894556\n",
      "Train Loss at iteration 6291: 0.043421107307459295\n",
      "Train Loss at iteration 6292: 0.04342100576049316\n",
      "Train Loss at iteration 6293: 0.043420904228044295\n",
      "Train Loss at iteration 6294: 0.04342080271010982\n",
      "Train Loss at iteration 6295: 0.0434207012066869\n",
      "Train Loss at iteration 6296: 0.04342059971777269\n",
      "Train Loss at iteration 6297: 0.04342049824336435\n",
      "Train Loss at iteration 6298: 0.043420396783458985\n",
      "Train Loss at iteration 6299: 0.043420295338053805\n",
      "Train Loss at iteration 6300: 0.04342019390714594\n",
      "Train Loss at iteration 6301: 0.04342009249073253\n",
      "Train Loss at iteration 6302: 0.04341999108881077\n",
      "Train Loss at iteration 6303: 0.04341988970137779\n",
      "Train Loss at iteration 6304: 0.043419788328430776\n",
      "Train Loss at iteration 6305: 0.043419686969966884\n",
      "Train Loss at iteration 6306: 0.04341958562598324\n",
      "Train Loss at iteration 6307: 0.04341948429647706\n",
      "Train Loss at iteration 6308: 0.04341938298144549\n",
      "Train Loss at iteration 6309: 0.0434192816808857\n",
      "Train Loss at iteration 6310: 0.043419180394794865\n",
      "Train Loss at iteration 6311: 0.04341907912317014\n",
      "Train Loss at iteration 6312: 0.043418977866008714\n",
      "Train Loss at iteration 6313: 0.043418876623307755\n",
      "Train Loss at iteration 6314: 0.04341877539506443\n",
      "Train Loss at iteration 6315: 0.04341867418127592\n",
      "Train Loss at iteration 6316: 0.04341857298193941\n",
      "Train Loss at iteration 6317: 0.04341847179705206\n",
      "Train Loss at iteration 6318: 0.043418370626611066\n",
      "Train Loss at iteration 6319: 0.0434182694706136\n",
      "Train Loss at iteration 6320: 0.043418168329056865\n",
      "Train Loss at iteration 6321: 0.043418067201938006\n",
      "Train Loss at iteration 6322: 0.04341796608925423\n",
      "Train Loss at iteration 6323: 0.043417864991002725\n",
      "Train Loss at iteration 6324: 0.043417763907180676\n",
      "Train Loss at iteration 6325: 0.043417662837785274\n",
      "Train Loss at iteration 6326: 0.04341756178281368\n",
      "Train Loss at iteration 6327: 0.043417460742263124\n",
      "Train Loss at iteration 6328: 0.04341735971613078\n",
      "Train Loss at iteration 6329: 0.04341725870441383\n",
      "Train Loss at iteration 6330: 0.04341715770710948\n",
      "Train Loss at iteration 6331: 0.04341705672421492\n",
      "Train Loss at iteration 6332: 0.043416955755727356\n",
      "Train Loss at iteration 6333: 0.043416854801643964\n",
      "Train Loss at iteration 6334: 0.04341675386196197\n",
      "Train Loss at iteration 6335: 0.043416652936678554\n",
      "Train Loss at iteration 6336: 0.04341655202579091\n",
      "Train Loss at iteration 6337: 0.043416451129296256\n",
      "Train Loss at iteration 6338: 0.04341635024719178\n",
      "Train Loss at iteration 6339: 0.04341624937947469\n",
      "Train Loss at iteration 6340: 0.04341614852614221\n",
      "Train Loss at iteration 6341: 0.043416047687191504\n",
      "Train Loss at iteration 6342: 0.04341594686261981\n",
      "Train Loss at iteration 6343: 0.04341584605242434\n",
      "Train Loss at iteration 6344: 0.04341574525660228\n",
      "Train Loss at iteration 6345: 0.04341564447515085\n",
      "Train Loss at iteration 6346: 0.043415543708067265\n",
      "Train Loss at iteration 6347: 0.04341544295534872\n",
      "Train Loss at iteration 6348: 0.04341534221699245\n",
      "Train Loss at iteration 6349: 0.04341524149299565\n",
      "Train Loss at iteration 6350: 0.043415140783355545\n",
      "Train Loss at iteration 6351: 0.04341504008806935\n",
      "Train Loss at iteration 6352: 0.04341493940713428\n",
      "Train Loss at iteration 6353: 0.04341483874054755\n",
      "Train Loss at iteration 6354: 0.04341473808830638\n",
      "Train Loss at iteration 6355: 0.04341463745040799\n",
      "Train Loss at iteration 6356: 0.04341453682684959\n",
      "Train Loss at iteration 6357: 0.04341443621762844\n",
      "Train Loss at iteration 6358: 0.043414335622741704\n",
      "Train Loss at iteration 6359: 0.043414235042186645\n",
      "Train Loss at iteration 6360: 0.04341413447596049\n",
      "Train Loss at iteration 6361: 0.04341403392406044\n",
      "Train Loss at iteration 6362: 0.043413933386483736\n",
      "Train Loss at iteration 6363: 0.04341383286322761\n",
      "Train Loss at iteration 6364: 0.043413732354289275\n",
      "Train Loss at iteration 6365: 0.04341363185966598\n",
      "Train Loss at iteration 6366: 0.04341353137935493\n",
      "Train Loss at iteration 6367: 0.04341343091335337\n",
      "Train Loss at iteration 6368: 0.04341333046165854\n",
      "Train Loss at iteration 6369: 0.04341323002426767\n",
      "Train Loss at iteration 6370: 0.04341312960117798\n",
      "Train Loss at iteration 6371: 0.04341302919238671\n",
      "Train Loss at iteration 6372: 0.043412928797891104\n",
      "Train Loss at iteration 6373: 0.043412828417688405\n",
      "Train Loss at iteration 6374: 0.043412728051775824\n",
      "Train Loss at iteration 6375: 0.04341262770015062\n",
      "Train Loss at iteration 6376: 0.043412527362810024\n",
      "Train Loss at iteration 6377: 0.043412427039751296\n",
      "Train Loss at iteration 6378: 0.04341232673097164\n",
      "Train Loss at iteration 6379: 0.04341222643646833\n",
      "Train Loss at iteration 6380: 0.043412126156238594\n",
      "Train Loss at iteration 6381: 0.04341202589027968\n",
      "Train Loss at iteration 6382: 0.04341192563858885\n",
      "Train Loss at iteration 6383: 0.04341182540116333\n",
      "Train Loss at iteration 6384: 0.04341172517800035\n",
      "Train Loss at iteration 6385: 0.0434116249690972\n",
      "Train Loss at iteration 6386: 0.043411524774451093\n",
      "Train Loss at iteration 6387: 0.04341142459405931\n",
      "Train Loss at iteration 6388: 0.04341132442791907\n",
      "Train Loss at iteration 6389: 0.04341122427602763\n",
      "Train Loss at iteration 6390: 0.043411124138382266\n",
      "Train Loss at iteration 6391: 0.043411024014980204\n",
      "Train Loss at iteration 6392: 0.04341092390581872\n",
      "Train Loss at iteration 6393: 0.043410823810895054\n",
      "Train Loss at iteration 6394: 0.04341072373020646\n",
      "Train Loss at iteration 6395: 0.0434106236637502\n",
      "Train Loss at iteration 6396: 0.04341052361152354\n",
      "Train Loss at iteration 6397: 0.04341042357352372\n",
      "Train Loss at iteration 6398: 0.043410323549748026\n",
      "Train Loss at iteration 6399: 0.043410223540193686\n",
      "Train Loss at iteration 6400: 0.04341012354485799\n",
      "Train Loss at iteration 6401: 0.043410023563738166\n",
      "Train Loss at iteration 6402: 0.04340992359683151\n",
      "Train Loss at iteration 6403: 0.043409823644135266\n",
      "Train Loss at iteration 6404: 0.043409723705646726\n",
      "Train Loss at iteration 6405: 0.043409623781363114\n",
      "Train Loss at iteration 6406: 0.043409523871281726\n",
      "Train Loss at iteration 6407: 0.04340942397539982\n",
      "Train Loss at iteration 6408: 0.04340932409371465\n",
      "Train Loss at iteration 6409: 0.043409224226223515\n",
      "Train Loss at iteration 6410: 0.04340912437292367\n",
      "Train Loss at iteration 6411: 0.04340902453381237\n",
      "Train Loss at iteration 6412: 0.04340892470888691\n",
      "Train Loss at iteration 6413: 0.04340882489814455\n",
      "Train Loss at iteration 6414: 0.04340872510158257\n",
      "Train Loss at iteration 6415: 0.04340862531919823\n",
      "Train Loss at iteration 6416: 0.04340852555098883\n",
      "Train Loss at iteration 6417: 0.043408425796951616\n",
      "Train Loss at iteration 6418: 0.04340832605708389\n",
      "Train Loss at iteration 6419: 0.0434082263313829\n",
      "Train Loss at iteration 6420: 0.04340812661984596\n",
      "Train Loss at iteration 6421: 0.04340802692247031\n",
      "Train Loss at iteration 6422: 0.04340792723925327\n",
      "Train Loss at iteration 6423: 0.043407827570192105\n",
      "Train Loss at iteration 6424: 0.043407727915284085\n",
      "Train Loss at iteration 6425: 0.04340762827452649\n",
      "Train Loss at iteration 6426: 0.043407528647916624\n",
      "Train Loss at iteration 6427: 0.04340742903545177\n",
      "Train Loss at iteration 6428: 0.0434073294371292\n",
      "Train Loss at iteration 6429: 0.0434072298529462\n",
      "Train Loss at iteration 6430: 0.04340713028290005\n",
      "Train Loss at iteration 6431: 0.04340703072698806\n",
      "Train Loss at iteration 6432: 0.043406931185207505\n",
      "Train Loss at iteration 6433: 0.04340683165755567\n",
      "Train Loss at iteration 6434: 0.043406732144029854\n",
      "Train Loss at iteration 6435: 0.043406632644627345\n",
      "Train Loss at iteration 6436: 0.04340653315934542\n",
      "Train Loss at iteration 6437: 0.0434064336881814\n",
      "Train Loss at iteration 6438: 0.04340633423113256\n",
      "Train Loss at iteration 6439: 0.0434062347881962\n",
      "Train Loss at iteration 6440: 0.0434061353593696\n",
      "Train Loss at iteration 6441: 0.043406035944650076\n",
      "Train Loss at iteration 6442: 0.043405936544034913\n",
      "Train Loss at iteration 6443: 0.04340583715752142\n",
      "Train Loss at iteration 6444: 0.043405737785106875\n",
      "Train Loss at iteration 6445: 0.04340563842678859\n",
      "Train Loss at iteration 6446: 0.04340553908256386\n",
      "Train Loss at iteration 6447: 0.04340543975243\n",
      "Train Loss at iteration 6448: 0.04340534043638428\n",
      "Train Loss at iteration 6449: 0.04340524113442404\n",
      "Train Loss at iteration 6450: 0.043405141846546556\n",
      "Train Loss at iteration 6451: 0.043405042572749145\n",
      "Train Loss at iteration 6452: 0.04340494331302911\n",
      "Train Loss at iteration 6453: 0.04340484406738374\n",
      "Train Loss at iteration 6454: 0.04340474483581039\n",
      "Train Loss at iteration 6455: 0.043404645618306294\n",
      "Train Loss at iteration 6456: 0.04340454641486882\n",
      "Train Loss at iteration 6457: 0.04340444722549524\n",
      "Train Loss at iteration 6458: 0.04340434805018288\n",
      "Train Loss at iteration 6459: 0.04340424888892905\n",
      "Train Loss at iteration 6460: 0.04340414974173107\n",
      "Train Loss at iteration 6461: 0.04340405060858623\n",
      "Train Loss at iteration 6462: 0.04340395148949186\n",
      "Train Loss at iteration 6463: 0.04340385238444526\n",
      "Train Loss at iteration 6464: 0.04340375329344375\n",
      "Train Loss at iteration 6465: 0.043403654216484656\n",
      "Train Loss at iteration 6466: 0.04340355515356529\n",
      "Train Loss at iteration 6467: 0.04340345610468295\n",
      "Train Loss at iteration 6468: 0.043403357069834975\n",
      "Train Loss at iteration 6469: 0.043403258049018664\n",
      "Train Loss at iteration 6470: 0.043403159042231354\n",
      "Train Loss at iteration 6471: 0.04340306004947036\n",
      "Train Loss at iteration 6472: 0.04340296107073299\n",
      "Train Loss at iteration 6473: 0.04340286210601659\n",
      "Train Loss at iteration 6474: 0.043402763155318456\n",
      "Train Loss at iteration 6475: 0.04340266421863592\n",
      "Train Loss at iteration 6476: 0.043402565295966325\n",
      "Train Loss at iteration 6477: 0.043402466387306965\n",
      "Train Loss at iteration 6478: 0.04340236749265518\n",
      "Train Loss at iteration 6479: 0.0434022686120083\n",
      "Train Loss at iteration 6480: 0.04340216974536364\n",
      "Train Loss at iteration 6481: 0.043402070892718546\n",
      "Train Loss at iteration 6482: 0.043401972054070315\n",
      "Train Loss at iteration 6483: 0.04340187322941631\n",
      "Train Loss at iteration 6484: 0.04340177441875384\n",
      "Train Loss at iteration 6485: 0.04340167562208025\n",
      "Train Loss at iteration 6486: 0.043401576839392865\n",
      "Train Loss at iteration 6487: 0.043401478070689024\n",
      "Train Loss at iteration 6488: 0.04340137931596604\n",
      "Train Loss at iteration 6489: 0.04340128057522126\n",
      "Train Loss at iteration 6490: 0.04340118184845202\n",
      "Train Loss at iteration 6491: 0.04340108313565565\n",
      "Train Loss at iteration 6492: 0.0434009844368295\n",
      "Train Loss at iteration 6493: 0.043400885751970895\n",
      "Train Loss at iteration 6494: 0.04340078708107717\n",
      "Train Loss at iteration 6495: 0.04340068842414566\n",
      "Train Loss at iteration 6496: 0.04340058978117371\n",
      "Train Loss at iteration 6497: 0.04340049115215868\n",
      "Train Loss at iteration 6498: 0.043400392537097876\n",
      "Train Loss at iteration 6499: 0.043400293935988675\n",
      "Train Loss at iteration 6500: 0.04340019534882839\n",
      "Train Loss at iteration 6501: 0.04340009677561437\n",
      "Train Loss at iteration 6502: 0.04339999821634398\n",
      "Train Loss at iteration 6503: 0.043399899671014534\n",
      "Train Loss at iteration 6504: 0.0433998011396234\n",
      "Train Loss at iteration 6505: 0.04339970262216791\n",
      "Train Loss at iteration 6506: 0.043399604118645414\n",
      "Train Loss at iteration 6507: 0.043399505629053266\n",
      "Train Loss at iteration 6508: 0.04339940715338881\n",
      "Train Loss at iteration 6509: 0.04339930869164939\n",
      "Train Loss at iteration 6510: 0.043399210243832365\n",
      "Train Loss at iteration 6511: 0.04339911180993509\n",
      "Train Loss at iteration 6512: 0.0433990133899549\n",
      "Train Loss at iteration 6513: 0.04339891498388916\n",
      "Train Loss at iteration 6514: 0.043398816591735216\n",
      "Train Loss at iteration 6515: 0.043398718213490416\n",
      "Train Loss at iteration 6516: 0.04339861984915213\n",
      "Train Loss at iteration 6517: 0.043398521498717715\n",
      "Train Loss at iteration 6518: 0.043398423162184514\n",
      "Train Loss at iteration 6519: 0.04339832483954988\n",
      "Train Loss at iteration 6520: 0.043398226530811185\n",
      "Train Loss at iteration 6521: 0.043398128235965776\n",
      "Train Loss at iteration 6522: 0.043398029955011005\n",
      "Train Loss at iteration 6523: 0.04339793168794426\n",
      "Train Loss at iteration 6524: 0.04339783343476288\n",
      "Train Loss at iteration 6525: 0.04339773519546423\n",
      "Train Loss at iteration 6526: 0.043397636970045665\n",
      "Train Loss at iteration 6527: 0.04339753875850457\n",
      "Train Loss at iteration 6528: 0.04339744056083828\n",
      "Train Loss at iteration 6529: 0.04339734237704418\n",
      "Train Loss at iteration 6530: 0.04339724420711962\n",
      "Train Loss at iteration 6531: 0.043397146051061986\n",
      "Train Loss at iteration 6532: 0.04339704790886862\n",
      "Train Loss at iteration 6533: 0.043396949780536896\n",
      "Train Loss at iteration 6534: 0.04339685166606418\n",
      "Train Loss at iteration 6535: 0.043396753565447865\n",
      "Train Loss at iteration 6536: 0.04339665547868529\n",
      "Train Loss at iteration 6537: 0.04339655740577385\n",
      "Train Loss at iteration 6538: 0.043396459346710894\n",
      "Train Loss at iteration 6539: 0.043396361301493803\n",
      "Train Loss at iteration 6540: 0.04339626327011995\n",
      "Train Loss at iteration 6541: 0.0433961652525867\n",
      "Train Loss at iteration 6542: 0.043396067248891446\n",
      "Train Loss at iteration 6543: 0.04339596925903154\n",
      "Train Loss at iteration 6544: 0.04339587128300436\n",
      "Train Loss at iteration 6545: 0.0433957733208073\n",
      "Train Loss at iteration 6546: 0.04339567537243772\n",
      "Train Loss at iteration 6547: 0.04339557743789302\n",
      "Train Loss at iteration 6548: 0.043395479517170536\n",
      "Train Loss at iteration 6549: 0.04339538161026768\n",
      "Train Loss at iteration 6550: 0.04339528371718183\n",
      "Train Loss at iteration 6551: 0.043395185837910355\n",
      "Train Loss at iteration 6552: 0.04339508797245064\n",
      "Train Loss at iteration 6553: 0.043394990120800066\n",
      "Train Loss at iteration 6554: 0.04339489228295602\n",
      "Train Loss at iteration 6555: 0.043394794458915864\n",
      "Train Loss at iteration 6556: 0.043394696648677025\n",
      "Train Loss at iteration 6557: 0.04339459885223685\n",
      "Train Loss at iteration 6558: 0.043394501069592736\n",
      "Train Loss at iteration 6559: 0.043394403300742075\n",
      "Train Loss at iteration 6560: 0.043394305545682264\n",
      "Train Loss at iteration 6561: 0.043394207804410646\n",
      "Train Loss at iteration 6562: 0.043394110076924654\n",
      "Train Loss at iteration 6563: 0.04339401236322165\n",
      "Train Loss at iteration 6564: 0.04339391466329905\n",
      "Train Loss at iteration 6565: 0.04339381697715423\n",
      "Train Loss at iteration 6566: 0.04339371930478456\n",
      "Train Loss at iteration 6567: 0.043393621646187465\n",
      "Train Loss at iteration 6568: 0.04339352400136033\n",
      "Train Loss at iteration 6569: 0.043393426370300525\n",
      "Train Loss at iteration 6570: 0.04339332875300548\n",
      "Train Loss at iteration 6571: 0.04339323114947258\n",
      "Train Loss at iteration 6572: 0.04339313355969919\n",
      "Train Loss at iteration 6573: 0.04339303598368274\n",
      "Train Loss at iteration 6574: 0.04339293842142062\n",
      "Train Loss at iteration 6575: 0.04339284087291021\n",
      "Train Loss at iteration 6576: 0.04339274333814893\n",
      "Train Loss at iteration 6577: 0.043392645817134166\n",
      "Train Loss at iteration 6578: 0.043392548309863305\n",
      "Train Loss at iteration 6579: 0.043392450816333784\n",
      "Train Loss at iteration 6580: 0.04339235333654299\n",
      "Train Loss at iteration 6581: 0.043392255870488304\n",
      "Train Loss at iteration 6582: 0.043392158418167154\n",
      "Train Loss at iteration 6583: 0.04339206097957693\n",
      "Train Loss at iteration 6584: 0.043391963554715036\n",
      "Train Loss at iteration 6585: 0.04339186614357888\n",
      "Train Loss at iteration 6586: 0.04339176874616586\n",
      "Train Loss at iteration 6587: 0.0433916713624734\n",
      "Train Loss at iteration 6588: 0.0433915739924989\n",
      "Train Loss at iteration 6589: 0.043391476636239754\n",
      "Train Loss at iteration 6590: 0.043391379293693376\n",
      "Train Loss at iteration 6591: 0.04339128196485718\n",
      "Train Loss at iteration 6592: 0.04339118464972856\n",
      "Train Loss at iteration 6593: 0.04339108734830497\n",
      "Train Loss at iteration 6594: 0.04339099006058377\n",
      "Train Loss at iteration 6595: 0.04339089278656239\n",
      "Train Loss at iteration 6596: 0.04339079552623825\n",
      "Train Loss at iteration 6597: 0.04339069827960875\n",
      "Train Loss at iteration 6598: 0.043390601046671334\n",
      "Train Loss at iteration 6599: 0.04339050382742337\n",
      "Train Loss at iteration 6600: 0.043390406621862304\n",
      "Train Loss at iteration 6601: 0.04339030942998555\n",
      "Train Loss at iteration 6602: 0.043390212251790516\n",
      "Train Loss at iteration 6603: 0.043390115087274625\n",
      "Train Loss at iteration 6604: 0.04339001793643529\n",
      "Train Loss at iteration 6605: 0.04338992079926994\n",
      "Train Loss at iteration 6606: 0.043389823675775964\n",
      "Train Loss at iteration 6607: 0.043389726565950824\n",
      "Train Loss at iteration 6608: 0.04338962946979193\n",
      "Train Loss at iteration 6609: 0.04338953238729667\n",
      "Train Loss at iteration 6610: 0.0433894353184625\n",
      "Train Loss at iteration 6611: 0.04338933826328685\n",
      "Train Loss at iteration 6612: 0.043389241221767114\n",
      "Train Loss at iteration 6613: 0.04338914419390072\n",
      "Train Loss at iteration 6614: 0.043389047179685114\n",
      "Train Loss at iteration 6615: 0.04338895017911771\n",
      "Train Loss at iteration 6616: 0.043388853192195924\n",
      "Train Loss at iteration 6617: 0.043388756218917204\n",
      "Train Loss at iteration 6618: 0.043388659259278956\n",
      "Train Loss at iteration 6619: 0.04338856231327863\n",
      "Train Loss at iteration 6620: 0.04338846538091364\n",
      "Train Loss at iteration 6621: 0.043388368462181415\n",
      "Train Loss at iteration 6622: 0.04338827155707939\n",
      "Train Loss at iteration 6623: 0.043388174665605\n",
      "Train Loss at iteration 6624: 0.04338807778775567\n",
      "Train Loss at iteration 6625: 0.04338798092352885\n",
      "Train Loss at iteration 6626: 0.04338788407292194\n",
      "Train Loss at iteration 6627: 0.04338778723593239\n",
      "Train Loss at iteration 6628: 0.04338769041255764\n",
      "Train Loss at iteration 6629: 0.043387593602795135\n",
      "Train Loss at iteration 6630: 0.0433874968066423\n",
      "Train Loss at iteration 6631: 0.043387400024096544\n",
      "Train Loss at iteration 6632: 0.04338730325515534\n",
      "Train Loss at iteration 6633: 0.043387206499816124\n",
      "Train Loss at iteration 6634: 0.04338710975807632\n",
      "Train Loss at iteration 6635: 0.04338701302993337\n",
      "Train Loss at iteration 6636: 0.04338691631538471\n",
      "Train Loss at iteration 6637: 0.0433868196144278\n",
      "Train Loss at iteration 6638: 0.04338672292706007\n",
      "Train Loss at iteration 6639: 0.04338662625327896\n",
      "Train Loss at iteration 6640: 0.0433865295930819\n",
      "Train Loss at iteration 6641: 0.04338643294646635\n",
      "Train Loss at iteration 6642: 0.04338633631342976\n",
      "Train Loss at iteration 6643: 0.043386239693969555\n",
      "Train Loss at iteration 6644: 0.0433861430880832\n",
      "Train Loss at iteration 6645: 0.043386046495768125\n",
      "Train Loss at iteration 6646: 0.04338594991702179\n",
      "Train Loss at iteration 6647: 0.04338585335184163\n",
      "Train Loss at iteration 6648: 0.0433857568002251\n",
      "Train Loss at iteration 6649: 0.04338566026216965\n",
      "Train Loss at iteration 6650: 0.04338556373767272\n",
      "Train Loss at iteration 6651: 0.04338546722673177\n",
      "Train Loss at iteration 6652: 0.043385370729344244\n",
      "Train Loss at iteration 6653: 0.043385274245507595\n",
      "Train Loss at iteration 6654: 0.04338517777521929\n",
      "Train Loss at iteration 6655: 0.04338508131847676\n",
      "Train Loss at iteration 6656: 0.04338498487527746\n",
      "Train Loss at iteration 6657: 0.04338488844561886\n",
      "Train Loss at iteration 6658: 0.04338479202949841\n",
      "Train Loss at iteration 6659: 0.04338469562691356\n",
      "Train Loss at iteration 6660: 0.043384599237861754\n",
      "Train Loss at iteration 6661: 0.04338450286234047\n",
      "Train Loss at iteration 6662: 0.04338440650034715\n",
      "Train Loss at iteration 6663: 0.04338431015187928\n",
      "Train Loss at iteration 6664: 0.04338421381693428\n",
      "Train Loss at iteration 6665: 0.043384117495509626\n",
      "Train Loss at iteration 6666: 0.0433840211876028\n",
      "Train Loss at iteration 6667: 0.043383924893211225\n",
      "Train Loss at iteration 6668: 0.04338382861233238\n",
      "Train Loss at iteration 6669: 0.04338373234496374\n",
      "Train Loss at iteration 6670: 0.04338363609110274\n",
      "Train Loss at iteration 6671: 0.04338353985074687\n",
      "Train Loss at iteration 6672: 0.04338344362389356\n",
      "Train Loss at iteration 6673: 0.04338334741054032\n",
      "Train Loss at iteration 6674: 0.04338325121068458\n",
      "Train Loss at iteration 6675: 0.04338315502432382\n",
      "Train Loss at iteration 6676: 0.043383058851455505\n",
      "Train Loss at iteration 6677: 0.043382962692077094\n",
      "Train Loss at iteration 6678: 0.043382866546186075\n",
      "Train Loss at iteration 6679: 0.04338277041377989\n",
      "Train Loss at iteration 6680: 0.043382674294856045\n",
      "Train Loss at iteration 6681: 0.04338257818941198\n",
      "Train Loss at iteration 6682: 0.04338248209744518\n",
      "Train Loss at iteration 6683: 0.04338238601895309\n",
      "Train Loss at iteration 6684: 0.04338228995393321\n",
      "Train Loss at iteration 6685: 0.04338219390238301\n",
      "Train Loss at iteration 6686: 0.04338209786429995\n",
      "Train Loss at iteration 6687: 0.043382001839681515\n",
      "Train Loss at iteration 6688: 0.04338190582852518\n",
      "Train Loss at iteration 6689: 0.04338180983082841\n",
      "Train Loss at iteration 6690: 0.04338171384658869\n",
      "Train Loss at iteration 6691: 0.0433816178758035\n",
      "Train Loss at iteration 6692: 0.043381521918470306\n",
      "Train Loss at iteration 6693: 0.043381425974586586\n",
      "Train Loss at iteration 6694: 0.043381330044149825\n",
      "Train Loss at iteration 6695: 0.04338123412715752\n",
      "Train Loss at iteration 6696: 0.04338113822360711\n",
      "Train Loss at iteration 6697: 0.04338104233349609\n",
      "Train Loss at iteration 6698: 0.043380946456821966\n",
      "Train Loss at iteration 6699: 0.04338085059358218\n",
      "Train Loss at iteration 6700: 0.04338075474377427\n",
      "Train Loss at iteration 6701: 0.04338065890739567\n",
      "Train Loss at iteration 6702: 0.043380563084443866\n",
      "Train Loss at iteration 6703: 0.04338046727491636\n",
      "Train Loss at iteration 6704: 0.04338037147881064\n",
      "Train Loss at iteration 6705: 0.04338027569612418\n",
      "Train Loss at iteration 6706: 0.04338017992685446\n",
      "Train Loss at iteration 6707: 0.04338008417099899\n",
      "Train Loss at iteration 6708: 0.04337998842855524\n",
      "Train Loss at iteration 6709: 0.043379892699520695\n",
      "Train Loss at iteration 6710: 0.04337979698389287\n",
      "Train Loss at iteration 6711: 0.043379701281669215\n",
      "Train Loss at iteration 6712: 0.04337960559284726\n",
      "Train Loss at iteration 6713: 0.04337950991742447\n",
      "Train Loss at iteration 6714: 0.04337941425539833\n",
      "Train Loss at iteration 6715: 0.04337931860676637\n",
      "Train Loss at iteration 6716: 0.04337922297152604\n",
      "Train Loss at iteration 6717: 0.04337912734967486\n",
      "Train Loss at iteration 6718: 0.04337903174121032\n",
      "Train Loss at iteration 6719: 0.04337893614612991\n",
      "Train Loss at iteration 6720: 0.043378840564431115\n",
      "Train Loss at iteration 6721: 0.04337874499611145\n",
      "Train Loss at iteration 6722: 0.0433786494411684\n",
      "Train Loss at iteration 6723: 0.043378553899599465\n",
      "Train Loss at iteration 6724: 0.043378458371402154\n",
      "Train Loss at iteration 6725: 0.04337836285657394\n",
      "Train Loss at iteration 6726: 0.04337826735511235\n",
      "Train Loss at iteration 6727: 0.04337817186701485\n",
      "Train Loss at iteration 6728: 0.043378076392278975\n",
      "Train Loss at iteration 6729: 0.04337798093090221\n",
      "Train Loss at iteration 6730: 0.043377885482882066\n",
      "Train Loss at iteration 6731: 0.04337779004821604\n",
      "Train Loss at iteration 6732: 0.04337769462690162\n",
      "Train Loss at iteration 6733: 0.04337759921893632\n",
      "Train Loss at iteration 6734: 0.043377503824317674\n",
      "Train Loss at iteration 6735: 0.04337740844304314\n",
      "Train Loss at iteration 6736: 0.043377313075110245\n",
      "Train Loss at iteration 6737: 0.0433772177205165\n",
      "Train Loss at iteration 6738: 0.043377122379259406\n",
      "Train Loss at iteration 6739: 0.04337702705133647\n",
      "Train Loss at iteration 6740: 0.0433769317367452\n",
      "Train Loss at iteration 6741: 0.0433768364354831\n",
      "Train Loss at iteration 6742: 0.04337674114754768\n",
      "Train Loss at iteration 6743: 0.043376645872936465\n",
      "Train Loss at iteration 6744: 0.04337655061164694\n",
      "Train Loss at iteration 6745: 0.04337645536367664\n",
      "Train Loss at iteration 6746: 0.04337636012902307\n",
      "Train Loss at iteration 6747: 0.043376264907683745\n",
      "Train Loss at iteration 6748: 0.043376169699656165\n",
      "Train Loss at iteration 6749: 0.043376074504937856\n",
      "Train Loss at iteration 6750: 0.04337597932352632\n",
      "Train Loss at iteration 6751: 0.04337588415541909\n",
      "Train Loss at iteration 6752: 0.04337578900061367\n",
      "Train Loss at iteration 6753: 0.043375693859107584\n",
      "Train Loss at iteration 6754: 0.04337559873089834\n",
      "Train Loss at iteration 6755: 0.04337550361598345\n",
      "Train Loss at iteration 6756: 0.043375408514360454\n",
      "Train Loss at iteration 6757: 0.04337531342602686\n",
      "Train Loss at iteration 6758: 0.043375218350980176\n",
      "Train Loss at iteration 6759: 0.043375123289217936\n",
      "Train Loss at iteration 6760: 0.043375028240737655\n",
      "Train Loss at iteration 6761: 0.04337493320553685\n",
      "Train Loss at iteration 6762: 0.043374838183613054\n",
      "Train Loss at iteration 6763: 0.043374743174963794\n",
      "Train Loss at iteration 6764: 0.04337464817958656\n",
      "Train Loss at iteration 6765: 0.04337455319747891\n",
      "Train Loss at iteration 6766: 0.04337445822863836\n",
      "Train Loss at iteration 6767: 0.04337436327306243\n",
      "Train Loss at iteration 6768: 0.04337426833074865\n",
      "Train Loss at iteration 6769: 0.04337417340169455\n",
      "Train Loss at iteration 6770: 0.04337407848589763\n",
      "Train Loss at iteration 6771: 0.043373983583355466\n",
      "Train Loss at iteration 6772: 0.04337388869406554\n",
      "Train Loss at iteration 6773: 0.043373793818025405\n",
      "Train Loss at iteration 6774: 0.04337369895523259\n",
      "Train Loss at iteration 6775: 0.043373604105684624\n",
      "Train Loss at iteration 6776: 0.04337350926937902\n",
      "Train Loss at iteration 6777: 0.04337341444631333\n",
      "Train Loss at iteration 6778: 0.04337331963648508\n",
      "Train Loss at iteration 6779: 0.0433732248398918\n",
      "Train Loss at iteration 6780: 0.04337313005653103\n",
      "Train Loss at iteration 6781: 0.0433730352864003\n",
      "Train Loss at iteration 6782: 0.043372940529497125\n",
      "Train Loss at iteration 6783: 0.04337284578581907\n",
      "Train Loss at iteration 6784: 0.04337275105536366\n",
      "Train Loss at iteration 6785: 0.043372656338128424\n",
      "Train Loss at iteration 6786: 0.04337256163411092\n",
      "Train Loss at iteration 6787: 0.04337246694330866\n",
      "Train Loss at iteration 6788: 0.04337237226571918\n",
      "Train Loss at iteration 6789: 0.04337227760134004\n",
      "Train Loss at iteration 6790: 0.04337218295016878\n",
      "Train Loss at iteration 6791: 0.04337208831220292\n",
      "Train Loss at iteration 6792: 0.043371993687440005\n",
      "Train Loss at iteration 6793: 0.0433718990758776\n",
      "Train Loss at iteration 6794: 0.043371804477513216\n",
      "Train Loss at iteration 6795: 0.0433717098923444\n",
      "Train Loss at iteration 6796: 0.043371615320368706\n",
      "Train Loss at iteration 6797: 0.04337152076158369\n",
      "Train Loss at iteration 6798: 0.04337142621598687\n",
      "Train Loss at iteration 6799: 0.043371331683575806\n",
      "Train Loss at iteration 6800: 0.04337123716434803\n",
      "Train Loss at iteration 6801: 0.0433711426583011\n",
      "Train Loss at iteration 6802: 0.043371048165432566\n",
      "Train Loss at iteration 6803: 0.04337095368573996\n",
      "Train Loss at iteration 6804: 0.04337085921922085\n",
      "Train Loss at iteration 6805: 0.043370764765872774\n",
      "Train Loss at iteration 6806: 0.043370670325693264\n",
      "Train Loss at iteration 6807: 0.0433705758986799\n",
      "Train Loss at iteration 6808: 0.04337048148483022\n",
      "Train Loss at iteration 6809: 0.04337038708414177\n",
      "Train Loss at iteration 6810: 0.04337029269661211\n",
      "Train Loss at iteration 6811: 0.04337019832223878\n",
      "Train Loss at iteration 6812: 0.04337010396101935\n",
      "Train Loss at iteration 6813: 0.04337000961295135\n",
      "Train Loss at iteration 6814: 0.04336991527803235\n",
      "Train Loss at iteration 6815: 0.04336982095625991\n",
      "Train Loss at iteration 6816: 0.04336972664763158\n",
      "Train Loss at iteration 6817: 0.0433696323521449\n",
      "Train Loss at iteration 6818: 0.04336953806979745\n",
      "Train Loss at iteration 6819: 0.043369443800586785\n",
      "Train Loss at iteration 6820: 0.04336934954451044\n",
      "Train Loss at iteration 6821: 0.043369255301565976\n",
      "Train Loss at iteration 6822: 0.04336916107175098\n",
      "Train Loss at iteration 6823: 0.043369066855063004\n",
      "Train Loss at iteration 6824: 0.043368972651499575\n",
      "Train Loss at iteration 6825: 0.0433688784610583\n",
      "Train Loss at iteration 6826: 0.04336878428373671\n",
      "Train Loss at iteration 6827: 0.04336869011953238\n",
      "Train Loss at iteration 6828: 0.043368595968442857\n",
      "Train Loss at iteration 6829: 0.04336850183046572\n",
      "Train Loss at iteration 6830: 0.043368407705598536\n",
      "Train Loss at iteration 6831: 0.04336831359383885\n",
      "Train Loss at iteration 6832: 0.04336821949518424\n",
      "Train Loss at iteration 6833: 0.04336812540963226\n",
      "Train Loss at iteration 6834: 0.04336803133718049\n",
      "Train Loss at iteration 6835: 0.043367937277826514\n",
      "Train Loss at iteration 6836: 0.04336784323156785\n",
      "Train Loss at iteration 6837: 0.043367749198402104\n",
      "Train Loss at iteration 6838: 0.04336765517832684\n",
      "Train Loss at iteration 6839: 0.04336756117133961\n",
      "Train Loss at iteration 6840: 0.043367467177437995\n",
      "Train Loss at iteration 6841: 0.04336737319661957\n",
      "Train Loss at iteration 6842: 0.04336727922888189\n",
      "Train Loss at iteration 6843: 0.04336718527422256\n",
      "Train Loss at iteration 6844: 0.04336709133263909\n",
      "Train Loss at iteration 6845: 0.04336699740412912\n",
      "Train Loss at iteration 6846: 0.04336690348869019\n",
      "Train Loss at iteration 6847: 0.04336680958631989\n",
      "Train Loss at iteration 6848: 0.043366715697015765\n",
      "Train Loss at iteration 6849: 0.04336662182077542\n",
      "Train Loss at iteration 6850: 0.043366527957596414\n",
      "Train Loss at iteration 6851: 0.043366434107476336\n",
      "Train Loss at iteration 6852: 0.04336634027041274\n",
      "Train Loss at iteration 6853: 0.043366246446403245\n",
      "Train Loss at iteration 6854: 0.043366152635445375\n",
      "Train Loss at iteration 6855: 0.043366058837536754\n",
      "Train Loss at iteration 6856: 0.043365965052674954\n",
      "Train Loss at iteration 6857: 0.043365871280857525\n",
      "Train Loss at iteration 6858: 0.04336577752208208\n",
      "Train Loss at iteration 6859: 0.043365683776346184\n",
      "Train Loss at iteration 6860: 0.04336559004364741\n",
      "Train Loss at iteration 6861: 0.04336549632398337\n",
      "Train Loss at iteration 6862: 0.043365402617351625\n",
      "Train Loss at iteration 6863: 0.04336530892374977\n",
      "Train Loss at iteration 6864: 0.04336521524317537\n",
      "Train Loss at iteration 6865: 0.04336512157562602\n",
      "Train Loss at iteration 6866: 0.0433650279210993\n",
      "Train Loss at iteration 6867: 0.043364934279592836\n",
      "Train Loss at iteration 6868: 0.04336484065110415\n",
      "Train Loss at iteration 6869: 0.04336474703563087\n",
      "Train Loss at iteration 6870: 0.04336465343317057\n",
      "Train Loss at iteration 6871: 0.04336455984372085\n",
      "Train Loss at iteration 6872: 0.04336446626727929\n",
      "Train Loss at iteration 6873: 0.043364372703843475\n",
      "Train Loss at iteration 6874: 0.04336427915341099\n",
      "Train Loss at iteration 6875: 0.043364185615979434\n",
      "Train Loss at iteration 6876: 0.04336409209154642\n",
      "Train Loss at iteration 6877: 0.04336399858010951\n",
      "Train Loss at iteration 6878: 0.04336390508166631\n",
      "Train Loss at iteration 6879: 0.0433638115962144\n",
      "Train Loss at iteration 6880: 0.04336371812375139\n",
      "Train Loss at iteration 6881: 0.043363624664274854\n",
      "Train Loss at iteration 6882: 0.04336353121778241\n",
      "Train Loss at iteration 6883: 0.04336343778427164\n",
      "Train Loss at iteration 6884: 0.04336334436374013\n",
      "Train Loss at iteration 6885: 0.043363250956185505\n",
      "Train Loss at iteration 6886: 0.04336315756160534\n",
      "Train Loss at iteration 6887: 0.043363064179997225\n",
      "Train Loss at iteration 6888: 0.04336297081135878\n",
      "Train Loss at iteration 6889: 0.04336287745568759\n",
      "Train Loss at iteration 6890: 0.043362784112981265\n",
      "Train Loss at iteration 6891: 0.0433626907832374\n",
      "Train Loss at iteration 6892: 0.0433625974664536\n",
      "Train Loss at iteration 6893: 0.043362504162627456\n",
      "Train Loss at iteration 6894: 0.04336241087175657\n",
      "Train Loss at iteration 6895: 0.04336231759383855\n",
      "Train Loss at iteration 6896: 0.043362224328871\n",
      "Train Loss at iteration 6897: 0.043362131076851516\n",
      "Train Loss at iteration 6898: 0.0433620378377777\n",
      "Train Loss at iteration 6899: 0.04336194461164719\n",
      "Train Loss at iteration 6900: 0.043361851398457545\n",
      "Train Loss at iteration 6901: 0.04336175819820638\n",
      "Train Loss at iteration 6902: 0.04336166501089133\n",
      "Train Loss at iteration 6903: 0.043361571836509974\n",
      "Train Loss at iteration 6904: 0.04336147867505994\n",
      "Train Loss at iteration 6905: 0.04336138552653882\n",
      "Train Loss at iteration 6906: 0.04336129239094423\n",
      "Train Loss at iteration 6907: 0.043361199268273774\n",
      "Train Loss at iteration 6908: 0.043361106158525065\n",
      "Train Loss at iteration 6909: 0.0433610130616957\n",
      "Train Loss at iteration 6910: 0.043360919977783324\n",
      "Train Loss at iteration 6911: 0.04336082690678553\n",
      "Train Loss at iteration 6912: 0.043360733848699914\n",
      "Train Loss at iteration 6913: 0.04336064080352411\n",
      "Train Loss at iteration 6914: 0.04336054777125572\n",
      "Train Loss at iteration 6915: 0.04336045475189236\n",
      "Train Loss at iteration 6916: 0.04336036174543166\n",
      "Train Loss at iteration 6917: 0.043360268751871205\n",
      "Train Loss at iteration 6918: 0.04336017577120864\n",
      "Train Loss at iteration 6919: 0.043360082803441564\n",
      "Train Loss at iteration 6920: 0.0433599898485676\n",
      "Train Loss at iteration 6921: 0.043359896906584365\n",
      "Train Loss at iteration 6922: 0.043359803977489474\n",
      "Train Loss at iteration 6923: 0.04335971106128054\n",
      "Train Loss at iteration 6924: 0.0433596181579552\n",
      "Train Loss at iteration 6925: 0.043359525267511065\n",
      "Train Loss at iteration 6926: 0.04335943238994574\n",
      "Train Loss at iteration 6927: 0.04335933952525687\n",
      "Train Loss at iteration 6928: 0.04335924667344206\n",
      "Train Loss at iteration 6929: 0.04335915383449895\n",
      "Train Loss at iteration 6930: 0.04335906100842513\n",
      "Train Loss at iteration 6931: 0.04335896819521825\n",
      "Train Loss at iteration 6932: 0.04335887539487593\n",
      "Train Loss at iteration 6933: 0.04335878260739579\n",
      "Train Loss at iteration 6934: 0.043358689832775466\n",
      "Train Loss at iteration 6935: 0.04335859707101255\n",
      "Train Loss at iteration 6936: 0.0433585043221047\n",
      "Train Loss at iteration 6937: 0.04335841158604955\n",
      "Train Loss at iteration 6938: 0.04335831886284469\n",
      "Train Loss at iteration 6939: 0.04335822615248778\n",
      "Train Loss at iteration 6940: 0.04335813345497644\n",
      "Train Loss at iteration 6941: 0.0433580407703083\n",
      "Train Loss at iteration 6942: 0.04335794809848097\n",
      "Train Loss at iteration 6943: 0.0433578554394921\n",
      "Train Loss at iteration 6944: 0.04335776279333932\n",
      "Train Loss at iteration 6945: 0.043357670160020255\n",
      "Train Loss at iteration 6946: 0.04335757753953254\n",
      "Train Loss at iteration 6947: 0.0433574849318738\n",
      "Train Loss at iteration 6948: 0.043357392337041674\n",
      "Train Loss at iteration 6949: 0.0433572997550338\n",
      "Train Loss at iteration 6950: 0.04335720718584781\n",
      "Train Loss at iteration 6951: 0.04335711462948133\n",
      "Train Loss at iteration 6952: 0.043357022085932\n",
      "Train Loss at iteration 6953: 0.04335692955519746\n",
      "Train Loss at iteration 6954: 0.04335683703727535\n",
      "Train Loss at iteration 6955: 0.04335674453216329\n",
      "Train Loss at iteration 6956: 0.043356652039858924\n",
      "Train Loss at iteration 6957: 0.04335655956035989\n",
      "Train Loss at iteration 6958: 0.04335646709366384\n",
      "Train Loss at iteration 6959: 0.0433563746397684\n",
      "Train Loss at iteration 6960: 0.04335628219867122\n",
      "Train Loss at iteration 6961: 0.04335618977036992\n",
      "Train Loss at iteration 6962: 0.04335609735486216\n",
      "Train Loss at iteration 6963: 0.04335600495214557\n",
      "Train Loss at iteration 6964: 0.043355912562217803\n",
      "Train Loss at iteration 6965: 0.043355820185076485\n",
      "Train Loss at iteration 6966: 0.04335572782071928\n",
      "Train Loss at iteration 6967: 0.04335563546914382\n",
      "Train Loss at iteration 6968: 0.04335554313034773\n",
      "Train Loss at iteration 6969: 0.0433554508043287\n",
      "Train Loss at iteration 6970: 0.04335535849108434\n",
      "Train Loss at iteration 6971: 0.04335526619061231\n",
      "Train Loss at iteration 6972: 0.04335517390291024\n",
      "Train Loss at iteration 6973: 0.04335508162797579\n",
      "Train Loss at iteration 6974: 0.043354989365806625\n",
      "Train Loss at iteration 6975: 0.04335489711640037\n",
      "Train Loss at iteration 6976: 0.04335480487975467\n",
      "Train Loss at iteration 6977: 0.04335471265586719\n",
      "Train Loss at iteration 6978: 0.04335462044473557\n",
      "Train Loss at iteration 6979: 0.04335452824635746\n",
      "Train Loss at iteration 6980: 0.04335443606073052\n",
      "Train Loss at iteration 6981: 0.043354343887852385\n",
      "Train Loss at iteration 6982: 0.04335425172772073\n",
      "Train Loss at iteration 6983: 0.04335415958033319\n",
      "Train Loss at iteration 6984: 0.04335406744568743\n",
      "Train Loss at iteration 6985: 0.04335397532378108\n",
      "Train Loss at iteration 6986: 0.04335388321461183\n",
      "Train Loss at iteration 6987: 0.04335379111817731\n",
      "Train Loss at iteration 6988: 0.04335369903447519\n",
      "Train Loss at iteration 6989: 0.04335360696350311\n",
      "Train Loss at iteration 6990: 0.04335351490525873\n",
      "Train Loss at iteration 6991: 0.04335342285973971\n",
      "Train Loss at iteration 6992: 0.043353330826943724\n",
      "Train Loss at iteration 6993: 0.0433532388068684\n",
      "Train Loss at iteration 6994: 0.04335314679951142\n",
      "Train Loss at iteration 6995: 0.04335305480487043\n",
      "Train Loss at iteration 6996: 0.0433529628229431\n",
      "Train Loss at iteration 6997: 0.04335287085372709\n",
      "Train Loss at iteration 6998: 0.04335277889722004\n",
      "Train Loss at iteration 6999: 0.04335268695341964\n",
      "Train Loss at iteration 7000: 0.043352595022323547\n",
      "Train Loss at iteration 7001: 0.043352503103929414\n",
      "Train Loss at iteration 7002: 0.0433524111982349\n",
      "Train Loss at iteration 7003: 0.04335231930523768\n",
      "Train Loss at iteration 7004: 0.04335222742493543\n",
      "Train Loss at iteration 7005: 0.04335213555732577\n",
      "Train Loss at iteration 7006: 0.04335204370240643\n",
      "Train Loss at iteration 7007: 0.04335195186017501\n",
      "Train Loss at iteration 7008: 0.04335186003062922\n",
      "Train Loss at iteration 7009: 0.04335176821376672\n",
      "Train Loss at iteration 7010: 0.04335167640958517\n",
      "Train Loss at iteration 7011: 0.04335158461808224\n",
      "Train Loss at iteration 7012: 0.043351492839255594\n",
      "Train Loss at iteration 7013: 0.0433514010731029\n",
      "Train Loss at iteration 7014: 0.04335130931962185\n",
      "Train Loss at iteration 7015: 0.04335121757881009\n",
      "Train Loss at iteration 7016: 0.043351125850665306\n",
      "Train Loss at iteration 7017: 0.04335103413518517\n",
      "Train Loss at iteration 7018: 0.043350942432367326\n",
      "Train Loss at iteration 7019: 0.04335085074220949\n",
      "Train Loss at iteration 7020: 0.04335075906470931\n",
      "Train Loss at iteration 7021: 0.04335066739986445\n",
      "Train Loss at iteration 7022: 0.04335057574767263\n",
      "Train Loss at iteration 7023: 0.04335048410813146\n",
      "Train Loss at iteration 7024: 0.04335039248123865\n",
      "Train Loss at iteration 7025: 0.043350300866991875\n",
      "Train Loss at iteration 7026: 0.04335020926538882\n",
      "Train Loss at iteration 7027: 0.043350117676427165\n",
      "Train Loss at iteration 7028: 0.043350026100104556\n",
      "Train Loss at iteration 7029: 0.0433499345364187\n",
      "Train Loss at iteration 7030: 0.04334984298536726\n",
      "Train Loss at iteration 7031: 0.04334975144694793\n",
      "Train Loss at iteration 7032: 0.04334965992115838\n",
      "Train Loss at iteration 7033: 0.043349568407996274\n",
      "Train Loss at iteration 7034: 0.04334947690745933\n",
      "Train Loss at iteration 7035: 0.04334938541954521\n",
      "Train Loss at iteration 7036: 0.043349293944251584\n",
      "Train Loss at iteration 7037: 0.04334920248157615\n",
      "Train Loss at iteration 7038: 0.043349111031516584\n",
      "Train Loss at iteration 7039: 0.043349019594070594\n",
      "Train Loss at iteration 7040: 0.04334892816923583\n",
      "Train Loss at iteration 7041: 0.04334883675700998\n",
      "Train Loss at iteration 7042: 0.04334874535739076\n",
      "Train Loss at iteration 7043: 0.043348653970375815\n",
      "Train Loss at iteration 7044: 0.04334856259596286\n",
      "Train Loss at iteration 7045: 0.04334847123414958\n",
      "Train Loss at iteration 7046: 0.043348379884933654\n",
      "Train Loss at iteration 7047: 0.04334828854831276\n",
      "Train Loss at iteration 7048: 0.043348197224284624\n",
      "Train Loss at iteration 7049: 0.04334810591284689\n",
      "Train Loss at iteration 7050: 0.043348014613997274\n",
      "Train Loss at iteration 7051: 0.04334792332773346\n",
      "Train Loss at iteration 7052: 0.04334783205405315\n",
      "Train Loss at iteration 7053: 0.043347740792954\n",
      "Train Loss at iteration 7054: 0.04334764954443373\n",
      "Train Loss at iteration 7055: 0.043347558308490054\n",
      "Train Loss at iteration 7056: 0.04334746708512061\n",
      "Train Loss at iteration 7057: 0.04334737587432312\n",
      "Train Loss at iteration 7058: 0.0433472846760953\n",
      "Train Loss at iteration 7059: 0.04334719349043481\n",
      "Train Loss at iteration 7060: 0.043347102317339375\n",
      "Train Loss at iteration 7061: 0.043347011156806656\n",
      "Train Loss at iteration 7062: 0.043346920008834375\n",
      "Train Loss at iteration 7063: 0.043346828873420215\n",
      "Train Loss at iteration 7064: 0.04334673775056189\n",
      "Train Loss at iteration 7065: 0.04334664664025707\n",
      "Train Loss at iteration 7066: 0.04334655554250349\n",
      "Train Loss at iteration 7067: 0.04334646445729882\n",
      "Train Loss at iteration 7068: 0.04334637338464076\n",
      "Train Loss at iteration 7069: 0.043346282324527016\n",
      "Train Loss at iteration 7070: 0.04334619127695532\n",
      "Train Loss at iteration 7071: 0.04334610024192333\n",
      "Train Loss at iteration 7072: 0.04334600921942876\n",
      "Train Loss at iteration 7073: 0.04334591820946929\n",
      "Train Loss at iteration 7074: 0.04334582721204268\n",
      "Train Loss at iteration 7075: 0.04334573622714659\n",
      "Train Loss at iteration 7076: 0.043345645254778735\n",
      "Train Loss at iteration 7077: 0.04334555429493682\n",
      "Train Loss at iteration 7078: 0.04334546334761853\n",
      "Train Loss at iteration 7079: 0.0433453724128216\n",
      "Train Loss at iteration 7080: 0.04334528149054373\n",
      "Train Loss at iteration 7081: 0.04334519058078263\n",
      "Train Loss at iteration 7082: 0.043345099683535976\n",
      "Train Loss at iteration 7083: 0.04334500879880151\n",
      "Train Loss at iteration 7084: 0.043344917926576916\n",
      "Train Loss at iteration 7085: 0.04334482706685992\n",
      "Train Loss at iteration 7086: 0.043344736219648225\n",
      "Train Loss at iteration 7087: 0.04334464538493954\n",
      "Train Loss at iteration 7088: 0.04334455456273158\n",
      "Train Loss at iteration 7089: 0.043344463753022054\n",
      "Train Loss at iteration 7090: 0.04334437295580866\n",
      "Train Loss at iteration 7091: 0.04334428217108912\n",
      "Train Loss at iteration 7092: 0.04334419139886116\n",
      "Train Loss at iteration 7093: 0.04334410063912246\n",
      "Train Loss at iteration 7094: 0.043344009891870774\n",
      "Train Loss at iteration 7095: 0.04334391915710379\n",
      "Train Loss at iteration 7096: 0.043343828434819216\n",
      "Train Loss at iteration 7097: 0.04334373772501479\n",
      "Train Loss at iteration 7098: 0.04334364702768821\n",
      "Train Loss at iteration 7099: 0.04334355634283721\n",
      "Train Loss at iteration 7100: 0.043343465670459484\n",
      "Train Loss at iteration 7101: 0.043343375010552763\n",
      "Train Loss at iteration 7102: 0.04334328436311477\n",
      "Train Loss at iteration 7103: 0.0433431937281432\n",
      "Train Loss at iteration 7104: 0.0433431031056358\n",
      "Train Loss at iteration 7105: 0.04334301249559027\n",
      "Train Loss at iteration 7106: 0.043342921898004344\n",
      "Train Loss at iteration 7107: 0.043342831312875724\n",
      "Train Loss at iteration 7108: 0.04334274074020213\n",
      "Train Loss at iteration 7109: 0.043342650179981325\n",
      "Train Loss at iteration 7110: 0.043342559632210995\n",
      "Train Loss at iteration 7111: 0.043342469096888846\n",
      "Train Loss at iteration 7112: 0.043342378574012644\n",
      "Train Loss at iteration 7113: 0.04334228806358008\n",
      "Train Loss at iteration 7114: 0.043342197565588894\n",
      "Train Loss at iteration 7115: 0.04334210708003681\n",
      "Train Loss at iteration 7116: 0.04334201660692155\n",
      "Train Loss at iteration 7117: 0.04334192614624083\n",
      "Train Loss at iteration 7118: 0.04334183569799239\n",
      "Train Loss at iteration 7119: 0.04334174526217395\n",
      "Train Loss at iteration 7120: 0.04334165483878325\n",
      "Train Loss at iteration 7121: 0.043341564427818\n",
      "Train Loss at iteration 7122: 0.043341474029275934\n",
      "Train Loss at iteration 7123: 0.0433413836431548\n",
      "Train Loss at iteration 7124: 0.04334129326945228\n",
      "Train Loss at iteration 7125: 0.04334120290816615\n",
      "Train Loss at iteration 7126: 0.043341112559294126\n",
      "Train Loss at iteration 7127: 0.043341022222833925\n",
      "Train Loss at iteration 7128: 0.04334093189878332\n",
      "Train Loss at iteration 7129: 0.04334084158713999\n",
      "Train Loss at iteration 7130: 0.043340751287901685\n",
      "Train Loss at iteration 7131: 0.04334066100106616\n",
      "Train Loss at iteration 7132: 0.043340570726631125\n",
      "Train Loss at iteration 7133: 0.04334048046459432\n",
      "Train Loss at iteration 7134: 0.0433403902149535\n",
      "Train Loss at iteration 7135: 0.043340299977706366\n",
      "Train Loss at iteration 7136: 0.04334020975285068\n",
      "Train Loss at iteration 7137: 0.04334011954038415\n",
      "Train Loss at iteration 7138: 0.043340029340304544\n",
      "Train Loss at iteration 7139: 0.04333993915260958\n",
      "Train Loss at iteration 7140: 0.04333984897729701\n",
      "Train Loss at iteration 7141: 0.04333975881436456\n",
      "Train Loss at iteration 7142: 0.04333966866380998\n",
      "Train Loss at iteration 7143: 0.043339578525630994\n",
      "Train Loss at iteration 7144: 0.04333948839982535\n",
      "Train Loss at iteration 7145: 0.043339398286390794\n",
      "Train Loss at iteration 7146: 0.04333930818532506\n",
      "Train Loss at iteration 7147: 0.04333921809662588\n",
      "Train Loss at iteration 7148: 0.043339128020291016\n",
      "Train Loss at iteration 7149: 0.04333903795631821\n",
      "Train Loss at iteration 7150: 0.04333894790470519\n",
      "Train Loss at iteration 7151: 0.0433388578654497\n",
      "Train Loss at iteration 7152: 0.04333876783854949\n",
      "Train Loss at iteration 7153: 0.043338677824002324\n",
      "Train Loss at iteration 7154: 0.04333858782180592\n",
      "Train Loss at iteration 7155: 0.043338497831958034\n",
      "Train Loss at iteration 7156: 0.0433384078544564\n",
      "Train Loss at iteration 7157: 0.04333831788929877\n",
      "Train Loss at iteration 7158: 0.04333822793648291\n",
      "Train Loss at iteration 7159: 0.04333813799600655\n",
      "Train Loss at iteration 7160: 0.043338048067867445\n",
      "Train Loss at iteration 7161: 0.04333795815206334\n",
      "Train Loss at iteration 7162: 0.043337868248591975\n",
      "Train Loss at iteration 7163: 0.043337778357451125\n",
      "Train Loss at iteration 7164: 0.04333768847863852\n",
      "Train Loss at iteration 7165: 0.043337598612151904\n",
      "Train Loss at iteration 7166: 0.04333750875798906\n",
      "Train Loss at iteration 7167: 0.04333741891614771\n",
      "Train Loss at iteration 7168: 0.04333732908662562\n",
      "Train Loss at iteration 7169: 0.04333723926942055\n",
      "Train Loss at iteration 7170: 0.04333714946453021\n",
      "Train Loss at iteration 7171: 0.04333705967195243\n",
      "Train Loss at iteration 7172: 0.0433369698916849\n",
      "Train Loss at iteration 7173: 0.0433368801237254\n",
      "Train Loss at iteration 7174: 0.043336790368071684\n",
      "Train Loss at iteration 7175: 0.0433367006247215\n",
      "Train Loss at iteration 7176: 0.04333661089367262\n",
      "Train Loss at iteration 7177: 0.043336521174922794\n",
      "Train Loss at iteration 7178: 0.04333643146846978\n",
      "Train Loss at iteration 7179: 0.04333634177431132\n",
      "Train Loss at iteration 7180: 0.0433362520924452\n",
      "Train Loss at iteration 7181: 0.04333616242286917\n",
      "Train Loss at iteration 7182: 0.04333607276558097\n",
      "Train Loss at iteration 7183: 0.04333598312057838\n",
      "Train Loss at iteration 7184: 0.04333589348785916\n",
      "Train Loss at iteration 7185: 0.04333580386742107\n",
      "Train Loss at iteration 7186: 0.04333571425926187\n",
      "Train Loss at iteration 7187: 0.04333562466337932\n",
      "Train Loss at iteration 7188: 0.04333553507977119\n",
      "Train Loss at iteration 7189: 0.043335445508435225\n",
      "Train Loss at iteration 7190: 0.04333535594936922\n",
      "Train Loss at iteration 7191: 0.04333526640257091\n",
      "Train Loss at iteration 7192: 0.04333517686803807\n",
      "Train Loss at iteration 7193: 0.04333508734576848\n",
      "Train Loss at iteration 7194: 0.043334997835759864\n",
      "Train Loss at iteration 7195: 0.043334908338010056\n",
      "Train Loss at iteration 7196: 0.043334818852516756\n",
      "Train Loss at iteration 7197: 0.04333472937927777\n",
      "Train Loss at iteration 7198: 0.043334639918290854\n",
      "Train Loss at iteration 7199: 0.04333455046955378\n",
      "Train Loss at iteration 7200: 0.0433344610330643\n",
      "Train Loss at iteration 7201: 0.0433343716088202\n",
      "Train Loss at iteration 7202: 0.043334282196819264\n",
      "Train Loss at iteration 7203: 0.04333419279705924\n",
      "Train Loss at iteration 7204: 0.043334103409537895\n",
      "Train Loss at iteration 7205: 0.04333401403425303\n",
      "Train Loss at iteration 7206: 0.04333392467120238\n",
      "Train Loss at iteration 7207: 0.04333383532038374\n",
      "Train Loss at iteration 7208: 0.043333745981794895\n",
      "Train Loss at iteration 7209: 0.04333365665543359\n",
      "Train Loss at iteration 7210: 0.043333567341297605\n",
      "Train Loss at iteration 7211: 0.043333478039384726\n",
      "Train Loss at iteration 7212: 0.04333338874969273\n",
      "Train Loss at iteration 7213: 0.04333329947221938\n",
      "Train Loss at iteration 7214: 0.04333321020696246\n",
      "Train Loss at iteration 7215: 0.04333312095391974\n",
      "Train Loss at iteration 7216: 0.04333303171308901\n",
      "Train Loss at iteration 7217: 0.04333294248446803\n",
      "Train Loss at iteration 7218: 0.04333285326805459\n",
      "Train Loss at iteration 7219: 0.04333276406384647\n",
      "Train Loss at iteration 7220: 0.04333267487184144\n",
      "Train Loss at iteration 7221: 0.043332585692037295\n",
      "Train Loss at iteration 7222: 0.043332496524431795\n",
      "Train Loss at iteration 7223: 0.04333240736902274\n",
      "Train Loss at iteration 7224: 0.04333231822580789\n",
      "Train Loss at iteration 7225: 0.043332229094785034\n",
      "Train Loss at iteration 7226: 0.04333213997595198\n",
      "Train Loss at iteration 7227: 0.04333205086930648\n",
      "Train Loss at iteration 7228: 0.043331961774846335\n",
      "Train Loss at iteration 7229: 0.04333187269256931\n",
      "Train Loss at iteration 7230: 0.043331783622473204\n",
      "Train Loss at iteration 7231: 0.043331694564555795\n",
      "Train Loss at iteration 7232: 0.043331605518814875\n",
      "Train Loss at iteration 7233: 0.04333151648524823\n",
      "Train Loss at iteration 7234: 0.04333142746385363\n",
      "Train Loss at iteration 7235: 0.04333133845462889\n",
      "Train Loss at iteration 7236: 0.04333124945757177\n",
      "Train Loss at iteration 7237: 0.043331160472680076\n",
      "Train Loss at iteration 7238: 0.043331071499951586\n",
      "Train Loss at iteration 7239: 0.04333098253938409\n",
      "Train Loss at iteration 7240: 0.04333089359097538\n",
      "Train Loss at iteration 7241: 0.04333080465472325\n",
      "Train Loss at iteration 7242: 0.04333071573062549\n",
      "Train Loss at iteration 7243: 0.043330626818679885\n",
      "Train Loss at iteration 7244: 0.04333053791888422\n",
      "Train Loss at iteration 7245: 0.04333044903123629\n",
      "Train Loss at iteration 7246: 0.04333036015573392\n",
      "Train Loss at iteration 7247: 0.04333027129237485\n",
      "Train Loss at iteration 7248: 0.0433301824411569\n",
      "Train Loss at iteration 7249: 0.04333009360207789\n",
      "Train Loss at iteration 7250: 0.04333000477513555\n",
      "Train Loss at iteration 7251: 0.043329915960327724\n",
      "Train Loss at iteration 7252: 0.043329827157652205\n",
      "Train Loss at iteration 7253: 0.043329738367106775\n",
      "Train Loss at iteration 7254: 0.043329649588689234\n",
      "Train Loss at iteration 7255: 0.04332956082239737\n",
      "Train Loss at iteration 7256: 0.04332947206822901\n",
      "Train Loss at iteration 7257: 0.043329383326181914\n",
      "Train Loss at iteration 7258: 0.043329294596253905\n",
      "Train Loss at iteration 7259: 0.043329205878442766\n",
      "Train Loss at iteration 7260: 0.04332911717274632\n",
      "Train Loss at iteration 7261: 0.04332902847916234\n",
      "Train Loss at iteration 7262: 0.04332893979768864\n",
      "Train Loss at iteration 7263: 0.04332885112832303\n",
      "Train Loss at iteration 7264: 0.0433287624710633\n",
      "Train Loss at iteration 7265: 0.04332867382590724\n",
      "Train Loss at iteration 7266: 0.04332858519285267\n",
      "Train Loss at iteration 7267: 0.0433284965718974\n",
      "Train Loss at iteration 7268: 0.04332840796303922\n",
      "Train Loss at iteration 7269: 0.04332831936627592\n",
      "Train Loss at iteration 7270: 0.043328230781605324\n",
      "Train Loss at iteration 7271: 0.04332814220902524\n",
      "Train Loss at iteration 7272: 0.043328053648533454\n",
      "Train Loss at iteration 7273: 0.0433279651001278\n",
      "Train Loss at iteration 7274: 0.04332787656380607\n",
      "Train Loss at iteration 7275: 0.04332778803956606\n",
      "Train Loss at iteration 7276: 0.04332769952740559\n",
      "Train Loss at iteration 7277: 0.04332761102732246\n",
      "Train Loss at iteration 7278: 0.04332752253931449\n",
      "Train Loss at iteration 7279: 0.04332743406337948\n",
      "Train Loss at iteration 7280: 0.04332734559951524\n",
      "Train Loss at iteration 7281: 0.043327257147719574\n",
      "Train Loss at iteration 7282: 0.04332716870799031\n",
      "Train Loss at iteration 7283: 0.04332708028032525\n",
      "Train Loss at iteration 7284: 0.043326991864722206\n",
      "Train Loss at iteration 7285: 0.04332690346117898\n",
      "Train Loss at iteration 7286: 0.043326815069693395\n",
      "Train Loss at iteration 7287: 0.04332672669026326\n",
      "Train Loss at iteration 7288: 0.043326638322886385\n",
      "Train Loss at iteration 7289: 0.04332654996756061\n",
      "Train Loss at iteration 7290: 0.043326461624283705\n",
      "Train Loss at iteration 7291: 0.04332637329305351\n",
      "Train Loss at iteration 7292: 0.04332628497386785\n",
      "Train Loss at iteration 7293: 0.04332619666672452\n",
      "Train Loss at iteration 7294: 0.04332610837162135\n",
      "Train Loss at iteration 7295: 0.04332602008855615\n",
      "Train Loss at iteration 7296: 0.043325931817526746\n",
      "Train Loss at iteration 7297: 0.04332584355853093\n",
      "Train Loss at iteration 7298: 0.043325755311566565\n",
      "Train Loss at iteration 7299: 0.04332566707663142\n",
      "Train Loss at iteration 7300: 0.04332557885372336\n",
      "Train Loss at iteration 7301: 0.04332549064284017\n",
      "Train Loss at iteration 7302: 0.04332540244397969\n",
      "Train Loss at iteration 7303: 0.04332531425713973\n",
      "Train Loss at iteration 7304: 0.043325226082318126\n",
      "Train Loss at iteration 7305: 0.043325137919512674\n",
      "Train Loss at iteration 7306: 0.04332504976872121\n",
      "Train Loss at iteration 7307: 0.04332496162994157\n",
      "Train Loss at iteration 7308: 0.04332487350317158\n",
      "Train Loss at iteration 7309: 0.043324785388409025\n",
      "Train Loss at iteration 7310: 0.04332469728565177\n",
      "Train Loss at iteration 7311: 0.043324609194897626\n",
      "Train Loss at iteration 7312: 0.043324521116144404\n",
      "Train Loss at iteration 7313: 0.04332443304938994\n",
      "Train Loss at iteration 7314: 0.043324344994632065\n",
      "Train Loss at iteration 7315: 0.0433242569518686\n",
      "Train Loss at iteration 7316: 0.043324168921097396\n",
      "Train Loss at iteration 7317: 0.043324080902316243\n",
      "Train Loss at iteration 7318: 0.043323992895522995\n",
      "Train Loss at iteration 7319: 0.04332390490071545\n",
      "Train Loss at iteration 7320: 0.04332381691789149\n",
      "Train Loss at iteration 7321: 0.04332372894704889\n",
      "Train Loss at iteration 7322: 0.04332364098818551\n",
      "Train Loss at iteration 7323: 0.043323553041299166\n",
      "Train Loss at iteration 7324: 0.0433234651063877\n",
      "Train Loss at iteration 7325: 0.04332337718344896\n",
      "Train Loss at iteration 7326: 0.04332328927248074\n",
      "Train Loss at iteration 7327: 0.0433232013734809\n",
      "Train Loss at iteration 7328: 0.043323113486447254\n",
      "Train Loss at iteration 7329: 0.043323025611377644\n",
      "Train Loss at iteration 7330: 0.043322937748269924\n",
      "Train Loss at iteration 7331: 0.04332284989712191\n",
      "Train Loss at iteration 7332: 0.043322762057931424\n",
      "Train Loss at iteration 7333: 0.04332267423069631\n",
      "Train Loss at iteration 7334: 0.04332258641541442\n",
      "Train Loss at iteration 7335: 0.043322498612083586\n",
      "Train Loss at iteration 7336: 0.04332241082070164\n",
      "Train Loss at iteration 7337: 0.04332232304126641\n",
      "Train Loss at iteration 7338: 0.04332223527377574\n",
      "Train Loss at iteration 7339: 0.04332214751822747\n",
      "Train Loss at iteration 7340: 0.043322059774619455\n",
      "Train Loss at iteration 7341: 0.043321972042949505\n",
      "Train Loss at iteration 7342: 0.04332188432321549\n",
      "Train Loss at iteration 7343: 0.04332179661541521\n",
      "Train Loss at iteration 7344: 0.04332170891954655\n",
      "Train Loss at iteration 7345: 0.04332162123560732\n",
      "Train Loss at iteration 7346: 0.04332153356359538\n",
      "Train Loss at iteration 7347: 0.04332144590350857\n",
      "Train Loss at iteration 7348: 0.04332135825534471\n",
      "Train Loss at iteration 7349: 0.043321270619101675\n",
      "Train Loss at iteration 7350: 0.04332118299477729\n",
      "Train Loss at iteration 7351: 0.04332109538236942\n",
      "Train Loss at iteration 7352: 0.043321007781875885\n",
      "Train Loss at iteration 7353: 0.04332092019329454\n",
      "Train Loss at iteration 7354: 0.04332083261662322\n",
      "Train Loss at iteration 7355: 0.043320745051859795\n",
      "Train Loss at iteration 7356: 0.0433206574990021\n",
      "Train Loss at iteration 7357: 0.04332056995804798\n",
      "Train Loss at iteration 7358: 0.04332048242899529\n",
      "Train Loss at iteration 7359: 0.04332039491184186\n",
      "Train Loss at iteration 7360: 0.043320307406585545\n",
      "Train Loss at iteration 7361: 0.04332021991322421\n",
      "Train Loss at iteration 7362: 0.0433201324317557\n",
      "Train Loss at iteration 7363: 0.04332004496217785\n",
      "Train Loss at iteration 7364: 0.043319957504488533\n",
      "Train Loss at iteration 7365: 0.04331987005868557\n",
      "Train Loss at iteration 7366: 0.04331978262476684\n",
      "Train Loss at iteration 7367: 0.04331969520273018\n",
      "Train Loss at iteration 7368: 0.04331960779257344\n",
      "Train Loss at iteration 7369: 0.04331952039429448\n",
      "Train Loss at iteration 7370: 0.04331943300789117\n",
      "Train Loss at iteration 7371: 0.04331934563336134\n",
      "Train Loss at iteration 7372: 0.04331925827070284\n",
      "Train Loss at iteration 7373: 0.043319170919913566\n",
      "Train Loss at iteration 7374: 0.04331908358099132\n",
      "Train Loss at iteration 7375: 0.043318996253933977\n",
      "Train Loss at iteration 7376: 0.04331890893873941\n",
      "Train Loss at iteration 7377: 0.04331882163540546\n",
      "Train Loss at iteration 7378: 0.043318734343929986\n",
      "Train Loss at iteration 7379: 0.043318647064310864\n",
      "Train Loss at iteration 7380: 0.043318559796545926\n",
      "Train Loss at iteration 7381: 0.043318472540633035\n",
      "Train Loss at iteration 7382: 0.04331838529657007\n",
      "Train Loss at iteration 7383: 0.04331829806435486\n",
      "Train Loss at iteration 7384: 0.0433182108439853\n",
      "Train Loss at iteration 7385: 0.04331812363545923\n",
      "Train Loss at iteration 7386: 0.04331803643877451\n",
      "Train Loss at iteration 7387: 0.043317949253929004\n",
      "Train Loss at iteration 7388: 0.043317862080920574\n",
      "Train Loss at iteration 7389: 0.0433177749197471\n",
      "Train Loss at iteration 7390: 0.04331768777040641\n",
      "Train Loss at iteration 7391: 0.04331760063289641\n",
      "Train Loss at iteration 7392: 0.043317513507214925\n",
      "Train Loss at iteration 7393: 0.04331742639335984\n",
      "Train Loss at iteration 7394: 0.043317339291329006\n",
      "Train Loss at iteration 7395: 0.0433172522011203\n",
      "Train Loss at iteration 7396: 0.0433171651227316\n",
      "Train Loss at iteration 7397: 0.043317078056160745\n",
      "Train Loss at iteration 7398: 0.04331699100140562\n",
      "Train Loss at iteration 7399: 0.043316903958464104\n",
      "Train Loss at iteration 7400: 0.04331681692733403\n",
      "Train Loss at iteration 7401: 0.04331672990801328\n",
      "Train Loss at iteration 7402: 0.04331664290049975\n",
      "Train Loss at iteration 7403: 0.04331655590479127\n",
      "Train Loss at iteration 7404: 0.04331646892088573\n",
      "Train Loss at iteration 7405: 0.043316381948781\n",
      "Train Loss at iteration 7406: 0.043316294988474946\n",
      "Train Loss at iteration 7407: 0.04331620803996545\n",
      "Train Loss at iteration 7408: 0.04331612110325036\n",
      "Train Loss at iteration 7409: 0.043316034178327564\n",
      "Train Loss at iteration 7410: 0.04331594726519493\n",
      "Train Loss at iteration 7411: 0.04331586036385035\n",
      "Train Loss at iteration 7412: 0.04331577347429168\n",
      "Train Loss at iteration 7413: 0.043315686596516786\n",
      "Train Loss at iteration 7414: 0.04331559973052356\n",
      "Train Loss at iteration 7415: 0.043315512876309865\n",
      "Train Loss at iteration 7416: 0.043315426033873575\n",
      "Train Loss at iteration 7417: 0.04331533920321259\n",
      "Train Loss at iteration 7418: 0.043315252384324744\n",
      "Train Loss at iteration 7419: 0.04331516557720796\n",
      "Train Loss at iteration 7420: 0.043315078781860084\n",
      "Train Loss at iteration 7421: 0.04331499199827899\n",
      "Train Loss at iteration 7422: 0.043314905226462586\n",
      "Train Loss at iteration 7423: 0.043314818466408736\n",
      "Train Loss at iteration 7424: 0.04331473171811531\n",
      "Train Loss at iteration 7425: 0.0433146449815802\n",
      "Train Loss at iteration 7426: 0.043314558256801285\n",
      "Train Loss at iteration 7427: 0.04331447154377643\n",
      "Train Loss at iteration 7428: 0.04331438484250354\n",
      "Train Loss at iteration 7429: 0.04331429815298048\n",
      "Train Loss at iteration 7430: 0.04331421147520513\n",
      "Train Loss at iteration 7431: 0.04331412480917538\n",
      "Train Loss at iteration 7432: 0.04331403815488912\n",
      "Train Loss at iteration 7433: 0.04331395151234422\n",
      "Train Loss at iteration 7434: 0.04331386488153857\n",
      "Train Loss at iteration 7435: 0.04331377826247006\n",
      "Train Loss at iteration 7436: 0.043313691655136564\n",
      "Train Loss at iteration 7437: 0.04331360505953597\n",
      "Train Loss at iteration 7438: 0.043313518475666175\n",
      "Train Loss at iteration 7439: 0.04331343190352505\n",
      "Train Loss at iteration 7440: 0.0433133453431105\n",
      "Train Loss at iteration 7441: 0.04331325879442039\n",
      "Train Loss at iteration 7442: 0.04331317225745262\n",
      "Train Loss at iteration 7443: 0.04331308573220507\n",
      "Train Loss at iteration 7444: 0.04331299921867563\n",
      "Train Loss at iteration 7445: 0.043312912716862204\n",
      "Train Loss at iteration 7446: 0.04331282622676268\n",
      "Train Loss at iteration 7447: 0.043312739748374934\n",
      "Train Loss at iteration 7448: 0.04331265328169685\n",
      "Train Loss at iteration 7449: 0.04331256682672635\n",
      "Train Loss at iteration 7450: 0.043312480383461305\n",
      "Train Loss at iteration 7451: 0.0433123939518996\n",
      "Train Loss at iteration 7452: 0.04331230753203912\n",
      "Train Loss at iteration 7453: 0.043312221123877795\n",
      "Train Loss at iteration 7454: 0.04331213472741351\n",
      "Train Loss at iteration 7455: 0.04331204834264411\n",
      "Train Loss at iteration 7456: 0.04331196196956755\n",
      "Train Loss at iteration 7457: 0.0433118756081817\n",
      "Train Loss at iteration 7458: 0.04331178925848445\n",
      "Train Loss at iteration 7459: 0.0433117029204737\n",
      "Train Loss at iteration 7460: 0.043311616594147354\n",
      "Train Loss at iteration 7461: 0.043311530279503296\n",
      "Train Loss at iteration 7462: 0.043311443976539424\n",
      "Train Loss at iteration 7463: 0.04331135768525365\n",
      "Train Loss at iteration 7464: 0.04331127140564386\n",
      "Train Loss at iteration 7465: 0.04331118513770796\n",
      "Train Loss at iteration 7466: 0.043311098881443834\n",
      "Train Loss at iteration 7467: 0.04331101263684938\n",
      "Train Loss at iteration 7468: 0.04331092640392253\n",
      "Train Loss at iteration 7469: 0.04331084018266115\n",
      "Train Loss at iteration 7470: 0.04331075397306316\n",
      "Train Loss at iteration 7471: 0.04331066777512646\n",
      "Train Loss at iteration 7472: 0.04331058158884894\n",
      "Train Loss at iteration 7473: 0.0433104954142285\n",
      "Train Loss at iteration 7474: 0.04331040925126307\n",
      "Train Loss at iteration 7475: 0.04331032309995053\n",
      "Train Loss at iteration 7476: 0.04331023696028878\n",
      "Train Loss at iteration 7477: 0.04331015083227574\n",
      "Train Loss at iteration 7478: 0.0433100647159093\n",
      "Train Loss at iteration 7479: 0.043309978611187384\n",
      "Train Loss at iteration 7480: 0.04330989251810787\n",
      "Train Loss at iteration 7481: 0.043309806436668694\n",
      "Train Loss at iteration 7482: 0.04330972036686775\n",
      "Train Loss at iteration 7483: 0.043309634308702924\n",
      "Train Loss at iteration 7484: 0.043309548262172155\n",
      "Train Loss at iteration 7485: 0.043309462227273325\n",
      "Train Loss at iteration 7486: 0.04330937620400437\n",
      "Train Loss at iteration 7487: 0.04330929019236319\n",
      "Train Loss at iteration 7488: 0.04330920419234766\n",
      "Train Loss at iteration 7489: 0.043309118203955735\n",
      "Train Loss at iteration 7490: 0.0433090322271853\n",
      "Train Loss at iteration 7491: 0.043308946262034284\n",
      "Train Loss at iteration 7492: 0.043308860308500584\n",
      "Train Loss at iteration 7493: 0.0433087743665821\n",
      "Train Loss at iteration 7494: 0.04330868843627676\n",
      "Train Loss at iteration 7495: 0.04330860251758249\n",
      "Train Loss at iteration 7496: 0.04330851661049717\n",
      "Train Loss at iteration 7497: 0.043308430715018754\n",
      "Train Loss at iteration 7498: 0.04330834483114512\n",
      "Train Loss at iteration 7499: 0.04330825895887419\n",
      "Train Loss at iteration 7500: 0.04330817309820388\n",
      "Train Loss at iteration 7501: 0.04330808724913213\n",
      "Train Loss at iteration 7502: 0.04330800141165681\n",
      "Train Loss at iteration 7503: 0.04330791558577587\n",
      "Train Loss at iteration 7504: 0.043307829771487207\n",
      "Train Loss at iteration 7505: 0.04330774396878875\n",
      "Train Loss at iteration 7506: 0.04330765817767842\n",
      "Train Loss at iteration 7507: 0.04330757239815414\n",
      "Train Loss at iteration 7508: 0.04330748663021379\n",
      "Train Loss at iteration 7509: 0.04330740087385533\n",
      "Train Loss at iteration 7510: 0.043307315129076665\n",
      "Train Loss at iteration 7511: 0.04330722939587571\n",
      "Train Loss at iteration 7512: 0.0433071436742504\n",
      "Train Loss at iteration 7513: 0.043307057964198634\n",
      "Train Loss at iteration 7514: 0.04330697226571835\n",
      "Train Loss at iteration 7515: 0.04330688657880748\n",
      "Train Loss at iteration 7516: 0.0433068009034639\n",
      "Train Loss at iteration 7517: 0.04330671523968557\n",
      "Train Loss at iteration 7518: 0.04330662958747042\n",
      "Train Loss at iteration 7519: 0.04330654394681635\n",
      "Train Loss at iteration 7520: 0.043306458317721286\n",
      "Train Loss at iteration 7521: 0.043306372700183164\n",
      "Train Loss at iteration 7522: 0.04330628709419992\n",
      "Train Loss at iteration 7523: 0.04330620149976946\n",
      "Train Loss at iteration 7524: 0.04330611591688968\n",
      "Train Loss at iteration 7525: 0.04330603034555857\n",
      "Train Loss at iteration 7526: 0.04330594478577401\n",
      "Train Loss at iteration 7527: 0.04330585923753395\n",
      "Train Loss at iteration 7528: 0.04330577370083631\n",
      "Train Loss at iteration 7529: 0.04330568817567902\n",
      "Train Loss at iteration 7530: 0.04330560266206\n",
      "Train Loss at iteration 7531: 0.043305517159977186\n",
      "Train Loss at iteration 7532: 0.04330543166942851\n",
      "Train Loss at iteration 7533: 0.043305346190411896\n",
      "Train Loss at iteration 7534: 0.04330526072292528\n",
      "Train Loss at iteration 7535: 0.04330517526696658\n",
      "Train Loss at iteration 7536: 0.04330508982253374\n",
      "Train Loss at iteration 7537: 0.04330500438962468\n",
      "Train Loss at iteration 7538: 0.043304918968237356\n",
      "Train Loss at iteration 7539: 0.043304833558369685\n",
      "Train Loss at iteration 7540: 0.04330474816001959\n",
      "Train Loss at iteration 7541: 0.04330466277318501\n",
      "Train Loss at iteration 7542: 0.0433045773978639\n",
      "Train Loss at iteration 7543: 0.04330449203405415\n",
      "Train Loss at iteration 7544: 0.04330440668175374\n",
      "Train Loss at iteration 7545: 0.043304321340960565\n",
      "Train Loss at iteration 7546: 0.043304236011672596\n",
      "Train Loss at iteration 7547: 0.043304150693887766\n",
      "Train Loss at iteration 7548: 0.043304065387604\n",
      "Train Loss at iteration 7549: 0.04330398009281923\n",
      "Train Loss at iteration 7550: 0.043303894809531396\n",
      "Train Loss at iteration 7551: 0.04330380953773844\n",
      "Train Loss at iteration 7552: 0.0433037242774383\n",
      "Train Loss at iteration 7553: 0.04330363902862892\n",
      "Train Loss at iteration 7554: 0.04330355379130823\n",
      "Train Loss at iteration 7555: 0.04330346856547417\n",
      "Train Loss at iteration 7556: 0.0433033833511247\n",
      "Train Loss at iteration 7557: 0.04330329814825772\n",
      "Train Loss at iteration 7558: 0.043303212956871215\n",
      "Train Loss at iteration 7559: 0.04330312777696309\n",
      "Train Loss at iteration 7560: 0.043303042608531316\n",
      "Train Loss at iteration 7561: 0.04330295745157382\n",
      "Train Loss at iteration 7562: 0.043302872306088556\n",
      "Train Loss at iteration 7563: 0.043302787172073455\n",
      "Train Loss at iteration 7564: 0.04330270204952645\n",
      "Train Loss at iteration 7565: 0.043302616938445525\n",
      "Train Loss at iteration 7566: 0.04330253183882859\n",
      "Train Loss at iteration 7567: 0.043302446750673594\n",
      "Train Loss at iteration 7568: 0.043302361673978496\n",
      "Train Loss at iteration 7569: 0.04330227660874123\n",
      "Train Loss at iteration 7570: 0.043302191554959756\n",
      "Train Loss at iteration 7571: 0.043302106512632006\n",
      "Train Loss at iteration 7572: 0.04330202148175594\n",
      "Train Loss at iteration 7573: 0.043301936462329486\n",
      "Train Loss at iteration 7574: 0.04330185145435063\n",
      "Train Loss at iteration 7575: 0.043301766457817276\n",
      "Train Loss at iteration 7576: 0.0433016814727274\n",
      "Train Loss at iteration 7577: 0.04330159649907894\n",
      "Train Loss at iteration 7578: 0.04330151153686986\n",
      "Train Loss at iteration 7579: 0.043301426586098105\n",
      "Train Loss at iteration 7580: 0.04330134164676161\n",
      "Train Loss at iteration 7581: 0.04330125671885836\n",
      "Train Loss at iteration 7582: 0.04330117180238626\n",
      "Train Loss at iteration 7583: 0.043301086897343306\n",
      "Train Loss at iteration 7584: 0.04330100200372744\n",
      "Train Loss at iteration 7585: 0.043300917121536595\n",
      "Train Loss at iteration 7586: 0.04330083225076873\n",
      "Train Loss at iteration 7587: 0.04330074739142182\n",
      "Train Loss at iteration 7588: 0.04330066254349381\n",
      "Train Loss at iteration 7589: 0.043300577706982646\n",
      "Train Loss at iteration 7590: 0.04330049288188629\n",
      "Train Loss at iteration 7591: 0.043300408068202706\n",
      "Train Loss at iteration 7592: 0.04330032326592983\n",
      "Train Loss at iteration 7593: 0.04330023847506563\n",
      "Train Loss at iteration 7594: 0.043300153695608065\n",
      "Train Loss at iteration 7595: 0.0433000689275551\n",
      "Train Loss at iteration 7596: 0.04329998417090469\n",
      "Train Loss at iteration 7597: 0.04329989942565477\n",
      "Train Loss at iteration 7598: 0.04329981469180333\n",
      "Train Loss at iteration 7599: 0.043299729969348305\n",
      "Train Loss at iteration 7600: 0.04329964525828767\n",
      "Train Loss at iteration 7601: 0.04329956055861938\n",
      "Train Loss at iteration 7602: 0.0432994758703414\n",
      "Train Loss at iteration 7603: 0.04329939119345169\n",
      "Train Loss at iteration 7604: 0.043299306527948195\n",
      "Train Loss at iteration 7605: 0.04329922187382892\n",
      "Train Loss at iteration 7606: 0.043299137231091786\n",
      "Train Loss at iteration 7607: 0.04329905259973476\n",
      "Train Loss at iteration 7608: 0.04329896797975584\n",
      "Train Loss at iteration 7609: 0.04329888337115294\n",
      "Train Loss at iteration 7610: 0.04329879877392406\n",
      "Train Loss at iteration 7611: 0.04329871418806717\n",
      "Train Loss at iteration 7612: 0.04329862961358019\n",
      "Train Loss at iteration 7613: 0.04329854505046112\n",
      "Train Loss at iteration 7614: 0.04329846049870794\n",
      "Train Loss at iteration 7615: 0.04329837595831859\n",
      "Train Loss at iteration 7616: 0.04329829142929104\n",
      "Train Loss at iteration 7617: 0.04329820691162327\n",
      "Train Loss at iteration 7618: 0.043298122405313236\n",
      "Train Loss at iteration 7619: 0.04329803791035892\n",
      "Train Loss at iteration 7620: 0.043297953426758254\n",
      "Train Loss at iteration 7621: 0.04329786895450927\n",
      "Train Loss at iteration 7622: 0.04329778449360987\n",
      "Train Loss at iteration 7623: 0.043297700044058085\n",
      "Train Loss at iteration 7624: 0.04329761560585183\n",
      "Train Loss at iteration 7625: 0.0432975311789891\n",
      "Train Loss at iteration 7626: 0.043297446763467896\n",
      "Train Loss at iteration 7627: 0.04329736235928615\n",
      "Train Loss at iteration 7628: 0.04329727796644185\n",
      "Train Loss at iteration 7629: 0.04329719358493297\n",
      "Train Loss at iteration 7630: 0.04329710921475746\n",
      "Train Loss at iteration 7631: 0.04329702485591332\n",
      "Train Loss at iteration 7632: 0.04329694050839852\n",
      "Train Loss at iteration 7633: 0.043296856172211025\n",
      "Train Loss at iteration 7634: 0.043296771847348835\n",
      "Train Loss at iteration 7635: 0.04329668753380989\n",
      "Train Loss at iteration 7636: 0.043296603231592186\n",
      "Train Loss at iteration 7637: 0.0432965189406937\n",
      "Train Loss at iteration 7638: 0.0432964346611124\n",
      "Train Loss at iteration 7639: 0.04329635039284627\n",
      "Train Loss at iteration 7640: 0.04329626613589328\n",
      "Train Loss at iteration 7641: 0.043296181890251424\n",
      "Train Loss at iteration 7642: 0.04329609765591866\n",
      "Train Loss at iteration 7643: 0.043296013432892966\n",
      "Train Loss at iteration 7644: 0.04329592922117234\n",
      "Train Loss at iteration 7645: 0.04329584502075476\n",
      "Train Loss at iteration 7646: 0.04329576083163819\n",
      "Train Loss at iteration 7647: 0.04329567665382062\n",
      "Train Loss at iteration 7648: 0.04329559248730004\n",
      "Train Loss at iteration 7649: 0.04329550833207442\n",
      "Train Loss at iteration 7650: 0.04329542418814174\n",
      "Train Loss at iteration 7651: 0.043295340055499995\n",
      "Train Loss at iteration 7652: 0.043295255934147146\n",
      "Train Loss at iteration 7653: 0.04329517182408119\n",
      "Train Loss at iteration 7654: 0.04329508772530012\n",
      "Train Loss at iteration 7655: 0.043295003637801914\n",
      "Train Loss at iteration 7656: 0.04329491956158454\n",
      "Train Loss at iteration 7657: 0.04329483549664601\n",
      "Train Loss at iteration 7658: 0.04329475144298429\n",
      "Train Loss at iteration 7659: 0.04329466740059738\n",
      "Train Loss at iteration 7660: 0.04329458336948324\n",
      "Train Loss at iteration 7661: 0.04329449934963988\n",
      "Train Loss at iteration 7662: 0.04329441534106528\n",
      "Train Loss at iteration 7663: 0.04329433134375744\n",
      "Train Loss at iteration 7664: 0.04329424735771433\n",
      "Train Loss at iteration 7665: 0.04329416338293394\n",
      "Train Loss at iteration 7666: 0.04329407941941428\n",
      "Train Loss at iteration 7667: 0.04329399546715332\n",
      "Train Loss at iteration 7668: 0.043293911526149045\n",
      "Train Loss at iteration 7669: 0.04329382759639946\n",
      "Train Loss at iteration 7670: 0.04329374367790254\n",
      "Train Loss at iteration 7671: 0.043293659770656294\n",
      "Train Loss at iteration 7672: 0.04329357587465871\n",
      "Train Loss at iteration 7673: 0.04329349198990778\n",
      "Train Loss at iteration 7674: 0.04329340811640147\n",
      "Train Loss at iteration 7675: 0.0432933242541378\n",
      "Train Loss at iteration 7676: 0.043293240403114774\n",
      "Train Loss at iteration 7677: 0.04329315656333036\n",
      "Train Loss at iteration 7678: 0.04329307273478256\n",
      "Train Loss at iteration 7679: 0.043292988917469376\n",
      "Train Loss at iteration 7680: 0.043292905111388794\n",
      "Train Loss at iteration 7681: 0.043292821316538804\n",
      "Train Loss at iteration 7682: 0.043292737532917416\n",
      "Train Loss at iteration 7683: 0.04329265376052262\n",
      "Train Loss at iteration 7684: 0.043292569999352416\n",
      "Train Loss at iteration 7685: 0.0432924862494048\n",
      "Train Loss at iteration 7686: 0.04329240251067777\n",
      "Train Loss at iteration 7687: 0.04329231878316932\n",
      "Train Loss at iteration 7688: 0.043292235066877446\n",
      "Train Loss at iteration 7689: 0.043292151361800146\n",
      "Train Loss at iteration 7690: 0.04329206766793545\n",
      "Train Loss at iteration 7691: 0.043291983985281314\n",
      "Train Loss at iteration 7692: 0.04329190031383575\n",
      "Train Loss at iteration 7693: 0.04329181665359678\n",
      "Train Loss at iteration 7694: 0.04329173300456238\n",
      "Train Loss at iteration 7695: 0.043291649366730564\n",
      "Train Loss at iteration 7696: 0.04329156574009933\n",
      "Train Loss at iteration 7697: 0.043291482124666686\n",
      "Train Loss at iteration 7698: 0.043291398520430624\n",
      "Train Loss at iteration 7699: 0.04329131492738916\n",
      "Train Loss at iteration 7700: 0.0432912313455403\n",
      "Train Loss at iteration 7701: 0.043291147774882015\n",
      "Train Loss at iteration 7702: 0.04329106421541235\n",
      "Train Loss at iteration 7703: 0.0432909806671293\n",
      "Train Loss at iteration 7704: 0.04329089713003085\n",
      "Train Loss at iteration 7705: 0.043290813604115026\n",
      "Train Loss at iteration 7706: 0.043290730089379836\n",
      "Train Loss at iteration 7707: 0.043290646585823274\n",
      "Train Loss at iteration 7708: 0.04329056309344336\n",
      "Train Loss at iteration 7709: 0.04329047961223807\n",
      "Train Loss at iteration 7710: 0.04329039614220545\n",
      "Train Loss at iteration 7711: 0.04329031268334349\n",
      "Train Loss at iteration 7712: 0.04329022923565021\n",
      "Train Loss at iteration 7713: 0.04329014579912361\n",
      "Train Loss at iteration 7714: 0.043290062373761685\n",
      "Train Loss at iteration 7715: 0.04328997895956248\n",
      "Train Loss at iteration 7716: 0.04328989555652397\n",
      "Train Loss at iteration 7717: 0.0432898121646442\n",
      "Train Loss at iteration 7718: 0.043289728783921164\n",
      "Train Loss at iteration 7719: 0.04328964541435286\n",
      "Train Loss at iteration 7720: 0.043289562055937314\n",
      "Train Loss at iteration 7721: 0.04328947870867255\n",
      "Train Loss at iteration 7722: 0.04328939537255656\n",
      "Train Loss at iteration 7723: 0.043289312047587375\n",
      "Train Loss at iteration 7724: 0.04328922873376301\n",
      "Train Loss at iteration 7725: 0.04328914543108144\n",
      "Train Loss at iteration 7726: 0.043289062139540736\n",
      "Train Loss at iteration 7727: 0.04328897885913887\n",
      "Train Loss at iteration 7728: 0.043288895589873894\n",
      "Train Loss at iteration 7729: 0.04328881233174378\n",
      "Train Loss at iteration 7730: 0.04328872908474659\n",
      "Train Loss at iteration 7731: 0.04328864584888032\n",
      "Train Loss at iteration 7732: 0.04328856262414297\n",
      "Train Loss at iteration 7733: 0.04328847941053259\n",
      "Train Loss at iteration 7734: 0.04328839620804718\n",
      "Train Loss at iteration 7735: 0.04328831301668477\n",
      "Train Loss at iteration 7736: 0.04328822983644336\n",
      "Train Loss at iteration 7737: 0.04328814666732098\n",
      "Train Loss at iteration 7738: 0.043288063509315654\n",
      "Train Loss at iteration 7739: 0.043287980362425385\n",
      "Train Loss at iteration 7740: 0.04328789722664822\n",
      "Train Loss at iteration 7741: 0.043287814101982174\n",
      "Train Loss at iteration 7742: 0.043287730988425255\n",
      "Train Loss at iteration 7743: 0.043287647885975486\n",
      "Train Loss at iteration 7744: 0.0432875647946309\n",
      "Train Loss at iteration 7745: 0.043287481714389515\n",
      "Train Loss at iteration 7746: 0.043287398645249345\n",
      "Train Loss at iteration 7747: 0.043287315587208434\n",
      "Train Loss at iteration 7748: 0.04328723254026479\n",
      "Train Loss at iteration 7749: 0.04328714950441645\n",
      "Train Loss at iteration 7750: 0.04328706647966144\n",
      "Train Loss at iteration 7751: 0.04328698346599774\n",
      "Train Loss at iteration 7752: 0.043286900463423456\n",
      "Train Loss at iteration 7753: 0.043286817471936544\n",
      "Train Loss at iteration 7754: 0.043286734491535056\n",
      "Train Loss at iteration 7755: 0.04328665152221704\n",
      "Train Loss at iteration 7756: 0.0432865685639805\n",
      "Train Loss at iteration 7757: 0.043286485616823446\n",
      "Train Loss at iteration 7758: 0.04328640268074395\n",
      "Train Loss at iteration 7759: 0.04328631975574001\n",
      "Train Loss at iteration 7760: 0.04328623684180966\n",
      "Train Loss at iteration 7761: 0.04328615393895095\n",
      "Train Loss at iteration 7762: 0.04328607104716189\n",
      "Train Loss at iteration 7763: 0.043285988166440505\n",
      "Train Loss at iteration 7764: 0.04328590529678484\n",
      "Train Loss at iteration 7765: 0.043285822438192914\n",
      "Train Loss at iteration 7766: 0.04328573959066277\n",
      "Train Loss at iteration 7767: 0.04328565675419243\n",
      "Train Loss at iteration 7768: 0.04328557392877995\n",
      "Train Loss at iteration 7769: 0.04328549111442333\n",
      "Train Loss at iteration 7770: 0.043285408311120634\n",
      "Train Loss at iteration 7771: 0.04328532551886987\n",
      "Train Loss at iteration 7772: 0.04328524273766907\n",
      "Train Loss at iteration 7773: 0.0432851599675163\n",
      "Train Loss at iteration 7774: 0.043285077208409566\n",
      "Train Loss at iteration 7775: 0.04328499446034693\n",
      "Train Loss at iteration 7776: 0.0432849117233264\n",
      "Train Loss at iteration 7777: 0.04328482899734603\n",
      "Train Loss at iteration 7778: 0.043284746282403835\n",
      "Train Loss at iteration 7779: 0.04328466357849788\n",
      "Train Loss at iteration 7780: 0.0432845808856262\n",
      "Train Loss at iteration 7781: 0.04328449820378681\n",
      "Train Loss at iteration 7782: 0.043284415532977766\n",
      "Train Loss at iteration 7783: 0.04328433287319711\n",
      "Train Loss at iteration 7784: 0.04328425022444287\n",
      "Train Loss at iteration 7785: 0.04328416758671308\n",
      "Train Loss at iteration 7786: 0.04328408496000579\n",
      "Train Loss at iteration 7787: 0.04328400234431906\n",
      "Train Loss at iteration 7788: 0.0432839197396509\n",
      "Train Loss at iteration 7789: 0.04328383714599936\n",
      "Train Loss at iteration 7790: 0.04328375456336248\n",
      "Train Loss at iteration 7791: 0.043283671991738326\n",
      "Train Loss at iteration 7792: 0.043283589431124904\n",
      "Train Loss at iteration 7793: 0.04328350688152027\n",
      "Train Loss at iteration 7794: 0.04328342434292249\n",
      "Train Loss at iteration 7795: 0.04328334181532959\n",
      "Train Loss at iteration 7796: 0.0432832592987396\n",
      "Train Loss at iteration 7797: 0.04328317679315057\n",
      "Train Loss at iteration 7798: 0.04328309429856057\n",
      "Train Loss at iteration 7799: 0.04328301181496762\n",
      "Train Loss at iteration 7800: 0.04328292934236978\n",
      "Train Loss at iteration 7801: 0.0432828468807651\n",
      "Train Loss at iteration 7802: 0.0432827644301516\n",
      "Train Loss at iteration 7803: 0.04328268199052735\n",
      "Train Loss at iteration 7804: 0.0432825995618904\n",
      "Train Loss at iteration 7805: 0.0432825171442388\n",
      "Train Loss at iteration 7806: 0.04328243473757057\n",
      "Train Loss at iteration 7807: 0.04328235234188378\n",
      "Train Loss at iteration 7808: 0.04328226995717649\n",
      "Train Loss at iteration 7809: 0.043282187583446714\n",
      "Train Loss at iteration 7810: 0.04328210522069255\n",
      "Train Loss at iteration 7811: 0.04328202286891201\n",
      "Train Loss at iteration 7812: 0.04328194052810316\n",
      "Train Loss at iteration 7813: 0.04328185819826406\n",
      "Train Loss at iteration 7814: 0.043281775879392734\n",
      "Train Loss at iteration 7815: 0.043281693571487256\n",
      "Train Loss at iteration 7816: 0.04328161127454569\n",
      "Train Loss at iteration 7817: 0.043281528988566055\n",
      "Train Loss at iteration 7818: 0.043281446713546425\n",
      "Train Loss at iteration 7819: 0.04328136444948486\n",
      "Train Loss at iteration 7820: 0.043281282196379396\n",
      "Train Loss at iteration 7821: 0.0432811999542281\n",
      "Train Loss at iteration 7822: 0.04328111772302903\n",
      "Train Loss at iteration 7823: 0.04328103550278024\n",
      "Train Loss at iteration 7824: 0.043280953293479765\n",
      "Train Loss at iteration 7825: 0.04328087109512569\n",
      "Train Loss at iteration 7826: 0.04328078890771605\n",
      "Train Loss at iteration 7827: 0.04328070673124892\n",
      "Train Loss at iteration 7828: 0.04328062456572234\n",
      "Train Loss at iteration 7829: 0.04328054241113439\n",
      "Train Loss at iteration 7830: 0.04328046026748311\n",
      "Train Loss at iteration 7831: 0.04328037813476656\n",
      "Train Loss at iteration 7832: 0.043280296012982805\n",
      "Train Loss at iteration 7833: 0.04328021390212991\n",
      "Train Loss at iteration 7834: 0.04328013180220592\n",
      "Train Loss at iteration 7835: 0.04328004971320891\n",
      "Train Loss at iteration 7836: 0.04327996763513691\n",
      "Train Loss at iteration 7837: 0.043279885567988036\n",
      "Train Loss at iteration 7838: 0.04327980351176031\n",
      "Train Loss at iteration 7839: 0.04327972146645181\n",
      "Train Loss at iteration 7840: 0.043279639432060577\n",
      "Train Loss at iteration 7841: 0.04327955740858469\n",
      "Train Loss at iteration 7842: 0.04327947539602223\n",
      "Train Loss at iteration 7843: 0.04327939339437121\n",
      "Train Loss at iteration 7844: 0.043279311403629755\n",
      "Train Loss at iteration 7845: 0.04327922942379589\n",
      "Train Loss at iteration 7846: 0.043279147454867675\n",
      "Train Loss at iteration 7847: 0.04327906549684321\n",
      "Train Loss at iteration 7848: 0.043278983549720526\n",
      "Train Loss at iteration 7849: 0.04327890161349771\n",
      "Train Loss at iteration 7850: 0.043278819688172834\n",
      "Train Loss at iteration 7851: 0.04327873777374394\n",
      "Train Loss at iteration 7852: 0.043278655870209105\n",
      "Train Loss at iteration 7853: 0.043278573977566424\n",
      "Train Loss at iteration 7854: 0.04327849209581392\n",
      "Train Loss at iteration 7855: 0.04327841022494969\n",
      "Train Loss at iteration 7856: 0.04327832836497181\n",
      "Train Loss at iteration 7857: 0.04327824651587831\n",
      "Train Loss at iteration 7858: 0.043278164677667305\n",
      "Train Loss at iteration 7859: 0.04327808285033683\n",
      "Train Loss at iteration 7860: 0.043278001033885\n",
      "Train Loss at iteration 7861: 0.04327791922830984\n",
      "Train Loss at iteration 7862: 0.04327783743360943\n",
      "Train Loss at iteration 7863: 0.04327775564978186\n",
      "Train Loss at iteration 7864: 0.0432776738768252\n",
      "Train Loss at iteration 7865: 0.043277592114737515\n",
      "Train Loss at iteration 7866: 0.043277510363516875\n",
      "Train Loss at iteration 7867: 0.04327742862316136\n",
      "Train Loss at iteration 7868: 0.04327734689366904\n",
      "Train Loss at iteration 7869: 0.04327726517503801\n",
      "Train Loss at iteration 7870: 0.04327718346726629\n",
      "Train Loss at iteration 7871: 0.043277101770352\n",
      "Train Loss at iteration 7872: 0.043277020084293225\n",
      "Train Loss at iteration 7873: 0.043276938409088014\n",
      "Train Loss at iteration 7874: 0.04327685674473445\n",
      "Train Loss at iteration 7875: 0.04327677509123062\n",
      "Train Loss at iteration 7876: 0.04327669344857458\n",
      "Train Loss at iteration 7877: 0.043276611816764426\n",
      "Train Loss at iteration 7878: 0.04327653019579823\n",
      "Train Loss at iteration 7879: 0.04327644858567408\n",
      "Train Loss at iteration 7880: 0.04327636698639002\n",
      "Train Loss at iteration 7881: 0.04327628539794418\n",
      "Train Loss at iteration 7882: 0.04327620382033462\n",
      "Train Loss at iteration 7883: 0.043276122253559396\n",
      "Train Loss at iteration 7884: 0.043276040697616605\n",
      "Train Loss at iteration 7885: 0.04327595915250433\n",
      "Train Loss at iteration 7886: 0.04327587761822066\n",
      "Train Loss at iteration 7887: 0.04327579609476366\n",
      "Train Loss at iteration 7888: 0.04327571458213145\n",
      "Train Loss at iteration 7889: 0.04327563308032205\n",
      "Train Loss at iteration 7890: 0.04327555158933359\n",
      "Train Loss at iteration 7891: 0.04327547010916413\n",
      "Train Loss at iteration 7892: 0.043275388639811764\n",
      "Train Loss at iteration 7893: 0.0432753071812746\n",
      "Train Loss at iteration 7894: 0.04327522573355066\n",
      "Train Loss at iteration 7895: 0.043275144296638084\n",
      "Train Loss at iteration 7896: 0.043275062870534946\n",
      "Train Loss at iteration 7897: 0.0432749814552393\n",
      "Train Loss at iteration 7898: 0.043274900050749285\n",
      "Train Loss at iteration 7899: 0.04327481865706294\n",
      "Train Loss at iteration 7900: 0.04327473727417838\n",
      "Train Loss at iteration 7901: 0.043274655902093694\n",
      "Train Loss at iteration 7902: 0.04327457454080695\n",
      "Train Loss at iteration 7903: 0.043274493190316246\n",
      "Train Loss at iteration 7904: 0.04327441185061966\n",
      "Train Loss at iteration 7905: 0.04327433052171531\n",
      "Train Loss at iteration 7906: 0.04327424920360125\n",
      "Train Loss at iteration 7907: 0.04327416789627559\n",
      "Train Loss at iteration 7908: 0.04327408659973642\n",
      "Train Loss at iteration 7909: 0.04327400531398183\n",
      "Train Loss at iteration 7910: 0.04327392403900989\n",
      "Train Loss at iteration 7911: 0.04327384277481873\n",
      "Train Loss at iteration 7912: 0.043273761521406405\n",
      "Train Loss at iteration 7913: 0.043273680278771036\n",
      "Train Loss at iteration 7914: 0.043273599046910693\n",
      "Train Loss at iteration 7915: 0.043273517825823477\n",
      "Train Loss at iteration 7916: 0.043273436615507484\n",
      "Train Loss at iteration 7917: 0.043273355415960814\n",
      "Train Loss at iteration 7918: 0.04327327422718156\n",
      "Train Loss at iteration 7919: 0.04327319304916779\n",
      "Train Loss at iteration 7920: 0.04327311188191762\n",
      "Train Loss at iteration 7921: 0.04327303072542915\n",
      "Train Loss at iteration 7922: 0.04327294957970047\n",
      "Train Loss at iteration 7923: 0.04327286844472968\n",
      "Train Loss at iteration 7924: 0.04327278732051486\n",
      "Train Loss at iteration 7925: 0.04327270620705413\n",
      "Train Loss at iteration 7926: 0.04327262510434557\n",
      "Train Loss at iteration 7927: 0.04327254401238729\n",
      "Train Loss at iteration 7928: 0.04327246293117738\n",
      "Train Loss at iteration 7929: 0.04327238186071395\n",
      "Train Loss at iteration 7930: 0.04327230080099507\n",
      "Train Loss at iteration 7931: 0.04327221975201887\n",
      "Train Loss at iteration 7932: 0.043272138713783444\n",
      "Train Loss at iteration 7933: 0.043272057686286884\n",
      "Train Loss at iteration 7934: 0.0432719766695273\n",
      "Train Loss at iteration 7935: 0.043271895663502764\n",
      "Train Loss at iteration 7936: 0.043271814668211415\n",
      "Train Loss at iteration 7937: 0.04327173368365134\n",
      "Train Loss at iteration 7938: 0.04327165270982064\n",
      "Train Loss at iteration 7939: 0.04327157174671741\n",
      "Train Loss at iteration 7940: 0.043271490794339784\n",
      "Train Loss at iteration 7941: 0.04327140985268583\n",
      "Train Loss at iteration 7942: 0.04327132892175365\n",
      "Train Loss at iteration 7943: 0.043271248001541376\n",
      "Train Loss at iteration 7944: 0.043271167092047085\n",
      "Train Loss at iteration 7945: 0.04327108619326892\n",
      "Train Loss at iteration 7946: 0.04327100530520495\n",
      "Train Loss at iteration 7947: 0.04327092442785329\n",
      "Train Loss at iteration 7948: 0.04327084356121205\n",
      "Train Loss at iteration 7949: 0.04327076270527933\n",
      "Train Loss at iteration 7950: 0.04327068186005324\n",
      "Train Loss at iteration 7951: 0.0432706010255319\n",
      "Train Loss at iteration 7952: 0.0432705202017134\n",
      "Train Loss at iteration 7953: 0.04327043938859587\n",
      "Train Loss at iteration 7954: 0.043270358586177386\n",
      "Train Loss at iteration 7955: 0.04327027779445607\n",
      "Train Loss at iteration 7956: 0.04327019701343006\n",
      "Train Loss at iteration 7957: 0.04327011624309742\n",
      "Train Loss at iteration 7958: 0.04327003548345629\n",
      "Train Loss at iteration 7959: 0.043269954734504776\n",
      "Train Loss at iteration 7960: 0.04326987399624098\n",
      "Train Loss at iteration 7961: 0.04326979326866301\n",
      "Train Loss at iteration 7962: 0.04326971255176898\n",
      "Train Loss at iteration 7963: 0.04326963184555703\n",
      "Train Loss at iteration 7964: 0.043269551150025246\n",
      "Train Loss at iteration 7965: 0.043269470465171725\n",
      "Train Loss at iteration 7966: 0.043269389790994615\n",
      "Train Loss at iteration 7967: 0.043269309127492016\n",
      "Train Loss at iteration 7968: 0.04326922847466205\n",
      "Train Loss at iteration 7969: 0.043269147832502804\n",
      "Train Loss at iteration 7970: 0.043269067201012416\n",
      "Train Loss at iteration 7971: 0.043268986580188996\n",
      "Train Loss at iteration 7972: 0.04326890597003066\n",
      "Train Loss at iteration 7973: 0.04326882537053553\n",
      "Train Loss at iteration 7974: 0.04326874478170171\n",
      "Train Loss at iteration 7975: 0.043268664203527325\n",
      "Train Loss at iteration 7976: 0.04326858363601047\n",
      "Train Loss at iteration 7977: 0.043268503079149305\n",
      "Train Loss at iteration 7978: 0.04326842253294193\n",
      "Train Loss at iteration 7979: 0.043268341997386446\n",
      "Train Loss at iteration 7980: 0.04326826147248099\n",
      "Train Loss at iteration 7981: 0.04326818095822367\n",
      "Train Loss at iteration 7982: 0.04326810045461262\n",
      "Train Loss at iteration 7983: 0.043268019961645934\n",
      "Train Loss at iteration 7984: 0.04326793947932177\n",
      "Train Loss at iteration 7985: 0.04326785900763821\n",
      "Train Loss at iteration 7986: 0.043267778546593416\n",
      "Train Loss at iteration 7987: 0.04326769809618546\n",
      "Train Loss at iteration 7988: 0.04326761765641251\n",
      "Train Loss at iteration 7989: 0.04326753722727267\n",
      "Train Loss at iteration 7990: 0.04326745680876404\n",
      "Train Loss at iteration 7991: 0.04326737640088479\n",
      "Train Loss at iteration 7992: 0.043267296003632987\n",
      "Train Loss at iteration 7993: 0.04326721561700681\n",
      "Train Loss at iteration 7994: 0.04326713524100436\n",
      "Train Loss at iteration 7995: 0.043267054875623746\n",
      "Train Loss at iteration 7996: 0.04326697452086311\n",
      "Train Loss at iteration 7997: 0.0432668941767206\n",
      "Train Loss at iteration 7998: 0.043266813843194286\n",
      "Train Loss at iteration 7999: 0.04326673352028234\n",
      "Train Loss at iteration 8000: 0.043266653207982876\n",
      "Train Loss at iteration 8001: 0.043266572906293994\n",
      "Train Loss at iteration 8002: 0.043266492615213877\n",
      "Train Loss at iteration 8003: 0.043266412334740614\n",
      "Train Loss at iteration 8004: 0.04326633206487233\n",
      "Train Loss at iteration 8005: 0.04326625180560716\n",
      "Train Loss at iteration 8006: 0.04326617155694327\n",
      "Train Loss at iteration 8007: 0.043266091318878715\n",
      "Train Loss at iteration 8008: 0.04326601109141169\n",
      "Train Loss at iteration 8009: 0.04326593087454029\n",
      "Train Loss at iteration 8010: 0.04326585066826267\n",
      "Train Loss at iteration 8011: 0.04326577047257693\n",
      "Train Loss at iteration 8012: 0.043265690287481236\n",
      "Train Loss at iteration 8013: 0.043265610112973696\n",
      "Train Loss at iteration 8014: 0.04326552994905245\n",
      "Train Loss at iteration 8015: 0.04326544979571563\n",
      "Train Loss at iteration 8016: 0.04326536965296136\n",
      "Train Loss at iteration 8017: 0.043265289520787786\n",
      "Train Loss at iteration 8018: 0.04326520939919304\n",
      "Train Loss at iteration 8019: 0.04326512928817525\n",
      "Train Loss at iteration 8020: 0.043265049187732545\n",
      "Train Loss at iteration 8021: 0.04326496909786308\n",
      "Train Loss at iteration 8022: 0.04326488901856496\n",
      "Train Loss at iteration 8023: 0.043264808949836364\n",
      "Train Loss at iteration 8024: 0.043264728891675375\n",
      "Train Loss at iteration 8025: 0.043264648844080184\n",
      "Train Loss at iteration 8026: 0.043264568807048875\n",
      "Train Loss at iteration 8027: 0.04326448878057963\n",
      "Train Loss at iteration 8028: 0.04326440876467055\n",
      "Train Loss at iteration 8029: 0.043264328759319806\n",
      "Train Loss at iteration 8030: 0.04326424876452552\n",
      "Train Loss at iteration 8031: 0.043264168780285814\n",
      "Train Loss at iteration 8032: 0.043264088806598856\n",
      "Train Loss at iteration 8033: 0.043264008843462765\n",
      "Train Loss at iteration 8034: 0.04326392889087569\n",
      "Train Loss at iteration 8035: 0.04326384894883578\n",
      "Train Loss at iteration 8036: 0.04326376901734116\n",
      "Train Loss at iteration 8037: 0.043263689096389966\n",
      "Train Loss at iteration 8038: 0.04326360918598036\n",
      "Train Loss at iteration 8039: 0.043263529286110464\n",
      "Train Loss at iteration 8040: 0.04326344939677844\n",
      "Train Loss at iteration 8041: 0.043263369517982424\n",
      "Train Loss at iteration 8042: 0.043263289649720545\n",
      "Train Loss at iteration 8043: 0.04326320979199096\n",
      "Train Loss at iteration 8044: 0.043263129944791805\n",
      "Train Loss at iteration 8045: 0.04326305010812123\n",
      "Train Loss at iteration 8046: 0.04326297028197737\n",
      "Train Loss at iteration 8047: 0.04326289046635839\n",
      "Train Loss at iteration 8048: 0.04326281066126242\n",
      "Train Loss at iteration 8049: 0.0432627308666876\n",
      "Train Loss at iteration 8050: 0.043262651082632095\n",
      "Train Loss at iteration 8051: 0.043262571309094035\n",
      "Train Loss at iteration 8052: 0.04326249154607157\n",
      "Train Loss at iteration 8053: 0.04326241179356284\n",
      "Train Loss at iteration 8054: 0.04326233205156601\n",
      "Train Loss at iteration 8055: 0.043262252320079235\n",
      "Train Loss at iteration 8056: 0.043262172599100625\n",
      "Train Loss at iteration 8057: 0.04326209288862836\n",
      "Train Loss at iteration 8058: 0.043262013188660575\n",
      "Train Loss at iteration 8059: 0.04326193349919543\n",
      "Train Loss at iteration 8060: 0.043261853820231067\n",
      "Train Loss at iteration 8061: 0.04326177415176565\n",
      "Train Loss at iteration 8062: 0.04326169449379731\n",
      "Train Loss at iteration 8063: 0.0432616148463242\n",
      "Train Loss at iteration 8064: 0.043261535209344476\n",
      "Train Loss at iteration 8065: 0.0432614555828563\n",
      "Train Loss at iteration 8066: 0.04326137596685782\n",
      "Train Loss at iteration 8067: 0.043261296361347165\n",
      "Train Loss at iteration 8068: 0.043261216766322524\n",
      "Train Loss at iteration 8069: 0.04326113718178203\n",
      "Train Loss at iteration 8070: 0.043261057607723835\n",
      "Train Loss at iteration 8071: 0.0432609780441461\n",
      "Train Loss at iteration 8072: 0.04326089849104699\n",
      "Train Loss at iteration 8073: 0.04326081894842463\n",
      "Train Loss at iteration 8074: 0.04326073941627719\n",
      "Train Loss at iteration 8075: 0.043260659894602836\n",
      "Train Loss at iteration 8076: 0.043260580383399715\n",
      "Train Loss at iteration 8077: 0.043260500882665975\n",
      "Train Loss at iteration 8078: 0.043260421392399794\n",
      "Train Loss at iteration 8079: 0.0432603419125993\n",
      "Train Loss at iteration 8080: 0.04326026244326268\n",
      "Train Loss at iteration 8081: 0.04326018298438808\n",
      "Train Loss at iteration 8082: 0.04326010353597365\n",
      "Train Loss at iteration 8083: 0.04326002409801756\n",
      "Train Loss at iteration 8084: 0.04325994467051796\n",
      "Train Loss at iteration 8085: 0.04325986525347302\n",
      "Train Loss at iteration 8086: 0.04325978584688089\n",
      "Train Loss at iteration 8087: 0.04325970645073974\n",
      "Train Loss at iteration 8088: 0.04325962706504771\n",
      "Train Loss at iteration 8089: 0.04325954768980299\n",
      "Train Loss at iteration 8090: 0.04325946832500372\n",
      "Train Loss at iteration 8091: 0.04325938897064807\n",
      "Train Loss at iteration 8092: 0.0432593096267342\n",
      "Train Loss at iteration 8093: 0.043259230293260266\n",
      "Train Loss at iteration 8094: 0.04325915097022444\n",
      "Train Loss at iteration 8095: 0.04325907165762489\n",
      "Train Loss at iteration 8096: 0.04325899235545977\n",
      "Train Loss at iteration 8097: 0.043258913063727235\n",
      "Train Loss at iteration 8098: 0.04325883378242546\n",
      "Train Loss at iteration 8099: 0.043258754511552616\n",
      "Train Loss at iteration 8100: 0.043258675251106875\n",
      "Train Loss at iteration 8101: 0.04325859600108637\n",
      "Train Loss at iteration 8102: 0.04325851676148929\n",
      "Train Loss at iteration 8103: 0.0432584375323138\n",
      "Train Loss at iteration 8104: 0.043258358313558065\n",
      "Train Loss at iteration 8105: 0.04325827910522025\n",
      "Train Loss at iteration 8106: 0.04325819990729852\n",
      "Train Loss at iteration 8107: 0.04325812071979106\n",
      "Train Loss at iteration 8108: 0.043258041542696\n",
      "Train Loss at iteration 8109: 0.043257962376011545\n",
      "Train Loss at iteration 8110: 0.04325788321973585\n",
      "Train Loss at iteration 8111: 0.04325780407386709\n",
      "Train Loss at iteration 8112: 0.04325772493840342\n",
      "Train Loss at iteration 8113: 0.04325764581334303\n",
      "Train Loss at iteration 8114: 0.04325756669868408\n",
      "Train Loss at iteration 8115: 0.04325748759442473\n",
      "Train Loss at iteration 8116: 0.04325740850056317\n",
      "Train Loss at iteration 8117: 0.04325732941709756\n",
      "Train Loss at iteration 8118: 0.04325725034402608\n",
      "Train Loss at iteration 8119: 0.043257171281346894\n",
      "Train Loss at iteration 8120: 0.043257092229058186\n",
      "Train Loss at iteration 8121: 0.04325701318715811\n",
      "Train Loss at iteration 8122: 0.04325693415564486\n",
      "Train Loss at iteration 8123: 0.04325685513451659\n",
      "Train Loss at iteration 8124: 0.0432567761237715\n",
      "Train Loss at iteration 8125: 0.043256697123407746\n",
      "Train Loss at iteration 8126: 0.04325661813342349\n",
      "Train Loss at iteration 8127: 0.043256539153816935\n",
      "Train Loss at iteration 8128: 0.04325646018458624\n",
      "Train Loss at iteration 8129: 0.04325638122572959\n",
      "Train Loss at iteration 8130: 0.043256302277245154\n",
      "Train Loss at iteration 8131: 0.0432562233391311\n",
      "Train Loss at iteration 8132: 0.04325614441138564\n",
      "Train Loss at iteration 8133: 0.043256065494006915\n",
      "Train Loss at iteration 8134: 0.043255986586993124\n",
      "Train Loss at iteration 8135: 0.04325590769034243\n",
      "Train Loss at iteration 8136: 0.04325582880405302\n",
      "Train Loss at iteration 8137: 0.04325574992812307\n",
      "Train Loss at iteration 8138: 0.043255671062550766\n",
      "Train Loss at iteration 8139: 0.043255592207334284\n",
      "Train Loss at iteration 8140: 0.0432555133624718\n",
      "Train Loss at iteration 8141: 0.043255434527961496\n",
      "Train Loss at iteration 8142: 0.043255355703801554\n",
      "Train Loss at iteration 8143: 0.04325527688999017\n",
      "Train Loss at iteration 8144: 0.0432551980865255\n",
      "Train Loss at iteration 8145: 0.043255119293405725\n",
      "Train Loss at iteration 8146: 0.04325504051062906\n",
      "Train Loss at iteration 8147: 0.04325496173819366\n",
      "Train Loss at iteration 8148: 0.043254882976097725\n",
      "Train Loss at iteration 8149: 0.043254804224339416\n",
      "Train Loss at iteration 8150: 0.043254725482916936\n",
      "Train Loss at iteration 8151: 0.04325464675182847\n",
      "Train Loss at iteration 8152: 0.04325456803107217\n",
      "Train Loss at iteration 8153: 0.04325448932064628\n",
      "Train Loss at iteration 8154: 0.04325441062054894\n",
      "Train Loss at iteration 8155: 0.043254331930778336\n",
      "Train Loss at iteration 8156: 0.04325425325133267\n",
      "Train Loss at iteration 8157: 0.04325417458221014\n",
      "Train Loss at iteration 8158: 0.0432540959234089\n",
      "Train Loss at iteration 8159: 0.04325401727492716\n",
      "Train Loss at iteration 8160: 0.0432539386367631\n",
      "Train Loss at iteration 8161: 0.04325386000891492\n",
      "Train Loss at iteration 8162: 0.043253781391380794\n",
      "Train Loss at iteration 8163: 0.0432537027841589\n",
      "Train Loss at iteration 8164: 0.043253624187247465\n",
      "Train Loss at iteration 8165: 0.04325354560064465\n",
      "Train Loss at iteration 8166: 0.04325346702434864\n",
      "Train Loss at iteration 8167: 0.04325338845835765\n",
      "Train Loss at iteration 8168: 0.04325330990266984\n",
      "Train Loss at iteration 8169: 0.04325323135728343\n",
      "Train Loss at iteration 8170: 0.043253152822196585\n",
      "Train Loss at iteration 8171: 0.04325307429740752\n",
      "Train Loss at iteration 8172: 0.04325299578291441\n",
      "Train Loss at iteration 8173: 0.04325291727871546\n",
      "Train Loss at iteration 8174: 0.04325283878480886\n",
      "Train Loss at iteration 8175: 0.0432527603011928\n",
      "Train Loss at iteration 8176: 0.04325268182786547\n",
      "Train Loss at iteration 8177: 0.043252603364825074\n",
      "Train Loss at iteration 8178: 0.04325252491206979\n",
      "Train Loss at iteration 8179: 0.043252446469597826\n",
      "Train Loss at iteration 8180: 0.04325236803740738\n",
      "Train Loss at iteration 8181: 0.04325228961549664\n",
      "Train Loss at iteration 8182: 0.04325221120386379\n",
      "Train Loss at iteration 8183: 0.04325213280250705\n",
      "Train Loss at iteration 8184: 0.04325205441142461\n",
      "Train Loss at iteration 8185: 0.04325197603061465\n",
      "Train Loss at iteration 8186: 0.04325189766007538\n",
      "Train Loss at iteration 8187: 0.04325181929980501\n",
      "Train Loss at iteration 8188: 0.04325174094980172\n",
      "Train Loss at iteration 8189: 0.04325166261006371\n",
      "Train Loss at iteration 8190: 0.04325158428058917\n",
      "Train Loss at iteration 8191: 0.043251505961376335\n",
      "Train Loss at iteration 8192: 0.04325142765242337\n",
      "Train Loss at iteration 8193: 0.043251349353728485\n",
      "Train Loss at iteration 8194: 0.04325127106528988\n",
      "Train Loss at iteration 8195: 0.043251192787105756\n",
      "Train Loss at iteration 8196: 0.043251114519174315\n",
      "Train Loss at iteration 8197: 0.04325103626149376\n",
      "Train Loss at iteration 8198: 0.043250958014062285\n",
      "Train Loss at iteration 8199: 0.0432508797768781\n",
      "Train Loss at iteration 8200: 0.0432508015499394\n",
      "Train Loss at iteration 8201: 0.043250723333244395\n",
      "Train Loss at iteration 8202: 0.043250645126791276\n",
      "Train Loss at iteration 8203: 0.04325056693057827\n",
      "Train Loss at iteration 8204: 0.04325048874460356\n",
      "Train Loss at iteration 8205: 0.04325041056886536\n",
      "Train Loss at iteration 8206: 0.04325033240336187\n",
      "Train Loss at iteration 8207: 0.04325025424809129\n",
      "Train Loss at iteration 8208: 0.043250176103051846\n",
      "Train Loss at iteration 8209: 0.04325009796824171\n",
      "Train Loss at iteration 8210: 0.04325001984365912\n",
      "Train Loss at iteration 8211: 0.04324994172930227\n",
      "Train Loss at iteration 8212: 0.04324986362516937\n",
      "Train Loss at iteration 8213: 0.0432497855312586\n",
      "Train Loss at iteration 8214: 0.04324970744756821\n",
      "Train Loss at iteration 8215: 0.04324962937409639\n",
      "Train Loss at iteration 8216: 0.04324955131084134\n",
      "Train Loss at iteration 8217: 0.04324947325780128\n",
      "Train Loss at iteration 8218: 0.04324939521497441\n",
      "Train Loss at iteration 8219: 0.04324931718235896\n",
      "Train Loss at iteration 8220: 0.04324923915995311\n",
      "Train Loss at iteration 8221: 0.043249161147755094\n",
      "Train Loss at iteration 8222: 0.04324908314576309\n",
      "Train Loss at iteration 8223: 0.04324900515397535\n",
      "Train Loss at iteration 8224: 0.04324892717239007\n",
      "Train Loss at iteration 8225: 0.04324884920100544\n",
      "Train Loss at iteration 8226: 0.04324877123981971\n",
      "Train Loss at iteration 8227: 0.04324869328883106\n",
      "Train Loss at iteration 8228: 0.04324861534803771\n",
      "Train Loss at iteration 8229: 0.04324853741743788\n",
      "Train Loss at iteration 8230: 0.04324845949702979\n",
      "Train Loss at iteration 8231: 0.04324838158681164\n",
      "Train Loss at iteration 8232: 0.04324830368678166\n",
      "Train Loss at iteration 8233: 0.04324822579693804\n",
      "Train Loss at iteration 8234: 0.043248147917278996\n",
      "Train Loss at iteration 8235: 0.043248070047802775\n",
      "Train Loss at iteration 8236: 0.04324799218850757\n",
      "Train Loss at iteration 8237: 0.04324791433939159\n",
      "Train Loss at iteration 8238: 0.04324783650045307\n",
      "Train Loss at iteration 8239: 0.04324775867169022\n",
      "Train Loss at iteration 8240: 0.04324768085310126\n",
      "Train Loss at iteration 8241: 0.043247603044684384\n",
      "Train Loss at iteration 8242: 0.04324752524643784\n",
      "Train Loss at iteration 8243: 0.043247447458359824\n",
      "Train Loss at iteration 8244: 0.04324736968044857\n",
      "Train Loss at iteration 8245: 0.043247291912702296\n",
      "Train Loss at iteration 8246: 0.043247214155119215\n",
      "Train Loss at iteration 8247: 0.043247136407697534\n",
      "Train Loss at iteration 8248: 0.0432470586704355\n",
      "Train Loss at iteration 8249: 0.04324698094333132\n",
      "Train Loss at iteration 8250: 0.043246903226383214\n",
      "Train Loss at iteration 8251: 0.0432468255195894\n",
      "Train Loss at iteration 8252: 0.0432467478229481\n",
      "Train Loss at iteration 8253: 0.04324667013645754\n",
      "Train Loss at iteration 8254: 0.04324659246011595\n",
      "Train Loss at iteration 8255: 0.04324651479392154\n",
      "Train Loss at iteration 8256: 0.04324643713787253\n",
      "Train Loss at iteration 8257: 0.043246359491967146\n",
      "Train Loss at iteration 8258: 0.043246281856203626\n",
      "Train Loss at iteration 8259: 0.04324620423058017\n",
      "Train Loss at iteration 8260: 0.043246126615095026\n",
      "Train Loss at iteration 8261: 0.0432460490097464\n",
      "Train Loss at iteration 8262: 0.043245971414532534\n",
      "Train Loss at iteration 8263: 0.04324589382945163\n",
      "Train Loss at iteration 8264: 0.04324581625450194\n",
      "Train Loss at iteration 8265: 0.04324573868968167\n",
      "Train Loss at iteration 8266: 0.04324566113498906\n",
      "Train Loss at iteration 8267: 0.043245583590422326\n",
      "Train Loss at iteration 8268: 0.0432455060559797\n",
      "Train Loss at iteration 8269: 0.04324542853165941\n",
      "Train Loss at iteration 8270: 0.043245351017459674\n",
      "Train Loss at iteration 8271: 0.04324527351337875\n",
      "Train Loss at iteration 8272: 0.043245196019414824\n",
      "Train Loss at iteration 8273: 0.04324511853556617\n",
      "Train Loss at iteration 8274: 0.04324504106183098\n",
      "Train Loss at iteration 8275: 0.043244963598207495\n",
      "Train Loss at iteration 8276: 0.04324488614469395\n",
      "Train Loss at iteration 8277: 0.043244808701288565\n",
      "Train Loss at iteration 8278: 0.0432447312679896\n",
      "Train Loss at iteration 8279: 0.04324465384479524\n",
      "Train Loss at iteration 8280: 0.04324457643170376\n",
      "Train Loss at iteration 8281: 0.04324449902871337\n",
      "Train Loss at iteration 8282: 0.043244421635822296\n",
      "Train Loss at iteration 8283: 0.04324434425302879\n",
      "Train Loss at iteration 8284: 0.04324426688033107\n",
      "Train Loss at iteration 8285: 0.04324418951772737\n",
      "Train Loss at iteration 8286: 0.04324411216521593\n",
      "Train Loss at iteration 8287: 0.043244034822794986\n",
      "Train Loss at iteration 8288: 0.04324395749046278\n",
      "Train Loss at iteration 8289: 0.043243880168217515\n",
      "Train Loss at iteration 8290: 0.04324380285605745\n",
      "Train Loss at iteration 8291: 0.04324372555398081\n",
      "Train Loss at iteration 8292: 0.043243648261985845\n",
      "Train Loss at iteration 8293: 0.04324357098007078\n",
      "Train Loss at iteration 8294: 0.043243493708233854\n",
      "Train Loss at iteration 8295: 0.0432434164464733\n",
      "Train Loss at iteration 8296: 0.04324333919478737\n",
      "Train Loss at iteration 8297: 0.04324326195317429\n",
      "Train Loss at iteration 8298: 0.043243184721632286\n",
      "Train Loss at iteration 8299: 0.04324310750015962\n",
      "Train Loss at iteration 8300: 0.04324303028875451\n",
      "Train Loss at iteration 8301: 0.043242953087415215\n",
      "Train Loss at iteration 8302: 0.04324287589613996\n",
      "Train Loss at iteration 8303: 0.04324279871492699\n",
      "Train Loss at iteration 8304: 0.043242721543774544\n",
      "Train Loss at iteration 8305: 0.04324264438268085\n",
      "Train Loss at iteration 8306: 0.04324256723164417\n",
      "Train Loss at iteration 8307: 0.04324249009066273\n",
      "Train Loss at iteration 8308: 0.04324241295973479\n",
      "Train Loss at iteration 8309: 0.04324233583885859\n",
      "Train Loss at iteration 8310: 0.043242258728032335\n",
      "Train Loss at iteration 8311: 0.04324218162725429\n",
      "Train Loss at iteration 8312: 0.04324210453652271\n",
      "Train Loss at iteration 8313: 0.043242027455835844\n",
      "Train Loss at iteration 8314: 0.04324195038519191\n",
      "Train Loss at iteration 8315: 0.043241873324589154\n",
      "Train Loss at iteration 8316: 0.04324179627402584\n",
      "Train Loss at iteration 8317: 0.04324171923350021\n",
      "Train Loss at iteration 8318: 0.043241642203010494\n",
      "Train Loss at iteration 8319: 0.043241565182554946\n",
      "Train Loss at iteration 8320: 0.043241488172131796\n",
      "Train Loss at iteration 8321: 0.04324141117173933\n",
      "Train Loss at iteration 8322: 0.043241334181375754\n",
      "Train Loss at iteration 8323: 0.04324125720103933\n",
      "Train Loss at iteration 8324: 0.043241180230728316\n",
      "Train Loss at iteration 8325: 0.04324110327044093\n",
      "Train Loss at iteration 8326: 0.04324102632017546\n",
      "Train Loss at iteration 8327: 0.043240949379930116\n",
      "Train Loss at iteration 8328: 0.043240872449703156\n",
      "Train Loss at iteration 8329: 0.04324079552949286\n",
      "Train Loss at iteration 8330: 0.04324071861929744\n",
      "Train Loss at iteration 8331: 0.04324064171911515\n",
      "Train Loss at iteration 8332: 0.043240564828944256\n",
      "Train Loss at iteration 8333: 0.04324048794878301\n",
      "Train Loss at iteration 8334: 0.043240411078629644\n",
      "Train Loss at iteration 8335: 0.043240334218482425\n",
      "Train Loss at iteration 8336: 0.04324025736833959\n",
      "Train Loss at iteration 8337: 0.04324018052819941\n",
      "Train Loss at iteration 8338: 0.04324010369806011\n",
      "Train Loss at iteration 8339: 0.04324002687791998\n",
      "Train Loss at iteration 8340: 0.043239950067777234\n",
      "Train Loss at iteration 8341: 0.04323987326763015\n",
      "Train Loss at iteration 8342: 0.04323979647747698\n",
      "Train Loss at iteration 8343: 0.04323971969731596\n",
      "Train Loss at iteration 8344: 0.04323964292714536\n",
      "Train Loss at iteration 8345: 0.043239566166963436\n",
      "Train Loss at iteration 8346: 0.04323948941676843\n",
      "Train Loss at iteration 8347: 0.043239412676558606\n",
      "Train Loss at iteration 8348: 0.04323933594633222\n",
      "Train Loss at iteration 8349: 0.043239259226087526\n",
      "Train Loss at iteration 8350: 0.04323918251582279\n",
      "Train Loss at iteration 8351: 0.04323910581553625\n",
      "Train Loss at iteration 8352: 0.043239029125226175\n",
      "Train Loss at iteration 8353: 0.043238952444890816\n",
      "Train Loss at iteration 8354: 0.04323887577452844\n",
      "Train Loss at iteration 8355: 0.04323879911413729\n",
      "Train Loss at iteration 8356: 0.04323872246371564\n",
      "Train Loss at iteration 8357: 0.04323864582326174\n",
      "Train Loss at iteration 8358: 0.04323856919277387\n",
      "Train Loss at iteration 8359: 0.04323849257225025\n",
      "Train Loss at iteration 8360: 0.04323841596168917\n",
      "Train Loss at iteration 8361: 0.04323833936108888\n",
      "Train Loss at iteration 8362: 0.04323826277044764\n",
      "Train Loss at iteration 8363: 0.04323818618976371\n",
      "Train Loss at iteration 8364: 0.04323810961903537\n",
      "Train Loss at iteration 8365: 0.04323803305826086\n",
      "Train Loss at iteration 8366: 0.043237956507438446\n",
      "Train Loss at iteration 8367: 0.043237879966566375\n",
      "Train Loss at iteration 8368: 0.04323780343564295\n",
      "Train Loss at iteration 8369: 0.043237726914666395\n",
      "Train Loss at iteration 8370: 0.043237650403634996\n",
      "Train Loss at iteration 8371: 0.04323757390254702\n",
      "Train Loss at iteration 8372: 0.043237497411400705\n",
      "Train Loss at iteration 8373: 0.04323742093019433\n",
      "Train Loss at iteration 8374: 0.04323734445892617\n",
      "Train Loss at iteration 8375: 0.04323726799759448\n",
      "Train Loss at iteration 8376: 0.04323719154619753\n",
      "Train Loss at iteration 8377: 0.043237115104733576\n",
      "Train Loss at iteration 8378: 0.04323703867320088\n",
      "Train Loss at iteration 8379: 0.04323696225159774\n",
      "Train Loss at iteration 8380: 0.04323688583992239\n",
      "Train Loss at iteration 8381: 0.04323680943817311\n",
      "Train Loss at iteration 8382: 0.043236733046348175\n",
      "Train Loss at iteration 8383: 0.04323665666444583\n",
      "Train Loss at iteration 8384: 0.04323658029246436\n",
      "Train Loss at iteration 8385: 0.043236503930402036\n",
      "Train Loss at iteration 8386: 0.04323642757825713\n",
      "Train Loss at iteration 8387: 0.0432363512360279\n",
      "Train Loss at iteration 8388: 0.04323627490371261\n",
      "Train Loss at iteration 8389: 0.04323619858130954\n",
      "Train Loss at iteration 8390: 0.043236122268816964\n",
      "Train Loss at iteration 8391: 0.04323604596623314\n",
      "Train Loss at iteration 8392: 0.043235969673556356\n",
      "Train Loss at iteration 8393: 0.04323589339078487\n",
      "Train Loss at iteration 8394: 0.04323581711791696\n",
      "Train Loss at iteration 8395: 0.0432357408549509\n",
      "Train Loss at iteration 8396: 0.04323566460188495\n",
      "Train Loss at iteration 8397: 0.043235588358717394\n",
      "Train Loss at iteration 8398: 0.043235512125446504\n",
      "Train Loss at iteration 8399: 0.043235435902070546\n",
      "Train Loss at iteration 8400: 0.04323535968858781\n",
      "Train Loss at iteration 8401: 0.04323528348499655\n",
      "Train Loss at iteration 8402: 0.043235207291295055\n",
      "Train Loss at iteration 8403: 0.0432351311074816\n",
      "Train Loss at iteration 8404: 0.04323505493355444\n",
      "Train Loss at iteration 8405: 0.043234978769511885\n",
      "Train Loss at iteration 8406: 0.04323490261535217\n",
      "Train Loss at iteration 8407: 0.0432348264710736\n",
      "Train Loss at iteration 8408: 0.043234750336674455\n",
      "Train Loss at iteration 8409: 0.043234674212153\n",
      "Train Loss at iteration 8410: 0.0432345980975075\n",
      "Train Loss at iteration 8411: 0.04323452199273626\n",
      "Train Loss at iteration 8412: 0.04323444589783753\n",
      "Train Loss at iteration 8413: 0.04323436981280961\n",
      "Train Loss at iteration 8414: 0.043234293737650774\n",
      "Train Loss at iteration 8415: 0.04323421767235929\n",
      "Train Loss at iteration 8416: 0.043234141616933446\n",
      "Train Loss at iteration 8417: 0.043234065571371535\n",
      "Train Loss at iteration 8418: 0.04323398953567181\n",
      "Train Loss at iteration 8419: 0.04323391350983257\n",
      "Train Loss at iteration 8420: 0.04323383749385209\n",
      "Train Loss at iteration 8421: 0.04323376148772864\n",
      "Train Loss at iteration 8422: 0.043233685491460526\n",
      "Train Loss at iteration 8423: 0.043233609505046004\n",
      "Train Loss at iteration 8424: 0.04323353352848338\n",
      "Train Loss at iteration 8425: 0.043233457561770924\n",
      "Train Loss at iteration 8426: 0.04323338160490691\n",
      "Train Loss at iteration 8427: 0.04323330565788963\n",
      "Train Loss at iteration 8428: 0.04323322972071739\n",
      "Train Loss at iteration 8429: 0.043233153793388435\n",
      "Train Loss at iteration 8430: 0.04323307787590107\n",
      "Train Loss at iteration 8431: 0.04323300196825356\n",
      "Train Loss at iteration 8432: 0.04323292607044423\n",
      "Train Loss at iteration 8433: 0.04323285018247131\n",
      "Train Loss at iteration 8434: 0.04323277430433315\n",
      "Train Loss at iteration 8435: 0.04323269843602799\n",
      "Train Loss at iteration 8436: 0.043232622577554114\n",
      "Train Loss at iteration 8437: 0.04323254672890984\n",
      "Train Loss at iteration 8438: 0.043232470890093425\n",
      "Train Loss at iteration 8439: 0.04323239506110318\n",
      "Train Loss at iteration 8440: 0.04323231924193737\n",
      "Train Loss at iteration 8441: 0.043232243432594304\n",
      "Train Loss at iteration 8442: 0.04323216763307226\n",
      "Train Loss at iteration 8443: 0.04323209184336951\n",
      "Train Loss at iteration 8444: 0.04323201606348438\n",
      "Train Loss at iteration 8445: 0.04323194029341514\n",
      "Train Loss at iteration 8446: 0.04323186453316007\n",
      "Train Loss at iteration 8447: 0.04323178878271747\n",
      "Train Loss at iteration 8448: 0.04323171304208563\n",
      "Train Loss at iteration 8449: 0.043231637311262845\n",
      "Train Loss at iteration 8450: 0.04323156159024741\n",
      "Train Loss at iteration 8451: 0.04323148587903758\n",
      "Train Loss at iteration 8452: 0.043231410177631685\n",
      "Train Loss at iteration 8453: 0.04323133448602801\n",
      "Train Loss at iteration 8454: 0.04323125880422485\n",
      "Train Loss at iteration 8455: 0.043231183132220484\n",
      "Train Loss at iteration 8456: 0.04323110747001321\n",
      "Train Loss at iteration 8457: 0.04323103181760132\n",
      "Train Loss at iteration 8458: 0.04323095617498312\n",
      "Train Loss at iteration 8459: 0.043230880542156895\n",
      "Train Loss at iteration 8460: 0.043230804919120944\n",
      "Train Loss at iteration 8461: 0.04323072930587355\n",
      "Train Loss at iteration 8462: 0.04323065370241302\n",
      "Train Loss at iteration 8463: 0.04323057810873764\n",
      "Train Loss at iteration 8464: 0.04323050252484571\n",
      "Train Loss at iteration 8465: 0.043230426950735534\n",
      "Train Loss at iteration 8466: 0.04323035138640541\n",
      "Train Loss at iteration 8467: 0.0432302758318536\n",
      "Train Loss at iteration 8468: 0.043230200287078455\n",
      "Train Loss at iteration 8469: 0.04323012475207822\n",
      "Train Loss at iteration 8470: 0.04323004922685123\n",
      "Train Loss at iteration 8471: 0.043229973711395774\n",
      "Train Loss at iteration 8472: 0.043229898205710146\n",
      "Train Loss at iteration 8473: 0.043229822709792635\n",
      "Train Loss at iteration 8474: 0.04322974722364156\n",
      "Train Loss at iteration 8475: 0.04322967174725522\n",
      "Train Loss at iteration 8476: 0.0432295962806319\n",
      "Train Loss at iteration 8477: 0.04322952082376991\n",
      "Train Loss at iteration 8478: 0.043229445376667536\n",
      "Train Loss at iteration 8479: 0.04322936993932311\n",
      "Train Loss at iteration 8480: 0.043229294511734906\n",
      "Train Loss at iteration 8481: 0.043229219093901226\n",
      "Train Loss at iteration 8482: 0.04322914368582038\n",
      "Train Loss at iteration 8483: 0.04322906828749067\n",
      "Train Loss at iteration 8484: 0.043228992898910405\n",
      "Train Loss at iteration 8485: 0.043228917520077875\n",
      "Train Loss at iteration 8486: 0.043228842150991396\n",
      "Train Loss at iteration 8487: 0.043228766791649255\n",
      "Train Loss at iteration 8488: 0.04322869144204978\n",
      "Train Loss at iteration 8489: 0.043228616102191246\n",
      "Train Loss at iteration 8490: 0.04322854077207198\n",
      "Train Loss at iteration 8491: 0.043228465451690276\n",
      "Train Loss at iteration 8492: 0.043228390141044445\n",
      "Train Loss at iteration 8493: 0.04322831484013279\n",
      "Train Loss at iteration 8494: 0.04322823954895361\n",
      "Train Loss at iteration 8495: 0.043228164267505226\n",
      "Train Loss at iteration 8496: 0.04322808899578594\n",
      "Train Loss at iteration 8497: 0.04322801373379405\n",
      "Train Loss at iteration 8498: 0.04322793848152787\n",
      "Train Loss at iteration 8499: 0.04322786323898572\n",
      "Train Loss at iteration 8500: 0.043227788006165876\n",
      "Train Loss at iteration 8501: 0.04322771278306668\n",
      "Train Loss at iteration 8502: 0.043227637569686415\n",
      "Train Loss at iteration 8503: 0.0432275623660234\n",
      "Train Loss at iteration 8504: 0.04322748717207596\n",
      "Train Loss at iteration 8505: 0.043227411987842385\n",
      "Train Loss at iteration 8506: 0.04322733681332098\n",
      "Train Loss at iteration 8507: 0.043227261648510075\n",
      "Train Loss at iteration 8508: 0.043227186493407975\n",
      "Train Loss at iteration 8509: 0.04322711134801297\n",
      "Train Loss at iteration 8510: 0.043227036212323405\n",
      "Train Loss at iteration 8511: 0.04322696108633757\n",
      "Train Loss at iteration 8512: 0.04322688597005377\n",
      "Train Loss at iteration 8513: 0.04322681086347034\n",
      "Train Loss at iteration 8514: 0.04322673576658558\n",
      "Train Loss at iteration 8515: 0.04322666067939781\n",
      "Train Loss at iteration 8516: 0.04322658560190534\n",
      "Train Loss at iteration 8517: 0.04322651053410647\n",
      "Train Loss at iteration 8518: 0.04322643547599954\n",
      "Train Loss at iteration 8519: 0.04322636042758285\n",
      "Train Loss at iteration 8520: 0.04322628538885471\n",
      "Train Loss at iteration 8521: 0.04322621035981344\n",
      "Train Loss at iteration 8522: 0.04322613534045735\n",
      "Train Loss at iteration 8523: 0.043226060330784764\n",
      "Train Loss at iteration 8524: 0.043225985330794005\n",
      "Train Loss at iteration 8525: 0.04322591034048337\n",
      "Train Loss at iteration 8526: 0.04322583535985119\n",
      "Train Loss at iteration 8527: 0.043225760388895786\n",
      "Train Loss at iteration 8528: 0.04322568542761544\n",
      "Train Loss at iteration 8529: 0.043225610476008515\n",
      "Train Loss at iteration 8530: 0.04322553553407332\n",
      "Train Loss at iteration 8531: 0.04322546060180816\n",
      "Train Loss at iteration 8532: 0.04322538567921134\n",
      "Train Loss at iteration 8533: 0.04322531076628121\n",
      "Train Loss at iteration 8534: 0.043225235863016076\n",
      "Train Loss at iteration 8535: 0.04322516096941426\n",
      "Train Loss at iteration 8536: 0.043225086085474075\n",
      "Train Loss at iteration 8537: 0.04322501121119385\n",
      "Train Loss at iteration 8538: 0.043224936346571895\n",
      "Train Loss at iteration 8539: 0.043224861491606544\n",
      "Train Loss at iteration 8540: 0.04322478664629611\n",
      "Train Loss at iteration 8541: 0.04322471181063893\n",
      "Train Loss at iteration 8542: 0.043224636984633295\n",
      "Train Loss at iteration 8543: 0.043224562168277546\n",
      "Train Loss at iteration 8544: 0.043224487361570016\n",
      "Train Loss at iteration 8545: 0.04322441256450901\n",
      "Train Loss at iteration 8546: 0.04322433777709287\n",
      "Train Loss at iteration 8547: 0.0432242629993199\n",
      "Train Loss at iteration 8548: 0.04322418823118843\n",
      "Train Loss at iteration 8549: 0.043224113472696786\n",
      "Train Loss at iteration 8550: 0.043224038723843304\n",
      "Train Loss at iteration 8551: 0.043223963984626294\n",
      "Train Loss at iteration 8552: 0.04322388925504409\n",
      "Train Loss at iteration 8553: 0.043223814535095\n",
      "Train Loss at iteration 8554: 0.04322373982477737\n",
      "Train Loss at iteration 8555: 0.043223665124089525\n",
      "Train Loss at iteration 8556: 0.043223590433029785\n",
      "Train Loss at iteration 8557: 0.04322351575159647\n",
      "Train Loss at iteration 8558: 0.043223441079787935\n",
      "Train Loss at iteration 8559: 0.043223366417602466\n",
      "Train Loss at iteration 8560: 0.04322329176503843\n",
      "Train Loss at iteration 8561: 0.043223217122094135\n",
      "Train Loss at iteration 8562: 0.04322314248876792\n",
      "Train Loss at iteration 8563: 0.0432230678650581\n",
      "Train Loss at iteration 8564: 0.04322299325096302\n",
      "Train Loss at iteration 8565: 0.043222918646481\n",
      "Train Loss at iteration 8566: 0.04322284405161036\n",
      "Train Loss at iteration 8567: 0.04322276946634945\n",
      "Train Loss at iteration 8568: 0.0432226948906966\n",
      "Train Loss at iteration 8569: 0.04322262032465013\n",
      "Train Loss at iteration 8570: 0.04322254576820838\n",
      "Train Loss at iteration 8571: 0.043222471221369664\n",
      "Train Loss at iteration 8572: 0.04322239668413233\n",
      "Train Loss at iteration 8573: 0.04322232215649472\n",
      "Train Loss at iteration 8574: 0.04322224763845515\n",
      "Train Loss at iteration 8575: 0.04322217313001194\n",
      "Train Loss at iteration 8576: 0.04322209863116347\n",
      "Train Loss at iteration 8577: 0.043222024141908025\n",
      "Train Loss at iteration 8578: 0.043221949662243964\n",
      "Train Loss at iteration 8579: 0.0432218751921696\n",
      "Train Loss at iteration 8580: 0.043221800731683295\n",
      "Train Loss at iteration 8581: 0.04322172628078338\n",
      "Train Loss at iteration 8582: 0.04322165183946817\n",
      "Train Loss at iteration 8583: 0.04322157740773602\n",
      "Train Loss at iteration 8584: 0.043221502985585254\n",
      "Train Loss at iteration 8585: 0.04322142857301422\n",
      "Train Loss at iteration 8586: 0.04322135417002124\n",
      "Train Loss at iteration 8587: 0.04322127977660466\n",
      "Train Loss at iteration 8588: 0.04322120539276283\n",
      "Train Loss at iteration 8589: 0.043221131018494056\n",
      "Train Loss at iteration 8590: 0.043221056653796705\n",
      "Train Loss at iteration 8591: 0.04322098229866908\n",
      "Train Loss at iteration 8592: 0.04322090795310957\n",
      "Train Loss at iteration 8593: 0.043220833617116476\n",
      "Train Loss at iteration 8594: 0.04322075929068816\n",
      "Train Loss at iteration 8595: 0.04322068497382293\n",
      "Train Loss at iteration 8596: 0.04322061066651915\n",
      "Train Loss at iteration 8597: 0.04322053636877517\n",
      "Train Loss at iteration 8598: 0.043220462080589314\n",
      "Train Loss at iteration 8599: 0.043220387801959914\n",
      "Train Loss at iteration 8600: 0.043220313532885336\n",
      "Train Loss at iteration 8601: 0.043220239273363895\n",
      "Train Loss at iteration 8602: 0.04322016502339396\n",
      "Train Loss at iteration 8603: 0.043220090782973855\n",
      "Train Loss at iteration 8604: 0.04322001655210193\n",
      "Train Loss at iteration 8605: 0.043219942330776515\n",
      "Train Loss at iteration 8606: 0.043219868118995974\n",
      "Train Loss at iteration 8607: 0.04321979391675863\n",
      "Train Loss at iteration 8608: 0.043219719724062834\n",
      "Train Loss at iteration 8609: 0.043219645540906945\n",
      "Train Loss at iteration 8610: 0.04321957136728929\n",
      "Train Loss at iteration 8611: 0.04321949720320821\n",
      "Train Loss at iteration 8612: 0.04321942304866207\n",
      "Train Loss at iteration 8613: 0.04321934890364919\n",
      "Train Loss at iteration 8614: 0.04321927476816795\n",
      "Train Loss at iteration 8615: 0.04321920064221666\n",
      "Train Loss at iteration 8616: 0.0432191265257937\n",
      "Train Loss at iteration 8617: 0.04321905241889739\n",
      "Train Loss at iteration 8618: 0.043218978321526075\n",
      "Train Loss at iteration 8619: 0.04321890423367812\n",
      "Train Loss at iteration 8620: 0.043218830155351884\n",
      "Train Loss at iteration 8621: 0.04321875608654569\n",
      "Train Loss at iteration 8622: 0.04321868202725788\n",
      "Train Loss at iteration 8623: 0.04321860797748682\n",
      "Train Loss at iteration 8624: 0.043218533937230866\n",
      "Train Loss at iteration 8625: 0.04321845990648835\n",
      "Train Loss at iteration 8626: 0.043218385885257636\n",
      "Train Loss at iteration 8627: 0.043218311873537064\n",
      "Train Loss at iteration 8628: 0.04321823787132499\n",
      "Train Loss at iteration 8629: 0.04321816387861975\n",
      "Train Loss at iteration 8630: 0.043218089895419726\n",
      "Train Loss at iteration 8631: 0.04321801592172324\n",
      "Train Loss at iteration 8632: 0.04321794195752865\n",
      "Train Loss at iteration 8633: 0.043217868002834316\n",
      "Train Loss at iteration 8634: 0.0432177940576386\n",
      "Train Loss at iteration 8635: 0.043217720121939814\n",
      "Train Loss at iteration 8636: 0.043217646195736356\n",
      "Train Loss at iteration 8637: 0.04321757227902655\n",
      "Train Loss at iteration 8638: 0.04321749837180878\n",
      "Train Loss at iteration 8639: 0.04321742447408136\n",
      "Train Loss at iteration 8640: 0.04321735058584266\n",
      "Train Loss at iteration 8641: 0.043217276707091065\n",
      "Train Loss at iteration 8642: 0.043217202837824896\n",
      "Train Loss at iteration 8643: 0.043217128978042516\n",
      "Train Loss at iteration 8644: 0.04321705512774227\n",
      "Train Loss at iteration 8645: 0.04321698128692254\n",
      "Train Loss at iteration 8646: 0.043216907455581655\n",
      "Train Loss at iteration 8647: 0.043216833633718005\n",
      "Train Loss at iteration 8648: 0.04321675982132991\n",
      "Train Loss at iteration 8649: 0.04321668601841573\n",
      "Train Loss at iteration 8650: 0.043216612224973854\n",
      "Train Loss at iteration 8651: 0.04321653844100261\n",
      "Train Loss at iteration 8652: 0.04321646466650037\n",
      "Train Loss at iteration 8653: 0.0432163909014655\n",
      "Train Loss at iteration 8654: 0.043216317145896334\n",
      "Train Loss at iteration 8655: 0.04321624339979125\n",
      "Train Loss at iteration 8656: 0.043216169663148604\n",
      "Train Loss at iteration 8657: 0.04321609593596676\n",
      "Train Loss at iteration 8658: 0.04321602221824407\n",
      "Train Loss at iteration 8659: 0.043215948509978896\n",
      "Train Loss at iteration 8660: 0.043215874811169584\n",
      "Train Loss at iteration 8661: 0.043215801121814536\n",
      "Train Loss at iteration 8662: 0.04321572744191207\n",
      "Train Loss at iteration 8663: 0.04321565377146058\n",
      "Train Loss at iteration 8664: 0.04321558011045842\n",
      "Train Loss at iteration 8665: 0.04321550645890392\n",
      "Train Loss at iteration 8666: 0.043215432816795483\n",
      "Train Loss at iteration 8667: 0.04321535918413146\n",
      "Train Loss at iteration 8668: 0.04321528556091021\n",
      "Train Loss at iteration 8669: 0.0432152119471301\n",
      "Train Loss at iteration 8670: 0.0432151383427895\n",
      "Train Loss at iteration 8671: 0.04321506474788675\n",
      "Train Loss at iteration 8672: 0.04321499116242024\n",
      "Train Loss at iteration 8673: 0.04321491758638832\n",
      "Train Loss at iteration 8674: 0.04321484401978937\n",
      "Train Loss at iteration 8675: 0.043214770462621746\n",
      "Train Loss at iteration 8676: 0.04321469691488381\n",
      "Train Loss at iteration 8677: 0.04321462337657394\n",
      "Train Loss at iteration 8678: 0.0432145498476905\n",
      "Train Loss at iteration 8679: 0.04321447632823184\n",
      "Train Loss at iteration 8680: 0.04321440281819634\n",
      "Train Loss at iteration 8681: 0.043214329317582366\n",
      "Train Loss at iteration 8682: 0.0432142558263883\n",
      "Train Loss at iteration 8683: 0.04321418234461249\n",
      "Train Loss at iteration 8684: 0.043214108872253305\n",
      "Train Loss at iteration 8685: 0.04321403540930913\n",
      "Train Loss at iteration 8686: 0.04321396195577832\n",
      "Train Loss at iteration 8687: 0.043213888511659235\n",
      "Train Loss at iteration 8688: 0.04321381507695028\n",
      "Train Loss at iteration 8689: 0.0432137416516498\n",
      "Train Loss at iteration 8690: 0.04321366823575616\n",
      "Train Loss at iteration 8691: 0.043213594829267746\n",
      "Train Loss at iteration 8692: 0.04321352143218292\n",
      "Train Loss at iteration 8693: 0.04321344804450005\n",
      "Train Loss at iteration 8694: 0.043213374666217524\n",
      "Train Loss at iteration 8695: 0.04321330129733369\n",
      "Train Loss at iteration 8696: 0.04321322793784694\n",
      "Train Loss at iteration 8697: 0.04321315458775563\n",
      "Train Loss at iteration 8698: 0.043213081247058166\n",
      "Train Loss at iteration 8699: 0.043213007915752874\n",
      "Train Loss at iteration 8700: 0.043212934593838175\n",
      "Train Loss at iteration 8701: 0.0432128612813124\n",
      "Train Loss at iteration 8702: 0.04321278797817396\n",
      "Train Loss at iteration 8703: 0.043212714684421186\n",
      "Train Loss at iteration 8704: 0.04321264140005251\n",
      "Train Loss at iteration 8705: 0.04321256812506625\n",
      "Train Loss at iteration 8706: 0.04321249485946082\n",
      "Train Loss at iteration 8707: 0.04321242160323458\n",
      "Train Loss at iteration 8708: 0.04321234835638591\n",
      "Train Loss at iteration 8709: 0.043212275118913186\n",
      "Train Loss at iteration 8710: 0.04321220189081479\n",
      "Train Loss at iteration 8711: 0.04321212867208909\n",
      "Train Loss at iteration 8712: 0.043212055462734454\n",
      "Train Loss at iteration 8713: 0.043211982262749284\n",
      "Train Loss at iteration 8714: 0.04321190907213195\n",
      "Train Loss at iteration 8715: 0.04321183589088082\n",
      "Train Loss at iteration 8716: 0.04321176271899428\n",
      "Train Loss at iteration 8717: 0.04321168955647071\n",
      "Train Loss at iteration 8718: 0.04321161640330848\n",
      "Train Loss at iteration 8719: 0.04321154325950599\n",
      "Train Loss at iteration 8720: 0.0432114701250616\n",
      "Train Loss at iteration 8721: 0.0432113969999737\n",
      "Train Loss at iteration 8722: 0.04321132388424067\n",
      "Train Loss at iteration 8723: 0.04321125077786087\n",
      "Train Loss at iteration 8724: 0.04321117768083272\n",
      "Train Loss at iteration 8725: 0.043211104593154574\n",
      "Train Loss at iteration 8726: 0.04321103151482483\n",
      "Train Loss at iteration 8727: 0.04321095844584186\n",
      "Train Loss at iteration 8728: 0.04321088538620404\n",
      "Train Loss at iteration 8729: 0.04321081233590976\n",
      "Train Loss at iteration 8730: 0.04321073929495741\n",
      "Train Loss at iteration 8731: 0.04321066626334536\n",
      "Train Loss at iteration 8732: 0.043210593241072\n",
      "Train Loss at iteration 8733: 0.04321052022813572\n",
      "Train Loss at iteration 8734: 0.04321044722453489\n",
      "Train Loss at iteration 8735: 0.04321037423026792\n",
      "Train Loss at iteration 8736: 0.04321030124533317\n",
      "Train Loss at iteration 8737: 0.04321022826972903\n",
      "Train Loss at iteration 8738: 0.04321015530345389\n",
      "Train Loss at iteration 8739: 0.04321008234650613\n",
      "Train Loss at iteration 8740: 0.04321000939888414\n",
      "Train Loss at iteration 8741: 0.04320993646058632\n",
      "Train Loss at iteration 8742: 0.043209863531611055\n",
      "Train Loss at iteration 8743: 0.0432097906119567\n",
      "Train Loss at iteration 8744: 0.04320971770162166\n",
      "Train Loss at iteration 8745: 0.04320964480060434\n",
      "Train Loss at iteration 8746: 0.04320957190890312\n",
      "Train Loss at iteration 8747: 0.04320949902651637\n",
      "Train Loss at iteration 8748: 0.04320942615344251\n",
      "Train Loss at iteration 8749: 0.0432093532896799\n",
      "Train Loss at iteration 8750: 0.04320928043522695\n",
      "Train Loss at iteration 8751: 0.04320920759008202\n",
      "Train Loss at iteration 8752: 0.04320913475424353\n",
      "Train Loss at iteration 8753: 0.043209061927709874\n",
      "Train Loss at iteration 8754: 0.043208989110479414\n",
      "Train Loss at iteration 8755: 0.043208916302550554\n",
      "Train Loss at iteration 8756: 0.04320884350392171\n",
      "Train Loss at iteration 8757: 0.04320877071459124\n",
      "Train Loss at iteration 8758: 0.04320869793455753\n",
      "Train Loss at iteration 8759: 0.04320862516381901\n",
      "Train Loss at iteration 8760: 0.04320855240237404\n",
      "Train Loss at iteration 8761: 0.043208479650221024\n",
      "Train Loss at iteration 8762: 0.04320840690735837\n",
      "Train Loss at iteration 8763: 0.04320833417378444\n",
      "Train Loss at iteration 8764: 0.04320826144949766\n",
      "Train Loss at iteration 8765: 0.043208188734496396\n",
      "Train Loss at iteration 8766: 0.04320811602877905\n",
      "Train Loss at iteration 8767: 0.04320804333234404\n",
      "Train Loss at iteration 8768: 0.04320797064518974\n",
      "Train Loss at iteration 8769: 0.04320789796731454\n",
      "Train Loss at iteration 8770: 0.043207825298716855\n",
      "Train Loss at iteration 8771: 0.04320775263939506\n",
      "Train Loss at iteration 8772: 0.04320767998934758\n",
      "Train Loss at iteration 8773: 0.04320760734857277\n",
      "Train Loss at iteration 8774: 0.043207534717069056\n",
      "Train Loss at iteration 8775: 0.04320746209483482\n",
      "Train Loss at iteration 8776: 0.04320738948186848\n",
      "Train Loss at iteration 8777: 0.043207316878168436\n",
      "Train Loss at iteration 8778: 0.04320724428373306\n",
      "Train Loss at iteration 8779: 0.04320717169856075\n",
      "Train Loss at iteration 8780: 0.04320709912264993\n",
      "Train Loss at iteration 8781: 0.04320702655599899\n",
      "Train Loss at iteration 8782: 0.043206953998606315\n",
      "Train Loss at iteration 8783: 0.04320688145047033\n",
      "Train Loss at iteration 8784: 0.0432068089115894\n",
      "Train Loss at iteration 8785: 0.043206736381961965\n",
      "Train Loss at iteration 8786: 0.0432066638615864\n",
      "Train Loss at iteration 8787: 0.04320659135046111\n",
      "Train Loss at iteration 8788: 0.0432065188485845\n",
      "Train Loss at iteration 8789: 0.04320644635595497\n",
      "Train Loss at iteration 8790: 0.04320637387257093\n",
      "Train Loss at iteration 8791: 0.04320630139843076\n",
      "Train Loss at iteration 8792: 0.043206228933532896\n",
      "Train Loss at iteration 8793: 0.0432061564778757\n",
      "Train Loss at iteration 8794: 0.04320608403145761\n",
      "Train Loss at iteration 8795: 0.043206011594277026\n",
      "Train Loss at iteration 8796: 0.04320593916633233\n",
      "Train Loss at iteration 8797: 0.043205866747621946\n",
      "Train Loss at iteration 8798: 0.04320579433814426\n",
      "Train Loss at iteration 8799: 0.043205721937897686\n",
      "Train Loss at iteration 8800: 0.043205649546880644\n",
      "Train Loss at iteration 8801: 0.04320557716509151\n",
      "Train Loss at iteration 8802: 0.04320550479252873\n",
      "Train Loss at iteration 8803: 0.04320543242919065\n",
      "Train Loss at iteration 8804: 0.04320536007507573\n",
      "Train Loss at iteration 8805: 0.04320528773018236\n",
      "Train Loss at iteration 8806: 0.043205215394508945\n",
      "Train Loss at iteration 8807: 0.04320514306805388\n",
      "Train Loss at iteration 8808: 0.043205070750815595\n",
      "Train Loss at iteration 8809: 0.04320499844279249\n",
      "Train Loss at iteration 8810: 0.043204926143982976\n",
      "Train Loss at iteration 8811: 0.043204853854385425\n",
      "Train Loss at iteration 8812: 0.0432047815739983\n",
      "Train Loss at iteration 8813: 0.04320470930281998\n",
      "Train Loss at iteration 8814: 0.04320463704084889\n",
      "Train Loss at iteration 8815: 0.04320456478808341\n",
      "Train Loss at iteration 8816: 0.043204492544521994\n",
      "Train Loss at iteration 8817: 0.04320442031016302\n",
      "Train Loss at iteration 8818: 0.04320434808500489\n",
      "Train Loss at iteration 8819: 0.04320427586904606\n",
      "Train Loss at iteration 8820: 0.0432042036622849\n",
      "Train Loss at iteration 8821: 0.04320413146471983\n",
      "Train Loss at iteration 8822: 0.04320405927634927\n",
      "Train Loss at iteration 8823: 0.043203987097171624\n",
      "Train Loss at iteration 8824: 0.04320391492718531\n",
      "Train Loss at iteration 8825: 0.04320384276638875\n",
      "Train Loss at iteration 8826: 0.04320377061478034\n",
      "Train Loss at iteration 8827: 0.04320369847235849\n",
      "Train Loss at iteration 8828: 0.04320362633912163\n",
      "Train Loss at iteration 8829: 0.04320355421506816\n",
      "Train Loss at iteration 8830: 0.04320348210019651\n",
      "Train Loss at iteration 8831: 0.04320340999450511\n",
      "Train Loss at iteration 8832: 0.043203337897992314\n",
      "Train Loss at iteration 8833: 0.04320326581065661\n",
      "Train Loss at iteration 8834: 0.04320319373249634\n",
      "Train Loss at iteration 8835: 0.04320312166350997\n",
      "Train Loss at iteration 8836: 0.04320304960369591\n",
      "Train Loss at iteration 8837: 0.04320297755305257\n",
      "Train Loss at iteration 8838: 0.04320290551157836\n",
      "Train Loss at iteration 8839: 0.043202833479271716\n",
      "Train Loss at iteration 8840: 0.04320276145613104\n",
      "Train Loss at iteration 8841: 0.04320268944215473\n",
      "Train Loss at iteration 8842: 0.043202617437341266\n",
      "Train Loss at iteration 8843: 0.043202545441688986\n",
      "Train Loss at iteration 8844: 0.04320247345519637\n",
      "Train Loss at iteration 8845: 0.04320240147786183\n",
      "Train Loss at iteration 8846: 0.043202329509683765\n",
      "Train Loss at iteration 8847: 0.043202257550660574\n",
      "Train Loss at iteration 8848: 0.04320218560079073\n",
      "Train Loss at iteration 8849: 0.04320211366007262\n",
      "Train Loss at iteration 8850: 0.04320204172850467\n",
      "Train Loss at iteration 8851: 0.04320196980608531\n",
      "Train Loss at iteration 8852: 0.043201897892812945\n",
      "Train Loss at iteration 8853: 0.043201825988686\n",
      "Train Loss at iteration 8854: 0.04320175409370291\n",
      "Train Loss at iteration 8855: 0.04320168220786209\n",
      "Train Loss at iteration 8856: 0.04320161033116194\n",
      "Train Loss at iteration 8857: 0.043201538463600926\n",
      "Train Loss at iteration 8858: 0.04320146660517744\n",
      "Train Loss at iteration 8859: 0.04320139475588991\n",
      "Train Loss at iteration 8860: 0.04320132291573676\n",
      "Train Loss at iteration 8861: 0.043201251084716424\n",
      "Train Loss at iteration 8862: 0.04320117926282731\n",
      "Train Loss at iteration 8863: 0.043201107450067854\n",
      "Train Loss at iteration 8864: 0.043201035646436466\n",
      "Train Loss at iteration 8865: 0.04320096385193159\n",
      "Train Loss at iteration 8866: 0.04320089206655166\n",
      "Train Loss at iteration 8867: 0.04320082029029506\n",
      "Train Loss at iteration 8868: 0.04320074852316025\n",
      "Train Loss at iteration 8869: 0.043200676765145656\n",
      "Train Loss at iteration 8870: 0.043200605016249685\n",
      "Train Loss at iteration 8871: 0.043200533276470776\n",
      "Train Loss at iteration 8872: 0.04320046154580737\n",
      "Train Loss at iteration 8873: 0.04320038982425784\n",
      "Train Loss at iteration 8874: 0.04320031811182068\n",
      "Train Loss at iteration 8875: 0.043200246408494285\n",
      "Train Loss at iteration 8876: 0.04320017471427708\n",
      "Train Loss at iteration 8877: 0.04320010302916753\n",
      "Train Loss at iteration 8878: 0.04320003135316402\n",
      "Train Loss at iteration 8879: 0.04319995968626499\n",
      "Train Loss at iteration 8880: 0.04319988802846887\n",
      "Train Loss at iteration 8881: 0.04319981637977411\n",
      "Train Loss at iteration 8882: 0.04319974474017912\n",
      "Train Loss at iteration 8883: 0.04319967310968234\n",
      "Train Loss at iteration 8884: 0.04319960148828219\n",
      "Train Loss at iteration 8885: 0.04319952987597711\n",
      "Train Loss at iteration 8886: 0.04319945827276552\n",
      "Train Loss at iteration 8887: 0.04319938667864587\n",
      "Train Loss at iteration 8888: 0.04319931509361658\n",
      "Train Loss at iteration 8889: 0.043199243517676095\n",
      "Train Loss at iteration 8890: 0.04319917195082282\n",
      "Train Loss at iteration 8891: 0.04319910039305521\n",
      "Train Loss at iteration 8892: 0.043199028844371705\n",
      "Train Loss at iteration 8893: 0.04319895730477071\n",
      "Train Loss at iteration 8894: 0.043198885774250684\n",
      "Train Loss at iteration 8895: 0.04319881425281005\n",
      "Train Loss at iteration 8896: 0.04319874274044725\n",
      "Train Loss at iteration 8897: 0.04319867123716071\n",
      "Train Loss at iteration 8898: 0.04319859974294888\n",
      "Train Loss at iteration 8899: 0.04319852825781017\n",
      "Train Loss at iteration 8900: 0.04319845678174303\n",
      "Train Loss at iteration 8901: 0.043198385314745895\n",
      "Train Loss at iteration 8902: 0.043198313856817216\n",
      "Train Loss at iteration 8903: 0.043198242407955396\n",
      "Train Loss at iteration 8904: 0.04319817096815891\n",
      "Train Loss at iteration 8905: 0.04319809953742615\n",
      "Train Loss at iteration 8906: 0.0431980281157556\n",
      "Train Loss at iteration 8907: 0.043197956703145676\n",
      "Train Loss at iteration 8908: 0.043197885299594826\n",
      "Train Loss at iteration 8909: 0.043197813905101454\n",
      "Train Loss at iteration 8910: 0.043197742519664045\n",
      "Train Loss at iteration 8911: 0.043197671143281005\n",
      "Train Loss at iteration 8912: 0.0431975997759508\n",
      "Train Loss at iteration 8913: 0.04319752841767185\n",
      "Train Loss at iteration 8914: 0.04319745706844259\n",
      "Train Loss at iteration 8915: 0.04319738572826147\n",
      "Train Loss at iteration 8916: 0.04319731439712694\n",
      "Train Loss at iteration 8917: 0.043197243075037424\n",
      "Train Loss at iteration 8918: 0.04319717176199138\n",
      "Train Loss at iteration 8919: 0.04319710045798723\n",
      "Train Loss at iteration 8920: 0.043197029163023426\n",
      "Train Loss at iteration 8921: 0.043196957877098416\n",
      "Train Loss at iteration 8922: 0.04319688660021063\n",
      "Train Loss at iteration 8923: 0.04319681533235851\n",
      "Train Loss at iteration 8924: 0.043196744073540516\n",
      "Train Loss at iteration 8925: 0.04319667282375507\n",
      "Train Loss at iteration 8926: 0.043196601583000635\n",
      "Train Loss at iteration 8927: 0.04319653035127564\n",
      "Train Loss at iteration 8928: 0.043196459128578536\n",
      "Train Loss at iteration 8929: 0.043196387914907756\n",
      "Train Loss at iteration 8930: 0.04319631671026176\n",
      "Train Loss at iteration 8931: 0.043196245514638985\n",
      "Train Loss at iteration 8932: 0.04319617432803788\n",
      "Train Loss at iteration 8933: 0.04319610315045688\n",
      "Train Loss at iteration 8934: 0.04319603198189446\n",
      "Train Loss at iteration 8935: 0.04319596082234903\n",
      "Train Loss at iteration 8936: 0.04319588967181905\n",
      "Train Loss at iteration 8937: 0.04319581853030297\n",
      "Train Loss at iteration 8938: 0.04319574739779925\n",
      "Train Loss at iteration 8939: 0.043195676274306316\n",
      "Train Loss at iteration 8940: 0.04319560515982261\n",
      "Train Loss at iteration 8941: 0.043195534054346604\n",
      "Train Loss at iteration 8942: 0.04319546295787674\n",
      "Train Loss at iteration 8943: 0.04319539187041145\n",
      "Train Loss at iteration 8944: 0.04319532079194919\n",
      "Train Loss at iteration 8945: 0.04319524972248843\n",
      "Train Loss at iteration 8946: 0.04319517866202758\n",
      "Train Loss at iteration 8947: 0.04319510761056513\n",
      "Train Loss at iteration 8948: 0.04319503656809951\n",
      "Train Loss at iteration 8949: 0.043194965534629176\n",
      "Train Loss at iteration 8950: 0.04319489451015258\n",
      "Train Loss at iteration 8951: 0.04319482349466814\n",
      "Train Loss at iteration 8952: 0.04319475248817435\n",
      "Train Loss at iteration 8953: 0.043194681490669656\n",
      "Train Loss at iteration 8954: 0.043194610502152495\n",
      "Train Loss at iteration 8955: 0.04319453952262133\n",
      "Train Loss at iteration 8956: 0.04319446855207461\n",
      "Train Loss at iteration 8957: 0.04319439759051077\n",
      "Train Loss at iteration 8958: 0.0431943266379283\n",
      "Train Loss at iteration 8959: 0.04319425569432562\n",
      "Train Loss at iteration 8960: 0.04319418475970119\n",
      "Train Loss at iteration 8961: 0.04319411383405347\n",
      "Train Loss at iteration 8962: 0.04319404291738092\n",
      "Train Loss at iteration 8963: 0.043193972009681995\n",
      "Train Loss at iteration 8964: 0.04319390111095514\n",
      "Train Loss at iteration 8965: 0.043193830221198805\n",
      "Train Loss at iteration 8966: 0.043193759340411454\n",
      "Train Loss at iteration 8967: 0.04319368846859155\n",
      "Train Loss at iteration 8968: 0.043193617605737526\n",
      "Train Loss at iteration 8969: 0.04319354675184787\n",
      "Train Loss at iteration 8970: 0.04319347590692102\n",
      "Train Loss at iteration 8971: 0.04319340507095543\n",
      "Train Loss at iteration 8972: 0.04319333424394957\n",
      "Train Loss at iteration 8973: 0.04319326342590188\n",
      "Train Loss at iteration 8974: 0.04319319261681083\n",
      "Train Loss at iteration 8975: 0.043193121816674875\n",
      "Train Loss at iteration 8976: 0.04319305102549247\n",
      "Train Loss at iteration 8977: 0.04319298024326209\n",
      "Train Loss at iteration 8978: 0.04319290946998217\n",
      "Train Loss at iteration 8979: 0.04319283870565119\n",
      "Train Loss at iteration 8980: 0.04319276795026758\n",
      "Train Loss at iteration 8981: 0.04319269720382984\n",
      "Train Loss at iteration 8982: 0.0431926264663364\n",
      "Train Loss at iteration 8983: 0.043192555737785736\n",
      "Train Loss at iteration 8984: 0.04319248501817629\n",
      "Train Loss at iteration 8985: 0.043192414307506546\n",
      "Train Loss at iteration 8986: 0.043192343605774945\n",
      "Train Loss at iteration 8987: 0.04319227291297997\n",
      "Train Loss at iteration 8988: 0.043192202229120076\n",
      "Train Loss at iteration 8989: 0.04319213155419371\n",
      "Train Loss at iteration 8990: 0.04319206088819935\n",
      "Train Loss at iteration 8991: 0.043191990231135464\n",
      "Train Loss at iteration 8992: 0.04319191958300048\n",
      "Train Loss at iteration 8993: 0.04319184894379291\n",
      "Train Loss at iteration 8994: 0.04319177831351118\n",
      "Train Loss at iteration 8995: 0.043191707692153775\n",
      "Train Loss at iteration 8996: 0.04319163707971915\n",
      "Train Loss at iteration 8997: 0.04319156647620577\n",
      "Train Loss at iteration 8998: 0.043191495881612106\n",
      "Train Loss at iteration 8999: 0.04319142529593662\n",
      "Train Loss at iteration 9000: 0.04319135471917778\n",
      "Train Loss at iteration 9001: 0.04319128415133404\n",
      "Train Loss at iteration 9002: 0.04319121359240386\n",
      "Train Loss at iteration 9003: 0.043191143042385756\n",
      "Train Loss at iteration 9004: 0.04319107250127813\n",
      "Train Loss at iteration 9005: 0.043191001969079476\n",
      "Train Loss at iteration 9006: 0.043190931445788276\n",
      "Train Loss at iteration 9007: 0.04319086093140299\n",
      "Train Loss at iteration 9008: 0.043190790425922075\n",
      "Train Loss at iteration 9009: 0.043190719929344\n",
      "Train Loss at iteration 9010: 0.04319064944166724\n",
      "Train Loss at iteration 9011: 0.043190578962890254\n",
      "Train Loss at iteration 9012: 0.043190508493011516\n",
      "Train Loss at iteration 9013: 0.04319043803202952\n",
      "Train Loss at iteration 9014: 0.04319036757994269\n",
      "Train Loss at iteration 9015: 0.04319029713674954\n",
      "Train Loss at iteration 9016: 0.04319022670244849\n",
      "Train Loss at iteration 9017: 0.043190156277038066\n",
      "Train Loss at iteration 9018: 0.043190085860516696\n",
      "Train Loss at iteration 9019: 0.043190015452882875\n",
      "Train Loss at iteration 9020: 0.043189945054135065\n",
      "Train Loss at iteration 9021: 0.04318987466427175\n",
      "Train Loss at iteration 9022: 0.04318980428329137\n",
      "Train Loss at iteration 9023: 0.04318973391119243\n",
      "Train Loss at iteration 9024: 0.04318966354797338\n",
      "Train Loss at iteration 9025: 0.04318959319363272\n",
      "Train Loss at iteration 9026: 0.0431895228481689\n",
      "Train Loss at iteration 9027: 0.04318945251158039\n",
      "Train Loss at iteration 9028: 0.043189382183865685\n",
      "Train Loss at iteration 9029: 0.04318931186502324\n",
      "Train Loss at iteration 9030: 0.04318924155505155\n",
      "Train Loss at iteration 9031: 0.043189171253949055\n",
      "Train Loss at iteration 9032: 0.04318910096171426\n",
      "Train Loss at iteration 9033: 0.04318903067834563\n",
      "Train Loss at iteration 9034: 0.04318896040384165\n",
      "Train Loss at iteration 9035: 0.04318889013820078\n",
      "Train Loss at iteration 9036: 0.043188819881421495\n",
      "Train Loss at iteration 9037: 0.0431887496335023\n",
      "Train Loss at iteration 9038: 0.043188679394441626\n",
      "Train Loss at iteration 9039: 0.04318860916423798\n",
      "Train Loss at iteration 9040: 0.04318853894288985\n",
      "Train Loss at iteration 9041: 0.04318846873039568\n",
      "Train Loss at iteration 9042: 0.04318839852675397\n",
      "Train Loss at iteration 9043: 0.0431883283319632\n",
      "Train Loss at iteration 9044: 0.043188258146021834\n",
      "Train Loss at iteration 9045: 0.04318818796892835\n",
      "Train Loss at iteration 9046: 0.04318811780068124\n",
      "Train Loss at iteration 9047: 0.04318804764127899\n",
      "Train Loss at iteration 9048: 0.04318797749072006\n",
      "Train Loss at iteration 9049: 0.04318790734900293\n",
      "Train Loss at iteration 9050: 0.04318783721612608\n",
      "Train Loss at iteration 9051: 0.04318776709208802\n",
      "Train Loss at iteration 9052: 0.043187696976887194\n",
      "Train Loss at iteration 9053: 0.043187626870522106\n",
      "Train Loss at iteration 9054: 0.043187556772991215\n",
      "Train Loss at iteration 9055: 0.04318748668429302\n",
      "Train Loss at iteration 9056: 0.043187416604426\n",
      "Train Loss at iteration 9057: 0.04318734653338864\n",
      "Train Loss at iteration 9058: 0.04318727647117942\n",
      "Train Loss at iteration 9059: 0.04318720641779681\n",
      "Train Loss at iteration 9060: 0.043187136373239295\n",
      "Train Loss at iteration 9061: 0.04318706633750538\n",
      "Train Loss at iteration 9062: 0.04318699631059353\n",
      "Train Loss at iteration 9063: 0.04318692629250225\n",
      "Train Loss at iteration 9064: 0.04318685628322999\n",
      "Train Loss at iteration 9065: 0.04318678628277526\n",
      "Train Loss at iteration 9066: 0.04318671629113654\n",
      "Train Loss at iteration 9067: 0.043186646308312295\n",
      "Train Loss at iteration 9068: 0.043186576334301036\n",
      "Train Loss at iteration 9069: 0.043186506369101256\n",
      "Train Loss at iteration 9070: 0.04318643641271141\n",
      "Train Loss at iteration 9071: 0.04318636646512999\n",
      "Train Loss at iteration 9072: 0.04318629652635551\n",
      "Train Loss at iteration 9073: 0.04318622659638643\n",
      "Train Loss at iteration 9074: 0.043186156675221236\n",
      "Train Loss at iteration 9075: 0.04318608676285844\n",
      "Train Loss at iteration 9076: 0.0431860168592965\n",
      "Train Loss at iteration 9077: 0.04318594696453393\n",
      "Train Loss at iteration 9078: 0.0431858770785692\n",
      "Train Loss at iteration 9079: 0.04318580720140081\n",
      "Train Loss at iteration 9080: 0.043185737333027226\n",
      "Train Loss at iteration 9081: 0.04318566747344697\n",
      "Train Loss at iteration 9082: 0.04318559762265851\n",
      "Train Loss at iteration 9083: 0.04318552778066034\n",
      "Train Loss at iteration 9084: 0.04318545794745095\n",
      "Train Loss at iteration 9085: 0.04318538812302882\n",
      "Train Loss at iteration 9086: 0.043185318307392466\n",
      "Train Loss at iteration 9087: 0.043185248500540376\n",
      "Train Loss at iteration 9088: 0.04318517870247101\n",
      "Train Loss at iteration 9089: 0.04318510891318289\n",
      "Train Loss at iteration 9090: 0.043185039132674484\n",
      "Train Loss at iteration 9091: 0.04318496936094429\n",
      "Train Loss at iteration 9092: 0.043184899597990815\n",
      "Train Loss at iteration 9093: 0.04318482984381256\n",
      "Train Loss at iteration 9094: 0.04318476009840798\n",
      "Train Loss at iteration 9095: 0.043184690361775586\n",
      "Train Loss at iteration 9096: 0.04318462063391389\n",
      "Train Loss at iteration 9097: 0.04318455091482136\n",
      "Train Loss at iteration 9098: 0.0431844812044965\n",
      "Train Loss at iteration 9099: 0.0431844115029378\n",
      "Train Loss at iteration 9100: 0.04318434181014376\n",
      "Train Loss at iteration 9101: 0.04318427212611288\n",
      "Train Loss at iteration 9102: 0.04318420245084364\n",
      "Train Loss at iteration 9103: 0.043184132784334535\n",
      "Train Loss at iteration 9104: 0.043184063126584085\n",
      "Train Loss at iteration 9105: 0.04318399347759076\n",
      "Train Loss at iteration 9106: 0.04318392383735307\n",
      "Train Loss at iteration 9107: 0.043183854205869496\n",
      "Train Loss at iteration 9108: 0.04318378458313856\n",
      "Train Loss at iteration 9109: 0.04318371496915873\n",
      "Train Loss at iteration 9110: 0.04318364536392853\n",
      "Train Loss at iteration 9111: 0.043183575767446435\n",
      "Train Loss at iteration 9112: 0.04318350617971098\n",
      "Train Loss at iteration 9113: 0.04318343660072061\n",
      "Train Loss at iteration 9114: 0.04318336703047385\n",
      "Train Loss at iteration 9115: 0.04318329746896922\n",
      "Train Loss at iteration 9116: 0.043183227916205186\n",
      "Train Loss at iteration 9117: 0.04318315837218028\n",
      "Train Loss at iteration 9118: 0.04318308883689295\n",
      "Train Loss at iteration 9119: 0.043183019310341735\n",
      "Train Loss at iteration 9120: 0.043182949792525134\n",
      "Train Loss at iteration 9121: 0.043182880283441645\n",
      "Train Loss at iteration 9122: 0.043182810783089755\n",
      "Train Loss at iteration 9123: 0.043182741291467985\n",
      "Train Loss at iteration 9124: 0.043182671808574824\n",
      "Train Loss at iteration 9125: 0.043182602334408765\n",
      "Train Loss at iteration 9126: 0.04318253286896833\n",
      "Train Loss at iteration 9127: 0.04318246341225201\n",
      "Train Loss at iteration 9128: 0.04318239396425831\n",
      "Train Loss at iteration 9129: 0.04318232452498572\n",
      "Train Loss at iteration 9130: 0.04318225509443277\n",
      "Train Loss at iteration 9131: 0.043182185672597954\n",
      "Train Loss at iteration 9132: 0.04318211625947976\n",
      "Train Loss at iteration 9133: 0.0431820468550767\n",
      "Train Loss at iteration 9134: 0.04318197745938728\n",
      "Train Loss at iteration 9135: 0.04318190807241\n",
      "Train Loss at iteration 9136: 0.043181838694143375\n",
      "Train Loss at iteration 9137: 0.0431817693245859\n",
      "Train Loss at iteration 9138: 0.0431816999637361\n",
      "Train Loss at iteration 9139: 0.043181630611592456\n",
      "Train Loss at iteration 9140: 0.04318156126815348\n",
      "Train Loss at iteration 9141: 0.043181491933417686\n",
      "Train Loss at iteration 9142: 0.04318142260738358\n",
      "Train Loss at iteration 9143: 0.04318135329004966\n",
      "Train Loss at iteration 9144: 0.04318128398141443\n",
      "Train Loss at iteration 9145: 0.043181214681476406\n",
      "Train Loss at iteration 9146: 0.0431811453902341\n",
      "Train Loss at iteration 9147: 0.04318107610768601\n",
      "Train Loss at iteration 9148: 0.043181006833830665\n",
      "Train Loss at iteration 9149: 0.04318093756866653\n",
      "Train Loss at iteration 9150: 0.043180868312192165\n",
      "Train Loss at iteration 9151: 0.043180799064406035\n",
      "Train Loss at iteration 9152: 0.043180729825306685\n",
      "Train Loss at iteration 9153: 0.0431806605948926\n",
      "Train Loss at iteration 9154: 0.043180591373162304\n",
      "Train Loss at iteration 9155: 0.043180522160114296\n",
      "Train Loss at iteration 9156: 0.043180452955747094\n",
      "Train Loss at iteration 9157: 0.0431803837600592\n",
      "Train Loss at iteration 9158: 0.04318031457304914\n",
      "Train Loss at iteration 9159: 0.043180245394715426\n",
      "Train Loss at iteration 9160: 0.043180176225056544\n",
      "Train Loss at iteration 9161: 0.043180107064071015\n",
      "Train Loss at iteration 9162: 0.04318003791175737\n",
      "Train Loss at iteration 9163: 0.04317996876811413\n",
      "Train Loss at iteration 9164: 0.04317989963313975\n",
      "Train Loss at iteration 9165: 0.0431798305068328\n",
      "Train Loss at iteration 9166: 0.04317976138919178\n",
      "Train Loss at iteration 9167: 0.04317969228021519\n",
      "Train Loss at iteration 9168: 0.04317962317990155\n",
      "Train Loss at iteration 9169: 0.04317955408824938\n",
      "Train Loss at iteration 9170: 0.04317948500525719\n",
      "Train Loss at iteration 9171: 0.04317941593092348\n",
      "Train Loss at iteration 9172: 0.0431793468652468\n",
      "Train Loss at iteration 9173: 0.04317927780822562\n",
      "Train Loss at iteration 9174: 0.04317920875985849\n",
      "Train Loss at iteration 9175: 0.04317913972014393\n",
      "Train Loss at iteration 9176: 0.04317907068908043\n",
      "Train Loss at iteration 9177: 0.043179001666666515\n",
      "Train Loss at iteration 9178: 0.04317893265290071\n",
      "Train Loss at iteration 9179: 0.04317886364778152\n",
      "Train Loss at iteration 9180: 0.04317879465130747\n",
      "Train Loss at iteration 9181: 0.04317872566347709\n",
      "Train Loss at iteration 9182: 0.04317865668428888\n",
      "Train Loss at iteration 9183: 0.04317858771374134\n",
      "Train Loss at iteration 9184: 0.043178518751833025\n",
      "Train Loss at iteration 9185: 0.043178449798562446\n",
      "Train Loss at iteration 9186: 0.04317838085392811\n",
      "Train Loss at iteration 9187: 0.04317831191792853\n",
      "Train Loss at iteration 9188: 0.043178242990562256\n",
      "Train Loss at iteration 9189: 0.04317817407182778\n",
      "Train Loss at iteration 9190: 0.04317810516172362\n",
      "Train Loss at iteration 9191: 0.04317803626024831\n",
      "Train Loss at iteration 9192: 0.04317796736740038\n",
      "Train Loss at iteration 9193: 0.04317789848317832\n",
      "Train Loss at iteration 9194: 0.04317782960758068\n",
      "Train Loss at iteration 9195: 0.04317776074060596\n",
      "Train Loss at iteration 9196: 0.043177691882252706\n",
      "Train Loss at iteration 9197: 0.043177623032519416\n",
      "Train Loss at iteration 9198: 0.043177554191404635\n",
      "Train Loss at iteration 9199: 0.04317748535890685\n",
      "Train Loss at iteration 9200: 0.043177416535024625\n",
      "Train Loss at iteration 9201: 0.04317734771975646\n",
      "Train Loss at iteration 9202: 0.04317727891310088\n",
      "Train Loss at iteration 9203: 0.043177210115056416\n",
      "Train Loss at iteration 9204: 0.04317714132562159\n",
      "Train Loss at iteration 9205: 0.043177072544794914\n",
      "Train Loss at iteration 9206: 0.04317700377257492\n",
      "Train Loss at iteration 9207: 0.043176935008960145\n",
      "Train Loss at iteration 9208: 0.04317686625394909\n",
      "Train Loss at iteration 9209: 0.043176797507540304\n",
      "Train Loss at iteration 9210: 0.0431767287697323\n",
      "Train Loss at iteration 9211: 0.04317666004052362\n",
      "Train Loss at iteration 9212: 0.04317659131991276\n",
      "Train Loss at iteration 9213: 0.043176522607898275\n",
      "Train Loss at iteration 9214: 0.04317645390447866\n",
      "Train Loss at iteration 9215: 0.04317638520965249\n",
      "Train Loss at iteration 9216: 0.04317631652341824\n",
      "Train Loss at iteration 9217: 0.04317624784577447\n",
      "Train Loss at iteration 9218: 0.0431761791767197\n",
      "Train Loss at iteration 9219: 0.043176110516252454\n",
      "Train Loss at iteration 9220: 0.043176041864371274\n",
      "Train Loss at iteration 9221: 0.043175973221074665\n",
      "Train Loss at iteration 9222: 0.04317590458636119\n",
      "Train Loss at iteration 9223: 0.043175835960229336\n",
      "Train Loss at iteration 9224: 0.04317576734267766\n",
      "Train Loss at iteration 9225: 0.043175698733704686\n",
      "Train Loss at iteration 9226: 0.043175630133308954\n",
      "Train Loss at iteration 9227: 0.04317556154148898\n",
      "Train Loss at iteration 9228: 0.04317549295824329\n",
      "Train Loss at iteration 9229: 0.043175424383570436\n",
      "Train Loss at iteration 9230: 0.04317535581746892\n",
      "Train Loss at iteration 9231: 0.04317528725993731\n",
      "Train Loss at iteration 9232: 0.0431752187109741\n",
      "Train Loss at iteration 9233: 0.043175150170577846\n",
      "Train Loss at iteration 9234: 0.04317508163874708\n",
      "Train Loss at iteration 9235: 0.043175013115480325\n",
      "Train Loss at iteration 9236: 0.043174944600776116\n",
      "Train Loss at iteration 9237: 0.04317487609463298\n",
      "Train Loss at iteration 9238: 0.04317480759704948\n",
      "Train Loss at iteration 9239: 0.04317473910802411\n",
      "Train Loss at iteration 9240: 0.043174670627555436\n",
      "Train Loss at iteration 9241: 0.043174602155641964\n",
      "Train Loss at iteration 9242: 0.043174533692282246\n",
      "Train Loss at iteration 9243: 0.04317446523747481\n",
      "Train Loss at iteration 9244: 0.043174396791218206\n",
      "Train Loss at iteration 9245: 0.04317432835351094\n",
      "Train Loss at iteration 9246: 0.04317425992435159\n",
      "Train Loss at iteration 9247: 0.04317419150373864\n",
      "Train Loss at iteration 9248: 0.043174123091670666\n",
      "Train Loss at iteration 9249: 0.043174054688146196\n",
      "Train Loss at iteration 9250: 0.04317398629316374\n",
      "Train Loss at iteration 9251: 0.043173917906721884\n",
      "Train Loss at iteration 9252: 0.04317384952881912\n",
      "Train Loss at iteration 9253: 0.04317378115945401\n",
      "Train Loss at iteration 9254: 0.04317371279862509\n",
      "Train Loss at iteration 9255: 0.043173644446330886\n",
      "Train Loss at iteration 9256: 0.04317357610256995\n",
      "Train Loss at iteration 9257: 0.04317350776734081\n",
      "Train Loss at iteration 9258: 0.04317343944064202\n",
      "Train Loss at iteration 9259: 0.0431733711224721\n",
      "Train Loss at iteration 9260: 0.04317330281282959\n",
      "Train Loss at iteration 9261: 0.04317323451171305\n",
      "Train Loss at iteration 9262: 0.043173166219121005\n",
      "Train Loss at iteration 9263: 0.043173097935051995\n",
      "Train Loss at iteration 9264: 0.043173029659504564\n",
      "Train Loss at iteration 9265: 0.04317296139247727\n",
      "Train Loss at iteration 9266: 0.043172893133968604\n",
      "Train Loss at iteration 9267: 0.04317282488397716\n",
      "Train Loss at iteration 9268: 0.04317275664250145\n",
      "Train Loss at iteration 9269: 0.04317268840954004\n",
      "Train Loss at iteration 9270: 0.04317262018509146\n",
      "Train Loss at iteration 9271: 0.043172551969154226\n",
      "Train Loss at iteration 9272: 0.04317248376172692\n",
      "Train Loss at iteration 9273: 0.04317241556280807\n",
      "Train Loss at iteration 9274: 0.04317234737239621\n",
      "Train Loss at iteration 9275: 0.04317227919048991\n",
      "Train Loss at iteration 9276: 0.04317221101708768\n",
      "Train Loss at iteration 9277: 0.0431721428521881\n",
      "Train Loss at iteration 9278: 0.04317207469578967\n",
      "Train Loss at iteration 9279: 0.04317200654789098\n",
      "Train Loss at iteration 9280: 0.04317193840849054\n",
      "Train Loss at iteration 9281: 0.04317187027758692\n",
      "Train Loss at iteration 9282: 0.04317180215517866\n",
      "Train Loss at iteration 9283: 0.043171734041264294\n",
      "Train Loss at iteration 9284: 0.04317166593584237\n",
      "Train Loss at iteration 9285: 0.04317159783891144\n",
      "Train Loss at iteration 9286: 0.04317152975047006\n",
      "Train Loss at iteration 9287: 0.04317146167051677\n",
      "Train Loss at iteration 9288: 0.043171393599050104\n",
      "Train Loss at iteration 9289: 0.043171325536068623\n",
      "Train Loss at iteration 9290: 0.043171257481570864\n",
      "Train Loss at iteration 9291: 0.04317118943555538\n",
      "Train Loss at iteration 9292: 0.043171121398020734\n",
      "Train Loss at iteration 9293: 0.04317105336896546\n",
      "Train Loss at iteration 9294: 0.04317098534838811\n",
      "Train Loss at iteration 9295: 0.043170917336287216\n",
      "Train Loss at iteration 9296: 0.043170849332661354\n",
      "Train Loss at iteration 9297: 0.04317078133750907\n",
      "Train Loss at iteration 9298: 0.0431707133508289\n",
      "Train Loss at iteration 9299: 0.04317064537261941\n",
      "Train Loss at iteration 9300: 0.04317057740287912\n",
      "Train Loss at iteration 9301: 0.04317050944160661\n",
      "Train Loss at iteration 9302: 0.04317044148880043\n",
      "Train Loss at iteration 9303: 0.043170373544459115\n",
      "Train Loss at iteration 9304: 0.04317030560858124\n",
      "Train Loss at iteration 9305: 0.04317023768116532\n",
      "Train Loss at iteration 9306: 0.04317016976220994\n",
      "Train Loss at iteration 9307: 0.043170101851713646\n",
      "Train Loss at iteration 9308: 0.043170033949674974\n",
      "Train Loss at iteration 9309: 0.043169966056092496\n",
      "Train Loss at iteration 9310: 0.04316989817096477\n",
      "Train Loss at iteration 9311: 0.04316983029429032\n",
      "Train Loss at iteration 9312: 0.04316976242606772\n",
      "Train Loss at iteration 9313: 0.04316969456629552\n",
      "Train Loss at iteration 9314: 0.04316962671497229\n",
      "Train Loss at iteration 9315: 0.04316955887209656\n",
      "Train Loss at iteration 9316: 0.04316949103766688\n",
      "Train Loss at iteration 9317: 0.043169423211681844\n",
      "Train Loss at iteration 9318: 0.04316935539413997\n",
      "Train Loss at iteration 9319: 0.04316928758503983\n",
      "Train Loss at iteration 9320: 0.04316921978437997\n",
      "Train Loss at iteration 9321: 0.04316915199215897\n",
      "Train Loss at iteration 9322: 0.043169084208375355\n",
      "Train Loss at iteration 9323: 0.0431690164330277\n",
      "Train Loss at iteration 9324: 0.04316894866611455\n",
      "Train Loss at iteration 9325: 0.04316888090763447\n",
      "Train Loss at iteration 9326: 0.04316881315758603\n",
      "Train Loss at iteration 9327: 0.04316874541596776\n",
      "Train Loss at iteration 9328: 0.04316867768277823\n",
      "Train Loss at iteration 9329: 0.043168609958016024\n",
      "Train Loss at iteration 9330: 0.04316854224167966\n",
      "Train Loss at iteration 9331: 0.04316847453376771\n",
      "Train Loss at iteration 9332: 0.04316840683427876\n",
      "Train Loss at iteration 9333: 0.04316833914321131\n",
      "Train Loss at iteration 9334: 0.043168271460563984\n",
      "Train Loss at iteration 9335: 0.043168203786335325\n",
      "Train Loss at iteration 9336: 0.04316813612052386\n",
      "Train Loss at iteration 9337: 0.04316806846312818\n",
      "Train Loss at iteration 9338: 0.043168000814146824\n",
      "Train Loss at iteration 9339: 0.043167933173578386\n",
      "Train Loss at iteration 9340: 0.0431678655414214\n",
      "Train Loss at iteration 9341: 0.04316779791767443\n",
      "Train Loss at iteration 9342: 0.043167730302336074\n",
      "Train Loss at iteration 9343: 0.04316766269540483\n",
      "Train Loss at iteration 9344: 0.043167595096879306\n",
      "Train Loss at iteration 9345: 0.043167527506758056\n",
      "Train Loss at iteration 9346: 0.043167459925039633\n",
      "Train Loss at iteration 9347: 0.043167392351722615\n",
      "Train Loss at iteration 9348: 0.043167324786805544\n",
      "Train Loss at iteration 9349: 0.043167257230287\n",
      "Train Loss at iteration 9350: 0.04316718968216554\n",
      "Train Loss at iteration 9351: 0.04316712214243974\n",
      "Train Loss at iteration 9352: 0.043167054611108154\n",
      "Train Loss at iteration 9353: 0.04316698708816936\n",
      "Train Loss at iteration 9354: 0.043166919573621905\n",
      "Train Loss at iteration 9355: 0.04316685206746435\n",
      "Train Loss at iteration 9356: 0.04316678456969528\n",
      "Train Loss at iteration 9357: 0.043166717080313256\n",
      "Train Loss at iteration 9358: 0.04316664959931683\n",
      "Train Loss at iteration 9359: 0.04316658212670459\n",
      "Train Loss at iteration 9360: 0.04316651466247509\n",
      "Train Loss at iteration 9361: 0.04316644720662689\n",
      "Train Loss at iteration 9362: 0.04316637975915859\n",
      "Train Loss at iteration 9363: 0.043166312320068706\n",
      "Train Loss at iteration 9364: 0.043166244889355845\n",
      "Train Loss at iteration 9365: 0.04316617746701856\n",
      "Train Loss at iteration 9366: 0.04316611005305543\n",
      "Train Loss at iteration 9367: 0.04316604264746501\n",
      "Train Loss at iteration 9368: 0.04316597525024588\n",
      "Train Loss at iteration 9369: 0.043165907861396596\n",
      "Train Loss at iteration 9370: 0.043165840480915735\n",
      "Train Loss at iteration 9371: 0.043165773108801875\n",
      "Train Loss at iteration 9372: 0.04316570574505358\n",
      "Train Loss at iteration 9373: 0.04316563838966941\n",
      "Train Loss at iteration 9374: 0.04316557104264794\n",
      "Train Loss at iteration 9375: 0.04316550370398776\n",
      "Train Loss at iteration 9376: 0.0431654363736874\n",
      "Train Loss at iteration 9377: 0.04316536905174548\n",
      "Train Loss at iteration 9378: 0.04316530173816052\n",
      "Train Loss at iteration 9379: 0.043165234432931146\n",
      "Train Loss at iteration 9380: 0.04316516713605589\n",
      "Train Loss at iteration 9381: 0.04316509984753335\n",
      "Train Loss at iteration 9382: 0.043165032567362066\n",
      "Train Loss at iteration 9383: 0.043164965295540654\n",
      "Train Loss at iteration 9384: 0.04316489803206765\n",
      "Train Loss at iteration 9385: 0.04316483077694164\n",
      "Train Loss at iteration 9386: 0.04316476353016121\n",
      "Train Loss at iteration 9387: 0.043164696291724904\n",
      "Train Loss at iteration 9388: 0.04316462906163133\n",
      "Train Loss at iteration 9389: 0.043164561839879045\n",
      "Train Loss at iteration 9390: 0.04316449462646662\n",
      "Train Loss at iteration 9391: 0.04316442742139264\n",
      "Train Loss at iteration 9392: 0.043164360224655675\n",
      "Train Loss at iteration 9393: 0.043164293036254305\n",
      "Train Loss at iteration 9394: 0.04316422585618711\n",
      "Train Loss at iteration 9395: 0.04316415868445264\n",
      "Train Loss at iteration 9396: 0.043164091521049504\n",
      "Train Loss at iteration 9397: 0.043164024365976265\n",
      "Train Loss at iteration 9398: 0.04316395721923149\n",
      "Train Loss at iteration 9399: 0.043163890080813755\n",
      "Train Loss at iteration 9400: 0.04316382295072167\n",
      "Train Loss at iteration 9401: 0.04316375582895377\n",
      "Train Loss at iteration 9402: 0.04316368871550867\n",
      "Train Loss at iteration 9403: 0.04316362161038493\n",
      "Train Loss at iteration 9404: 0.04316355451358112\n",
      "Train Loss at iteration 9405: 0.043163487425095834\n",
      "Train Loss at iteration 9406: 0.04316342034492765\n",
      "Train Loss at iteration 9407: 0.043163353273075133\n",
      "Train Loss at iteration 9408: 0.04316328620953687\n",
      "Train Loss at iteration 9409: 0.043163219154311455\n",
      "Train Loss at iteration 9410: 0.04316315210739745\n",
      "Train Loss at iteration 9411: 0.04316308506879344\n",
      "Train Loss at iteration 9412: 0.043163018038498\n",
      "Train Loss at iteration 9413: 0.04316295101650973\n",
      "Train Loss at iteration 9414: 0.04316288400282718\n",
      "Train Loss at iteration 9415: 0.04316281699744897\n",
      "Train Loss at iteration 9416: 0.043162750000373645\n",
      "Train Loss at iteration 9417: 0.043162683011599805\n",
      "Train Loss at iteration 9418: 0.04316261603112603\n",
      "Train Loss at iteration 9419: 0.043162549058950915\n",
      "Train Loss at iteration 9420: 0.04316248209507303\n",
      "Train Loss at iteration 9421: 0.043162415139490935\n",
      "Train Loss at iteration 9422: 0.043162348192203256\n",
      "Train Loss at iteration 9423: 0.04316228125320856\n",
      "Train Loss at iteration 9424: 0.043162214322505416\n",
      "Train Loss at iteration 9425: 0.04316214740009242\n",
      "Train Loss at iteration 9426: 0.04316208048596816\n",
      "Train Loss at iteration 9427: 0.04316201358013123\n",
      "Train Loss at iteration 9428: 0.043161946682580166\n",
      "Train Loss at iteration 9429: 0.043161879793313616\n",
      "Train Loss at iteration 9430: 0.04316181291233012\n",
      "Train Loss at iteration 9431: 0.04316174603962829\n",
      "Train Loss at iteration 9432: 0.04316167917520671\n",
      "Train Loss at iteration 9433: 0.043161612319063944\n",
      "Train Loss at iteration 9434: 0.0431615454711986\n",
      "Train Loss at iteration 9435: 0.04316147863160926\n",
      "Train Loss at iteration 9436: 0.0431614118002945\n",
      "Train Loss at iteration 9437: 0.04316134497725293\n",
      "Train Loss at iteration 9438: 0.04316127816248311\n",
      "Train Loss at iteration 9439: 0.04316121135598364\n",
      "Train Loss at iteration 9440: 0.04316114455775313\n",
      "Train Loss at iteration 9441: 0.043161077767790115\n",
      "Train Loss at iteration 9442: 0.04316101098609323\n",
      "Train Loss at iteration 9443: 0.043160944212661055\n",
      "Train Loss at iteration 9444: 0.04316087744749217\n",
      "Train Loss at iteration 9445: 0.04316081069058516\n",
      "Train Loss at iteration 9446: 0.04316074394193863\n",
      "Train Loss at iteration 9447: 0.043160677201551166\n",
      "Train Loss at iteration 9448: 0.04316061046942134\n",
      "Train Loss at iteration 9449: 0.043160543745547765\n",
      "Train Loss at iteration 9450: 0.04316047702992902\n",
      "Train Loss at iteration 9451: 0.043160410322563694\n",
      "Train Loss at iteration 9452: 0.043160343623450385\n",
      "Train Loss at iteration 9453: 0.043160276932587685\n",
      "Train Loss at iteration 9454: 0.043160210249974185\n",
      "Train Loss at iteration 9455: 0.04316014357560847\n",
      "Train Loss at iteration 9456: 0.043160076909489135\n",
      "Train Loss at iteration 9457: 0.04316001025161477\n",
      "Train Loss at iteration 9458: 0.043159943601983976\n",
      "Train Loss at iteration 9459: 0.043159876960595346\n",
      "Train Loss at iteration 9460: 0.043159810327447465\n",
      "Train Loss at iteration 9461: 0.04315974370253893\n",
      "Train Loss at iteration 9462: 0.04315967708586833\n",
      "Train Loss at iteration 9463: 0.043159610477434274\n",
      "Train Loss at iteration 9464: 0.043159543877235335\n",
      "Train Loss at iteration 9465: 0.04315947728527012\n",
      "Train Loss at iteration 9466: 0.043159410701537226\n",
      "Train Loss at iteration 9467: 0.04315934412603526\n",
      "Train Loss at iteration 9468: 0.043159277558762775\n",
      "Train Loss at iteration 9469: 0.04315921099971841\n",
      "Train Loss at iteration 9470: 0.04315914444890074\n",
      "Train Loss at iteration 9471: 0.043159077906308366\n",
      "Train Loss at iteration 9472: 0.04315901137193988\n",
      "Train Loss at iteration 9473: 0.04315894484579387\n",
      "Train Loss at iteration 9474: 0.043158878327868966\n",
      "Train Loss at iteration 9475: 0.04315881181816372\n",
      "Train Loss at iteration 9476: 0.04315874531667678\n",
      "Train Loss at iteration 9477: 0.0431586788234067\n",
      "Train Loss at iteration 9478: 0.0431586123383521\n",
      "Train Loss at iteration 9479: 0.04315854586151158\n",
      "Train Loss at iteration 9480: 0.04315847939288372\n",
      "Train Loss at iteration 9481: 0.043158412932467125\n",
      "Train Loss at iteration 9482: 0.043158346480260415\n",
      "Train Loss at iteration 9483: 0.04315828003626217\n",
      "Train Loss at iteration 9484: 0.04315821360047099\n",
      "Train Loss at iteration 9485: 0.04315814717288548\n",
      "Train Loss at iteration 9486: 0.043158080753504226\n",
      "Train Loss at iteration 9487: 0.04315801434232584\n",
      "Train Loss at iteration 9488: 0.043157947939348934\n",
      "Train Loss at iteration 9489: 0.043157881544572084\n",
      "Train Loss at iteration 9490: 0.043157815157993926\n",
      "Train Loss at iteration 9491: 0.04315774877961302\n",
      "Train Loss at iteration 9492: 0.043157682409427994\n",
      "Train Loss at iteration 9493: 0.043157616047437446\n",
      "Train Loss at iteration 9494: 0.043157549693639975\n",
      "Train Loss at iteration 9495: 0.043157483348034174\n",
      "Train Loss at iteration 9496: 0.043157417010618655\n",
      "Train Loss at iteration 9497: 0.04315735068139202\n",
      "Train Loss at iteration 9498: 0.04315728436035288\n",
      "Train Loss at iteration 9499: 0.04315721804749983\n",
      "Train Loss at iteration 9500: 0.04315715174283147\n",
      "Train Loss at iteration 9501: 0.043157085446346416\n",
      "Train Loss at iteration 9502: 0.04315701915804325\n",
      "Train Loss at iteration 9503: 0.0431569528779206\n",
      "Train Loss at iteration 9504: 0.04315688660597706\n",
      "Train Loss at iteration 9505: 0.04315682034221124\n",
      "Train Loss at iteration 9506: 0.043156754086621736\n",
      "Train Loss at iteration 9507: 0.04315668783920717\n",
      "Train Loss at iteration 9508: 0.043156621599966125\n",
      "Train Loss at iteration 9509: 0.043156555368897224\n",
      "Train Loss at iteration 9510: 0.043156489145999066\n",
      "Train Loss at iteration 9511: 0.04315642293127026\n",
      "Train Loss at iteration 9512: 0.04315635672470941\n",
      "Train Loss at iteration 9513: 0.04315629052631514\n",
      "Train Loss at iteration 9514: 0.04315622433608602\n",
      "Train Loss at iteration 9515: 0.0431561581540207\n",
      "Train Loss at iteration 9516: 0.04315609198011775\n",
      "Train Loss at iteration 9517: 0.04315602581437582\n",
      "Train Loss at iteration 9518: 0.04315595965679347\n",
      "Train Loss at iteration 9519: 0.043155893507369335\n",
      "Train Loss at iteration 9520: 0.04315582736610203\n",
      "Train Loss at iteration 9521: 0.04315576123299015\n",
      "Train Loss at iteration 9522: 0.04315569510803233\n",
      "Train Loss at iteration 9523: 0.043155628991227137\n",
      "Train Loss at iteration 9524: 0.04315556288257322\n",
      "Train Loss at iteration 9525: 0.043155496782069154\n",
      "Train Loss at iteration 9526: 0.04315543068971358\n",
      "Train Loss at iteration 9527: 0.04315536460550509\n",
      "Train Loss at iteration 9528: 0.0431552985294423\n",
      "Train Loss at iteration 9529: 0.04315523246152382\n",
      "Train Loss at iteration 9530: 0.043155166401748275\n",
      "Train Loss at iteration 9531: 0.043155100350114266\n",
      "Train Loss at iteration 9532: 0.0431550343066204\n",
      "Train Loss at iteration 9533: 0.04315496827126529\n",
      "Train Loss at iteration 9534: 0.04315490224404755\n",
      "Train Loss at iteration 9535: 0.043154836224965805\n",
      "Train Loss at iteration 9536: 0.04315477021401865\n",
      "Train Loss at iteration 9537: 0.04315470421120471\n",
      "Train Loss at iteration 9538: 0.04315463821652258\n",
      "Train Loss at iteration 9539: 0.043154572229970906\n",
      "Train Loss at iteration 9540: 0.043154506251548284\n",
      "Train Loss at iteration 9541: 0.04315444028125332\n",
      "Train Loss at iteration 9542: 0.043154374319084636\n",
      "Train Loss at iteration 9543: 0.043154308365040854\n",
      "Train Loss at iteration 9544: 0.04315424241912057\n",
      "Train Loss at iteration 9545: 0.043154176481322415\n",
      "Train Loss at iteration 9546: 0.04315411055164502\n",
      "Train Loss at iteration 9547: 0.04315404463008696\n",
      "Train Loss at iteration 9548: 0.043153978716646876\n",
      "Train Loss at iteration 9549: 0.043153912811323386\n",
      "Train Loss at iteration 9550: 0.043153846914115106\n",
      "Train Loss at iteration 9551: 0.04315378102502064\n",
      "Train Loss at iteration 9552: 0.043153715144038624\n",
      "Train Loss at iteration 9553: 0.04315364927116764\n",
      "Train Loss at iteration 9554: 0.04315358340640636\n",
      "Train Loss at iteration 9555: 0.04315351754975336\n",
      "Train Loss at iteration 9556: 0.043153451701207275\n",
      "Train Loss at iteration 9557: 0.04315338586076671\n",
      "Train Loss at iteration 9558: 0.043153320028430295\n",
      "Train Loss at iteration 9559: 0.04315325420419665\n",
      "Train Loss at iteration 9560: 0.04315318838806438\n",
      "Train Loss at iteration 9561: 0.04315312258003213\n",
      "Train Loss at iteration 9562: 0.04315305678009849\n",
      "Train Loss at iteration 9563: 0.043152990988262106\n",
      "Train Loss at iteration 9564: 0.04315292520452158\n",
      "Train Loss at iteration 9565: 0.04315285942887554\n",
      "Train Loss at iteration 9566: 0.0431527936613226\n",
      "Train Loss at iteration 9567: 0.04315272790186138\n",
      "Train Loss at iteration 9568: 0.043152662150490524\n",
      "Train Loss at iteration 9569: 0.04315259640720864\n",
      "Train Loss at iteration 9570: 0.04315253067201433\n",
      "Train Loss at iteration 9571: 0.04315246494490624\n",
      "Train Loss at iteration 9572: 0.04315239922588297\n",
      "Train Loss at iteration 9573: 0.04315233351494318\n",
      "Train Loss at iteration 9574: 0.04315226781208545\n",
      "Train Loss at iteration 9575: 0.043152202117308434\n",
      "Train Loss at iteration 9576: 0.04315213643061075\n",
      "Train Loss at iteration 9577: 0.043152070751991\n",
      "Train Loss at iteration 9578: 0.04315200508144782\n",
      "Train Loss at iteration 9579: 0.043151939418979844\n",
      "Train Loss at iteration 9580: 0.043151873764585694\n",
      "Train Loss at iteration 9581: 0.043151808118263975\n",
      "Train Loss at iteration 9582: 0.04315174248001334\n",
      "Train Loss at iteration 9583: 0.043151676849832385\n",
      "Train Loss at iteration 9584: 0.043151611227719766\n",
      "Train Loss at iteration 9585: 0.04315154561367409\n",
      "Train Loss at iteration 9586: 0.043151480007693976\n",
      "Train Loss at iteration 9587: 0.043151414409778065\n",
      "Train Loss at iteration 9588: 0.04315134881992498\n",
      "Train Loss at iteration 9589: 0.04315128323813334\n",
      "Train Loss at iteration 9590: 0.04315121766440178\n",
      "Train Loss at iteration 9591: 0.04315115209872892\n",
      "Train Loss at iteration 9592: 0.04315108654111339\n",
      "Train Loss at iteration 9593: 0.04315102099155383\n",
      "Train Loss at iteration 9594: 0.04315095545004884\n",
      "Train Loss at iteration 9595: 0.043150889916597074\n",
      "Train Loss at iteration 9596: 0.043150824391197155\n",
      "Train Loss at iteration 9597: 0.04315075887384769\n",
      "Train Loss at iteration 9598: 0.04315069336454734\n",
      "Train Loss at iteration 9599: 0.04315062786329471\n",
      "Train Loss at iteration 9600: 0.04315056237008845\n",
      "Train Loss at iteration 9601: 0.04315049688492717\n",
      "Train Loss at iteration 9602: 0.043150431407809504\n",
      "Train Loss at iteration 9603: 0.0431503659387341\n",
      "Train Loss at iteration 9604: 0.043150300477699556\n",
      "Train Loss at iteration 9605: 0.043150235024704525\n",
      "Train Loss at iteration 9606: 0.04315016957974765\n",
      "Train Loss at iteration 9607: 0.04315010414282752\n",
      "Train Loss at iteration 9608: 0.04315003871394281\n",
      "Train Loss at iteration 9609: 0.04314997329309212\n",
      "Train Loss at iteration 9610: 0.0431499078802741\n",
      "Train Loss at iteration 9611: 0.043149842475487386\n",
      "Train Loss at iteration 9612: 0.0431497770787306\n",
      "Train Loss at iteration 9613: 0.04314971169000237\n",
      "Train Loss at iteration 9614: 0.043149646309301334\n",
      "Train Loss at iteration 9615: 0.043149580936626124\n",
      "Train Loss at iteration 9616: 0.04314951557197536\n",
      "Train Loss at iteration 9617: 0.04314945021534773\n",
      "Train Loss at iteration 9618: 0.043149384866741795\n",
      "Train Loss at iteration 9619: 0.04314931952615624\n",
      "Train Loss at iteration 9620: 0.04314925419358968\n",
      "Train Loss at iteration 9621: 0.04314918886904075\n",
      "Train Loss at iteration 9622: 0.04314912355250808\n",
      "Train Loss at iteration 9623: 0.04314905824399032\n",
      "Train Loss at iteration 9624: 0.04314899294348609\n",
      "Train Loss at iteration 9625: 0.04314892765099405\n",
      "Train Loss at iteration 9626: 0.0431488623665128\n",
      "Train Loss at iteration 9627: 0.043148797090041005\n",
      "Train Loss at iteration 9628: 0.043148731821577274\n",
      "Train Loss at iteration 9629: 0.04314866656112028\n",
      "Train Loss at iteration 9630: 0.04314860130866863\n",
      "Train Loss at iteration 9631: 0.04314853606422096\n",
      "Train Loss at iteration 9632: 0.04314847082777594\n",
      "Train Loss at iteration 9633: 0.04314840559933219\n",
      "Train Loss at iteration 9634: 0.043148340378888324\n",
      "Train Loss at iteration 9635: 0.04314827516644301\n",
      "Train Loss at iteration 9636: 0.04314820996199487\n",
      "Train Loss at iteration 9637: 0.04314814476554256\n",
      "Train Loss at iteration 9638: 0.04314807957708471\n",
      "Train Loss at iteration 9639: 0.04314801439661995\n",
      "Train Loss at iteration 9640: 0.043147949224146935\n",
      "Train Loss at iteration 9641: 0.04314788405966429\n",
      "Train Loss at iteration 9642: 0.04314781890317066\n",
      "Train Loss at iteration 9643: 0.04314775375466469\n",
      "Train Loss at iteration 9644: 0.043147688614145016\n",
      "Train Loss at iteration 9645: 0.043147623481610275\n",
      "Train Loss at iteration 9646: 0.04314755835705913\n",
      "Train Loss at iteration 9647: 0.04314749324049019\n",
      "Train Loss at iteration 9648: 0.043147428131902095\n",
      "Train Loss at iteration 9649: 0.04314736303129353\n",
      "Train Loss at iteration 9650: 0.04314729793866309\n",
      "Train Loss at iteration 9651: 0.043147232854009454\n",
      "Train Loss at iteration 9652: 0.043147167777331234\n",
      "Train Loss at iteration 9653: 0.043147102708627094\n",
      "Train Loss at iteration 9654: 0.04314703764789567\n",
      "Train Loss at iteration 9655: 0.04314697259513561\n",
      "Train Loss at iteration 9656: 0.043146907550345524\n",
      "Train Loss at iteration 9657: 0.0431468425135241\n",
      "Train Loss at iteration 9658: 0.04314677748466996\n",
      "Train Loss at iteration 9659: 0.04314671246378176\n",
      "Train Loss at iteration 9660: 0.04314664745085813\n",
      "Train Loss at iteration 9661: 0.04314658244589773\n",
      "Train Loss at iteration 9662: 0.04314651744889919\n",
      "Train Loss at iteration 9663: 0.043146452459861165\n",
      "Train Loss at iteration 9664: 0.04314638747878229\n",
      "Train Loss at iteration 9665: 0.04314632250566123\n",
      "Train Loss at iteration 9666: 0.043146257540496606\n",
      "Train Loss at iteration 9667: 0.04314619258328708\n",
      "Train Loss at iteration 9668: 0.0431461276340313\n",
      "Train Loss at iteration 9669: 0.0431460626927279\n",
      "Train Loss at iteration 9670: 0.04314599775937555\n",
      "Train Loss at iteration 9671: 0.04314593283397287\n",
      "Train Loss at iteration 9672: 0.04314586791651853\n",
      "Train Loss at iteration 9673: 0.04314580300701115\n",
      "Train Loss at iteration 9674: 0.04314573810544941\n",
      "Train Loss at iteration 9675: 0.04314567321183194\n",
      "Train Loss at iteration 9676: 0.043145608326157395\n",
      "Train Loss at iteration 9677: 0.0431455434484244\n",
      "Train Loss at iteration 9678: 0.04314547857863163\n",
      "Train Loss at iteration 9679: 0.04314541371677773\n",
      "Train Loss at iteration 9680: 0.04314534886286136\n",
      "Train Loss at iteration 9681: 0.043145284016881155\n",
      "Train Loss at iteration 9682: 0.043145219178835764\n",
      "Train Loss at iteration 9683: 0.04314515434872384\n",
      "Train Loss at iteration 9684: 0.04314508952654401\n",
      "Train Loss at iteration 9685: 0.04314502471229498\n",
      "Train Loss at iteration 9686: 0.043144959905975354\n",
      "Train Loss at iteration 9687: 0.0431448951075838\n",
      "Train Loss at iteration 9688: 0.043144830317118965\n",
      "Train Loss at iteration 9689: 0.04314476553457951\n",
      "Train Loss at iteration 9690: 0.04314470075996407\n",
      "Train Loss at iteration 9691: 0.04314463599327131\n",
      "Train Loss at iteration 9692: 0.04314457123449989\n",
      "Train Loss at iteration 9693: 0.04314450648364845\n",
      "Train Loss at iteration 9694: 0.043144441740715626\n",
      "Train Loss at iteration 9695: 0.04314437700570011\n",
      "Train Loss at iteration 9696: 0.04314431227860052\n",
      "Train Loss at iteration 9697: 0.04314424755941553\n",
      "Train Loss at iteration 9698: 0.043144182848143794\n",
      "Train Loss at iteration 9699: 0.04314411814478396\n",
      "Train Loss at iteration 9700: 0.04314405344933468\n",
      "Train Loss at iteration 9701: 0.04314398876179462\n",
      "Train Loss at iteration 9702: 0.043143924082162416\n",
      "Train Loss at iteration 9703: 0.043143859410436726\n",
      "Train Loss at iteration 9704: 0.043143794746616226\n",
      "Train Loss at iteration 9705: 0.04314373009069956\n",
      "Train Loss at iteration 9706: 0.043143665442685386\n",
      "Train Loss at iteration 9707: 0.043143600802572346\n",
      "Train Loss at iteration 9708: 0.0431435361703591\n",
      "Train Loss at iteration 9709: 0.043143471546044324\n",
      "Train Loss at iteration 9710: 0.04314340692962666\n",
      "Train Loss at iteration 9711: 0.04314334232110476\n",
      "Train Loss at iteration 9712: 0.0431432777204773\n",
      "Train Loss at iteration 9713: 0.04314321312774293\n",
      "Train Loss at iteration 9714: 0.043143148542900286\n",
      "Train Loss at iteration 9715: 0.043143083965948056\n",
      "Train Loss at iteration 9716: 0.04314301939688488\n",
      "Train Loss at iteration 9717: 0.04314295483570941\n",
      "Train Loss at iteration 9718: 0.04314289028242034\n",
      "Train Loss at iteration 9719: 0.043142825737016305\n",
      "Train Loss at iteration 9720: 0.04314276119949594\n",
      "Train Loss at iteration 9721: 0.043142696669857955\n",
      "Train Loss at iteration 9722: 0.04314263214810098\n",
      "Train Loss at iteration 9723: 0.04314256763422368\n",
      "Train Loss at iteration 9724: 0.04314250312822473\n",
      "Train Loss at iteration 9725: 0.04314243863010275\n",
      "Train Loss at iteration 9726: 0.043142374139856436\n",
      "Train Loss at iteration 9727: 0.04314230965748444\n",
      "Train Loss at iteration 9728: 0.04314224518298543\n",
      "Train Loss at iteration 9729: 0.04314218071635805\n",
      "Train Loss at iteration 9730: 0.043142116257600976\n",
      "Train Loss at iteration 9731: 0.04314205180671287\n",
      "Train Loss at iteration 9732: 0.04314198736369238\n",
      "Train Loss at iteration 9733: 0.04314192292853819\n",
      "Train Loss at iteration 9734: 0.04314185850124895\n",
      "Train Loss at iteration 9735: 0.043141794081823316\n",
      "Train Loss at iteration 9736: 0.04314172967025997\n",
      "Train Loss at iteration 9737: 0.043141665266557556\n",
      "Train Loss at iteration 9738: 0.043141600870714766\n",
      "Train Loss at iteration 9739: 0.04314153648273022\n",
      "Train Loss at iteration 9740: 0.04314147210260263\n",
      "Train Loss at iteration 9741: 0.04314140773033061\n",
      "Train Loss at iteration 9742: 0.043141343365912875\n",
      "Train Loss at iteration 9743: 0.04314127900934806\n",
      "Train Loss at iteration 9744: 0.043141214660634836\n",
      "Train Loss at iteration 9745: 0.04314115031977187\n",
      "Train Loss at iteration 9746: 0.04314108598675784\n",
      "Train Loss at iteration 9747: 0.04314102166159138\n",
      "Train Loss at iteration 9748: 0.04314095734427118\n",
      "Train Loss at iteration 9749: 0.0431408930347959\n",
      "Train Loss at iteration 9750: 0.043140828733164216\n",
      "Train Loss at iteration 9751: 0.043140764439374785\n",
      "Train Loss at iteration 9752: 0.043140700153426276\n",
      "Train Loss at iteration 9753: 0.043140635875317364\n",
      "Train Loss at iteration 9754: 0.0431405716050467\n",
      "Train Loss at iteration 9755: 0.04314050734261296\n",
      "Train Loss at iteration 9756: 0.04314044308801482\n",
      "Train Loss at iteration 9757: 0.04314037884125095\n",
      "Train Loss at iteration 9758: 0.043140314602320005\n",
      "Train Loss at iteration 9759: 0.04314025037122064\n",
      "Train Loss at iteration 9760: 0.043140186147951565\n",
      "Train Loss at iteration 9761: 0.043140121932511426\n",
      "Train Loss at iteration 9762: 0.043140057724898886\n",
      "Train Loss at iteration 9763: 0.04313999352511263\n",
      "Train Loss at iteration 9764: 0.04313992933315131\n",
      "Train Loss at iteration 9765: 0.04313986514901363\n",
      "Train Loss at iteration 9766: 0.04313980097269821\n",
      "Train Loss at iteration 9767: 0.04313973680420377\n",
      "Train Loss at iteration 9768: 0.04313967264352896\n",
      "Train Loss at iteration 9769: 0.04313960849067244\n",
      "Train Loss at iteration 9770: 0.0431395443456329\n",
      "Train Loss at iteration 9771: 0.04313948020840901\n",
      "Train Loss at iteration 9772: 0.04313941607899942\n",
      "Train Loss at iteration 9773: 0.04313935195740284\n",
      "Train Loss at iteration 9774: 0.0431392878436179\n",
      "Train Loss at iteration 9775: 0.043139223737643305\n",
      "Train Loss at iteration 9776: 0.04313915963947772\n",
      "Train Loss at iteration 9777: 0.04313909554911981\n",
      "Train Loss at iteration 9778: 0.04313903146656825\n",
      "Train Loss at iteration 9779: 0.04313896739182172\n",
      "Train Loss at iteration 9780: 0.0431389033248789\n",
      "Train Loss at iteration 9781: 0.04313883926573845\n",
      "Train Loss at iteration 9782: 0.043138775214399044\n",
      "Train Loss at iteration 9783: 0.04313871117085938\n",
      "Train Loss at iteration 9784: 0.043138647135118106\n",
      "Train Loss at iteration 9785: 0.043138583107173896\n",
      "Train Loss at iteration 9786: 0.043138519087025456\n",
      "Train Loss at iteration 9787: 0.043138455074671435\n",
      "Train Loss at iteration 9788: 0.04313839107011051\n",
      "Train Loss at iteration 9789: 0.043138327073341365\n",
      "Train Loss at iteration 9790: 0.04313826308436268\n",
      "Train Loss at iteration 9791: 0.04313819910317312\n",
      "Train Loss at iteration 9792: 0.043138135129771375\n",
      "Train Loss at iteration 9793: 0.04313807116415611\n",
      "Train Loss at iteration 9794: 0.04313800720632602\n",
      "Train Loss at iteration 9795: 0.04313794325627975\n",
      "Train Loss at iteration 9796: 0.043137879314016005\n",
      "Train Loss at iteration 9797: 0.043137815379533456\n",
      "Train Loss at iteration 9798: 0.04313775145283078\n",
      "Train Loss at iteration 9799: 0.04313768753390667\n",
      "Train Loss at iteration 9800: 0.043137623622759774\n",
      "Train Loss at iteration 9801: 0.043137559719388804\n",
      "Train Loss at iteration 9802: 0.043137495823792414\n",
      "Train Loss at iteration 9803: 0.04313743193596931\n",
      "Train Loss at iteration 9804: 0.04313736805591814\n",
      "Train Loss at iteration 9805: 0.043137304183637606\n",
      "Train Loss at iteration 9806: 0.04313724031912637\n",
      "Train Loss at iteration 9807: 0.04313717646238314\n",
      "Train Loss at iteration 9808: 0.04313711261340657\n",
      "Train Loss at iteration 9809: 0.043137048772195354\n",
      "Train Loss at iteration 9810: 0.04313698493874817\n",
      "Train Loss at iteration 9811: 0.043136921113063706\n",
      "Train Loss at iteration 9812: 0.043136857295140635\n",
      "Train Loss at iteration 9813: 0.04313679348497765\n",
      "Train Loss at iteration 9814: 0.04313672968257342\n",
      "Train Loss at iteration 9815: 0.04313666588792662\n",
      "Train Loss at iteration 9816: 0.04313660210103597\n",
      "Train Loss at iteration 9817: 0.04313653832190011\n",
      "Train Loss at iteration 9818: 0.04313647455051776\n",
      "Train Loss at iteration 9819: 0.04313641078688756\n",
      "Train Loss at iteration 9820: 0.043136347031008236\n",
      "Train Loss at iteration 9821: 0.043136283282878454\n",
      "Train Loss at iteration 9822: 0.0431362195424969\n",
      "Train Loss at iteration 9823: 0.043136155809862256\n",
      "Train Loss at iteration 9824: 0.043136092084973196\n",
      "Train Loss at iteration 9825: 0.043136028367828434\n",
      "Train Loss at iteration 9826: 0.04313596465842663\n",
      "Train Loss at iteration 9827: 0.04313590095676648\n",
      "Train Loss at iteration 9828: 0.043135837262846664\n",
      "Train Loss at iteration 9829: 0.04313577357666588\n",
      "Train Loss at iteration 9830: 0.04313570989822279\n",
      "Train Loss at iteration 9831: 0.043135646227516096\n",
      "Train Loss at iteration 9832: 0.043135582564544486\n",
      "Train Loss at iteration 9833: 0.04313551890930664\n",
      "Train Loss at iteration 9834: 0.04313545526180126\n",
      "Train Loss at iteration 9835: 0.043135391622027015\n",
      "Train Loss at iteration 9836: 0.043135327989982604\n",
      "Train Loss at iteration 9837: 0.043135264365666705\n",
      "Train Loss at iteration 9838: 0.04313520074907802\n",
      "Train Loss at iteration 9839: 0.043135137140215225\n",
      "Train Loss at iteration 9840: 0.04313507353907701\n",
      "Train Loss at iteration 9841: 0.043135009945662064\n",
      "Train Loss at iteration 9842: 0.04313494635996908\n",
      "Train Loss at iteration 9843: 0.04313488278199675\n",
      "Train Loss at iteration 9844: 0.043134819211743745\n",
      "Train Loss at iteration 9845: 0.04313475564920879\n",
      "Train Loss at iteration 9846: 0.04313469209439053\n",
      "Train Loss at iteration 9847: 0.043134628547287696\n",
      "Train Loss at iteration 9848: 0.04313456500789895\n",
      "Train Loss at iteration 9849: 0.043134501476223\n",
      "Train Loss at iteration 9850: 0.04313443795225851\n",
      "Train Loss at iteration 9851: 0.04313437443600421\n",
      "Train Loss at iteration 9852: 0.04313431092745877\n",
      "Train Loss at iteration 9853: 0.043134247426620885\n",
      "Train Loss at iteration 9854: 0.04313418393348924\n",
      "Train Loss at iteration 9855: 0.043134120448062536\n",
      "Train Loss at iteration 9856: 0.04313405697033946\n",
      "Train Loss at iteration 9857: 0.04313399350031871\n",
      "Train Loss at iteration 9858: 0.04313393003799898\n",
      "Train Loss at iteration 9859: 0.04313386658337894\n",
      "Train Loss at iteration 9860: 0.043133803136457316\n",
      "Train Loss at iteration 9861: 0.043133739697232776\n",
      "Train Loss at iteration 9862: 0.043133676265704045\n",
      "Train Loss at iteration 9863: 0.04313361284186976\n",
      "Train Loss at iteration 9864: 0.04313354942572869\n",
      "Train Loss at iteration 9865: 0.04313348601727947\n",
      "Train Loss at iteration 9866: 0.04313342261652083\n",
      "Train Loss at iteration 9867: 0.04313335922345145\n",
      "Train Loss at iteration 9868: 0.043133295838070004\n",
      "Train Loss at iteration 9869: 0.04313323246037523\n",
      "Train Loss at iteration 9870: 0.0431331690903658\n",
      "Train Loss at iteration 9871: 0.043133105728040404\n",
      "Train Loss at iteration 9872: 0.04313304237339775\n",
      "Train Loss at iteration 9873: 0.043132979026436535\n",
      "Train Loss at iteration 9874: 0.04313291568715545\n",
      "Train Loss at iteration 9875: 0.04313285235555319\n",
      "Train Loss at iteration 9876: 0.04313278903162845\n",
      "Train Loss at iteration 9877: 0.043132725715379946\n",
      "Train Loss at iteration 9878: 0.043132662406806355\n",
      "Train Loss at iteration 9879: 0.043132599105906395\n",
      "Train Loss at iteration 9880: 0.043132535812678736\n",
      "Train Loss at iteration 9881: 0.043132472527122094\n",
      "Train Loss at iteration 9882: 0.043132409249235164\n",
      "Train Loss at iteration 9883: 0.04313234597901664\n",
      "Train Loss at iteration 9884: 0.04313228271646524\n",
      "Train Loss at iteration 9885: 0.04313221946157964\n",
      "Train Loss at iteration 9886: 0.04313215621435856\n",
      "Train Loss at iteration 9887: 0.04313209297480069\n",
      "Train Loss at iteration 9888: 0.043132029742904705\n",
      "Train Loss at iteration 9889: 0.04313196651866934\n",
      "Train Loss at iteration 9890: 0.043131903302093294\n",
      "Train Loss at iteration 9891: 0.04313184009317525\n",
      "Train Loss at iteration 9892: 0.04313177689191392\n",
      "Train Loss at iteration 9893: 0.043131713698308\n",
      "Train Loss at iteration 9894: 0.043131650512356186\n",
      "Train Loss at iteration 9895: 0.04313158733405719\n",
      "Train Loss at iteration 9896: 0.04313152416340971\n",
      "Train Loss at iteration 9897: 0.043131461000412444\n",
      "Train Loss at iteration 9898: 0.043131397845064104\n",
      "Train Loss at iteration 9899: 0.04313133469736339\n",
      "Train Loss at iteration 9900: 0.043131271557309\n",
      "Train Loss at iteration 9901: 0.043131208424899634\n",
      "Train Loss at iteration 9902: 0.04313114530013401\n",
      "Train Loss at iteration 9903: 0.0431310821830108\n",
      "Train Loss at iteration 9904: 0.043131019073528745\n",
      "Train Loss at iteration 9905: 0.04313095597168653\n",
      "Train Loss at iteration 9906: 0.04313089287748287\n",
      "Train Loss at iteration 9907: 0.043130829790916446\n",
      "Train Loss at iteration 9908: 0.043130766711985985\n",
      "Train Loss at iteration 9909: 0.04313070364069019\n",
      "Train Loss at iteration 9910: 0.04313064057702776\n",
      "Train Loss at iteration 9911: 0.04313057752099739\n",
      "Train Loss at iteration 9912: 0.04313051447259781\n",
      "Train Loss at iteration 9913: 0.0431304514318277\n",
      "Train Loss at iteration 9914: 0.043130388398685784\n",
      "Train Loss at iteration 9915: 0.043130325373170755\n",
      "Train Loss at iteration 9916: 0.04313026235528134\n",
      "Train Loss at iteration 9917: 0.04313019934501623\n",
      "Train Loss at iteration 9918: 0.043130136342374135\n",
      "Train Loss at iteration 9919: 0.043130073347353755\n",
      "Train Loss at iteration 9920: 0.04313001035995382\n",
      "Train Loss at iteration 9921: 0.04312994738017302\n",
      "Train Loss at iteration 9922: 0.04312988440801005\n",
      "Train Loss at iteration 9923: 0.04312982144346364\n",
      "Train Loss at iteration 9924: 0.043129758486532506\n",
      "Train Loss at iteration 9925: 0.04312969553721532\n",
      "Train Loss at iteration 9926: 0.04312963259551082\n",
      "Train Loss at iteration 9927: 0.04312956966141772\n",
      "Train Loss at iteration 9928: 0.0431295067349347\n",
      "Train Loss at iteration 9929: 0.04312944381606051\n",
      "Train Loss at iteration 9930: 0.04312938090479382\n",
      "Train Loss at iteration 9931: 0.043129318001133364\n",
      "Train Loss at iteration 9932: 0.043129255105077834\n",
      "Train Loss at iteration 9933: 0.043129192216625964\n",
      "Train Loss at iteration 9934: 0.04312912933577645\n",
      "Train Loss at iteration 9935: 0.04312906646252801\n",
      "Train Loss at iteration 9936: 0.04312900359687935\n",
      "Train Loss at iteration 9937: 0.04312894073882917\n",
      "Train Loss at iteration 9938: 0.0431288778883762\n",
      "Train Loss at iteration 9939: 0.04312881504551916\n",
      "Train Loss at iteration 9940: 0.04312875221025672\n",
      "Train Loss at iteration 9941: 0.04312868938258765\n",
      "Train Loss at iteration 9942: 0.04312862656251061\n",
      "Train Loss at iteration 9943: 0.04312856375002436\n",
      "Train Loss at iteration 9944: 0.04312850094512755\n",
      "Train Loss at iteration 9945: 0.04312843814781897\n",
      "Train Loss at iteration 9946: 0.04312837535809728\n",
      "Train Loss at iteration 9947: 0.04312831257596121\n",
      "Train Loss at iteration 9948: 0.04312824980140948\n",
      "Train Loss at iteration 9949: 0.043128187034440796\n",
      "Train Loss at iteration 9950: 0.04312812427505386\n",
      "Train Loss at iteration 9951: 0.04312806152324742\n",
      "Train Loss at iteration 9952: 0.043127998779020155\n",
      "Train Loss at iteration 9953: 0.043127936042370814\n",
      "Train Loss at iteration 9954: 0.043127873313298086\n",
      "Train Loss at iteration 9955: 0.0431278105918007\n",
      "Train Loss at iteration 9956: 0.043127747877877355\n",
      "Train Loss at iteration 9957: 0.043127685171526806\n",
      "Train Loss at iteration 9958: 0.04312762247274772\n",
      "Train Loss at iteration 9959: 0.04312755978153884\n",
      "Train Loss at iteration 9960: 0.04312749709789889\n",
      "Train Loss at iteration 9961: 0.04312743442182658\n",
      "Train Loss at iteration 9962: 0.0431273717533206\n",
      "Train Loss at iteration 9963: 0.04312730909237972\n",
      "Train Loss at iteration 9964: 0.04312724643900262\n",
      "Train Loss at iteration 9965: 0.04312718379318802\n",
      "Train Loss at iteration 9966: 0.04312712115493465\n",
      "Train Loss at iteration 9967: 0.04312705852424123\n",
      "Train Loss at iteration 9968: 0.04312699590110646\n",
      "Train Loss at iteration 9969: 0.0431269332855291\n",
      "Train Loss at iteration 9970: 0.04312687067750781\n",
      "Train Loss at iteration 9971: 0.043126808077041355\n",
      "Train Loss at iteration 9972: 0.04312674548412843\n",
      "Train Loss at iteration 9973: 0.043126682898767775\n",
      "Train Loss at iteration 9974: 0.04312662032095809\n",
      "Train Loss at iteration 9975: 0.04312655775069811\n",
      "Train Loss at iteration 9976: 0.04312649518798656\n",
      "Train Loss at iteration 9977: 0.04312643263282214\n",
      "Train Loss at iteration 9978: 0.04312637008520358\n",
      "Train Loss at iteration 9979: 0.04312630754512961\n",
      "Train Loss at iteration 9980: 0.043126245012598936\n",
      "Train Loss at iteration 9981: 0.043126182487610305\n",
      "Train Loss at iteration 9982: 0.04312611997016243\n",
      "Train Loss at iteration 9983: 0.043126057460254004\n",
      "Train Loss at iteration 9984: 0.043125994957883766\n",
      "Train Loss at iteration 9985: 0.04312593246305046\n",
      "Train Loss at iteration 9986: 0.04312586997575279\n",
      "Train Loss at iteration 9987: 0.04312580749598947\n",
      "Train Loss at iteration 9988: 0.04312574502375925\n",
      "Train Loss at iteration 9989: 0.04312568255906084\n",
      "Train Loss at iteration 9990: 0.04312562010189296\n",
      "Train Loss at iteration 9991: 0.04312555765225433\n",
      "Train Loss at iteration 9992: 0.04312549521014369\n",
      "Train Loss at iteration 9993: 0.04312543277555975\n",
      "Train Loss at iteration 9994: 0.04312537034850124\n",
      "Train Loss at iteration 9995: 0.04312530792896688\n",
      "Train Loss at iteration 9996: 0.04312524551695541\n",
      "Train Loss at iteration 9997: 0.043125183112465534\n",
      "Train Loss at iteration 9998: 0.04312512071549599\n",
      "Train Loss at iteration 9999: 0.04312505832604551\n",
      "Train Loss at iteration 10000: 0.043124995944112815\n",
      "Train Loss at iteration 10001: 0.04312493356969663\n",
      "Train Loss at iteration 10002: 0.04312487120279568\n",
      "Train Loss at iteration 10003: 0.043124808843408684\n",
      "Train Loss at iteration 10004: 0.04312474649153439\n",
      "Train Loss at iteration 10005: 0.0431246841471715\n",
      "Train Loss at iteration 10006: 0.043124621810318765\n",
      "Train Loss at iteration 10007: 0.043124559480974906\n",
      "Train Loss at iteration 10008: 0.04312449715913864\n",
      "Train Loss at iteration 10009: 0.043124434844808705\n",
      "Train Loss at iteration 10010: 0.043124372537983825\n",
      "Train Loss at iteration 10011: 0.04312431023866273\n",
      "Train Loss at iteration 10012: 0.04312424794684415\n",
      "Train Loss at iteration 10013: 0.043124185662526814\n",
      "Train Loss at iteration 10014: 0.043124123385709444\n",
      "Train Loss at iteration 10015: 0.0431240611163908\n",
      "Train Loss at iteration 10016: 0.043123998854569565\n",
      "Train Loss at iteration 10017: 0.043123936600244495\n",
      "Train Loss at iteration 10018: 0.043123874353414325\n",
      "Train Loss at iteration 10019: 0.04312381211407777\n",
      "Train Loss at iteration 10020: 0.043123749882233574\n",
      "Train Loss at iteration 10021: 0.043123687657880466\n",
      "Train Loss at iteration 10022: 0.043123625441017166\n",
      "Train Loss at iteration 10023: 0.04312356323164242\n",
      "Train Loss at iteration 10024: 0.04312350102975494\n",
      "Train Loss at iteration 10025: 0.04312343883535348\n",
      "Train Loss at iteration 10026: 0.04312337664843676\n",
      "Train Loss at iteration 10027: 0.04312331446900353\n",
      "Train Loss at iteration 10028: 0.04312325229705249\n",
      "Train Loss at iteration 10029: 0.04312319013258239\n",
      "Train Loss at iteration 10030: 0.04312312797559197\n",
      "Train Loss at iteration 10031: 0.043123065826079963\n",
      "Train Loss at iteration 10032: 0.04312300368404509\n",
      "Train Loss at iteration 10033: 0.043122941549486093\n",
      "Train Loss at iteration 10034: 0.04312287942240169\n",
      "Train Loss at iteration 10035: 0.04312281730279065\n",
      "Train Loss at iteration 10036: 0.04312275519065167\n",
      "Train Loss at iteration 10037: 0.04312269308598352\n",
      "Train Loss at iteration 10038: 0.04312263098878489\n",
      "Train Loss at iteration 10039: 0.04312256889905456\n",
      "Train Loss at iteration 10040: 0.043122506816791244\n",
      "Train Loss at iteration 10041: 0.04312244474199367\n",
      "Train Loss at iteration 10042: 0.043122382674660585\n",
      "Train Loss at iteration 10043: 0.04312232061479074\n",
      "Train Loss at iteration 10044: 0.04312225856238284\n",
      "Train Loss at iteration 10045: 0.04312219651743563\n",
      "Train Loss at iteration 10046: 0.04312213447994786\n",
      "Train Loss at iteration 10047: 0.04312207244991827\n",
      "Train Loss at iteration 10048: 0.04312201042734558\n",
      "Train Loss at iteration 10049: 0.04312194841222853\n",
      "Train Loss at iteration 10050: 0.043121886404565865\n",
      "Train Loss at iteration 10051: 0.04312182440435631\n",
      "Train Loss at iteration 10052: 0.04312176241159862\n",
      "Train Loss at iteration 10053: 0.04312170042629153\n",
      "Train Loss at iteration 10054: 0.04312163844843377\n",
      "Train Loss at iteration 10055: 0.043121576478024094\n",
      "Train Loss at iteration 10056: 0.043121514515061216\n",
      "Train Loss at iteration 10057: 0.043121452559543895\n",
      "Train Loss at iteration 10058: 0.04312139061147086\n",
      "Train Loss at iteration 10059: 0.04312132867084086\n",
      "Train Loss at iteration 10060: 0.043121266737652625\n",
      "Train Loss at iteration 10061: 0.043121204811904915\n",
      "Train Loss at iteration 10062: 0.043121142893596455\n",
      "Train Loss at iteration 10063: 0.04312108098272597\n",
      "Train Loss at iteration 10064: 0.043121019079292215\n",
      "Train Loss at iteration 10065: 0.043120957183293944\n",
      "Train Loss at iteration 10066: 0.0431208952947299\n",
      "Train Loss at iteration 10067: 0.043120833413598784\n",
      "Train Loss at iteration 10068: 0.043120771539899384\n",
      "Train Loss at iteration 10069: 0.043120709673630415\n",
      "Train Loss at iteration 10070: 0.04312064781479062\n",
      "Train Loss at iteration 10071: 0.04312058596337877\n",
      "Train Loss at iteration 10072: 0.04312052411939358\n",
      "Train Loss at iteration 10073: 0.043120462282833796\n",
      "Train Loss at iteration 10074: 0.04312040045369816\n",
      "Train Loss at iteration 10075: 0.04312033863198542\n",
      "Train Loss at iteration 10076: 0.04312027681769432\n",
      "Train Loss at iteration 10077: 0.04312021501082361\n",
      "Train Loss at iteration 10078: 0.043120153211372025\n",
      "Train Loss at iteration 10079: 0.04312009141933831\n",
      "Train Loss at iteration 10080: 0.0431200296347212\n",
      "Train Loss at iteration 10081: 0.04311996785751947\n",
      "Train Loss at iteration 10082: 0.04311990608773183\n",
      "Train Loss at iteration 10083: 0.043119844325357044\n",
      "Train Loss at iteration 10084: 0.04311978257039386\n",
      "Train Loss at iteration 10085: 0.043119720822841004\n",
      "Train Loss at iteration 10086: 0.04311965908269725\n",
      "Train Loss at iteration 10087: 0.04311959734996132\n",
      "Train Loss at iteration 10088: 0.04311953562463197\n",
      "Train Loss at iteration 10089: 0.04311947390670795\n",
      "Train Loss at iteration 10090: 0.04311941219618799\n",
      "Train Loss at iteration 10091: 0.04311935049307086\n",
      "Train Loss at iteration 10092: 0.0431192887973553\n",
      "Train Loss at iteration 10093: 0.04311922710904004\n",
      "Train Loss at iteration 10094: 0.04311916542812386\n",
      "Train Loss at iteration 10095: 0.04311910375460548\n",
      "Train Loss at iteration 10096: 0.04311904208848363\n",
      "Train Loss at iteration 10097: 0.04311898042975712\n",
      "Train Loss at iteration 10098: 0.043118918778424664\n",
      "Train Loss at iteration 10099: 0.04311885713448499\n",
      "Train Loss at iteration 10100: 0.04311879549793688\n",
      "Train Loss at iteration 10101: 0.04311873386877908\n",
      "Train Loss at iteration 10102: 0.04311867224701031\n",
      "Train Loss at iteration 10103: 0.04311861063262935\n",
      "Train Loss at iteration 10104: 0.04311854902563494\n",
      "Train Loss at iteration 10105: 0.04311848742602583\n",
      "Train Loss at iteration 10106: 0.043118425833800766\n",
      "Train Loss at iteration 10107: 0.04311836424895851\n",
      "Train Loss at iteration 10108: 0.04311830267149779\n",
      "Train Loss at iteration 10109: 0.04311824110141739\n",
      "Train Loss at iteration 10110: 0.04311817953871604\n",
      "Train Loss at iteration 10111: 0.04311811798339249\n",
      "Train Loss at iteration 10112: 0.0431180564354455\n",
      "Train Loss at iteration 10113: 0.04311799489487382\n",
      "Train Loss at iteration 10114: 0.04311793336167619\n",
      "Train Loss at iteration 10115: 0.04311787183585138\n",
      "Train Loss at iteration 10116: 0.04311781031739814\n",
      "Train Loss at iteration 10117: 0.04311774880631522\n",
      "Train Loss at iteration 10118: 0.043117687302601355\n",
      "Train Loss at iteration 10119: 0.04311762580625532\n",
      "Train Loss at iteration 10120: 0.043117564317275876\n",
      "Train Loss at iteration 10121: 0.04311750283566177\n",
      "Train Loss at iteration 10122: 0.04311744136141172\n",
      "Train Loss at iteration 10123: 0.04311737989452453\n",
      "Train Loss at iteration 10124: 0.043117318434998934\n",
      "Train Loss at iteration 10125: 0.04311725698283369\n",
      "Train Loss at iteration 10126: 0.04311719553802753\n",
      "Train Loss at iteration 10127: 0.04311713410057925\n",
      "Train Loss at iteration 10128: 0.043117072670487575\n",
      "Train Loss at iteration 10129: 0.04311701124775126\n",
      "Train Loss at iteration 10130: 0.043116949832369085\n",
      "Train Loss at iteration 10131: 0.04311688842433979\n",
      "Train Loss at iteration 10132: 0.043116827023662124\n",
      "Train Loss at iteration 10133: 0.04311676563033484\n",
      "Train Loss at iteration 10134: 0.04311670424435673\n",
      "Train Loss at iteration 10135: 0.04311664286572652\n",
      "Train Loss at iteration 10136: 0.043116581494442974\n",
      "Train Loss at iteration 10137: 0.043116520130504835\n",
      "Train Loss at iteration 10138: 0.04311645877391089\n",
      "Train Loss at iteration 10139: 0.043116397424659884\n",
      "Train Loss at iteration 10140: 0.04311633608275057\n",
      "Train Loss at iteration 10141: 0.04311627474818171\n",
      "Train Loss at iteration 10142: 0.043116213420952045\n",
      "Train Loss at iteration 10143: 0.043116152101060366\n",
      "Train Loss at iteration 10144: 0.04311609078850541\n",
      "Train Loss at iteration 10145: 0.04311602948328594\n",
      "Train Loss at iteration 10146: 0.04311596818540072\n",
      "Train Loss at iteration 10147: 0.0431159068948485\n",
      "Train Loss at iteration 10148: 0.043115845611628054\n",
      "Train Loss at iteration 10149: 0.043115784335738136\n",
      "Train Loss at iteration 10150: 0.043115723067177494\n",
      "Train Loss at iteration 10151: 0.04311566180594491\n",
      "Train Loss at iteration 10152: 0.043115600552039134\n",
      "Train Loss at iteration 10153: 0.04311553930545891\n",
      "Train Loss at iteration 10154: 0.04311547806620303\n",
      "Train Loss at iteration 10155: 0.04311541683427023\n",
      "Train Loss at iteration 10156: 0.043115355609659296\n",
      "Train Loss at iteration 10157: 0.043115294392368955\n",
      "Train Loss at iteration 10158: 0.043115233182398016\n",
      "Train Loss at iteration 10159: 0.0431151719797452\n",
      "Train Loss at iteration 10160: 0.043115110784409286\n",
      "Train Loss at iteration 10161: 0.04311504959638903\n",
      "Train Loss at iteration 10162: 0.0431149884156832\n",
      "Train Loss at iteration 10163: 0.04311492724229057\n",
      "Train Loss at iteration 10164: 0.043114866076209894\n",
      "Train Loss at iteration 10165: 0.043114804917439915\n",
      "Train Loss at iteration 10166: 0.043114743765979444\n",
      "Train Loss at iteration 10167: 0.04311468262182719\n",
      "Train Loss at iteration 10168: 0.043114621484981956\n",
      "Train Loss at iteration 10169: 0.04311456035544249\n",
      "Train Loss at iteration 10170: 0.04311449923320756\n",
      "Train Loss at iteration 10171: 0.04311443811827594\n",
      "Train Loss at iteration 10172: 0.04311437701064638\n",
      "Train Loss at iteration 10173: 0.04311431591031765\n",
      "Train Loss at iteration 10174: 0.04311425481728853\n",
      "Train Loss at iteration 10175: 0.04311419373155776\n",
      "Train Loss at iteration 10176: 0.043114132653124124\n",
      "Train Loss at iteration 10177: 0.04311407158198638\n",
      "Train Loss at iteration 10178: 0.043114010518143314\n",
      "Train Loss at iteration 10179: 0.043113949461593665\n",
      "Train Loss at iteration 10180: 0.04311388841233621\n",
      "Train Loss at iteration 10181: 0.043113827370369714\n",
      "Train Loss at iteration 10182: 0.04311376633569295\n",
      "Train Loss at iteration 10183: 0.043113705308304696\n",
      "Train Loss at iteration 10184: 0.0431136442882037\n",
      "Train Loss at iteration 10185: 0.04311358327538873\n",
      "Train Loss at iteration 10186: 0.04311352226985856\n",
      "Train Loss at iteration 10187: 0.043113461271611975\n",
      "Train Loss at iteration 10188: 0.04311340028064771\n",
      "Train Loss at iteration 10189: 0.04311333929696458\n",
      "Train Loss at iteration 10190: 0.043113278320561305\n",
      "Train Loss at iteration 10191: 0.04311321735143668\n",
      "Train Loss at iteration 10192: 0.04311315638958947\n",
      "Train Loss at iteration 10193: 0.04311309543501844\n",
      "Train Loss at iteration 10194: 0.04311303448772237\n",
      "Train Loss at iteration 10195: 0.04311297354770002\n",
      "Train Loss at iteration 10196: 0.04311291261495019\n",
      "Train Loss at iteration 10197: 0.0431128516894716\n",
      "Train Loss at iteration 10198: 0.043112790771263065\n",
      "Train Loss at iteration 10199: 0.04311272986032333\n",
      "Train Loss at iteration 10200: 0.04311266895665116\n",
      "Train Loss at iteration 10201: 0.04311260806024537\n",
      "Train Loss at iteration 10202: 0.04311254717110468\n",
      "Train Loss at iteration 10203: 0.0431124862892279\n",
      "Train Loss at iteration 10204: 0.04311242541461377\n",
      "Train Loss at iteration 10205: 0.043112364547261084\n",
      "Train Loss at iteration 10206: 0.043112303687168625\n",
      "Train Loss at iteration 10207: 0.04311224283433514\n",
      "Train Loss at iteration 10208: 0.04311218198875941\n",
      "Train Loss at iteration 10209: 0.04311212115044021\n",
      "Train Loss at iteration 10210: 0.043112060319376325\n",
      "Train Loss at iteration 10211: 0.0431119994955665\n",
      "Train Loss at iteration 10212: 0.04311193867900955\n",
      "Train Loss at iteration 10213: 0.04311187786970422\n",
      "Train Loss at iteration 10214: 0.04311181706764928\n",
      "Train Loss at iteration 10215: 0.04311175627284353\n",
      "Train Loss at iteration 10216: 0.04311169548528573\n",
      "Train Loss at iteration 10217: 0.04311163470497464\n",
      "Train Loss at iteration 10218: 0.04311157393190906\n",
      "Train Loss at iteration 10219: 0.04311151316608775\n",
      "Train Loss at iteration 10220: 0.043111452407509514\n",
      "Train Loss at iteration 10221: 0.04311139165617308\n",
      "Train Loss at iteration 10222: 0.043111330912077264\n",
      "Train Loss at iteration 10223: 0.043111270175220824\n",
      "Train Loss at iteration 10224: 0.04311120944560253\n",
      "Train Loss at iteration 10225: 0.043111148723221186\n",
      "Train Loss at iteration 10226: 0.04311108800807554\n",
      "Train Loss at iteration 10227: 0.04311102730016439\n",
      "Train Loss at iteration 10228: 0.043110966599486523\n",
      "Train Loss at iteration 10229: 0.043110905906040664\n",
      "Train Loss at iteration 10230: 0.04311084521982565\n",
      "Train Loss at iteration 10231: 0.043110784540840226\n",
      "Train Loss at iteration 10232: 0.043110723869083184\n",
      "Train Loss at iteration 10233: 0.04311066320455329\n",
      "Train Loss at iteration 10234: 0.04311060254724934\n",
      "Train Loss at iteration 10235: 0.043110541897170104\n",
      "Train Loss at iteration 10236: 0.04311048125431437\n",
      "Train Loss at iteration 10237: 0.043110420618680895\n",
      "Train Loss at iteration 10238: 0.04311035999026847\n",
      "Train Loss at iteration 10239: 0.043110299369075886\n",
      "Train Loss at iteration 10240: 0.043110238755101914\n",
      "Train Loss at iteration 10241: 0.043110178148345314\n",
      "Train Loss at iteration 10242: 0.043110117548804915\n",
      "Train Loss at iteration 10243: 0.04311005695647946\n",
      "Train Loss at iteration 10244: 0.043109996371367726\n",
      "Train Loss at iteration 10245: 0.04310993579346853\n",
      "Train Loss at iteration 10246: 0.04310987522278063\n",
      "Train Loss at iteration 10247: 0.04310981465930279\n",
      "Train Loss at iteration 10248: 0.04310975410303382\n",
      "Train Loss at iteration 10249: 0.04310969355397249\n",
      "Train Loss at iteration 10250: 0.0431096330121176\n",
      "Train Loss at iteration 10251: 0.04310957247746791\n",
      "Train Loss at iteration 10252: 0.04310951195002221\n",
      "Train Loss at iteration 10253: 0.043109451429779275\n",
      "Train Loss at iteration 10254: 0.04310939091673791\n",
      "Train Loss at iteration 10255: 0.04310933041089688\n",
      "Train Loss at iteration 10256: 0.04310926991225497\n",
      "Train Loss at iteration 10257: 0.04310920942081097\n",
      "Train Loss at iteration 10258: 0.04310914893656366\n",
      "Train Loss at iteration 10259: 0.043109088459511835\n",
      "Train Loss at iteration 10260: 0.043109027989654256\n",
      "Train Loss at iteration 10261: 0.04310896752698973\n",
      "Train Loss at iteration 10262: 0.043108907071517044\n",
      "Train Loss at iteration 10263: 0.04310884662323494\n",
      "Train Loss at iteration 10264: 0.04310878618214228\n",
      "Train Loss at iteration 10265: 0.04310872574823778\n",
      "Train Loss at iteration 10266: 0.04310866532152026\n",
      "Train Loss at iteration 10267: 0.043108604901988486\n",
      "Train Loss at iteration 10268: 0.043108544489641264\n",
      "Train Loss at iteration 10269: 0.043108484084477365\n",
      "Train Loss at iteration 10270: 0.0431084236864956\n",
      "Train Loss at iteration 10271: 0.04310836329569473\n",
      "Train Loss at iteration 10272: 0.04310830291207353\n",
      "Train Loss at iteration 10273: 0.043108242535630836\n",
      "Train Loss at iteration 10274: 0.0431081821663654\n",
      "Train Loss at iteration 10275: 0.04310812180427602\n",
      "Train Loss at iteration 10276: 0.043108061449361475\n",
      "Train Loss at iteration 10277: 0.043108001101620566\n",
      "Train Loss at iteration 10278: 0.04310794076105207\n",
      "Train Loss at iteration 10279: 0.04310788042765479\n",
      "Train Loss at iteration 10280: 0.04310782010142749\n",
      "Train Loss at iteration 10281: 0.04310775978236899\n",
      "Train Loss at iteration 10282: 0.043107699470478045\n",
      "Train Loss at iteration 10283: 0.04310763916575348\n",
      "Train Loss at iteration 10284: 0.04310757886819406\n",
      "Train Loss at iteration 10285: 0.04310751857779858\n",
      "Train Loss at iteration 10286: 0.043107458294565845\n",
      "Train Loss at iteration 10287: 0.04310739801849462\n",
      "Train Loss at iteration 10288: 0.043107337749583724\n",
      "Train Loss at iteration 10289: 0.04310727748783191\n",
      "Train Loss at iteration 10290: 0.043107217233238015\n",
      "Train Loss at iteration 10291: 0.0431071569858008\n",
      "Train Loss at iteration 10292: 0.04310709674551907\n",
      "Train Loss at iteration 10293: 0.04310703651239159\n",
      "Train Loss at iteration 10294: 0.04310697628641719\n",
      "Train Loss at iteration 10295: 0.04310691606759463\n",
      "Train Loss at iteration 10296: 0.04310685585592273\n",
      "Train Loss at iteration 10297: 0.043106795651400256\n",
      "Train Loss at iteration 10298: 0.04310673545402602\n",
      "Train Loss at iteration 10299: 0.043106675263798806\n",
      "Train Loss at iteration 10300: 0.04310661508071741\n",
      "Train Loss at iteration 10301: 0.04310655490478064\n",
      "Train Loss at iteration 10302: 0.043106494735987246\n",
      "Train Loss at iteration 10303: 0.04310643457433607\n",
      "Train Loss at iteration 10304: 0.04310637441982588\n",
      "Train Loss at iteration 10305: 0.04310631427245548\n",
      "Train Loss at iteration 10306: 0.043106254132223666\n",
      "Train Loss at iteration 10307: 0.04310619399912921\n",
      "Train Loss at iteration 10308: 0.04310613387317093\n",
      "Train Loss at iteration 10309: 0.043106073754347614\n",
      "Train Loss at iteration 10310: 0.04310601364265807\n",
      "Train Loss at iteration 10311: 0.04310595353810108\n",
      "Train Loss at iteration 10312: 0.04310589344067545\n",
      "Train Loss at iteration 10313: 0.04310583335037995\n",
      "Train Loss at iteration 10314: 0.043105773267213404\n",
      "Train Loss at iteration 10315: 0.04310571319117459\n",
      "Train Loss at iteration 10316: 0.04310565312226233\n",
      "Train Loss at iteration 10317: 0.0431055930604754\n",
      "Train Loss at iteration 10318: 0.043105533005812595\n",
      "Train Loss at iteration 10319: 0.043105472958272724\n",
      "Train Loss at iteration 10320: 0.04310541291785457\n",
      "Train Loss at iteration 10321: 0.043105352884556955\n",
      "Train Loss at iteration 10322: 0.04310529285837865\n",
      "Train Loss at iteration 10323: 0.04310523283931847\n",
      "Train Loss at iteration 10324: 0.04310517282737519\n",
      "Train Loss at iteration 10325: 0.04310511282254764\n",
      "Train Loss at iteration 10326: 0.043105052824834615\n",
      "Train Loss at iteration 10327: 0.04310499283423492\n",
      "Train Loss at iteration 10328: 0.0431049328507473\n",
      "Train Loss at iteration 10329: 0.043104872874370606\n",
      "Train Loss at iteration 10330: 0.043104812905103616\n",
      "Train Loss at iteration 10331: 0.04310475294294516\n",
      "Train Loss at iteration 10332: 0.04310469298789401\n",
      "Train Loss at iteration 10333: 0.043104633039948964\n",
      "Train Loss at iteration 10334: 0.04310457309910884\n",
      "Train Loss at iteration 10335: 0.04310451316537242\n",
      "Train Loss at iteration 10336: 0.04310445323873853\n",
      "Train Loss at iteration 10337: 0.04310439331920595\n",
      "Train Loss at iteration 10338: 0.04310433340677349\n",
      "Train Loss at iteration 10339: 0.04310427350143994\n",
      "Train Loss at iteration 10340: 0.04310421360320411\n",
      "Train Loss at iteration 10341: 0.0431041537120648\n",
      "Train Loss at iteration 10342: 0.04310409382802083\n",
      "Train Loss at iteration 10343: 0.04310403395107097\n",
      "Train Loss at iteration 10344: 0.04310397408121405\n",
      "Train Loss at iteration 10345: 0.04310391421844887\n",
      "Train Loss at iteration 10346: 0.04310385436277422\n",
      "Train Loss at iteration 10347: 0.0431037945141889\n",
      "Train Loss at iteration 10348: 0.04310373467269174\n",
      "Train Loss at iteration 10349: 0.04310367483828151\n",
      "Train Loss at iteration 10350: 0.043103615010957025\n",
      "Train Loss at iteration 10351: 0.04310355519071712\n",
      "Train Loss at iteration 10352: 0.04310349537756056\n",
      "Train Loss at iteration 10353: 0.04310343557148615\n",
      "Train Loss at iteration 10354: 0.043103375772492736\n",
      "Train Loss at iteration 10355: 0.043103315980579066\n",
      "Train Loss at iteration 10356: 0.04310325619574399\n",
      "Train Loss at iteration 10357: 0.043103196417986316\n",
      "Train Loss at iteration 10358: 0.04310313664730479\n",
      "Train Loss at iteration 10359: 0.04310307688369828\n",
      "Train Loss at iteration 10360: 0.04310301712716558\n",
      "Train Loss at iteration 10361: 0.04310295737770548\n",
      "Train Loss at iteration 10362: 0.04310289763531679\n",
      "Train Loss at iteration 10363: 0.043102837899998324\n",
      "Train Loss at iteration 10364: 0.04310277817174889\n",
      "Train Loss at iteration 10365: 0.043102718450567276\n",
      "Train Loss at iteration 10366: 0.04310265873645231\n",
      "Train Loss at iteration 10367: 0.04310259902940281\n",
      "Train Loss at iteration 10368: 0.043102539329417544\n",
      "Train Loss at iteration 10369: 0.043102479636495356\n",
      "Train Loss at iteration 10370: 0.04310241995063504\n",
      "Train Loss at iteration 10371: 0.043102360271835394\n",
      "Train Loss at iteration 10372: 0.04310230060009526\n",
      "Train Loss at iteration 10373: 0.04310224093541341\n",
      "Train Loss at iteration 10374: 0.04310218127778867\n",
      "Train Loss at iteration 10375: 0.04310212162721984\n",
      "Train Loss at iteration 10376: 0.04310206198370575\n",
      "Train Loss at iteration 10377: 0.0431020023472452\n",
      "Train Loss at iteration 10378: 0.04310194271783699\n",
      "Train Loss at iteration 10379: 0.04310188309547994\n",
      "Train Loss at iteration 10380: 0.04310182348017284\n",
      "Train Loss at iteration 10381: 0.04310176387191453\n",
      "Train Loss at iteration 10382: 0.043101704270703804\n",
      "Train Loss at iteration 10383: 0.043101644676539486\n",
      "Train Loss at iteration 10384: 0.04310158508942037\n",
      "Train Loss at iteration 10385: 0.04310152550934528\n",
      "Train Loss at iteration 10386: 0.04310146593631303\n",
      "Train Loss at iteration 10387: 0.04310140637032241\n",
      "Train Loss at iteration 10388: 0.04310134681137226\n",
      "Train Loss at iteration 10389: 0.043101287259461364\n",
      "Train Loss at iteration 10390: 0.04310122771458856\n",
      "Train Loss at iteration 10391: 0.043101168176752656\n",
      "Train Loss at iteration 10392: 0.04310110864595246\n",
      "Train Loss at iteration 10393: 0.04310104912218676\n",
      "Train Loss at iteration 10394: 0.04310098960545441\n",
      "Train Loss at iteration 10395: 0.04310093009575422\n",
      "Train Loss at iteration 10396: 0.04310087059308497\n",
      "Train Loss at iteration 10397: 0.043100811097445506\n",
      "Train Loss at iteration 10398: 0.04310075160883462\n",
      "Train Loss at iteration 10399: 0.04310069212725116\n",
      "Train Loss at iteration 10400: 0.04310063265269389\n",
      "Train Loss at iteration 10401: 0.04310057318516167\n",
      "Train Loss at iteration 10402: 0.04310051372465329\n",
      "Train Loss at iteration 10403: 0.04310045427116757\n",
      "Train Loss at iteration 10404: 0.04310039482470333\n",
      "Train Loss at iteration 10405: 0.043100335385259377\n",
      "Train Loss at iteration 10406: 0.04310027595283456\n",
      "Train Loss at iteration 10407: 0.04310021652742764\n",
      "Train Loss at iteration 10408: 0.04310015710903746\n",
      "Train Loss at iteration 10409: 0.04310009769766286\n",
      "Train Loss at iteration 10410: 0.043100038293302625\n",
      "Train Loss at iteration 10411: 0.043099978895955585\n",
      "Train Loss at iteration 10412: 0.043099919505620546\n",
      "Train Loss at iteration 10413: 0.043099860122296337\n",
      "Train Loss at iteration 10414: 0.04309980074598177\n",
      "Train Loss at iteration 10415: 0.04309974137667567\n",
      "Train Loss at iteration 10416: 0.04309968201437684\n",
      "Train Loss at iteration 10417: 0.043099622659084104\n",
      "Train Loss at iteration 10418: 0.0430995633107963\n",
      "Train Loss at iteration 10419: 0.043099503969512225\n",
      "Train Loss at iteration 10420: 0.04309944463523069\n",
      "Train Loss at iteration 10421: 0.043099385307950544\n",
      "Train Loss at iteration 10422: 0.04309932598767058\n",
      "Train Loss at iteration 10423: 0.043099266674389625\n",
      "Train Loss at iteration 10424: 0.0430992073681065\n",
      "Train Loss at iteration 10425: 0.04309914806882004\n",
      "Train Loss at iteration 10426: 0.04309908877652903\n",
      "Train Loss at iteration 10427: 0.04309902949123232\n",
      "Train Loss at iteration 10428: 0.04309897021292873\n",
      "Train Loss at iteration 10429: 0.04309891094161705\n",
      "Train Loss at iteration 10430: 0.04309885167729613\n",
      "Train Loss at iteration 10431: 0.043098792419964796\n",
      "Train Loss at iteration 10432: 0.043098733169621845\n",
      "Train Loss at iteration 10433: 0.04309867392626611\n",
      "Train Loss at iteration 10434: 0.04309861468989642\n",
      "Train Loss at iteration 10435: 0.043098555460511584\n",
      "Train Loss at iteration 10436: 0.04309849623811042\n",
      "Train Loss at iteration 10437: 0.043098437022691775\n",
      "Train Loss at iteration 10438: 0.043098377814254456\n",
      "Train Loss at iteration 10439: 0.043098318612797275\n",
      "Train Loss at iteration 10440: 0.04309825941831908\n",
      "Train Loss at iteration 10441: 0.04309820023081868\n",
      "Train Loss at iteration 10442: 0.04309814105029489\n",
      "Train Loss at iteration 10443: 0.043098081876746554\n",
      "Train Loss at iteration 10444: 0.043098022710172476\n",
      "Train Loss at iteration 10445: 0.0430979635505715\n",
      "Train Loss at iteration 10446: 0.04309790439794242\n",
      "Train Loss at iteration 10447: 0.04309784525228408\n",
      "Train Loss at iteration 10448: 0.04309778611359532\n",
      "Train Loss at iteration 10449: 0.04309772698187495\n",
      "Train Loss at iteration 10450: 0.04309766785712179\n",
      "Train Loss at iteration 10451: 0.043097608739334665\n",
      "Train Loss at iteration 10452: 0.043097549628512394\n",
      "Train Loss at iteration 10453: 0.043097490524653836\n",
      "Train Loss at iteration 10454: 0.043097431427757786\n",
      "Train Loss at iteration 10455: 0.04309737233782307\n",
      "Train Loss at iteration 10456: 0.04309731325484852\n",
      "Train Loss at iteration 10457: 0.04309725417883298\n",
      "Train Loss at iteration 10458: 0.04309719510977525\n",
      "Train Loss at iteration 10459: 0.043097136047674185\n",
      "Train Loss at iteration 10460: 0.043097076992528585\n",
      "Train Loss at iteration 10461: 0.043097017944337276\n",
      "Train Loss at iteration 10462: 0.04309695890309912\n",
      "Train Loss at iteration 10463: 0.04309689986881291\n",
      "Train Loss at iteration 10464: 0.04309684084147748\n",
      "Train Loss at iteration 10465: 0.04309678182109169\n",
      "Train Loss at iteration 10466: 0.04309672280765432\n",
      "Train Loss at iteration 10467: 0.04309666380116424\n",
      "Train Loss at iteration 10468: 0.04309660480162025\n",
      "Train Loss at iteration 10469: 0.04309654580902119\n",
      "Train Loss at iteration 10470: 0.04309648682336589\n",
      "Train Loss at iteration 10471: 0.04309642784465319\n",
      "Train Loss at iteration 10472: 0.04309636887288188\n",
      "Train Loss at iteration 10473: 0.04309630990805085\n",
      "Train Loss at iteration 10474: 0.04309625095015888\n",
      "Train Loss at iteration 10475: 0.043096191999204815\n",
      "Train Loss at iteration 10476: 0.04309613305518751\n",
      "Train Loss at iteration 10477: 0.04309607411810575\n",
      "Train Loss at iteration 10478: 0.043096015187958414\n",
      "Train Loss at iteration 10479: 0.043095956264744294\n",
      "Train Loss at iteration 10480: 0.043095897348462246\n",
      "Train Loss at iteration 10481: 0.04309583843911109\n",
      "Train Loss at iteration 10482: 0.04309577953668965\n",
      "Train Loss at iteration 10483: 0.04309572064119678\n",
      "Train Loss at iteration 10484: 0.04309566175263131\n",
      "Train Loss at iteration 10485: 0.043095602870992046\n",
      "Train Loss at iteration 10486: 0.04309554399627784\n",
      "Train Loss at iteration 10487: 0.043095485128487536\n",
      "Train Loss at iteration 10488: 0.04309542626761994\n",
      "Train Loss at iteration 10489: 0.04309536741367389\n",
      "Train Loss at iteration 10490: 0.04309530856664824\n",
      "Train Loss at iteration 10491: 0.04309524972654181\n",
      "Train Loss at iteration 10492: 0.04309519089335345\n",
      "Train Loss at iteration 10493: 0.043095132067081966\n",
      "Train Loss at iteration 10494: 0.043095073247726184\n",
      "Train Loss at iteration 10495: 0.043095014435284996\n",
      "Train Loss at iteration 10496: 0.04309495562975718\n",
      "Train Loss at iteration 10497: 0.04309489683114161\n",
      "Train Loss at iteration 10498: 0.04309483803943709\n",
      "Train Loss at iteration 10499: 0.04309477925464247\n",
      "Train Loss at iteration 10500: 0.043094720476756575\n",
      "Train Loss at iteration 10501: 0.04309466170577826\n",
      "Train Loss at iteration 10502: 0.04309460294170635\n",
      "Train Loss at iteration 10503: 0.04309454418453967\n",
      "Train Loss at iteration 10504: 0.04309448543427708\n",
      "Train Loss at iteration 10505: 0.043094426690917384\n",
      "Train Loss at iteration 10506: 0.04309436795445945\n",
      "Train Loss at iteration 10507: 0.04309430922490212\n",
      "Train Loss at iteration 10508: 0.043094250502244205\n",
      "Train Loss at iteration 10509: 0.04309419178648455\n",
      "Train Loss at iteration 10510: 0.043094133077621985\n",
      "Train Loss at iteration 10511: 0.043094074375655364\n",
      "Train Loss at iteration 10512: 0.04309401568058353\n",
      "Train Loss at iteration 10513: 0.04309395699240529\n",
      "Train Loss at iteration 10514: 0.04309389831111952\n",
      "Train Loss at iteration 10515: 0.04309383963672503\n",
      "Train Loss at iteration 10516: 0.04309378096922067\n",
      "Train Loss at iteration 10517: 0.04309372230860529\n",
      "Train Loss at iteration 10518: 0.04309366365487769\n",
      "Train Loss at iteration 10519: 0.043093605008036766\n",
      "Train Loss at iteration 10520: 0.04309354636808132\n",
      "Train Loss at iteration 10521: 0.0430934877350102\n",
      "Train Loss at iteration 10522: 0.043093429108822245\n",
      "Train Loss at iteration 10523: 0.04309337048951629\n",
      "Train Loss at iteration 10524: 0.043093311877091194\n",
      "Train Loss at iteration 10525: 0.04309325327154578\n",
      "Train Loss at iteration 10526: 0.04309319467287888\n",
      "Train Loss at iteration 10527: 0.04309313608108937\n",
      "Train Loss at iteration 10528: 0.043093077496176066\n",
      "Train Loss at iteration 10529: 0.04309301891813781\n",
      "Train Loss at iteration 10530: 0.04309296034697345\n",
      "Train Loss at iteration 10531: 0.04309290178268183\n",
      "Train Loss at iteration 10532: 0.043092843225261794\n",
      "Train Loss at iteration 10533: 0.043092784674712156\n",
      "Train Loss at iteration 10534: 0.0430927261310318\n",
      "Train Loss at iteration 10535: 0.043092667594219536\n",
      "Train Loss at iteration 10536: 0.04309260906427424\n",
      "Train Loss at iteration 10537: 0.043092550541194714\n",
      "Train Loss at iteration 10538: 0.043092492024979834\n",
      "Train Loss at iteration 10539: 0.043092433515628435\n",
      "Train Loss at iteration 10540: 0.04309237501313936\n",
      "Train Loss at iteration 10541: 0.04309231651751145\n",
      "Train Loss at iteration 10542: 0.04309225802874353\n",
      "Train Loss at iteration 10543: 0.043092199546834485\n",
      "Train Loss at iteration 10544: 0.04309214107178314\n",
      "Train Loss at iteration 10545: 0.04309208260358832\n",
      "Train Loss at iteration 10546: 0.04309202414224891\n",
      "Train Loss at iteration 10547: 0.043091965687763724\n",
      "Train Loss at iteration 10548: 0.043091907240131615\n",
      "Train Loss at iteration 10549: 0.043091848799351434\n",
      "Train Loss at iteration 10550: 0.04309179036542202\n",
      "Train Loss at iteration 10551: 0.04309173193834224\n",
      "Train Loss at iteration 10552: 0.043091673518110905\n",
      "Train Loss at iteration 10553: 0.04309161510472688\n",
      "Train Loss at iteration 10554: 0.04309155669818902\n",
      "Train Loss at iteration 10555: 0.04309149829849616\n",
      "Train Loss at iteration 10556: 0.04309143990564714\n",
      "Train Loss at iteration 10557: 0.04309138151964083\n",
      "Train Loss at iteration 10558: 0.04309132314047607\n",
      "Train Loss at iteration 10559: 0.04309126476815169\n",
      "Train Loss at iteration 10560: 0.04309120640266655\n",
      "Train Loss at iteration 10561: 0.0430911480440195\n",
      "Train Loss at iteration 10562: 0.04309108969220937\n",
      "Train Loss at iteration 10563: 0.04309103134723505\n",
      "Train Loss at iteration 10564: 0.04309097300909535\n",
      "Train Loss at iteration 10565: 0.043090914677789145\n",
      "Train Loss at iteration 10566: 0.043090856353315256\n",
      "Train Loss at iteration 10567: 0.043090798035672555\n",
      "Train Loss at iteration 10568: 0.043090739724859876\n",
      "Train Loss at iteration 10569: 0.043090681420876076\n",
      "Train Loss at iteration 10570: 0.04309062312372001\n",
      "Train Loss at iteration 10571: 0.04309056483339053\n",
      "Train Loss at iteration 10572: 0.04309050654988648\n",
      "Train Loss at iteration 10573: 0.04309044827320669\n",
      "Train Loss at iteration 10574: 0.04309039000335006\n",
      "Train Loss at iteration 10575: 0.043090331740315395\n",
      "Train Loss at iteration 10576: 0.04309027348410157\n",
      "Train Loss at iteration 10577: 0.043090215234707414\n",
      "Train Loss at iteration 10578: 0.04309015699213181\n",
      "Train Loss at iteration 10579: 0.04309009875637358\n",
      "Train Loss at iteration 10580: 0.0430900405274316\n",
      "Train Loss at iteration 10581: 0.04308998230530471\n",
      "Train Loss at iteration 10582: 0.04308992408999176\n",
      "Train Loss at iteration 10583: 0.0430898658814916\n",
      "Train Loss at iteration 10584: 0.04308980767980311\n",
      "Train Loss at iteration 10585: 0.043089749484925104\n",
      "Train Loss at iteration 10586: 0.04308969129685646\n",
      "Train Loss at iteration 10587: 0.04308963311559602\n",
      "Train Loss at iteration 10588: 0.043089574941142655\n",
      "Train Loss at iteration 10589: 0.04308951677349519\n",
      "Train Loss at iteration 10590: 0.043089458612652495\n",
      "Train Loss at iteration 10591: 0.04308940045861344\n",
      "Train Loss at iteration 10592: 0.043089342311376844\n",
      "Train Loss at iteration 10593: 0.04308928417094158\n",
      "Train Loss at iteration 10594: 0.04308922603730651\n",
      "Train Loss at iteration 10595: 0.04308916791047049\n",
      "Train Loss at iteration 10596: 0.043089109790432366\n",
      "Train Loss at iteration 10597: 0.043089051677190994\n",
      "Train Loss at iteration 10598: 0.04308899357074523\n",
      "Train Loss at iteration 10599: 0.04308893547109392\n",
      "Train Loss at iteration 10600: 0.04308887737823594\n",
      "Train Loss at iteration 10601: 0.043088819292170136\n",
      "Train Loss at iteration 10602: 0.04308876121289536\n",
      "Train Loss at iteration 10603: 0.04308870314041048\n",
      "Train Loss at iteration 10604: 0.04308864507471434\n",
      "Train Loss at iteration 10605: 0.043088587015805806\n",
      "Train Loss at iteration 10606: 0.04308852896368373\n",
      "Train Loss at iteration 10607: 0.04308847091834697\n",
      "Train Loss at iteration 10608: 0.0430884128797944\n",
      "Train Loss at iteration 10609: 0.04308835484802485\n",
      "Train Loss at iteration 10610: 0.04308829682303719\n",
      "Train Loss at iteration 10611: 0.04308823880483029\n",
      "Train Loss at iteration 10612: 0.043088180793402986\n",
      "Train Loss at iteration 10613: 0.04308812278875415\n",
      "Train Loss at iteration 10614: 0.04308806479088265\n",
      "Train Loss at iteration 10615: 0.04308800679978732\n",
      "Train Loss at iteration 10616: 0.04308794881546704\n",
      "Train Loss at iteration 10617: 0.04308789083792066\n",
      "Train Loss at iteration 10618: 0.04308783286714705\n",
      "Train Loss at iteration 10619: 0.043087774903145054\n",
      "Train Loss at iteration 10620: 0.04308771694591354\n",
      "Train Loss at iteration 10621: 0.043087658995451375\n",
      "Train Loss at iteration 10622: 0.04308760105175741\n",
      "Train Loss at iteration 10623: 0.043087543114830505\n",
      "Train Loss at iteration 10624: 0.04308748518466954\n",
      "Train Loss at iteration 10625: 0.04308742726127334\n",
      "Train Loss at iteration 10626: 0.0430873693446408\n",
      "Train Loss at iteration 10627: 0.043087311434770764\n",
      "Train Loss at iteration 10628: 0.043087253531662095\n",
      "Train Loss at iteration 10629: 0.04308719563531366\n",
      "Train Loss at iteration 10630: 0.043087137745724306\n",
      "Train Loss at iteration 10631: 0.043087079862892924\n",
      "Train Loss at iteration 10632: 0.043087021986818366\n",
      "Train Loss at iteration 10633: 0.043086964117499474\n",
      "Train Loss at iteration 10634: 0.04308690625493513\n",
      "Train Loss at iteration 10635: 0.043086848399124196\n",
      "Train Loss at iteration 10636: 0.04308679055006552\n",
      "Train Loss at iteration 10637: 0.04308673270775799\n",
      "Train Loss at iteration 10638: 0.043086674872200456\n",
      "Train Loss at iteration 10639: 0.043086617043391784\n",
      "Train Loss at iteration 10640: 0.04308655922133083\n",
      "Train Loss at iteration 10641: 0.04308650140601648\n",
      "Train Loss at iteration 10642: 0.043086443597447566\n",
      "Train Loss at iteration 10643: 0.04308638579562298\n",
      "Train Loss at iteration 10644: 0.04308632800054157\n",
      "Train Loss at iteration 10645: 0.04308627021220222\n",
      "Train Loss at iteration 10646: 0.043086212430603774\n",
      "Train Loss at iteration 10647: 0.043086154655745106\n",
      "Train Loss at iteration 10648: 0.04308609688762509\n",
      "Train Loss at iteration 10649: 0.04308603912624258\n",
      "Train Loss at iteration 10650: 0.043085981371596443\n",
      "Train Loss at iteration 10651: 0.04308592362368556\n",
      "Train Loss at iteration 10652: 0.043085865882508775\n",
      "Train Loss at iteration 10653: 0.04308580814806497\n",
      "Train Loss at iteration 10654: 0.043085750420353015\n",
      "Train Loss at iteration 10655: 0.04308569269937177\n",
      "Train Loss at iteration 10656: 0.04308563498512008\n",
      "Train Loss at iteration 10657: 0.043085577277596855\n",
      "Train Loss at iteration 10658: 0.04308551957680094\n",
      "Train Loss at iteration 10659: 0.04308546188273121\n",
      "Train Loss at iteration 10660: 0.04308540419538651\n",
      "Train Loss at iteration 10661: 0.04308534651476574\n",
      "Train Loss at iteration 10662: 0.04308528884086775\n",
      "Train Loss at iteration 10663: 0.043085231173691424\n",
      "Train Loss at iteration 10664: 0.04308517351323562\n",
      "Train Loss at iteration 10665: 0.043085115859499194\n",
      "Train Loss at iteration 10666: 0.04308505821248103\n",
      "Train Loss at iteration 10667: 0.04308500057218001\n",
      "Train Loss at iteration 10668: 0.04308494293859498\n",
      "Train Loss at iteration 10669: 0.043084885311724816\n",
      "Train Loss at iteration 10670: 0.04308482769156839\n",
      "Train Loss at iteration 10671: 0.0430847700781246\n",
      "Train Loss at iteration 10672: 0.04308471247139226\n",
      "Train Loss at iteration 10673: 0.043084654871370295\n",
      "Train Loss at iteration 10674: 0.043084597278057524\n",
      "Train Loss at iteration 10675: 0.04308453969145287\n",
      "Train Loss at iteration 10676: 0.043084482111555175\n",
      "Train Loss at iteration 10677: 0.043084424538363315\n",
      "Train Loss at iteration 10678: 0.043084366971876155\n",
      "Train Loss at iteration 10679: 0.043084309412092577\n",
      "Train Loss at iteration 10680: 0.04308425185901147\n",
      "Train Loss at iteration 10681: 0.04308419431263166\n",
      "Train Loss at iteration 10682: 0.04308413677295206\n",
      "Train Loss at iteration 10683: 0.04308407923997153\n",
      "Train Loss at iteration 10684: 0.043084021713688934\n",
      "Train Loss at iteration 10685: 0.04308396419410316\n",
      "Train Loss at iteration 10686: 0.04308390668121308\n",
      "Train Loss at iteration 10687: 0.04308384917501755\n",
      "Train Loss at iteration 10688: 0.04308379167551545\n",
      "Train Loss at iteration 10689: 0.04308373418270567\n",
      "Train Loss at iteration 10690: 0.04308367669658707\n",
      "Train Loss at iteration 10691: 0.043083619217158524\n",
      "Train Loss at iteration 10692: 0.043083561744418915\n",
      "Train Loss at iteration 10693: 0.04308350427836711\n",
      "Train Loss at iteration 10694: 0.04308344681900199\n",
      "Train Loss at iteration 10695: 0.04308338936632242\n",
      "Train Loss at iteration 10696: 0.043083331920327296\n",
      "Train Loss at iteration 10697: 0.04308327448101546\n",
      "Train Loss at iteration 10698: 0.04308321704838581\n",
      "Train Loss at iteration 10699: 0.04308315962243723\n",
      "Train Loss at iteration 10700: 0.04308310220316858\n",
      "Train Loss at iteration 10701: 0.043083044790578735\n",
      "Train Loss at iteration 10702: 0.04308298738466657\n",
      "Train Loss at iteration 10703: 0.043082929985430986\n",
      "Train Loss at iteration 10704: 0.043082872592870844\n",
      "Train Loss at iteration 10705: 0.043082815206984995\n",
      "Train Loss at iteration 10706: 0.04308275782777238\n",
      "Train Loss at iteration 10707: 0.04308270045523181\n",
      "Train Loss at iteration 10708: 0.043082643089362195\n",
      "Train Loss at iteration 10709: 0.043082585730162395\n",
      "Train Loss at iteration 10710: 0.043082528377631324\n",
      "Train Loss at iteration 10711: 0.04308247103176782\n",
      "Train Loss at iteration 10712: 0.043082413692570785\n",
      "Train Loss at iteration 10713: 0.04308235636003909\n",
      "Train Loss at iteration 10714: 0.04308229903417162\n",
      "Train Loss at iteration 10715: 0.04308224171496725\n",
      "Train Loss at iteration 10716: 0.04308218440242485\n",
      "Train Loss at iteration 10717: 0.043082127096543306\n",
      "Train Loss at iteration 10718: 0.04308206979732151\n",
      "Train Loss at iteration 10719: 0.04308201250475833\n",
      "Train Loss at iteration 10720: 0.04308195521885264\n",
      "Train Loss at iteration 10721: 0.04308189793960333\n",
      "Train Loss at iteration 10722: 0.04308184066700927\n",
      "Train Loss at iteration 10723: 0.04308178340106935\n",
      "Train Loss at iteration 10724: 0.043081726141782455\n",
      "Train Loss at iteration 10725: 0.04308166888914746\n",
      "Train Loss at iteration 10726: 0.043081611643163244\n",
      "Train Loss at iteration 10727: 0.0430815544038287\n",
      "Train Loss at iteration 10728: 0.043081497171142694\n",
      "Train Loss at iteration 10729: 0.043081439945104105\n",
      "Train Loss at iteration 10730: 0.04308138272571183\n",
      "Train Loss at iteration 10731: 0.043081325512964744\n",
      "Train Loss at iteration 10732: 0.04308126830686173\n",
      "Train Loss at iteration 10733: 0.04308121110740168\n",
      "Train Loss at iteration 10734: 0.043081153914583455\n",
      "Train Loss at iteration 10735: 0.04308109672840596\n",
      "Train Loss at iteration 10736: 0.04308103954886807\n",
      "Train Loss at iteration 10737: 0.043080982375968656\n",
      "Train Loss at iteration 10738: 0.04308092520970661\n",
      "Train Loss at iteration 10739: 0.04308086805008083\n",
      "Train Loss at iteration 10740: 0.0430808108970902\n",
      "Train Loss at iteration 10741: 0.04308075375073357\n",
      "Train Loss at iteration 10742: 0.04308069661100986\n",
      "Train Loss at iteration 10743: 0.04308063947791793\n",
      "Train Loss at iteration 10744: 0.04308058235145669\n",
      "Train Loss at iteration 10745: 0.043080525231625005\n",
      "Train Loss at iteration 10746: 0.043080468118421755\n",
      "Train Loss at iteration 10747: 0.04308041101184585\n",
      "Train Loss at iteration 10748: 0.04308035391189617\n",
      "Train Loss at iteration 10749: 0.043080296818571576\n",
      "Train Loss at iteration 10750: 0.04308023973187098\n",
      "Train Loss at iteration 10751: 0.04308018265179325\n",
      "Train Loss at iteration 10752: 0.04308012557833729\n",
      "Train Loss at iteration 10753: 0.04308006851150198\n",
      "Train Loss at iteration 10754: 0.04308001145128619\n",
      "Train Loss at iteration 10755: 0.04307995439768884\n",
      "Train Loss at iteration 10756: 0.04307989735070879\n",
      "Train Loss at iteration 10757: 0.04307984031034493\n",
      "Train Loss at iteration 10758: 0.04307978327659616\n",
      "Train Loss at iteration 10759: 0.04307972624946136\n",
      "Train Loss at iteration 10760: 0.04307966922893942\n",
      "Train Loss at iteration 10761: 0.04307961221502922\n",
      "Train Loss at iteration 10762: 0.04307955520772965\n",
      "Train Loss at iteration 10763: 0.04307949820703962\n",
      "Train Loss at iteration 10764: 0.04307944121295799\n",
      "Train Loss at iteration 10765: 0.04307938422548367\n",
      "Train Loss at iteration 10766: 0.043079327244615544\n",
      "Train Loss at iteration 10767: 0.043079270270352486\n",
      "Train Loss at iteration 10768: 0.0430792133026934\n",
      "Train Loss at iteration 10769: 0.04307915634163717\n",
      "Train Loss at iteration 10770: 0.0430790993871827\n",
      "Train Loss at iteration 10771: 0.043079042439328856\n",
      "Train Loss at iteration 10772: 0.04307898549807455\n",
      "Train Loss at iteration 10773: 0.04307892856341866\n",
      "Train Loss at iteration 10774: 0.04307887163536006\n",
      "Train Loss at iteration 10775: 0.04307881471389769\n",
      "Train Loss at iteration 10776: 0.04307875779903039\n",
      "Train Loss at iteration 10777: 0.043078700890757084\n",
      "Train Loss at iteration 10778: 0.04307864398907666\n",
      "Train Loss at iteration 10779: 0.04307858709398798\n",
      "Train Loss at iteration 10780: 0.04307853020548996\n",
      "Train Loss at iteration 10781: 0.04307847332358152\n",
      "Train Loss at iteration 10782: 0.043078416448261485\n",
      "Train Loss at iteration 10783: 0.04307835957952881\n",
      "Train Loss at iteration 10784: 0.04307830271738234\n",
      "Train Loss at iteration 10785: 0.043078245861821\n",
      "Train Loss at iteration 10786: 0.04307818901284368\n",
      "Train Loss at iteration 10787: 0.04307813217044926\n",
      "Train Loss at iteration 10788: 0.04307807533463664\n",
      "Train Loss at iteration 10789: 0.04307801850540469\n",
      "Train Loss at iteration 10790: 0.043077961682752346\n",
      "Train Loss at iteration 10791: 0.04307790486667848\n",
      "Train Loss at iteration 10792: 0.04307784805718199\n",
      "Train Loss at iteration 10793: 0.043077791254261776\n",
      "Train Loss at iteration 10794: 0.04307773445791671\n",
      "Train Loss at iteration 10795: 0.04307767766814571\n",
      "Train Loss at iteration 10796: 0.04307762088494766\n",
      "Train Loss at iteration 10797: 0.04307756410832146\n",
      "Train Loss at iteration 10798: 0.043077507338266\n",
      "Train Loss at iteration 10799: 0.043077450574780185\n",
      "Train Loss at iteration 10800: 0.0430773938178629\n",
      "Train Loss at iteration 10801: 0.043077337067513044\n",
      "Train Loss at iteration 10802: 0.043077280323729504\n",
      "Train Loss at iteration 10803: 0.0430772235865112\n",
      "Train Loss at iteration 10804: 0.04307716685585702\n",
      "Train Loss at iteration 10805: 0.04307711013176584\n",
      "Train Loss at iteration 10806: 0.04307705341423657\n",
      "Train Loss at iteration 10807: 0.04307699670326813\n",
      "Train Loss at iteration 10808: 0.04307693999885939\n",
      "Train Loss at iteration 10809: 0.04307688330100925\n",
      "Train Loss at iteration 10810: 0.04307682660971661\n",
      "Train Loss at iteration 10811: 0.04307676992498038\n",
      "Train Loss at iteration 10812: 0.043076713246799445\n",
      "Train Loss at iteration 10813: 0.04307665657517269\n",
      "Train Loss at iteration 10814: 0.04307659991009903\n",
      "Train Loss at iteration 10815: 0.043076543251577386\n",
      "Train Loss at iteration 10816: 0.04307648659960661\n",
      "Train Loss at iteration 10817: 0.04307642995418564\n",
      "Train Loss at iteration 10818: 0.04307637331531335\n",
      "Train Loss at iteration 10819: 0.04307631668298866\n",
      "Train Loss at iteration 10820: 0.04307626005721045\n",
      "Train Loss at iteration 10821: 0.04307620343797764\n",
      "Train Loss at iteration 10822: 0.0430761468252891\n",
      "Train Loss at iteration 10823: 0.04307609021914377\n",
      "Train Loss at iteration 10824: 0.04307603361954051\n",
      "Train Loss at iteration 10825: 0.04307597702647824\n",
      "Train Loss at iteration 10826: 0.04307592043995588\n",
      "Train Loss at iteration 10827: 0.043075863859972294\n",
      "Train Loss at iteration 10828: 0.0430758072865264\n",
      "Train Loss at iteration 10829: 0.043075750719617105\n",
      "Train Loss at iteration 10830: 0.04307569415924332\n",
      "Train Loss at iteration 10831: 0.0430756376054039\n",
      "Train Loss at iteration 10832: 0.0430755810580978\n",
      "Train Loss at iteration 10833: 0.04307552451732389\n",
      "Train Loss at iteration 10834: 0.043075467983081095\n",
      "Train Loss at iteration 10835: 0.043075411455368307\n",
      "Train Loss at iteration 10836: 0.043075354934184414\n",
      "Train Loss at iteration 10837: 0.04307529841952834\n",
      "Train Loss at iteration 10838: 0.04307524191139897\n",
      "Train Loss at iteration 10839: 0.043075185409795234\n",
      "Train Loss at iteration 10840: 0.043075128914716015\n",
      "Train Loss at iteration 10841: 0.0430750724261602\n",
      "Train Loss at iteration 10842: 0.043075015944126736\n",
      "Train Loss at iteration 10843: 0.043074959468614504\n",
      "Train Loss at iteration 10844: 0.0430749029996224\n",
      "Train Loss at iteration 10845: 0.04307484653714933\n",
      "Train Loss at iteration 10846: 0.04307479008119422\n",
      "Train Loss at iteration 10847: 0.04307473363175596\n",
      "Train Loss at iteration 10848: 0.043074677188833455\n",
      "Train Loss at iteration 10849: 0.04307462075242559\n",
      "Train Loss at iteration 10850: 0.04307456432253132\n",
      "Train Loss at iteration 10851: 0.0430745078991495\n",
      "Train Loss at iteration 10852: 0.04307445148227908\n",
      "Train Loss at iteration 10853: 0.04307439507191891\n",
      "Train Loss at iteration 10854: 0.04307433866806795\n",
      "Train Loss at iteration 10855: 0.04307428227072509\n",
      "Train Loss at iteration 10856: 0.043074225879889225\n",
      "Train Loss at iteration 10857: 0.043074169495559256\n",
      "Train Loss at iteration 10858: 0.043074113117734125\n",
      "Train Loss at iteration 10859: 0.0430740567464127\n",
      "Train Loss at iteration 10860: 0.04307400038159391\n",
      "Train Loss at iteration 10861: 0.04307394402327666\n",
      "Train Loss at iteration 10862: 0.04307388767145986\n",
      "Train Loss at iteration 10863: 0.043073831326142405\n",
      "Train Loss at iteration 10864: 0.04307377498732321\n",
      "Train Loss at iteration 10865: 0.04307371865500118\n",
      "Train Loss at iteration 10866: 0.04307366232917524\n",
      "Train Loss at iteration 10867: 0.043073606009844286\n",
      "Train Loss at iteration 10868: 0.043073549697007225\n",
      "Train Loss at iteration 10869: 0.043073493390662965\n",
      "Train Loss at iteration 10870: 0.04307343709081042\n",
      "Train Loss at iteration 10871: 0.0430733807974485\n",
      "Train Loss at iteration 10872: 0.04307332451057611\n",
      "Train Loss at iteration 10873: 0.04307326823019216\n",
      "Train Loss at iteration 10874: 0.04307321195629557\n",
      "Train Loss at iteration 10875: 0.04307315568888523\n",
      "Train Loss at iteration 10876: 0.043073099427960074\n",
      "Train Loss at iteration 10877: 0.043073043173518996\n",
      "Train Loss at iteration 10878: 0.043072986925560906\n",
      "Train Loss at iteration 10879: 0.04307293068408473\n",
      "Train Loss at iteration 10880: 0.04307287444908937\n",
      "Train Loss at iteration 10881: 0.04307281822057373\n",
      "Train Loss at iteration 10882: 0.043072761998536734\n",
      "Train Loss at iteration 10883: 0.04307270578297728\n",
      "Train Loss at iteration 10884: 0.043072649573894305\n",
      "Train Loss at iteration 10885: 0.04307259337128669\n",
      "Train Loss at iteration 10886: 0.043072537175153386\n",
      "Train Loss at iteration 10887: 0.043072480985493246\n",
      "Train Loss at iteration 10888: 0.04307242480230525\n",
      "Train Loss at iteration 10889: 0.043072368625588255\n",
      "Train Loss at iteration 10890: 0.0430723124553412\n",
      "Train Loss at iteration 10891: 0.043072256291563\n",
      "Train Loss at iteration 10892: 0.04307220013425257\n",
      "Train Loss at iteration 10893: 0.04307214398340881\n",
      "Train Loss at iteration 10894: 0.043072087839030646\n",
      "Train Loss at iteration 10895: 0.04307203170111699\n",
      "Train Loss at iteration 10896: 0.04307197556966675\n",
      "Train Loss at iteration 10897: 0.043071919444678856\n",
      "Train Loss at iteration 10898: 0.0430718633261522\n",
      "Train Loss at iteration 10899: 0.04307180721408571\n",
      "Train Loss at iteration 10900: 0.04307175110847829\n",
      "Train Loss at iteration 10901: 0.04307169500932887\n",
      "Train Loss at iteration 10902: 0.04307163891663636\n",
      "Train Loss at iteration 10903: 0.04307158283039967\n",
      "Train Loss at iteration 10904: 0.043071526750617734\n",
      "Train Loss at iteration 10905: 0.04307147067728945\n",
      "Train Loss at iteration 10906: 0.04307141461041371\n",
      "Train Loss at iteration 10907: 0.04307135854998949\n",
      "Train Loss at iteration 10908: 0.043071302496015665\n",
      "Train Loss at iteration 10909: 0.043071246448491154\n",
      "Train Loss at iteration 10910: 0.043071190407414896\n",
      "Train Loss at iteration 10911: 0.04307113437278579\n",
      "Train Loss at iteration 10912: 0.04307107834460276\n",
      "Train Loss at iteration 10913: 0.0430710223228647\n",
      "Train Loss at iteration 10914: 0.04307096630757056\n",
      "Train Loss at iteration 10915: 0.043070910298719255\n",
      "Train Loss at iteration 10916: 0.043070854296309684\n",
      "Train Loss at iteration 10917: 0.043070798300340774\n",
      "Train Loss at iteration 10918: 0.04307074231081145\n",
      "Train Loss at iteration 10919: 0.04307068632772061\n",
      "Train Loss at iteration 10920: 0.043070630351067195\n",
      "Train Loss at iteration 10921: 0.04307057438085012\n",
      "Train Loss at iteration 10922: 0.043070518417068306\n",
      "Train Loss at iteration 10923: 0.043070462459720654\n",
      "Train Loss at iteration 10924: 0.0430704065088061\n",
      "Train Loss at iteration 10925: 0.04307035056432357\n",
      "Train Loss at iteration 10926: 0.04307029462627196\n",
      "Train Loss at iteration 10927: 0.0430702386946502\n",
      "Train Loss at iteration 10928: 0.043070182769457226\n",
      "Train Loss at iteration 10929: 0.04307012685069195\n",
      "Train Loss at iteration 10930: 0.04307007093835328\n",
      "Train Loss at iteration 10931: 0.043070015032440144\n",
      "Train Loss at iteration 10932: 0.04306995913295147\n",
      "Train Loss at iteration 10933: 0.043069903239886166\n",
      "Train Loss at iteration 10934: 0.043069847353243165\n",
      "Train Loss at iteration 10935: 0.04306979147302138\n",
      "Train Loss at iteration 10936: 0.043069735599219755\n",
      "Train Loss at iteration 10937: 0.04306967973183718\n",
      "Train Loss at iteration 10938: 0.043069623870872605\n",
      "Train Loss at iteration 10939: 0.04306956801632494\n",
      "Train Loss at iteration 10940: 0.04306951216819309\n",
      "Train Loss at iteration 10941: 0.04306945632647599\n",
      "Train Loss at iteration 10942: 0.04306940049117259\n",
      "Train Loss at iteration 10943: 0.04306934466228178\n",
      "Train Loss at iteration 10944: 0.04306928883980249\n",
      "Train Loss at iteration 10945: 0.04306923302373365\n",
      "Train Loss at iteration 10946: 0.04306917721407419\n",
      "Train Loss at iteration 10947: 0.043069121410823016\n",
      "Train Loss at iteration 10948: 0.043069065613979066\n",
      "Train Loss at iteration 10949: 0.04306900982354125\n",
      "Train Loss at iteration 10950: 0.043068954039508514\n",
      "Train Loss at iteration 10951: 0.04306889826187976\n",
      "Train Loss at iteration 10952: 0.04306884249065392\n",
      "Train Loss at iteration 10953: 0.043068786725829936\n",
      "Train Loss at iteration 10954: 0.04306873096740672\n",
      "Train Loss at iteration 10955: 0.04306867521538318\n",
      "Train Loss at iteration 10956: 0.043068619469758256\n",
      "Train Loss at iteration 10957: 0.04306856373053089\n",
      "Train Loss at iteration 10958: 0.04306850799769999\n",
      "Train Loss at iteration 10959: 0.04306845227126448\n",
      "Train Loss at iteration 10960: 0.04306839655122329\n",
      "Train Loss at iteration 10961: 0.04306834083757535\n",
      "Train Loss at iteration 10962: 0.0430682851303196\n",
      "Train Loss at iteration 10963: 0.043068229429454936\n",
      "Train Loss at iteration 10964: 0.04306817373498031\n",
      "Train Loss at iteration 10965: 0.043068118046894636\n",
      "Train Loss at iteration 10966: 0.04306806236519684\n",
      "Train Loss at iteration 10967: 0.04306800668988586\n",
      "Train Loss at iteration 10968: 0.04306795102096062\n",
      "Train Loss at iteration 10969: 0.04306789535842005\n",
      "Train Loss at iteration 10970: 0.04306783970226307\n",
      "Train Loss at iteration 10971: 0.04306778405248862\n",
      "Train Loss at iteration 10972: 0.043067728409095625\n",
      "Train Loss at iteration 10973: 0.043067672772083\n",
      "Train Loss at iteration 10974: 0.043067617141449695\n",
      "Train Loss at iteration 10975: 0.043067561517194625\n",
      "Train Loss at iteration 10976: 0.04306750589931672\n",
      "Train Loss at iteration 10977: 0.04306745028781491\n",
      "Train Loss at iteration 10978: 0.04306739468268813\n",
      "Train Loss at iteration 10979: 0.04306733908393531\n",
      "Train Loss at iteration 10980: 0.04306728349155538\n",
      "Train Loss at iteration 10981: 0.04306722790554727\n",
      "Train Loss at iteration 10982: 0.043067172325909904\n",
      "Train Loss at iteration 10983: 0.0430671167526422\n",
      "Train Loss at iteration 10984: 0.04306706118574314\n",
      "Train Loss at iteration 10985: 0.04306700562521159\n",
      "Train Loss at iteration 10986: 0.04306695007104652\n",
      "Train Loss at iteration 10987: 0.043066894523246864\n",
      "Train Loss at iteration 10988: 0.043066838981811524\n",
      "Train Loss at iteration 10989: 0.043066783446739476\n",
      "Train Loss at iteration 10990: 0.0430667279180296\n",
      "Train Loss at iteration 10991: 0.04306667239568086\n",
      "Train Loss at iteration 10992: 0.043066616879692196\n",
      "Train Loss at iteration 10993: 0.04306656137006252\n",
      "Train Loss at iteration 10994: 0.04306650586679076\n",
      "Train Loss at iteration 10995: 0.04306645036987588\n",
      "Train Loss at iteration 10996: 0.04306639487931679\n",
      "Train Loss at iteration 10997: 0.04306633939511241\n",
      "Train Loss at iteration 10998: 0.0430662839172617\n",
      "Train Loss at iteration 10999: 0.04306622844576359\n",
      "Train Loss at iteration 11000: 0.043066172980617004\n",
      "Train Loss at iteration 11001: 0.043066117521820886\n",
      "Train Loss at iteration 11002: 0.04306606206937416\n",
      "Train Loss at iteration 11003: 0.043066006623275754\n",
      "Train Loss at iteration 11004: 0.04306595118352463\n",
      "Train Loss at iteration 11005: 0.04306589575011968\n",
      "Train Loss at iteration 11006: 0.04306584032305989\n",
      "Train Loss at iteration 11007: 0.04306578490234416\n",
      "Train Loss at iteration 11008: 0.04306572948797144\n",
      "Train Loss at iteration 11009: 0.04306567407994066\n",
      "Train Loss at iteration 11010: 0.04306561867825075\n",
      "Train Loss at iteration 11011: 0.043065563282900655\n",
      "Train Loss at iteration 11012: 0.043065507893889306\n",
      "Train Loss at iteration 11013: 0.043065452511215636\n",
      "Train Loss at iteration 11014: 0.04306539713487859\n",
      "Train Loss at iteration 11015: 0.043065341764877106\n",
      "Train Loss at iteration 11016: 0.04306528640121011\n",
      "Train Loss at iteration 11017: 0.04306523104387655\n",
      "Train Loss at iteration 11018: 0.04306517569287536\n",
      "Train Loss at iteration 11019: 0.04306512034820546\n",
      "Train Loss at iteration 11020: 0.04306506500986582\n",
      "Train Loss at iteration 11021: 0.043065009677855356\n",
      "Train Loss at iteration 11022: 0.04306495435217301\n",
      "Train Loss at iteration 11023: 0.04306489903281772\n",
      "Train Loss at iteration 11024: 0.04306484371978843\n",
      "Train Loss at iteration 11025: 0.04306478841308406\n",
      "Train Loss at iteration 11026: 0.04306473311270357\n",
      "Train Loss at iteration 11027: 0.0430646778186459\n",
      "Train Loss at iteration 11028: 0.043064622530909966\n",
      "Train Loss at iteration 11029: 0.043064567249494715\n",
      "Train Loss at iteration 11030: 0.043064511974399096\n",
      "Train Loss at iteration 11031: 0.04306445670562206\n",
      "Train Loss at iteration 11032: 0.04306440144316252\n",
      "Train Loss at iteration 11033: 0.043064346187019424\n",
      "Train Loss at iteration 11034: 0.043064290937191725\n",
      "Train Loss at iteration 11035: 0.04306423569367836\n",
      "Train Loss at iteration 11036: 0.04306418045647825\n",
      "Train Loss at iteration 11037: 0.043064125225590354\n",
      "Train Loss at iteration 11038: 0.0430640700010136\n",
      "Train Loss at iteration 11039: 0.04306401478274695\n",
      "Train Loss at iteration 11040: 0.04306395957078932\n",
      "Train Loss at iteration 11041: 0.04306390436513967\n",
      "Train Loss at iteration 11042: 0.04306384916579693\n",
      "Train Loss at iteration 11043: 0.043063793972760056\n",
      "Train Loss at iteration 11044: 0.043063738786027976\n",
      "Train Loss at iteration 11045: 0.04306368360559964\n",
      "Train Loss at iteration 11046: 0.04306362843147399\n",
      "Train Loss at iteration 11047: 0.04306357326364996\n",
      "Train Loss at iteration 11048: 0.0430635181021265\n",
      "Train Loss at iteration 11049: 0.043063462946902546\n",
      "Train Loss at iteration 11050: 0.04306340779797706\n",
      "Train Loss at iteration 11051: 0.04306335265534897\n",
      "Train Loss at iteration 11052: 0.04306329751901721\n",
      "Train Loss at iteration 11053: 0.04306324238898074\n",
      "Train Loss at iteration 11054: 0.0430631872652385\n",
      "Train Loss at iteration 11055: 0.04306313214778943\n",
      "Train Loss at iteration 11056: 0.04306307703663248\n",
      "Train Loss at iteration 11057: 0.04306302193176659\n",
      "Train Loss at iteration 11058: 0.04306296683319071\n",
      "Train Loss at iteration 11059: 0.04306291174090377\n",
      "Train Loss at iteration 11060: 0.04306285665490474\n",
      "Train Loss at iteration 11061: 0.043062801575192544\n",
      "Train Loss at iteration 11062: 0.04306274650176614\n",
      "Train Loss at iteration 11063: 0.04306269143462446\n",
      "Train Loss at iteration 11064: 0.043062636373766464\n",
      "Train Loss at iteration 11065: 0.04306258131919108\n",
      "Train Loss at iteration 11066: 0.043062526270897274\n",
      "Train Loss at iteration 11067: 0.04306247122888398\n",
      "Train Loss at iteration 11068: 0.04306241619315015\n",
      "Train Loss at iteration 11069: 0.043062361163694725\n",
      "Train Loss at iteration 11070: 0.04306230614051666\n",
      "Train Loss at iteration 11071: 0.04306225112361489\n",
      "Train Loss at iteration 11072: 0.04306219611298838\n",
      "Train Loss at iteration 11073: 0.043062141108636064\n",
      "Train Loss at iteration 11074: 0.04306208611055688\n",
      "Train Loss at iteration 11075: 0.043062031118749795\n",
      "Train Loss at iteration 11076: 0.043061976133213754\n",
      "Train Loss at iteration 11077: 0.043061921153947705\n",
      "Train Loss at iteration 11078: 0.043061866180950585\n",
      "Train Loss at iteration 11079: 0.04306181121422134\n",
      "Train Loss at iteration 11080: 0.043061756253758944\n",
      "Train Loss at iteration 11081: 0.04306170129956232\n",
      "Train Loss at iteration 11082: 0.04306164635163042\n",
      "Train Loss at iteration 11083: 0.04306159140996221\n",
      "Train Loss at iteration 11084: 0.043061536474556635\n",
      "Train Loss at iteration 11085: 0.04306148154541263\n",
      "Train Loss at iteration 11086: 0.04306142662252915\n",
      "Train Loss at iteration 11087: 0.04306137170590516\n",
      "Train Loss at iteration 11088: 0.04306131679553959\n",
      "Train Loss at iteration 11089: 0.043061261891431404\n",
      "Train Loss at iteration 11090: 0.043061206993579536\n",
      "Train Loss at iteration 11091: 0.04306115210198295\n",
      "Train Loss at iteration 11092: 0.043061097216640615\n",
      "Train Loss at iteration 11093: 0.04306104233755144\n",
      "Train Loss at iteration 11094: 0.0430609874647144\n",
      "Train Loss at iteration 11095: 0.04306093259812846\n",
      "Train Loss at iteration 11096: 0.04306087773779254\n",
      "Train Loss at iteration 11097: 0.04306082288370561\n",
      "Train Loss at iteration 11098: 0.043060768035866615\n",
      "Train Loss at iteration 11099: 0.04306071319427452\n",
      "Train Loss at iteration 11100: 0.04306065835892826\n",
      "Train Loss at iteration 11101: 0.043060603529826796\n",
      "Train Loss at iteration 11102: 0.04306054870696909\n",
      "Train Loss at iteration 11103: 0.04306049389035409\n",
      "Train Loss at iteration 11104: 0.04306043907998072\n",
      "Train Loss at iteration 11105: 0.04306038427584798\n",
      "Train Loss at iteration 11106: 0.04306032947795479\n",
      "Train Loss at iteration 11107: 0.04306027468630012\n",
      "Train Loss at iteration 11108: 0.04306021990088292\n",
      "Train Loss at iteration 11109: 0.043060165121702146\n",
      "Train Loss at iteration 11110: 0.043060110348756744\n",
      "Train Loss at iteration 11111: 0.04306005558204567\n",
      "Train Loss at iteration 11112: 0.04306000082156788\n",
      "Train Loss at iteration 11113: 0.04305994606732233\n",
      "Train Loss at iteration 11114: 0.04305989131930797\n",
      "Train Loss at iteration 11115: 0.04305983657752376\n",
      "Train Loss at iteration 11116: 0.04305978184196868\n",
      "Train Loss at iteration 11117: 0.04305972711264163\n",
      "Train Loss at iteration 11118: 0.04305967238954161\n",
      "Train Loss at iteration 11119: 0.04305961767266756\n",
      "Train Loss at iteration 11120: 0.04305956296201843\n",
      "Train Loss at iteration 11121: 0.04305950825759319\n",
      "Train Loss at iteration 11122: 0.04305945355939079\n",
      "Train Loss at iteration 11123: 0.04305939886741019\n",
      "Train Loss at iteration 11124: 0.04305934418165033\n",
      "Train Loss at iteration 11125: 0.0430592895021102\n",
      "Train Loss at iteration 11126: 0.04305923482878871\n",
      "Train Loss at iteration 11127: 0.04305918016168487\n",
      "Train Loss at iteration 11128: 0.043059125500797595\n",
      "Train Loss at iteration 11129: 0.043059070846125876\n",
      "Train Loss at iteration 11130: 0.04305901619766863\n",
      "Train Loss at iteration 11131: 0.04305896155542486\n",
      "Train Loss at iteration 11132: 0.043058906919393486\n",
      "Train Loss at iteration 11133: 0.04305885228957349\n",
      "Train Loss at iteration 11134: 0.043058797665963836\n",
      "Train Loss at iteration 11135: 0.043058743048563455\n",
      "Train Loss at iteration 11136: 0.043058688437371324\n",
      "Train Loss at iteration 11137: 0.043058633832386395\n",
      "Train Loss at iteration 11138: 0.04305857923360764\n",
      "Train Loss at iteration 11139: 0.043058524641034004\n",
      "Train Loss at iteration 11140: 0.04305847005466445\n",
      "Train Loss at iteration 11141: 0.04305841547449794\n",
      "Train Loss at iteration 11142: 0.04305836090053344\n",
      "Train Loss at iteration 11143: 0.0430583063327699\n",
      "Train Loss at iteration 11144: 0.043058251771206285\n",
      "Train Loss at iteration 11145: 0.04305819721584156\n",
      "Train Loss at iteration 11146: 0.043058142666674674\n",
      "Train Loss at iteration 11147: 0.0430580881237046\n",
      "Train Loss at iteration 11148: 0.043058033586930285\n",
      "Train Loss at iteration 11149: 0.04305797905635069\n",
      "Train Loss at iteration 11150: 0.0430579245319648\n",
      "Train Loss at iteration 11151: 0.043057870013771565\n",
      "Train Loss at iteration 11152: 0.04305781550176994\n",
      "Train Loss at iteration 11153: 0.04305776099595888\n",
      "Train Loss at iteration 11154: 0.043057706496337354\n",
      "Train Loss at iteration 11155: 0.043057652002904345\n",
      "Train Loss at iteration 11156: 0.0430575975156588\n",
      "Train Loss at iteration 11157: 0.04305754303459966\n",
      "Train Loss at iteration 11158: 0.043057488559725916\n",
      "Train Loss at iteration 11159: 0.043057434091036524\n",
      "Train Loss at iteration 11160: 0.04305737962853044\n",
      "Train Loss at iteration 11161: 0.04305732517220663\n",
      "Train Loss at iteration 11162: 0.04305727072206408\n",
      "Train Loss at iteration 11163: 0.04305721627810171\n",
      "Train Loss at iteration 11164: 0.04305716184031853\n",
      "Train Loss at iteration 11165: 0.04305710740871346\n",
      "Train Loss at iteration 11166: 0.04305705298328549\n",
      "Train Loss at iteration 11167: 0.043056998564033604\n",
      "Train Loss at iteration 11168: 0.04305694415095672\n",
      "Train Loss at iteration 11169: 0.04305688974405383\n",
      "Train Loss at iteration 11170: 0.04305683534332392\n",
      "Train Loss at iteration 11171: 0.0430567809487659\n",
      "Train Loss at iteration 11172: 0.043056726560378775\n",
      "Train Loss at iteration 11173: 0.0430566721781615\n",
      "Train Loss at iteration 11174: 0.043056617802113055\n",
      "Train Loss at iteration 11175: 0.04305656343223237\n",
      "Train Loss at iteration 11176: 0.04305650906851845\n",
      "Train Loss at iteration 11177: 0.043056454710970235\n",
      "Train Loss at iteration 11178: 0.0430564003595867\n",
      "Train Loss at iteration 11179: 0.04305634601436683\n",
      "Train Loss at iteration 11180: 0.04305629167530957\n",
      "Train Loss at iteration 11181: 0.04305623734241388\n",
      "Train Loss at iteration 11182: 0.04305618301567874\n",
      "Train Loss at iteration 11183: 0.043056128695103134\n",
      "Train Loss at iteration 11184: 0.043056074380686\n",
      "Train Loss at iteration 11185: 0.043056020072426315\n",
      "Train Loss at iteration 11186: 0.043055965770323064\n",
      "Train Loss at iteration 11187: 0.04305591147437517\n",
      "Train Loss at iteration 11188: 0.04305585718458167\n",
      "Train Loss at iteration 11189: 0.04305580290094148\n",
      "Train Loss at iteration 11190: 0.04305574862345358\n",
      "Train Loss at iteration 11191: 0.04305569435211693\n",
      "Train Loss at iteration 11192: 0.043055640086930524\n",
      "Train Loss at iteration 11193: 0.04305558582789332\n",
      "Train Loss at iteration 11194: 0.04305553157500427\n",
      "Train Loss at iteration 11195: 0.04305547732826237\n",
      "Train Loss at iteration 11196: 0.04305542308766658\n",
      "Train Loss at iteration 11197: 0.043055368853215856\n",
      "Train Loss at iteration 11198: 0.04305531462490919\n",
      "Train Loss at iteration 11199: 0.04305526040274554\n",
      "Train Loss at iteration 11200: 0.043055206186723885\n",
      "Train Loss at iteration 11201: 0.04305515197684319\n",
      "Train Loss at iteration 11202: 0.043055097773102416\n",
      "Train Loss at iteration 11203: 0.04305504357550055\n",
      "Train Loss at iteration 11204: 0.04305498938403654\n",
      "Train Loss at iteration 11205: 0.04305493519870939\n",
      "Train Loss at iteration 11206: 0.04305488101951805\n",
      "Train Loss at iteration 11207: 0.0430548268464615\n",
      "Train Loss at iteration 11208: 0.043054772679538694\n",
      "Train Loss at iteration 11209: 0.043054718518748626\n",
      "Train Loss at iteration 11210: 0.04305466436409027\n",
      "Train Loss at iteration 11211: 0.043054610215562575\n",
      "Train Loss at iteration 11212: 0.04305455607316454\n",
      "Train Loss at iteration 11213: 0.043054501936895126\n",
      "Train Loss at iteration 11214: 0.04305444780675329\n",
      "Train Loss at iteration 11215: 0.04305439368273803\n",
      "Train Loss at iteration 11216: 0.043054339564848304\n",
      "Train Loss at iteration 11217: 0.043054285453083097\n",
      "Train Loss at iteration 11218: 0.04305423134744137\n",
      "Train Loss at iteration 11219: 0.04305417724792211\n",
      "Train Loss at iteration 11220: 0.04305412315452427\n",
      "Train Loss at iteration 11221: 0.043054069067246854\n",
      "Train Loss at iteration 11222: 0.04305401498608881\n",
      "Train Loss at iteration 11223: 0.043053960911049134\n",
      "Train Loss at iteration 11224: 0.043053906842126774\n",
      "Train Loss at iteration 11225: 0.043053852779320737\n",
      "Train Loss at iteration 11226: 0.04305379872262998\n",
      "Train Loss at iteration 11227: 0.04305374467205346\n",
      "Train Loss at iteration 11228: 0.04305369062759019\n",
      "Train Loss at iteration 11229: 0.043053636589239115\n",
      "Train Loss at iteration 11230: 0.04305358255699923\n",
      "Train Loss at iteration 11231: 0.0430535285308695\n",
      "Train Loss at iteration 11232: 0.04305347451084891\n",
      "Train Loss at iteration 11233: 0.04305342049693644\n",
      "Train Loss at iteration 11234: 0.043053366489131044\n",
      "Train Loss at iteration 11235: 0.04305331248743172\n",
      "Train Loss at iteration 11236: 0.04305325849183744\n",
      "Train Loss at iteration 11237: 0.043053204502347175\n",
      "Train Loss at iteration 11238: 0.04305315051895991\n",
      "Train Loss at iteration 11239: 0.043053096541674614\n",
      "Train Loss at iteration 11240: 0.04305304257049027\n",
      "Train Loss at iteration 11241: 0.04305298860540584\n",
      "Train Loss at iteration 11242: 0.04305293464642034\n",
      "Train Loss at iteration 11243: 0.04305288069353271\n",
      "Train Loss at iteration 11244: 0.043052826746741955\n",
      "Train Loss at iteration 11245: 0.04305277280604703\n",
      "Train Loss at iteration 11246: 0.04305271887144693\n",
      "Train Loss at iteration 11247: 0.043052664942940626\n",
      "Train Loss at iteration 11248: 0.04305261102052709\n",
      "Train Loss at iteration 11249: 0.04305255710420532\n",
      "Train Loss at iteration 11250: 0.04305250319397429\n",
      "Train Loss at iteration 11251: 0.043052449289832956\n",
      "Train Loss at iteration 11252: 0.043052395391780345\n",
      "Train Loss at iteration 11253: 0.0430523414998154\n",
      "Train Loss at iteration 11254: 0.0430522876139371\n",
      "Train Loss at iteration 11255: 0.043052233734144435\n",
      "Train Loss at iteration 11256: 0.04305217986043639\n",
      "Train Loss at iteration 11257: 0.04305212599281194\n",
      "Train Loss at iteration 11258: 0.043052072131270075\n",
      "Train Loss at iteration 11259: 0.04305201827580975\n",
      "Train Loss at iteration 11260: 0.04305196442642997\n",
      "Train Loss at iteration 11261: 0.04305191058312971\n",
      "Train Loss at iteration 11262: 0.04305185674590797\n",
      "Train Loss at iteration 11263: 0.04305180291476368\n",
      "Train Loss at iteration 11264: 0.043051749089695876\n",
      "Train Loss at iteration 11265: 0.04305169527070352\n",
      "Train Loss at iteration 11266: 0.04305164145778558\n",
      "Train Loss at iteration 11267: 0.04305158765094107\n",
      "Train Loss at iteration 11268: 0.04305153385016894\n",
      "Train Loss at iteration 11269: 0.04305148005546818\n",
      "Train Loss at iteration 11270: 0.04305142626683779\n",
      "Train Loss at iteration 11271: 0.04305137248427674\n",
      "Train Loss at iteration 11272: 0.04305131870778401\n",
      "Train Loss at iteration 11273: 0.0430512649373586\n",
      "Train Loss at iteration 11274: 0.04305121117299947\n",
      "Train Loss at iteration 11275: 0.04305115741470563\n",
      "Train Loss at iteration 11276: 0.04305110366247603\n",
      "Train Loss at iteration 11277: 0.04305104991630969\n",
      "Train Loss at iteration 11278: 0.04305099617620557\n",
      "Train Loss at iteration 11279: 0.043050942442162676\n",
      "Train Loss at iteration 11280: 0.04305088871417996\n",
      "Train Loss at iteration 11281: 0.04305083499225644\n",
      "Train Loss at iteration 11282: 0.04305078127639108\n",
      "Train Loss at iteration 11283: 0.043050727566582875\n",
      "Train Loss at iteration 11284: 0.04305067386283081\n",
      "Train Loss at iteration 11285: 0.04305062016513387\n",
      "Train Loss at iteration 11286: 0.04305056647349103\n",
      "Train Loss at iteration 11287: 0.04305051278790128\n",
      "Train Loss at iteration 11288: 0.043050459108363616\n",
      "Train Loss at iteration 11289: 0.04305040543487704\n",
      "Train Loss at iteration 11290: 0.04305035176744049\n",
      "Train Loss at iteration 11291: 0.043050298106052994\n",
      "Train Loss at iteration 11292: 0.04305024445071351\n",
      "Train Loss at iteration 11293: 0.04305019080142105\n",
      "Train Loss at iteration 11294: 0.04305013715817459\n",
      "Train Loss at iteration 11295: 0.043050083520973115\n",
      "Train Loss at iteration 11296: 0.043050029889815614\n",
      "Train Loss at iteration 11297: 0.043049976264701084\n",
      "Train Loss at iteration 11298: 0.0430499226456285\n",
      "Train Loss at iteration 11299: 0.043049869032596846\n",
      "Train Loss at iteration 11300: 0.043049815425605126\n",
      "Train Loss at iteration 11301: 0.04304976182465233\n",
      "Train Loss at iteration 11302: 0.04304970822973742\n",
      "Train Loss at iteration 11303: 0.04304965464085942\n",
      "Train Loss at iteration 11304: 0.043049601058017285\n",
      "Train Loss at iteration 11305: 0.043049547481210024\n",
      "Train Loss at iteration 11306: 0.04304949391043663\n",
      "Train Loss at iteration 11307: 0.04304944034569608\n",
      "Train Loss at iteration 11308: 0.043049386786987366\n",
      "Train Loss at iteration 11309: 0.04304933323430948\n",
      "Train Loss at iteration 11310: 0.04304927968766141\n",
      "Train Loss at iteration 11311: 0.043049226147042145\n",
      "Train Loss at iteration 11312: 0.043049172612450684\n",
      "Train Loss at iteration 11313: 0.04304911908388601\n",
      "Train Loss at iteration 11314: 0.04304906556134712\n",
      "Train Loss at iteration 11315: 0.04304901204483299\n",
      "Train Loss at iteration 11316: 0.04304895853434264\n",
      "Train Loss at iteration 11317: 0.04304890502987502\n",
      "Train Loss at iteration 11318: 0.04304885153142915\n",
      "Train Loss at iteration 11319: 0.043048798039004015\n",
      "Train Loss at iteration 11320: 0.043048744552598606\n",
      "Train Loss at iteration 11321: 0.043048691072211924\n",
      "Train Loss at iteration 11322: 0.043048637597842936\n",
      "Train Loss at iteration 11323: 0.04304858412949066\n",
      "Train Loss at iteration 11324: 0.04304853066715408\n",
      "Train Loss at iteration 11325: 0.04304847721083218\n",
      "Train Loss at iteration 11326: 0.04304842376052395\n",
      "Train Loss at iteration 11327: 0.04304837031622842\n",
      "Train Loss at iteration 11328: 0.04304831687794454\n",
      "Train Loss at iteration 11329: 0.04304826344567132\n",
      "Train Loss at iteration 11330: 0.04304821001940776\n",
      "Train Loss at iteration 11331: 0.04304815659915284\n",
      "Train Loss at iteration 11332: 0.04304810318490555\n",
      "Train Loss at iteration 11333: 0.043048049776664904\n",
      "Train Loss at iteration 11334: 0.04304799637442989\n",
      "Train Loss at iteration 11335: 0.0430479429781995\n",
      "Train Loss at iteration 11336: 0.04304788958797271\n",
      "Train Loss at iteration 11337: 0.04304783620374854\n",
      "Train Loss at iteration 11338: 0.04304778282552597\n",
      "Train Loss at iteration 11339: 0.043047729453304005\n",
      "Train Loss at iteration 11340: 0.04304767608708164\n",
      "Train Loss at iteration 11341: 0.04304762272685786\n",
      "Train Loss at iteration 11342: 0.04304756937263167\n",
      "Train Loss at iteration 11343: 0.04304751602440207\n",
      "Train Loss at iteration 11344: 0.04304746268216804\n",
      "Train Loss at iteration 11345: 0.043047409345928586\n",
      "Train Loss at iteration 11346: 0.0430473560156827\n",
      "Train Loss at iteration 11347: 0.04304730269142939\n",
      "Train Loss at iteration 11348: 0.04304724937316763\n",
      "Train Loss at iteration 11349: 0.04304719606089642\n",
      "Train Loss at iteration 11350: 0.043047142754614784\n",
      "Train Loss at iteration 11351: 0.04304708945432171\n",
      "Train Loss at iteration 11352: 0.043047036160016185\n",
      "Train Loss at iteration 11353: 0.043046982871697184\n",
      "Train Loss at iteration 11354: 0.043046929589363764\n",
      "Train Loss at iteration 11355: 0.04304687631301486\n",
      "Train Loss at iteration 11356: 0.04304682304264951\n",
      "Train Loss at iteration 11357: 0.0430467697782667\n",
      "Train Loss at iteration 11358: 0.04304671651986542\n",
      "Train Loss at iteration 11359: 0.04304666326744468\n",
      "Train Loss at iteration 11360: 0.0430466100210035\n",
      "Train Loss at iteration 11361: 0.04304655678054081\n",
      "Train Loss at iteration 11362: 0.04304650354605568\n",
      "Train Loss at iteration 11363: 0.04304645031754708\n",
      "Train Loss at iteration 11364: 0.04304639709501401\n",
      "Train Loss at iteration 11365: 0.04304634387845547\n",
      "Train Loss at iteration 11366: 0.04304629066787046\n",
      "Train Loss at iteration 11367: 0.04304623746325799\n",
      "Train Loss at iteration 11368: 0.04304618426461704\n",
      "Train Loss at iteration 11369: 0.04304613107194662\n",
      "Train Loss at iteration 11370: 0.043046077885245726\n",
      "Train Loss at iteration 11371: 0.04304602470451338\n",
      "Train Loss at iteration 11372: 0.043045971529748546\n",
      "Train Loss at iteration 11373: 0.04304591836095026\n",
      "Train Loss at iteration 11374: 0.04304586519811752\n",
      "Train Loss at iteration 11375: 0.04304581204124929\n",
      "Train Loss at iteration 11376: 0.0430457588903446\n",
      "Train Loss at iteration 11377: 0.043045705745402466\n",
      "Train Loss at iteration 11378: 0.043045652606421854\n",
      "Train Loss at iteration 11379: 0.0430455994734018\n",
      "Train Loss at iteration 11380: 0.04304554634634128\n",
      "Train Loss at iteration 11381: 0.04304549322523931\n",
      "Train Loss at iteration 11382: 0.043045440110094875\n",
      "Train Loss at iteration 11383: 0.04304538700090701\n",
      "Train Loss at iteration 11384: 0.043045333897674704\n",
      "Train Loss at iteration 11385: 0.04304528080039694\n",
      "Train Loss at iteration 11386: 0.043045227709072736\n",
      "Train Loss at iteration 11387: 0.0430451746237011\n",
      "Train Loss at iteration 11388: 0.04304512154428104\n",
      "Train Loss at iteration 11389: 0.04304506847081154\n",
      "Train Loss at iteration 11390: 0.04304501540329162\n",
      "Train Loss at iteration 11391: 0.04304496234172027\n",
      "Train Loss at iteration 11392: 0.043044909286096514\n",
      "Train Loss at iteration 11393: 0.043044856236419335\n",
      "Train Loss at iteration 11394: 0.043044803192687754\n",
      "Train Loss at iteration 11395: 0.04304475015490077\n",
      "Train Loss at iteration 11396: 0.043044697123057384\n",
      "Train Loss at iteration 11397: 0.043044644097156616\n",
      "Train Loss at iteration 11398: 0.04304459107719743\n",
      "Train Loss at iteration 11399: 0.04304453806317889\n",
      "Train Loss at iteration 11400: 0.04304448505509997\n",
      "Train Loss at iteration 11401: 0.04304443205295966\n",
      "Train Loss at iteration 11402: 0.043044379056756996\n",
      "Train Loss at iteration 11403: 0.04304432606649095\n",
      "Train Loss at iteration 11404: 0.043044273082160585\n",
      "Train Loss at iteration 11405: 0.04304422010376485\n",
      "Train Loss at iteration 11406: 0.04304416713130278\n",
      "Train Loss at iteration 11407: 0.04304411416477339\n",
      "Train Loss at iteration 11408: 0.04304406120417565\n",
      "Train Loss at iteration 11409: 0.043044008249508596\n",
      "Train Loss at iteration 11410: 0.04304395530077125\n",
      "Train Loss at iteration 11411: 0.04304390235796257\n",
      "Train Loss at iteration 11412: 0.04304384942108161\n",
      "Train Loss at iteration 11413: 0.043043796490127344\n",
      "Train Loss at iteration 11414: 0.04304374356509882\n",
      "Train Loss at iteration 11415: 0.04304369064599499\n",
      "Train Loss at iteration 11416: 0.043043637732814916\n",
      "Train Loss at iteration 11417: 0.04304358482555759\n",
      "Train Loss at iteration 11418: 0.04304353192422201\n",
      "Train Loss at iteration 11419: 0.04304347902880719\n",
      "Train Loss at iteration 11420: 0.04304342613931213\n",
      "Train Loss at iteration 11421: 0.04304337325573585\n",
      "Train Loss at iteration 11422: 0.04304332037807737\n",
      "Train Loss at iteration 11423: 0.04304326750633568\n",
      "Train Loss at iteration 11424: 0.043043214640509794\n",
      "Train Loss at iteration 11425: 0.04304316178059873\n",
      "Train Loss at iteration 11426: 0.043043108926601494\n",
      "Train Loss at iteration 11427: 0.04304305607851709\n",
      "Train Loss at iteration 11428: 0.04304300323634453\n",
      "Train Loss at iteration 11429: 0.04304295040008284\n",
      "Train Loss at iteration 11430: 0.04304289756973101\n",
      "Train Loss at iteration 11431: 0.04304284474528806\n",
      "Train Loss at iteration 11432: 0.043042791926753005\n",
      "Train Loss at iteration 11433: 0.04304273911412484\n",
      "Train Loss at iteration 11434: 0.04304268630740259\n",
      "Train Loss at iteration 11435: 0.04304263350658527\n",
      "Train Loss at iteration 11436: 0.0430425807116719\n",
      "Train Loss at iteration 11437: 0.043042527922661465\n",
      "Train Loss at iteration 11438: 0.04304247513955299\n",
      "Train Loss at iteration 11439: 0.04304242236234548\n",
      "Train Loss at iteration 11440: 0.04304236959103797\n",
      "Train Loss at iteration 11441: 0.04304231682562945\n",
      "Train Loss at iteration 11442: 0.04304226406611894\n",
      "Train Loss at iteration 11443: 0.04304221131250545\n",
      "Train Loss at iteration 11444: 0.043042158564788005\n",
      "Train Loss at iteration 11445: 0.04304210582296561\n",
      "Train Loss at iteration 11446: 0.04304205308703728\n",
      "Train Loss at iteration 11447: 0.043042000357002015\n",
      "Train Loss at iteration 11448: 0.04304194763285884\n",
      "Train Loss at iteration 11449: 0.04304189491460678\n",
      "Train Loss at iteration 11450: 0.04304184220224483\n",
      "Train Loss at iteration 11451: 0.04304178949577202\n",
      "Train Loss at iteration 11452: 0.043041736795187344\n",
      "Train Loss at iteration 11453: 0.04304168410048984\n",
      "Train Loss at iteration 11454: 0.043041631411678515\n",
      "Train Loss at iteration 11455: 0.04304157872875238\n",
      "Train Loss at iteration 11456: 0.04304152605171045\n",
      "Train Loss at iteration 11457: 0.04304147338055175\n",
      "Train Loss at iteration 11458: 0.04304142071527527\n",
      "Train Loss at iteration 11459: 0.04304136805588006\n",
      "Train Loss at iteration 11460: 0.0430413154023651\n",
      "Train Loss at iteration 11461: 0.04304126275472944\n",
      "Train Loss at iteration 11462: 0.043041210112972086\n",
      "Train Loss at iteration 11463: 0.04304115747709204\n",
      "Train Loss at iteration 11464: 0.04304110484708833\n",
      "Train Loss at iteration 11465: 0.04304105222295997\n",
      "Train Loss at iteration 11466: 0.04304099960470598\n",
      "Train Loss at iteration 11467: 0.043040946992325385\n",
      "Train Loss at iteration 11468: 0.04304089438581718\n",
      "Train Loss at iteration 11469: 0.04304084178518038\n",
      "Train Loss at iteration 11470: 0.04304078919041403\n",
      "Train Loss at iteration 11471: 0.043040736601517145\n",
      "Train Loss at iteration 11472: 0.04304068401848871\n",
      "Train Loss at iteration 11473: 0.04304063144132779\n",
      "Train Loss at iteration 11474: 0.04304057887003336\n",
      "Train Loss at iteration 11475: 0.04304052630460446\n",
      "Train Loss at iteration 11476: 0.04304047374504012\n",
      "Train Loss at iteration 11477: 0.04304042119133933\n",
      "Train Loss at iteration 11478: 0.04304036864350112\n",
      "Train Loss at iteration 11479: 0.04304031610152452\n",
      "Train Loss at iteration 11480: 0.04304026356540854\n",
      "Train Loss at iteration 11481: 0.043040211035152195\n",
      "Train Loss at iteration 11482: 0.04304015851075451\n",
      "Train Loss at iteration 11483: 0.04304010599221452\n",
      "Train Loss at iteration 11484: 0.043040053479531215\n",
      "Train Loss at iteration 11485: 0.04304000097270364\n",
      "Train Loss at iteration 11486: 0.043039948471730795\n",
      "Train Loss at iteration 11487: 0.04303989597661172\n",
      "Train Loss at iteration 11488: 0.04303984348734542\n",
      "Train Loss at iteration 11489: 0.04303979100393093\n",
      "Train Loss at iteration 11490: 0.043039738526367256\n",
      "Train Loss at iteration 11491: 0.04303968605465343\n",
      "Train Loss at iteration 11492: 0.043039633588788456\n",
      "Train Loss at iteration 11493: 0.04303958112877138\n",
      "Train Loss at iteration 11494: 0.0430395286746012\n",
      "Train Loss at iteration 11495: 0.04303947622627696\n",
      "Train Loss at iteration 11496: 0.04303942378379768\n",
      "Train Loss at iteration 11497: 0.043039371347162375\n",
      "Train Loss at iteration 11498: 0.04303931891637005\n",
      "Train Loss at iteration 11499: 0.04303926649141975\n",
      "Train Loss at iteration 11500: 0.0430392140723105\n",
      "Train Loss at iteration 11501: 0.04303916165904131\n",
      "Train Loss at iteration 11502: 0.043039109251611196\n",
      "Train Loss at iteration 11503: 0.0430390568500192\n",
      "Train Loss at iteration 11504: 0.04303900445426433\n",
      "Train Loss at iteration 11505: 0.04303895206434563\n",
      "Train Loss at iteration 11506: 0.0430388996802621\n",
      "Train Loss at iteration 11507: 0.04303884730201279\n",
      "Train Loss at iteration 11508: 0.04303879492959669\n",
      "Train Loss at iteration 11509: 0.043038742563012856\n",
      "Train Loss at iteration 11510: 0.04303869020226029\n",
      "Train Loss at iteration 11511: 0.04303863784733803\n",
      "Train Loss at iteration 11512: 0.04303858549824509\n",
      "Train Loss at iteration 11513: 0.0430385331549805\n",
      "Train Loss at iteration 11514: 0.04303848081754329\n",
      "Train Loss at iteration 11515: 0.04303842848593249\n",
      "Train Loss at iteration 11516: 0.0430383761601471\n",
      "Train Loss at iteration 11517: 0.043038323840186174\n",
      "Train Loss at iteration 11518: 0.043038271526048714\n",
      "Train Loss at iteration 11519: 0.043038219217733764\n",
      "Train Loss at iteration 11520: 0.04303816691524035\n",
      "Train Loss at iteration 11521: 0.04303811461856748\n",
      "Train Loss at iteration 11522: 0.043038062327714184\n",
      "Train Loss at iteration 11523: 0.0430380100426795\n",
      "Train Loss at iteration 11524: 0.043037957763462475\n",
      "Train Loss at iteration 11525: 0.04303790549006208\n",
      "Train Loss at iteration 11526: 0.0430378532224774\n",
      "Train Loss at iteration 11527: 0.04303780096070742\n",
      "Train Loss at iteration 11528: 0.04303774870475118\n",
      "Train Loss at iteration 11529: 0.04303769645460773\n",
      "Train Loss at iteration 11530: 0.043037644210276046\n",
      "Train Loss at iteration 11531: 0.0430375919717552\n",
      "Train Loss at iteration 11532: 0.04303753973904422\n",
      "Train Loss at iteration 11533: 0.04303748751214212\n",
      "Train Loss at iteration 11534: 0.04303743529104793\n",
      "Train Loss at iteration 11535: 0.04303738307576067\n",
      "Train Loss at iteration 11536: 0.04303733086627938\n",
      "Train Loss at iteration 11537: 0.04303727866260309\n",
      "Train Loss at iteration 11538: 0.04303722646473082\n",
      "Train Loss at iteration 11539: 0.04303717427266161\n",
      "Train Loss at iteration 11540: 0.043037122086394486\n",
      "Train Loss at iteration 11541: 0.04303706990592848\n",
      "Train Loss at iteration 11542: 0.0430370177312626\n",
      "Train Loss at iteration 11543: 0.04303696556239592\n",
      "Train Loss at iteration 11544: 0.043036913399327414\n",
      "Train Loss at iteration 11545: 0.04303686124205616\n",
      "Train Loss at iteration 11546: 0.04303680909058116\n",
      "Train Loss at iteration 11547: 0.04303675694490146\n",
      "Train Loss at iteration 11548: 0.043036704805016084\n",
      "Train Loss at iteration 11549: 0.043036652670924054\n",
      "Train Loss at iteration 11550: 0.04303660054262442\n",
      "Train Loss at iteration 11551: 0.04303654842011621\n",
      "Train Loss at iteration 11552: 0.04303649630339844\n",
      "Train Loss at iteration 11553: 0.04303644419247015\n",
      "Train Loss at iteration 11554: 0.043036392087330365\n",
      "Train Loss at iteration 11555: 0.04303633998797814\n",
      "Train Loss at iteration 11556: 0.04303628789441249\n",
      "Train Loss at iteration 11557: 0.04303623580663244\n",
      "Train Loss at iteration 11558: 0.043036183724637044\n",
      "Train Loss at iteration 11559: 0.043036131648425305\n",
      "Train Loss at iteration 11560: 0.04303607957799628\n",
      "Train Loss at iteration 11561: 0.04303602751334898\n",
      "Train Loss at iteration 11562: 0.04303597545448247\n",
      "Train Loss at iteration 11563: 0.043035923401395754\n",
      "Train Loss at iteration 11564: 0.04303587135408789\n",
      "Train Loss at iteration 11565: 0.043035819312557885\n",
      "Train Loss at iteration 11566: 0.0430357672768048\n",
      "Train Loss at iteration 11567: 0.04303571524682764\n",
      "Train Loss at iteration 11568: 0.04303566322262546\n",
      "Train Loss at iteration 11569: 0.04303561120419729\n",
      "Train Loss at iteration 11570: 0.043035559191542165\n",
      "Train Loss at iteration 11571: 0.043035507184659115\n",
      "Train Loss at iteration 11572: 0.04303545518354718\n",
      "Train Loss at iteration 11573: 0.043035403188205375\n",
      "Train Loss at iteration 11574: 0.043035351198632756\n",
      "Train Loss at iteration 11575: 0.04303529921482836\n",
      "Train Loss at iteration 11576: 0.04303524723679122\n",
      "Train Loss at iteration 11577: 0.043035195264520355\n",
      "Train Loss at iteration 11578: 0.043035143298014805\n",
      "Train Loss at iteration 11579: 0.04303509133727364\n",
      "Train Loss at iteration 11580: 0.04303503938229586\n",
      "Train Loss at iteration 11581: 0.043034987433080506\n",
      "Train Loss at iteration 11582: 0.04303493548962662\n",
      "Train Loss at iteration 11583: 0.04303488355193324\n",
      "Train Loss at iteration 11584: 0.04303483161999941\n",
      "Train Loss at iteration 11585: 0.04303477969382414\n",
      "Train Loss at iteration 11586: 0.04303472777340649\n",
      "Train Loss at iteration 11587: 0.04303467585874551\n",
      "Train Loss at iteration 11588: 0.0430346239498402\n",
      "Train Loss at iteration 11589: 0.04303457204668962\n",
      "Train Loss at iteration 11590: 0.04303452014929281\n",
      "Train Loss at iteration 11591: 0.04303446825764879\n",
      "Train Loss at iteration 11592: 0.043034416371756624\n",
      "Train Loss at iteration 11593: 0.04303436449161532\n",
      "Train Loss at iteration 11594: 0.043034312617223944\n",
      "Train Loss at iteration 11595: 0.04303426074858153\n",
      "Train Loss at iteration 11596: 0.0430342088856871\n",
      "Train Loss at iteration 11597: 0.04303415702853971\n",
      "Train Loss at iteration 11598: 0.04303410517713838\n",
      "Train Loss at iteration 11599: 0.04303405333148218\n",
      "Train Loss at iteration 11600: 0.04303400149157011\n",
      "Train Loss at iteration 11601: 0.04303394965740123\n",
      "Train Loss at iteration 11602: 0.043033897828974606\n",
      "Train Loss at iteration 11603: 0.04303384600628923\n",
      "Train Loss at iteration 11604: 0.043033794189344166\n",
      "Train Loss at iteration 11605: 0.043033742378138465\n",
      "Train Loss at iteration 11606: 0.04303369057267113\n",
      "Train Loss at iteration 11607: 0.043033638772941245\n",
      "Train Loss at iteration 11608: 0.04303358697894782\n",
      "Train Loss at iteration 11609: 0.043033535190689906\n",
      "Train Loss at iteration 11610: 0.04303348340816655\n",
      "Train Loss at iteration 11611: 0.04303343163137679\n",
      "Train Loss at iteration 11612: 0.04303337986031966\n",
      "Train Loss at iteration 11613: 0.043033328094994217\n",
      "Train Loss at iteration 11614: 0.04303327633539948\n",
      "Train Loss at iteration 11615: 0.04303322458153452\n",
      "Train Loss at iteration 11616: 0.04303317283339834\n",
      "Train Loss at iteration 11617: 0.04303312109099001\n",
      "Train Loss at iteration 11618: 0.04303306935430858\n",
      "Train Loss at iteration 11619: 0.04303301762335308\n",
      "Train Loss at iteration 11620: 0.04303296589812254\n",
      "Train Loss at iteration 11621: 0.04303291417861602\n",
      "Train Loss at iteration 11622: 0.043032862464832566\n",
      "Train Loss at iteration 11623: 0.0430328107567712\n",
      "Train Loss at iteration 11624: 0.043032759054430984\n",
      "Train Loss at iteration 11625: 0.043032707357810965\n",
      "Train Loss at iteration 11626: 0.043032655666910155\n",
      "Train Loss at iteration 11627: 0.04303260398172763\n",
      "Train Loss at iteration 11628: 0.04303255230226244\n",
      "Train Loss at iteration 11629: 0.0430325006285136\n",
      "Train Loss at iteration 11630: 0.04303244896048016\n",
      "Train Loss at iteration 11631: 0.04303239729816118\n",
      "Train Loss at iteration 11632: 0.04303234564155571\n",
      "Train Loss at iteration 11633: 0.043032293990662766\n",
      "Train Loss at iteration 11634: 0.04303224234548142\n",
      "Train Loss at iteration 11635: 0.0430321907060107\n",
      "Train Loss at iteration 11636: 0.04303213907224966\n",
      "Train Loss at iteration 11637: 0.043032087444197335\n",
      "Train Loss at iteration 11638: 0.04303203582185277\n",
      "Train Loss at iteration 11639: 0.043031984205215035\n",
      "Train Loss at iteration 11640: 0.043031932594283145\n",
      "Train Loss at iteration 11641: 0.043031880989056186\n",
      "Train Loss at iteration 11642: 0.04303182938953316\n",
      "Train Loss at iteration 11643: 0.043031777795713134\n",
      "Train Loss at iteration 11644: 0.04303172620759516\n",
      "Train Loss at iteration 11645: 0.04303167462517828\n",
      "Train Loss at iteration 11646: 0.043031623048461524\n",
      "Train Loss at iteration 11647: 0.04303157147744398\n",
      "Train Loss at iteration 11648: 0.043031519912124644\n",
      "Train Loss at iteration 11649: 0.04303146835250259\n",
      "Train Loss at iteration 11650: 0.04303141679857688\n",
      "Train Loss at iteration 11651: 0.043031365250346544\n",
      "Train Loss at iteration 11652: 0.04303131370781063\n",
      "Train Loss at iteration 11653: 0.04303126217096819\n",
      "Train Loss at iteration 11654: 0.04303121063981827\n",
      "Train Loss at iteration 11655: 0.0430311591143599\n",
      "Train Loss at iteration 11656: 0.04303110759459217\n",
      "Train Loss at iteration 11657: 0.043031056080514093\n",
      "Train Loss at iteration 11658: 0.04303100457212474\n",
      "Train Loss at iteration 11659: 0.04303095306942315\n",
      "Train Loss at iteration 11660: 0.043030901572408374\n",
      "Train Loss at iteration 11661: 0.043030850081079465\n",
      "Train Loss at iteration 11662: 0.04303079859543546\n",
      "Train Loss at iteration 11663: 0.043030747115475415\n",
      "Train Loss at iteration 11664: 0.04303069564119839\n",
      "Train Loss at iteration 11665: 0.04303064417260343\n",
      "Train Loss at iteration 11666: 0.043030592709689564\n",
      "Train Loss at iteration 11667: 0.04303054125245589\n",
      "Train Loss at iteration 11668: 0.0430304898009014\n",
      "Train Loss at iteration 11669: 0.04303043835502519\n",
      "Train Loss at iteration 11670: 0.043030386914826295\n",
      "Train Loss at iteration 11671: 0.04303033548030376\n",
      "Train Loss at iteration 11672: 0.04303028405145664\n",
      "Train Loss at iteration 11673: 0.043030232628284\n",
      "Train Loss at iteration 11674: 0.043030181210784876\n",
      "Train Loss at iteration 11675: 0.043030129798958316\n",
      "Train Loss at iteration 11676: 0.043030078392803386\n",
      "Train Loss at iteration 11677: 0.04303002699231913\n",
      "Train Loss at iteration 11678: 0.04302997559750459\n",
      "Train Loss at iteration 11679: 0.043029924208358855\n",
      "Train Loss at iteration 11680: 0.04302987282488093\n",
      "Train Loss at iteration 11681: 0.0430298214470699\n",
      "Train Loss at iteration 11682: 0.04302977007492482\n",
      "Train Loss at iteration 11683: 0.043029718708444714\n",
      "Train Loss at iteration 11684: 0.04302966734762865\n",
      "Train Loss at iteration 11685: 0.04302961599247569\n",
      "Train Loss at iteration 11686: 0.043029564642984894\n",
      "Train Loss at iteration 11687: 0.04302951329915529\n",
      "Train Loss at iteration 11688: 0.043029461960985935\n",
      "Train Loss at iteration 11689: 0.04302941062847591\n",
      "Train Loss at iteration 11690: 0.04302935930162424\n",
      "Train Loss at iteration 11691: 0.043029307980430004\n",
      "Train Loss at iteration 11692: 0.043029256664892226\n",
      "Train Loss at iteration 11693: 0.04302920535500999\n",
      "Train Loss at iteration 11694: 0.043029154050782344\n",
      "Train Loss at iteration 11695: 0.043029102752208316\n",
      "Train Loss at iteration 11696: 0.043029051459287\n",
      "Train Loss at iteration 11697: 0.04302900017201743\n",
      "Train Loss at iteration 11698: 0.04302894889039866\n",
      "Train Loss at iteration 11699: 0.04302889761442977\n",
      "Train Loss at iteration 11700: 0.04302884634410978\n",
      "Train Loss at iteration 11701: 0.043028795079437764\n",
      "Train Loss at iteration 11702: 0.04302874382041278\n",
      "Train Loss at iteration 11703: 0.043028692567033885\n",
      "Train Loss at iteration 11704: 0.043028641319300134\n",
      "Train Loss at iteration 11705: 0.04302859007721058\n",
      "Train Loss at iteration 11706: 0.04302853884076427\n",
      "Train Loss at iteration 11707: 0.04302848760996028\n",
      "Train Loss at iteration 11708: 0.043028436384797655\n",
      "Train Loss at iteration 11709: 0.04302838516527546\n",
      "Train Loss at iteration 11710: 0.043028333951392744\n",
      "Train Loss at iteration 11711: 0.04302828274314857\n",
      "Train Loss at iteration 11712: 0.04302823154054201\n",
      "Train Loss at iteration 11713: 0.04302818034357208\n",
      "Train Loss at iteration 11714: 0.04302812915223788\n",
      "Train Loss at iteration 11715: 0.04302807796653846\n",
      "Train Loss at iteration 11716: 0.04302802678647286\n",
      "Train Loss at iteration 11717: 0.04302797561204015\n",
      "Train Loss at iteration 11718: 0.04302792444323939\n",
      "Train Loss at iteration 11719: 0.04302787328006964\n",
      "Train Loss at iteration 11720: 0.04302782212252996\n",
      "Train Loss at iteration 11721: 0.043027770970619404\n",
      "Train Loss at iteration 11722: 0.04302771982433703\n",
      "Train Loss at iteration 11723: 0.043027668683681894\n",
      "Train Loss at iteration 11724: 0.04302761754865307\n",
      "Train Loss at iteration 11725: 0.043027566419249604\n",
      "Train Loss at iteration 11726: 0.04302751529547057\n",
      "Train Loss at iteration 11727: 0.04302746417731502\n",
      "Train Loss at iteration 11728: 0.043027413064782\n",
      "Train Loss at iteration 11729: 0.0430273619578706\n",
      "Train Loss at iteration 11730: 0.043027310856579855\n",
      "Train Loss at iteration 11731: 0.04302725976090885\n",
      "Train Loss at iteration 11732: 0.043027208670856625\n",
      "Train Loss at iteration 11733: 0.04302715758642225\n",
      "Train Loss at iteration 11734: 0.04302710650760478\n",
      "Train Loss at iteration 11735: 0.043027055434403286\n",
      "Train Loss at iteration 11736: 0.04302700436681682\n",
      "Train Loss at iteration 11737: 0.043026953304844455\n",
      "Train Loss at iteration 11738: 0.04302690224848524\n",
      "Train Loss at iteration 11739: 0.04302685119773824\n",
      "Train Loss at iteration 11740: 0.04302680015260253\n",
      "Train Loss at iteration 11741: 0.043026749113077155\n",
      "Train Loss at iteration 11742: 0.04302669807916119\n",
      "Train Loss at iteration 11743: 0.04302664705085369\n",
      "Train Loss at iteration 11744: 0.04302659602815373\n",
      "Train Loss at iteration 11745: 0.04302654501106035\n",
      "Train Loss at iteration 11746: 0.04302649399957264\n",
      "Train Loss at iteration 11747: 0.043026442993689636\n",
      "Train Loss at iteration 11748: 0.04302639199341044\n",
      "Train Loss at iteration 11749: 0.04302634099873408\n",
      "Train Loss at iteration 11750: 0.04302629000965962\n",
      "Train Loss at iteration 11751: 0.04302623902618616\n",
      "Train Loss at iteration 11752: 0.04302618804831272\n",
      "Train Loss at iteration 11753: 0.0430261370760384\n",
      "Train Loss at iteration 11754: 0.04302608610936224\n",
      "Train Loss at iteration 11755: 0.043026035148283306\n",
      "Train Loss at iteration 11756: 0.043025984192800674\n",
      "Train Loss at iteration 11757: 0.04302593324291341\n",
      "Train Loss at iteration 11758: 0.04302588229862058\n",
      "Train Loss at iteration 11759: 0.043025831359921246\n",
      "Train Loss at iteration 11760: 0.04302578042681446\n",
      "Train Loss at iteration 11761: 0.043025729499299295\n",
      "Train Loss at iteration 11762: 0.04302567857737483\n",
      "Train Loss at iteration 11763: 0.04302562766104012\n",
      "Train Loss at iteration 11764: 0.04302557675029424\n",
      "Train Loss at iteration 11765: 0.043025525845136234\n",
      "Train Loss at iteration 11766: 0.043025474945565204\n",
      "Train Loss at iteration 11767: 0.04302542405158018\n",
      "Train Loss at iteration 11768: 0.04302537316318025\n",
      "Train Loss at iteration 11769: 0.04302532228036447\n",
      "Train Loss at iteration 11770: 0.04302527140313192\n",
      "Train Loss at iteration 11771: 0.04302522053148165\n",
      "Train Loss at iteration 11772: 0.043025169665412755\n",
      "Train Loss at iteration 11773: 0.043025118804924274\n",
      "Train Loss at iteration 11774: 0.04302506795001529\n",
      "Train Loss at iteration 11775: 0.04302501710068487\n",
      "Train Loss at iteration 11776: 0.043024966256932064\n",
      "Train Loss at iteration 11777: 0.043024915418755964\n",
      "Train Loss at iteration 11778: 0.04302486458615564\n",
      "Train Loss at iteration 11779: 0.04302481375913013\n",
      "Train Loss at iteration 11780: 0.04302476293767852\n",
      "Train Loss at iteration 11781: 0.04302471212179989\n",
      "Train Loss at iteration 11782: 0.04302466131149331\n",
      "Train Loss at iteration 11783: 0.043024610506757835\n",
      "Train Loss at iteration 11784: 0.04302455970759254\n",
      "Train Loss at iteration 11785: 0.04302450891399648\n",
      "Train Loss at iteration 11786: 0.04302445812596875\n",
      "Train Loss at iteration 11787: 0.043024407343508396\n",
      "Train Loss at iteration 11788: 0.043024356566614505\n",
      "Train Loss at iteration 11789: 0.043024305795286155\n",
      "Train Loss at iteration 11790: 0.04302425502952239\n",
      "Train Loss at iteration 11791: 0.04302420426932228\n",
      "Train Loss at iteration 11792: 0.043024153514684936\n",
      "Train Loss at iteration 11793: 0.04302410276560939\n",
      "Train Loss at iteration 11794: 0.04302405202209473\n",
      "Train Loss at iteration 11795: 0.04302400128414001\n",
      "Train Loss at iteration 11796: 0.04302395055174433\n",
      "Train Loss at iteration 11797: 0.04302389982490672\n",
      "Train Loss at iteration 11798: 0.04302384910362629\n",
      "Train Loss at iteration 11799: 0.043023798387902094\n",
      "Train Loss at iteration 11800: 0.043023747677733225\n",
      "Train Loss at iteration 11801: 0.04302369697311872\n",
      "Train Loss at iteration 11802: 0.04302364627405767\n",
      "Train Loss at iteration 11803: 0.04302359558054915\n",
      "Train Loss at iteration 11804: 0.04302354489259222\n",
      "Train Loss at iteration 11805: 0.04302349421018597\n",
      "Train Loss at iteration 11806: 0.04302344353332945\n",
      "Train Loss at iteration 11807: 0.04302339286202177\n",
      "Train Loss at iteration 11808: 0.04302334219626198\n",
      "Train Loss at iteration 11809: 0.043023291536049134\n",
      "Train Loss at iteration 11810: 0.043023240881382346\n",
      "Train Loss at iteration 11811: 0.04302319023226063\n",
      "Train Loss at iteration 11812: 0.04302313958868314\n",
      "Train Loss at iteration 11813: 0.043023088950648894\n",
      "Train Loss at iteration 11814: 0.04302303831815697\n",
      "Train Loss at iteration 11815: 0.04302298769120647\n",
      "Train Loss at iteration 11816: 0.04302293706979644\n",
      "Train Loss at iteration 11817: 0.04302288645392597\n",
      "Train Loss at iteration 11818: 0.04302283584359412\n",
      "Train Loss at iteration 11819: 0.043022785238799995\n",
      "Train Loss at iteration 11820: 0.04302273463954264\n",
      "Train Loss at iteration 11821: 0.04302268404582114\n",
      "Train Loss at iteration 11822: 0.043022633457634574\n",
      "Train Loss at iteration 11823: 0.04302258287498201\n",
      "Train Loss at iteration 11824: 0.043022532297862535\n",
      "Train Loss at iteration 11825: 0.04302248172627521\n",
      "Train Loss at iteration 11826: 0.04302243116021913\n",
      "Train Loss at iteration 11827: 0.043022380599693336\n",
      "Train Loss at iteration 11828: 0.04302233004469695\n",
      "Train Loss at iteration 11829: 0.04302227949522902\n",
      "Train Loss at iteration 11830: 0.043022228951288634\n",
      "Train Loss at iteration 11831: 0.04302217841287487\n",
      "Train Loss at iteration 11832: 0.04302212787998678\n",
      "Train Loss at iteration 11833: 0.04302207735262347\n",
      "Train Loss at iteration 11834: 0.043022026830784016\n",
      "Train Loss at iteration 11835: 0.043021976314467476\n",
      "Train Loss at iteration 11836: 0.04302192580367295\n",
      "Train Loss at iteration 11837: 0.043021875298399494\n",
      "Train Loss at iteration 11838: 0.0430218247986462\n",
      "Train Loss at iteration 11839: 0.04302177430441214\n",
      "Train Loss at iteration 11840: 0.043021723815696404\n",
      "Train Loss at iteration 11841: 0.04302167333249805\n",
      "Train Loss at iteration 11842: 0.04302162285481617\n",
      "Train Loss at iteration 11843: 0.04302157238264985\n",
      "Train Loss at iteration 11844: 0.04302152191599816\n",
      "Train Loss at iteration 11845: 0.043021471454860155\n",
      "Train Loss at iteration 11846: 0.04302142099923496\n",
      "Train Loss at iteration 11847: 0.043021370549121626\n",
      "Train Loss at iteration 11848: 0.04302132010451924\n",
      "Train Loss at iteration 11849: 0.043021269665426874\n",
      "Train Loss at iteration 11850: 0.04302121923184361\n",
      "Train Loss at iteration 11851: 0.04302116880376854\n",
      "Train Loss at iteration 11852: 0.043021118381200735\n",
      "Train Loss at iteration 11853: 0.043021067964139285\n",
      "Train Loss at iteration 11854: 0.04302101755258325\n",
      "Train Loss at iteration 11855: 0.043020967146531724\n",
      "Train Loss at iteration 11856: 0.04302091674598379\n",
      "Train Loss at iteration 11857: 0.04302086635093851\n",
      "Train Loss at iteration 11858: 0.043020815961394995\n",
      "Train Loss at iteration 11859: 0.043020765577352314\n",
      "Train Loss at iteration 11860: 0.04302071519880954\n",
      "Train Loss at iteration 11861: 0.043020664825765755\n",
      "Train Loss at iteration 11862: 0.04302061445822006\n",
      "Train Loss at iteration 11863: 0.043020564096171506\n",
      "Train Loss at iteration 11864: 0.043020513739619196\n",
      "Train Loss at iteration 11865: 0.04302046338856221\n",
      "Train Loss at iteration 11866: 0.04302041304299963\n",
      "Train Loss at iteration 11867: 0.04302036270293054\n",
      "Train Loss at iteration 11868: 0.04302031236835401\n",
      "Train Loss at iteration 11869: 0.043020262039269126\n",
      "Train Loss at iteration 11870: 0.043020211715675\n",
      "Train Loss at iteration 11871: 0.04302016139757067\n",
      "Train Loss at iteration 11872: 0.043020111084955265\n",
      "Train Loss at iteration 11873: 0.04302006077782782\n",
      "Train Loss at iteration 11874: 0.04302001047618746\n",
      "Train Loss at iteration 11875: 0.043019960180033255\n",
      "Train Loss at iteration 11876: 0.04301990988936427\n",
      "Train Loss at iteration 11877: 0.04301985960417961\n",
      "Train Loss at iteration 11878: 0.04301980932447837\n",
      "Train Loss at iteration 11879: 0.04301975905025959\n",
      "Train Loss at iteration 11880: 0.043019708781522406\n",
      "Train Loss at iteration 11881: 0.04301965851826587\n",
      "Train Loss at iteration 11882: 0.04301960826048909\n",
      "Train Loss at iteration 11883: 0.04301955800819113\n",
      "Train Loss at iteration 11884: 0.04301950776137107\n",
      "Train Loss at iteration 11885: 0.043019457520028025\n",
      "Train Loss at iteration 11886: 0.04301940728416106\n",
      "Train Loss at iteration 11887: 0.04301935705376926\n",
      "Train Loss at iteration 11888: 0.04301930682885172\n",
      "Train Loss at iteration 11889: 0.04301925660940752\n",
      "Train Loss at iteration 11890: 0.04301920639543575\n",
      "Train Loss at iteration 11891: 0.0430191561869355\n",
      "Train Loss at iteration 11892: 0.043019105983905834\n",
      "Train Loss at iteration 11893: 0.04301905578634586\n",
      "Train Loss at iteration 11894: 0.04301900559425466\n",
      "Train Loss at iteration 11895: 0.04301895540763133\n",
      "Train Loss at iteration 11896: 0.043018905226474925\n",
      "Train Loss at iteration 11897: 0.04301885505078457\n",
      "Train Loss at iteration 11898: 0.04301880488055934\n",
      "Train Loss at iteration 11899: 0.04301875471579831\n",
      "Train Loss at iteration 11900: 0.04301870455650058\n",
      "Train Loss at iteration 11901: 0.04301865440266523\n",
      "Train Loss at iteration 11902: 0.043018604254291375\n",
      "Train Loss at iteration 11903: 0.04301855411137805\n",
      "Train Loss at iteration 11904: 0.043018503973924406\n",
      "Train Loss at iteration 11905: 0.043018453841929476\n",
      "Train Loss at iteration 11906: 0.04301840371539238\n",
      "Train Loss at iteration 11907: 0.0430183535943122\n",
      "Train Loss at iteration 11908: 0.043018303478688014\n",
      "Train Loss at iteration 11909: 0.04301825336851894\n",
      "Train Loss at iteration 11910: 0.04301820326380403\n",
      "Train Loss at iteration 11911: 0.043018153164542404\n",
      "Train Loss at iteration 11912: 0.04301810307073314\n",
      "Train Loss at iteration 11913: 0.043018052982375314\n",
      "Train Loss at iteration 11914: 0.04301800289946804\n",
      "Train Loss at iteration 11915: 0.043017952822010395\n",
      "Train Loss at iteration 11916: 0.04301790275000146\n",
      "Train Loss at iteration 11917: 0.04301785268344036\n",
      "Train Loss at iteration 11918: 0.043017802622326136\n",
      "Train Loss at iteration 11919: 0.043017752566657914\n",
      "Train Loss at iteration 11920: 0.04301770251643479\n",
      "Train Loss at iteration 11921: 0.04301765247165582\n",
      "Train Loss at iteration 11922: 0.04301760243232012\n",
      "Train Loss at iteration 11923: 0.04301755239842679\n",
      "Train Loss at iteration 11924: 0.04301750236997489\n",
      "Train Loss at iteration 11925: 0.04301745234696355\n",
      "Train Loss at iteration 11926: 0.04301740232939182\n",
      "Train Loss at iteration 11927: 0.04301735231725882\n",
      "Train Loss at iteration 11928: 0.043017302310563645\n",
      "Train Loss at iteration 11929: 0.043017252309305365\n",
      "Train Loss at iteration 11930: 0.0430172023134831\n",
      "Train Loss at iteration 11931: 0.043017152323095914\n",
      "Train Loss at iteration 11932: 0.043017102338142904\n",
      "Train Loss at iteration 11933: 0.043017052358623194\n",
      "Train Loss at iteration 11934: 0.04301700238453583\n",
      "Train Loss at iteration 11935: 0.04301695241587995\n",
      "Train Loss at iteration 11936: 0.04301690245265463\n",
      "Train Loss at iteration 11937: 0.043016852494858954\n",
      "Train Loss at iteration 11938: 0.04301680254249202\n",
      "Train Loss at iteration 11939: 0.04301675259555292\n",
      "Train Loss at iteration 11940: 0.043016702654040756\n",
      "Train Loss at iteration 11941: 0.043016652717954615\n",
      "Train Loss at iteration 11942: 0.0430166027872936\n",
      "Train Loss at iteration 11943: 0.0430165528620568\n",
      "Train Loss at iteration 11944: 0.0430165029422433\n",
      "Train Loss at iteration 11945: 0.04301645302785222\n",
      "Train Loss at iteration 11946: 0.04301640311888262\n",
      "Train Loss at iteration 11947: 0.04301635321533363\n",
      "Train Loss at iteration 11948: 0.04301630331720432\n",
      "Train Loss at iteration 11949: 0.0430162534244938\n",
      "Train Loss at iteration 11950: 0.04301620353720114\n",
      "Train Loss at iteration 11951: 0.04301615365532547\n",
      "Train Loss at iteration 11952: 0.043016103778865875\n",
      "Train Loss at iteration 11953: 0.043016053907821435\n",
      "Train Loss at iteration 11954: 0.04301600404219127\n",
      "Train Loss at iteration 11955: 0.043015954181974465\n",
      "Train Loss at iteration 11956: 0.0430159043271701\n",
      "Train Loss at iteration 11957: 0.04301585447777731\n",
      "Train Loss at iteration 11958: 0.04301580463379515\n",
      "Train Loss at iteration 11959: 0.04301575479522274\n",
      "Train Loss at iteration 11960: 0.04301570496205917\n",
      "Train Loss at iteration 11961: 0.04301565513430355\n",
      "Train Loss at iteration 11962: 0.04301560531195497\n",
      "Train Loss at iteration 11963: 0.04301555549501251\n",
      "Train Loss at iteration 11964: 0.04301550568347529\n",
      "Train Loss at iteration 11965: 0.043015455877342394\n",
      "Train Loss at iteration 11966: 0.04301540607661292\n",
      "Train Loss at iteration 11967: 0.043015356281285985\n",
      "Train Loss at iteration 11968: 0.04301530649136068\n",
      "Train Loss at iteration 11969: 0.043015256706836084\n",
      "Train Loss at iteration 11970: 0.04301520692771132\n",
      "Train Loss at iteration 11971: 0.04301515715398546\n",
      "Train Loss at iteration 11972: 0.043015107385657635\n",
      "Train Loss at iteration 11973: 0.04301505762272692\n",
      "Train Loss at iteration 11974: 0.04301500786519242\n",
      "Train Loss at iteration 11975: 0.04301495811305325\n",
      "Train Loss at iteration 11976: 0.04301490836630848\n",
      "Train Loss at iteration 11977: 0.043014858624957235\n",
      "Train Loss at iteration 11978: 0.043014808888998605\n",
      "Train Loss at iteration 11979: 0.04301475915843169\n",
      "Train Loss at iteration 11980: 0.04301470943325558\n",
      "Train Loss at iteration 11981: 0.04301465971346939\n",
      "Train Loss at iteration 11982: 0.043014609999072216\n",
      "Train Loss at iteration 11983: 0.04301456029006317\n",
      "Train Loss at iteration 11984: 0.043014510586441325\n",
      "Train Loss at iteration 11985: 0.04301446088820581\n",
      "Train Loss at iteration 11986: 0.04301441119535571\n",
      "Train Loss at iteration 11987: 0.04301436150789013\n",
      "Train Loss at iteration 11988: 0.043014311825808164\n",
      "Train Loss at iteration 11989: 0.043014262149108924\n",
      "Train Loss at iteration 11990: 0.043014212477791525\n",
      "Train Loss at iteration 11991: 0.043014162811855036\n",
      "Train Loss at iteration 11992: 0.04301411315129858\n",
      "Train Loss at iteration 11993: 0.043014063496121246\n",
      "Train Loss at iteration 11994: 0.04301401384632216\n",
      "Train Loss at iteration 11995: 0.043013964201900415\n",
      "Train Loss at iteration 11996: 0.04301391456285508\n",
      "Train Loss at iteration 11997: 0.043013864929185304\n",
      "Train Loss at iteration 11998: 0.04301381530089018\n",
      "Train Loss at iteration 11999: 0.04301376567796879\n",
      "Train Loss at iteration 12000: 0.04301371606042025\n",
      "Train Loss at iteration 12001: 0.04301366644824367\n",
      "Train Loss at iteration 12002: 0.043013616841438125\n",
      "Train Loss at iteration 12003: 0.043013567240002756\n",
      "Train Loss at iteration 12004: 0.04301351764393666\n",
      "Train Loss at iteration 12005: 0.04301346805323891\n",
      "Train Loss at iteration 12006: 0.043013418467908654\n",
      "Train Loss at iteration 12007: 0.04301336888794496\n",
      "Train Loss at iteration 12008: 0.04301331931334695\n",
      "Train Loss at iteration 12009: 0.04301326974411372\n",
      "Train Loss at iteration 12010: 0.04301322018024439\n",
      "Train Loss at iteration 12011: 0.04301317062173807\n",
      "Train Loss at iteration 12012: 0.04301312106859381\n",
      "Train Loss at iteration 12013: 0.04301307152081077\n",
      "Train Loss at iteration 12014: 0.04301302197838803\n",
      "Train Loss at iteration 12015: 0.04301297244132474\n",
      "Train Loss at iteration 12016: 0.043012922909619944\n",
      "Train Loss at iteration 12017: 0.04301287338327279\n",
      "Train Loss at iteration 12018: 0.043012823862282355\n",
      "Train Loss at iteration 12019: 0.04301277434664777\n",
      "Train Loss at iteration 12020: 0.04301272483636812\n",
      "Train Loss at iteration 12021: 0.043012675331442526\n",
      "Train Loss at iteration 12022: 0.0430126258318701\n",
      "Train Loss at iteration 12023: 0.04301257633764992\n",
      "Train Loss at iteration 12024: 0.043012526848781124\n",
      "Train Loss at iteration 12025: 0.04301247736526281\n",
      "Train Loss at iteration 12026: 0.04301242788709407\n",
      "Train Loss at iteration 12027: 0.04301237841427402\n",
      "Train Loss at iteration 12028: 0.0430123289468018\n",
      "Train Loss at iteration 12029: 0.04301227948467646\n",
      "Train Loss at iteration 12030: 0.04301223002789715\n",
      "Train Loss at iteration 12031: 0.04301218057646296\n",
      "Train Loss at iteration 12032: 0.043012131130373\n",
      "Train Loss at iteration 12033: 0.04301208168962638\n",
      "Train Loss at iteration 12034: 0.04301203225422221\n",
      "Train Loss at iteration 12035: 0.0430119828241596\n",
      "Train Loss at iteration 12036: 0.04301193339943766\n",
      "Train Loss at iteration 12037: 0.043011883980055475\n",
      "Train Loss at iteration 12038: 0.043011834566012194\n",
      "Train Loss at iteration 12039: 0.0430117851573069\n",
      "Train Loss at iteration 12040: 0.043011735753938705\n",
      "Train Loss at iteration 12041: 0.04301168635590673\n",
      "Train Loss at iteration 12042: 0.04301163696321006\n",
      "Train Loss at iteration 12043: 0.04301158757584784\n",
      "Train Loss at iteration 12044: 0.04301153819381914\n",
      "Train Loss at iteration 12045: 0.04301148881712311\n",
      "Train Loss at iteration 12046: 0.04301143944575884\n",
      "Train Loss at iteration 12047: 0.04301139007972543\n",
      "Train Loss at iteration 12048: 0.04301134071902201\n",
      "Train Loss at iteration 12049: 0.04301129136364769\n",
      "Train Loss at iteration 12050: 0.04301124201360155\n",
      "Train Loss at iteration 12051: 0.04301119266888275\n",
      "Train Loss at iteration 12052: 0.04301114332949035\n",
      "Train Loss at iteration 12053: 0.043011093995423515\n",
      "Train Loss at iteration 12054: 0.04301104466668131\n",
      "Train Loss at iteration 12055: 0.04301099534326288\n",
      "Train Loss at iteration 12056: 0.0430109460251673\n",
      "Train Loss at iteration 12057: 0.04301089671239373\n",
      "Train Loss at iteration 12058: 0.04301084740494123\n",
      "Train Loss at iteration 12059: 0.043010798102808945\n",
      "Train Loss at iteration 12060: 0.043010748805995994\n",
      "Train Loss at iteration 12061: 0.04301069951450146\n",
      "Train Loss at iteration 12062: 0.043010650228324475\n",
      "Train Loss at iteration 12063: 0.043010600947464145\n",
      "Train Loss at iteration 12064: 0.0430105516719196\n",
      "Train Loss at iteration 12065: 0.04301050240168993\n",
      "Train Loss at iteration 12066: 0.043010453136774256\n",
      "Train Loss at iteration 12067: 0.0430104038771717\n",
      "Train Loss at iteration 12068: 0.04301035462288136\n",
      "Train Loss at iteration 12069: 0.04301030537390236\n",
      "Train Loss at iteration 12070: 0.0430102561302338\n",
      "Train Loss at iteration 12071: 0.04301020689187481\n",
      "Train Loss at iteration 12072: 0.043010157658824516\n",
      "Train Loss at iteration 12073: 0.043010108431082016\n",
      "Train Loss at iteration 12074: 0.04301005920864641\n",
      "Train Loss at iteration 12075: 0.04301000999151684\n",
      "Train Loss at iteration 12076: 0.0430099607796924\n",
      "Train Loss at iteration 12077: 0.04300991157317221\n",
      "Train Loss at iteration 12078: 0.0430098623719554\n",
      "Train Loss at iteration 12079: 0.04300981317604106\n",
      "Train Loss at iteration 12080: 0.043009763985428336\n",
      "Train Loss at iteration 12081: 0.043009714800116304\n",
      "Train Loss at iteration 12082: 0.04300966562010413\n",
      "Train Loss at iteration 12083: 0.04300961644539087\n",
      "Train Loss at iteration 12084: 0.043009567275975685\n",
      "Train Loss at iteration 12085: 0.04300951811185768\n",
      "Train Loss at iteration 12086: 0.043009468953035965\n",
      "Train Loss at iteration 12087: 0.043009419799509674\n",
      "Train Loss at iteration 12088: 0.043009370651277905\n",
      "Train Loss at iteration 12089: 0.04300932150833976\n",
      "Train Loss at iteration 12090: 0.0430092723706944\n",
      "Train Loss at iteration 12091: 0.0430092232383409\n",
      "Train Loss at iteration 12092: 0.043009174111278414\n",
      "Train Loss at iteration 12093: 0.043009124989506035\n",
      "Train Loss at iteration 12094: 0.04300907587302288\n",
      "Train Loss at iteration 12095: 0.04300902676182807\n",
      "Train Loss at iteration 12096: 0.043008977655920724\n",
      "Train Loss at iteration 12097: 0.04300892855529996\n",
      "Train Loss at iteration 12098: 0.04300887945996491\n",
      "Train Loss at iteration 12099: 0.04300883036991468\n",
      "Train Loss at iteration 12100: 0.04300878128514838\n",
      "Train Loss at iteration 12101: 0.04300873220566513\n",
      "Train Loss at iteration 12102: 0.04300868313146407\n",
      "Train Loss at iteration 12103: 0.043008634062544286\n",
      "Train Loss at iteration 12104: 0.04300858499890493\n",
      "Train Loss at iteration 12105: 0.04300853594054511\n",
      "Train Loss at iteration 12106: 0.04300848688746392\n",
      "Train Loss at iteration 12107: 0.04300843783966051\n",
      "Train Loss at iteration 12108: 0.043008388797134\n",
      "Train Loss at iteration 12109: 0.04300833975988349\n",
      "Train Loss at iteration 12110: 0.04300829072790813\n",
      "Train Loss at iteration 12111: 0.04300824170120699\n",
      "Train Loss at iteration 12112: 0.043008192679779234\n",
      "Train Loss at iteration 12113: 0.043008143663623974\n",
      "Train Loss at iteration 12114: 0.04300809465274032\n",
      "Train Loss at iteration 12115: 0.043008045647127395\n",
      "Train Loss at iteration 12116: 0.04300799664678431\n",
      "Train Loss at iteration 12117: 0.043007947651710225\n",
      "Train Loss at iteration 12118: 0.043007898661904215\n",
      "Train Loss at iteration 12119: 0.043007849677365435\n",
      "Train Loss at iteration 12120: 0.043007800698092985\n",
      "Train Loss at iteration 12121: 0.043007751724085996\n",
      "Train Loss at iteration 12122: 0.04300770275534358\n",
      "Train Loss at iteration 12123: 0.04300765379186489\n",
      "Train Loss at iteration 12124: 0.04300760483364901\n",
      "Train Loss at iteration 12125: 0.043007555880695054\n",
      "Train Loss at iteration 12126: 0.043007506933002204\n",
      "Train Loss at iteration 12127: 0.04300745799056952\n",
      "Train Loss at iteration 12128: 0.04300740905339616\n",
      "Train Loss at iteration 12129: 0.04300736012148123\n",
      "Train Loss at iteration 12130: 0.04300731119482386\n",
      "Train Loss at iteration 12131: 0.04300726227342319\n",
      "Train Loss at iteration 12132: 0.04300721335727831\n",
      "Train Loss at iteration 12133: 0.04300716444638836\n",
      "Train Loss at iteration 12134: 0.043007115540752455\n",
      "Train Loss at iteration 12135: 0.04300706664036975\n",
      "Train Loss at iteration 12136: 0.04300701774523931\n",
      "Train Loss at iteration 12137: 0.043006968855360324\n",
      "Train Loss at iteration 12138: 0.04300691997073188\n",
      "Train Loss at iteration 12139: 0.04300687109135311\n",
      "Train Loss at iteration 12140: 0.04300682221722313\n",
      "Train Loss at iteration 12141: 0.043006773348341076\n",
      "Train Loss at iteration 12142: 0.043006724484706056\n",
      "Train Loss at iteration 12143: 0.04300667562631722\n",
      "Train Loss at iteration 12144: 0.04300662677317368\n",
      "Train Loss at iteration 12145: 0.04300657792527455\n",
      "Train Loss at iteration 12146: 0.04300652908261897\n",
      "Train Loss at iteration 12147: 0.04300648024520607\n",
      "Train Loss at iteration 12148: 0.043006431413034965\n",
      "Train Loss at iteration 12149: 0.043006382586104765\n",
      "Train Loss at iteration 12150: 0.04300633376441463\n",
      "Train Loss at iteration 12151: 0.043006284947963665\n",
      "Train Loss at iteration 12152: 0.04300623613675101\n",
      "Train Loss at iteration 12153: 0.04300618733077579\n",
      "Train Loss at iteration 12154: 0.043006138530037104\n",
      "Train Loss at iteration 12155: 0.043006089734534104\n",
      "Train Loss at iteration 12156: 0.04300604094426592\n",
      "Train Loss at iteration 12157: 0.043005992159231655\n",
      "Train Loss at iteration 12158: 0.04300594337943047\n",
      "Train Loss at iteration 12159: 0.043005894604861455\n",
      "Train Loss at iteration 12160: 0.04300584583552376\n",
      "Train Loss at iteration 12161: 0.04300579707141653\n",
      "Train Loss at iteration 12162: 0.043005748312538845\n",
      "Train Loss at iteration 12163: 0.043005699558889866\n",
      "Train Loss at iteration 12164: 0.04300565081046872\n",
      "Train Loss at iteration 12165: 0.04300560206727453\n",
      "Train Loss at iteration 12166: 0.043005553329306404\n",
      "Train Loss at iteration 12167: 0.043005504596563514\n",
      "Train Loss at iteration 12168: 0.04300545586904495\n",
      "Train Loss at iteration 12169: 0.043005407146749865\n",
      "Train Loss at iteration 12170: 0.04300535842967738\n",
      "Train Loss at iteration 12171: 0.04300530971782661\n",
      "Train Loss at iteration 12172: 0.043005261011196706\n",
      "Train Loss at iteration 12173: 0.04300521230978678\n",
      "Train Loss at iteration 12174: 0.04300516361359598\n",
      "Train Loss at iteration 12175: 0.043005114922623426\n",
      "Train Loss at iteration 12176: 0.043005066236868235\n",
      "Train Loss at iteration 12177: 0.04300501755632954\n",
      "Train Loss at iteration 12178: 0.04300496888100649\n",
      "Train Loss at iteration 12179: 0.04300492021089821\n",
      "Train Loss at iteration 12180: 0.043004871546003826\n",
      "Train Loss at iteration 12181: 0.04300482288632246\n",
      "Train Loss at iteration 12182: 0.043004774231853254\n",
      "Train Loss at iteration 12183: 0.04300472558259533\n",
      "Train Loss at iteration 12184: 0.04300467693854782\n",
      "Train Loss at iteration 12185: 0.043004628299709864\n",
      "Train Loss at iteration 12186: 0.043004579666080595\n",
      "Train Loss at iteration 12187: 0.04300453103765913\n",
      "Train Loss at iteration 12188: 0.043004482414444614\n",
      "Train Loss at iteration 12189: 0.043004433796436176\n",
      "Train Loss at iteration 12190: 0.04300438518363294\n",
      "Train Loss at iteration 12191: 0.04300433657603404\n",
      "Train Loss at iteration 12192: 0.04300428797363861\n",
      "Train Loss at iteration 12193: 0.043004239376445784\n",
      "Train Loss at iteration 12194: 0.04300419078445469\n",
      "Train Loss at iteration 12195: 0.04300414219766447\n",
      "Train Loss at iteration 12196: 0.04300409361607426\n",
      "Train Loss at iteration 12197: 0.043004045039683164\n",
      "Train Loss at iteration 12198: 0.04300399646849034\n",
      "Train Loss at iteration 12199: 0.04300394790249493\n",
      "Train Loss at iteration 12200: 0.04300389934169604\n",
      "Train Loss at iteration 12201: 0.04300385078609281\n",
      "Train Loss at iteration 12202: 0.04300380223568439\n",
      "Train Loss at iteration 12203: 0.043003753690469916\n",
      "Train Loss at iteration 12204: 0.0430037051504485\n",
      "Train Loss at iteration 12205: 0.04300365661561928\n",
      "Train Loss at iteration 12206: 0.04300360808598141\n",
      "Train Loss at iteration 12207: 0.043003559561534\n",
      "Train Loss at iteration 12208: 0.04300351104227619\n",
      "Train Loss at iteration 12209: 0.04300346252820713\n",
      "Train Loss at iteration 12210: 0.04300341401932594\n",
      "Train Loss at iteration 12211: 0.04300336551563177\n",
      "Train Loss at iteration 12212: 0.04300331701712372\n",
      "Train Loss at iteration 12213: 0.043003268523800965\n",
      "Train Loss at iteration 12214: 0.043003220035662634\n",
      "Train Loss at iteration 12215: 0.04300317155270785\n",
      "Train Loss at iteration 12216: 0.043003123074935735\n",
      "Train Loss at iteration 12217: 0.043003074602345454\n",
      "Train Loss at iteration 12218: 0.04300302613493614\n",
      "Train Loss at iteration 12219: 0.043002977672706916\n",
      "Train Loss at iteration 12220: 0.043002929215656925\n",
      "Train Loss at iteration 12221: 0.04300288076378529\n",
      "Train Loss at iteration 12222: 0.04300283231709116\n",
      "Train Loss at iteration 12223: 0.04300278387557368\n",
      "Train Loss at iteration 12224: 0.043002735439231984\n",
      "Train Loss at iteration 12225: 0.04300268700806518\n",
      "Train Loss at iteration 12226: 0.043002638582072435\n",
      "Train Loss at iteration 12227: 0.043002590161252886\n",
      "Train Loss at iteration 12228: 0.043002541745605666\n",
      "Train Loss at iteration 12229: 0.04300249333512989\n",
      "Train Loss at iteration 12230: 0.04300244492982473\n",
      "Train Loss at iteration 12231: 0.04300239652968932\n",
      "Train Loss at iteration 12232: 0.04300234813472278\n",
      "Train Loss at iteration 12233: 0.04300229974492424\n",
      "Train Loss at iteration 12234: 0.04300225136029287\n",
      "Train Loss at iteration 12235: 0.04300220298082778\n",
      "Train Loss at iteration 12236: 0.043002154606528134\n",
      "Train Loss at iteration 12237: 0.043002106237393055\n",
      "Train Loss at iteration 12238: 0.043002057873421685\n",
      "Train Loss at iteration 12239: 0.043002009514613165\n",
      "Train Loss at iteration 12240: 0.04300196116096662\n",
      "Train Loss at iteration 12241: 0.04300191281248122\n",
      "Train Loss at iteration 12242: 0.04300186446915607\n",
      "Train Loss at iteration 12243: 0.043001816130990335\n",
      "Train Loss at iteration 12244: 0.04300176779798315\n",
      "Train Loss at iteration 12245: 0.043001719470133634\n",
      "Train Loss at iteration 12246: 0.04300167114744095\n",
      "Train Loss at iteration 12247: 0.043001622829904246\n",
      "Train Loss at iteration 12248: 0.04300157451752262\n",
      "Train Loss at iteration 12249: 0.04300152621029526\n",
      "Train Loss at iteration 12250: 0.043001477908221286\n",
      "Train Loss at iteration 12251: 0.04300142961129985\n",
      "Train Loss at iteration 12252: 0.04300138131953007\n",
      "Train Loss at iteration 12253: 0.0430013330329111\n",
      "Train Loss at iteration 12254: 0.04300128475144208\n",
      "Train Loss at iteration 12255: 0.043001236475122155\n",
      "Train Loss at iteration 12256: 0.04300118820395048\n",
      "Train Loss at iteration 12257: 0.043001139937926154\n",
      "Train Loss at iteration 12258: 0.04300109167704836\n",
      "Train Loss at iteration 12259: 0.04300104342131623\n",
      "Train Loss at iteration 12260: 0.04300099517072889\n",
      "Train Loss at iteration 12261: 0.043000946925285506\n",
      "Train Loss at iteration 12262: 0.043000898684985224\n",
      "Train Loss at iteration 12263: 0.043000850449827155\n",
      "Train Loss at iteration 12264: 0.04300080221981044\n",
      "Train Loss at iteration 12265: 0.04300075399493426\n",
      "Train Loss at iteration 12266: 0.04300070577519774\n",
      "Train Loss at iteration 12267: 0.043000657560600015\n",
      "Train Loss at iteration 12268: 0.04300060935114024\n",
      "Train Loss at iteration 12269: 0.04300056114681755\n",
      "Train Loss at iteration 12270: 0.043000512947631096\n",
      "Train Loss at iteration 12271: 0.04300046475358001\n",
      "Train Loss at iteration 12272: 0.04300041656466344\n",
      "Train Loss at iteration 12273: 0.04300036838088055\n",
      "Train Loss at iteration 12274: 0.043000320202230456\n",
      "Train Loss at iteration 12275: 0.043000272028712316\n",
      "Train Loss at iteration 12276: 0.04300022386032527\n",
      "Train Loss at iteration 12277: 0.043000175697068474\n",
      "Train Loss at iteration 12278: 0.04300012753894106\n",
      "Train Loss at iteration 12279: 0.04300007938594217\n",
      "Train Loss at iteration 12280: 0.04300003123807096\n",
      "Train Loss at iteration 12281: 0.04299998309532658\n",
      "Train Loss at iteration 12282: 0.04299993495770815\n",
      "Train Loss at iteration 12283: 0.042999886825214856\n",
      "Train Loss at iteration 12284: 0.042999838697845794\n",
      "Train Loss at iteration 12285: 0.04299979057560015\n",
      "Train Loss at iteration 12286: 0.042999742458477055\n",
      "Train Loss at iteration 12287: 0.042999694346475645\n",
      "Train Loss at iteration 12288: 0.04299964623959508\n",
      "Train Loss at iteration 12289: 0.04299959813783451\n",
      "Train Loss at iteration 12290: 0.04299955004119307\n",
      "Train Loss at iteration 12291: 0.04299950194966991\n",
      "Train Loss at iteration 12292: 0.042999453863264196\n",
      "Train Loss at iteration 12293: 0.04299940578197503\n",
      "Train Loss at iteration 12294: 0.04299935770580161\n",
      "Train Loss at iteration 12295: 0.042999309634743056\n",
      "Train Loss at iteration 12296: 0.0429992615687985\n",
      "Train Loss at iteration 12297: 0.04299921350796712\n",
      "Train Loss at iteration 12298: 0.04299916545224806\n",
      "Train Loss at iteration 12299: 0.042999117401640446\n",
      "Train Loss at iteration 12300: 0.04299906935614345\n",
      "Train Loss at iteration 12301: 0.04299902131575619\n",
      "Train Loss at iteration 12302: 0.04299897328047787\n",
      "Train Loss at iteration 12303: 0.04299892525030759\n",
      "Train Loss at iteration 12304: 0.042998877225244495\n",
      "Train Loss at iteration 12305: 0.04299882920528775\n",
      "Train Loss at iteration 12306: 0.04299878119043652\n",
      "Train Loss at iteration 12307: 0.042998733180689926\n",
      "Train Loss at iteration 12308: 0.04299868517604714\n",
      "Train Loss at iteration 12309: 0.04299863717650728\n",
      "Train Loss at iteration 12310: 0.04299858918206954\n",
      "Train Loss at iteration 12311: 0.042998541192733046\n",
      "Train Loss at iteration 12312: 0.04299849320849693\n",
      "Train Loss at iteration 12313: 0.042998445229360374\n",
      "Train Loss at iteration 12314: 0.0429983972553225\n",
      "Train Loss at iteration 12315: 0.04299834928638247\n",
      "Train Loss at iteration 12316: 0.04299830132253945\n",
      "Train Loss at iteration 12317: 0.04299825336379255\n",
      "Train Loss at iteration 12318: 0.042998205410140974\n",
      "Train Loss at iteration 12319: 0.04299815746158383\n",
      "Train Loss at iteration 12320: 0.04299810951812028\n",
      "Train Loss at iteration 12321: 0.042998061579749496\n",
      "Train Loss at iteration 12322: 0.04299801364647059\n",
      "Train Loss at iteration 12323: 0.04299796571828275\n",
      "Train Loss at iteration 12324: 0.04299791779518512\n",
      "Train Loss at iteration 12325: 0.04299786987717683\n",
      "Train Loss at iteration 12326: 0.04299782196425706\n",
      "Train Loss at iteration 12327: 0.04299777405642494\n",
      "Train Loss at iteration 12328: 0.04299772615367963\n",
      "Train Loss at iteration 12329: 0.04299767825602028\n",
      "Train Loss at iteration 12330: 0.04299763036344605\n",
      "Train Loss at iteration 12331: 0.042997582475956087\n",
      "Train Loss at iteration 12332: 0.04299753459354954\n",
      "Train Loss at iteration 12333: 0.042997486716225566\n",
      "Train Loss at iteration 12334: 0.04299743884398332\n",
      "Train Loss at iteration 12335: 0.04299739097682195\n",
      "Train Loss at iteration 12336: 0.04299734311474062\n",
      "Train Loss at iteration 12337: 0.04299729525773845\n",
      "Train Loss at iteration 12338: 0.04299724740581465\n",
      "Train Loss at iteration 12339: 0.04299719955896832\n",
      "Train Loss at iteration 12340: 0.042997151717198645\n",
      "Train Loss at iteration 12341: 0.04299710388050477\n",
      "Train Loss at iteration 12342: 0.04299705604888585\n",
      "Train Loss at iteration 12343: 0.04299700822234103\n",
      "Train Loss at iteration 12344: 0.042996960400869484\n",
      "Train Loss at iteration 12345: 0.042996912584470345\n",
      "Train Loss at iteration 12346: 0.04299686477314278\n",
      "Train Loss at iteration 12347: 0.04299681696688595\n",
      "Train Loss at iteration 12348: 0.042996769165699\n",
      "Train Loss at iteration 12349: 0.042996721369581076\n",
      "Train Loss at iteration 12350: 0.04299667357853135\n",
      "Train Loss at iteration 12351: 0.04299662579254897\n",
      "Train Loss at iteration 12352: 0.042996578011633084\n",
      "Train Loss at iteration 12353: 0.04299653023578287\n",
      "Train Loss at iteration 12354: 0.04299648246499746\n",
      "Train Loss at iteration 12355: 0.04299643469927602\n",
      "Train Loss at iteration 12356: 0.04299638693861771\n",
      "Train Loss at iteration 12357: 0.04299633918302168\n",
      "Train Loss at iteration 12358: 0.04299629143248709\n",
      "Train Loss at iteration 12359: 0.042996243687013096\n",
      "Train Loss at iteration 12360: 0.04299619594659885\n",
      "Train Loss at iteration 12361: 0.042996148211243514\n",
      "Train Loss at iteration 12362: 0.04299610048094624\n",
      "Train Loss at iteration 12363: 0.042996052755706185\n",
      "Train Loss at iteration 12364: 0.04299600503552252\n",
      "Train Loss at iteration 12365: 0.04299595732039438\n",
      "Train Loss at iteration 12366: 0.042995909610320945\n",
      "Train Loss at iteration 12367: 0.04299586190530136\n",
      "Train Loss at iteration 12368: 0.042995814205334784\n",
      "Train Loss at iteration 12369: 0.04299576651042038\n",
      "Train Loss at iteration 12370: 0.042995718820557284\n",
      "Train Loss at iteration 12371: 0.04299567113574468\n",
      "Train Loss at iteration 12372: 0.04299562345598173\n",
      "Train Loss at iteration 12373: 0.042995575781267566\n",
      "Train Loss at iteration 12374: 0.04299552811160137\n",
      "Train Loss at iteration 12375: 0.042995480446982294\n",
      "Train Loss at iteration 12376: 0.04299543278740949\n",
      "Train Loss at iteration 12377: 0.04299538513288213\n",
      "Train Loss at iteration 12378: 0.042995337483399364\n",
      "Train Loss at iteration 12379: 0.042995289838960345\n",
      "Train Loss at iteration 12380: 0.04299524219956424\n",
      "Train Loss at iteration 12381: 0.04299519456521022\n",
      "Train Loss at iteration 12382: 0.04299514693589743\n",
      "Train Loss at iteration 12383: 0.04299509931162502\n",
      "Train Loss at iteration 12384: 0.04299505169239218\n",
      "Train Loss at iteration 12385: 0.04299500407819805\n",
      "Train Loss at iteration 12386: 0.042994956469041785\n",
      "Train Loss at iteration 12387: 0.042994908864922556\n",
      "Train Loss at iteration 12388: 0.04299486126583955\n",
      "Train Loss at iteration 12389: 0.04299481367179187\n",
      "Train Loss at iteration 12390: 0.042994766082778725\n",
      "Train Loss at iteration 12391: 0.04299471849879925\n",
      "Train Loss at iteration 12392: 0.042994670919852616\n",
      "Train Loss at iteration 12393: 0.04299462334593799\n",
      "Train Loss at iteration 12394: 0.04299457577705452\n",
      "Train Loss at iteration 12395: 0.04299452821320137\n",
      "Train Loss at iteration 12396: 0.04299448065437771\n",
      "Train Loss at iteration 12397: 0.04299443310058271\n",
      "Train Loss at iteration 12398: 0.042994385551815506\n",
      "Train Loss at iteration 12399: 0.04299433800807527\n",
      "Train Loss at iteration 12400: 0.04299429046936118\n",
      "Train Loss at iteration 12401: 0.04299424293567238\n",
      "Train Loss at iteration 12402: 0.04299419540700804\n",
      "Train Loss at iteration 12403: 0.04299414788336733\n",
      "Train Loss at iteration 12404: 0.0429941003647494\n",
      "Train Loss at iteration 12405: 0.042994052851153416\n",
      "Train Loss at iteration 12406: 0.04299400534257855\n",
      "Train Loss at iteration 12407: 0.04299395783902396\n",
      "Train Loss at iteration 12408: 0.0429939103404888\n",
      "Train Loss at iteration 12409: 0.04299386284697225\n",
      "Train Loss at iteration 12410: 0.04299381535847346\n",
      "Train Loss at iteration 12411: 0.0429937678749916\n",
      "Train Loss at iteration 12412: 0.04299372039652584\n",
      "Train Loss at iteration 12413: 0.042993672923075335\n",
      "Train Loss at iteration 12414: 0.04299362545463926\n",
      "Train Loss at iteration 12415: 0.04299357799121676\n",
      "Train Loss at iteration 12416: 0.042993530532807016\n",
      "Train Loss at iteration 12417: 0.04299348307940919\n",
      "Train Loss at iteration 12418: 0.04299343563102244\n",
      "Train Loss at iteration 12419: 0.04299338818764595\n",
      "Train Loss at iteration 12420: 0.042993340749278866\n",
      "Train Loss at iteration 12421: 0.04299329331592037\n",
      "Train Loss at iteration 12422: 0.042993245887569594\n",
      "Train Loss at iteration 12423: 0.04299319846422573\n",
      "Train Loss at iteration 12424: 0.04299315104588795\n",
      "Train Loss at iteration 12425: 0.042993103632555416\n",
      "Train Loss at iteration 12426: 0.04299305622422727\n",
      "Train Loss at iteration 12427: 0.042993008820902706\n",
      "Train Loss at iteration 12428: 0.04299296142258088\n",
      "Train Loss at iteration 12429: 0.042992914029260955\n",
      "Train Loss at iteration 12430: 0.042992866640942116\n",
      "Train Loss at iteration 12431: 0.042992819257623514\n",
      "Train Loss at iteration 12432: 0.0429927718793043\n",
      "Train Loss at iteration 12433: 0.042992724505983675\n",
      "Train Loss at iteration 12434: 0.04299267713766078\n",
      "Train Loss at iteration 12435: 0.04299262977433479\n",
      "Train Loss at iteration 12436: 0.042992582416004885\n",
      "Train Loss at iteration 12437: 0.04299253506267022\n",
      "Train Loss at iteration 12438: 0.04299248771432995\n",
      "Train Loss at iteration 12439: 0.04299244037098328\n",
      "Train Loss at iteration 12440: 0.042992393032629334\n",
      "Train Loss at iteration 12441: 0.042992345699267326\n",
      "Train Loss at iteration 12442: 0.04299229837089638\n",
      "Train Loss at iteration 12443: 0.04299225104751571\n",
      "Train Loss at iteration 12444: 0.04299220372912442\n",
      "Train Loss at iteration 12445: 0.042992156415721745\n",
      "Train Loss at iteration 12446: 0.042992109107306824\n",
      "Train Loss at iteration 12447: 0.04299206180387883\n",
      "Train Loss at iteration 12448: 0.04299201450543694\n",
      "Train Loss at iteration 12449: 0.042991967211980286\n",
      "Train Loss at iteration 12450: 0.04299191992350809\n",
      "Train Loss at iteration 12451: 0.0429918726400195\n",
      "Train Loss at iteration 12452: 0.04299182536151367\n",
      "Train Loss at iteration 12453: 0.04299177808798978\n",
      "Train Loss at iteration 12454: 0.04299173081944702\n",
      "Train Loss at iteration 12455: 0.04299168355588453\n",
      "Train Loss at iteration 12456: 0.04299163629730151\n",
      "Train Loss at iteration 12457: 0.042991589043697105\n",
      "Train Loss at iteration 12458: 0.0429915417950705\n",
      "Train Loss at iteration 12459: 0.04299149455142085\n",
      "Train Loss at iteration 12460: 0.04299144731274734\n",
      "Train Loss at iteration 12461: 0.04299140007904914\n",
      "Train Loss at iteration 12462: 0.04299135285032542\n",
      "Train Loss at iteration 12463: 0.04299130562657536\n",
      "Train Loss at iteration 12464: 0.0429912584077981\n",
      "Train Loss at iteration 12465: 0.042991211193992844\n",
      "Train Loss at iteration 12466: 0.042991163985158745\n",
      "Train Loss at iteration 12467: 0.04299111678129499\n",
      "Train Loss at iteration 12468: 0.042991069582400745\n",
      "Train Loss at iteration 12469: 0.042991022388475186\n",
      "Train Loss at iteration 12470: 0.04299097519951746\n",
      "Train Loss at iteration 12471: 0.042990928015526786\n",
      "Train Loss at iteration 12472: 0.042990880836502286\n",
      "Train Loss at iteration 12473: 0.04299083366244317\n",
      "Train Loss at iteration 12474: 0.0429907864933486\n",
      "Train Loss at iteration 12475: 0.042990739329217736\n",
      "Train Loss at iteration 12476: 0.04299069217004976\n",
      "Train Loss at iteration 12477: 0.042990645015843855\n",
      "Train Loss at iteration 12478: 0.04299059786659919\n",
      "Train Loss at iteration 12479: 0.04299055072231493\n",
      "Train Loss at iteration 12480: 0.04299050358299025\n",
      "Train Loss at iteration 12481: 0.04299045644862433\n",
      "Train Loss at iteration 12482: 0.04299040931921635\n",
      "Train Loss at iteration 12483: 0.042990362194765465\n",
      "Train Loss at iteration 12484: 0.04299031507527086\n",
      "Train Loss at iteration 12485: 0.042990267960731715\n",
      "Train Loss at iteration 12486: 0.0429902208511472\n",
      "Train Loss at iteration 12487: 0.04299017374651648\n",
      "Train Loss at iteration 12488: 0.04299012664683873\n",
      "Train Loss at iteration 12489: 0.042990079552113154\n",
      "Train Loss at iteration 12490: 0.0429900324623389\n",
      "Train Loss at iteration 12491: 0.042989985377515146\n",
      "Train Loss at iteration 12492: 0.04298993829764106\n",
      "Train Loss at iteration 12493: 0.04298989122271586\n",
      "Train Loss at iteration 12494: 0.042989844152738654\n",
      "Train Loss at iteration 12495: 0.04298979708770868\n",
      "Train Loss at iteration 12496: 0.042989750027625076\n",
      "Train Loss at iteration 12497: 0.042989702972487026\n",
      "Train Loss at iteration 12498: 0.042989655922293715\n",
      "Train Loss at iteration 12499: 0.04298960887704432\n",
      "Train Loss at iteration 12500: 0.042989561836738\n",
      "Train Loss at iteration 12501: 0.042989514801373946\n",
      "Train Loss at iteration 12502: 0.042989467770951334\n",
      "Train Loss at iteration 12503: 0.042989420745469346\n",
      "Train Loss at iteration 12504: 0.04298937372492715\n",
      "Train Loss at iteration 12505: 0.04298932670932392\n",
      "Train Loss at iteration 12506: 0.042989279698658844\n",
      "Train Loss at iteration 12507: 0.04298923269293109\n",
      "Train Loss at iteration 12508: 0.042989185692139824\n",
      "Train Loss at iteration 12509: 0.042989138696284276\n",
      "Train Loss at iteration 12510: 0.04298909170536356\n",
      "Train Loss at iteration 12511: 0.042989044719376895\n",
      "Train Loss at iteration 12512: 0.04298899773832344\n",
      "Train Loss at iteration 12513: 0.04298895076220238\n",
      "Train Loss at iteration 12514: 0.04298890379101289\n",
      "Train Loss at iteration 12515: 0.042988856824754146\n",
      "Train Loss at iteration 12516: 0.042988809863425334\n",
      "Train Loss at iteration 12517: 0.04298876290702565\n",
      "Train Loss at iteration 12518: 0.04298871595555424\n",
      "Train Loss at iteration 12519: 0.042988669009010286\n",
      "Train Loss at iteration 12520: 0.042988622067392984\n",
      "Train Loss at iteration 12521: 0.04298857513070151\n",
      "Train Loss at iteration 12522: 0.042988528198935054\n",
      "Train Loss at iteration 12523: 0.04298848127209277\n",
      "Train Loss at iteration 12524: 0.04298843435017384\n",
      "Train Loss at iteration 12525: 0.04298838743317747\n",
      "Train Loss at iteration 12526: 0.042988340521102814\n",
      "Train Loss at iteration 12527: 0.04298829361394906\n",
      "Train Loss at iteration 12528: 0.0429882467117154\n",
      "Train Loss at iteration 12529: 0.04298819981440101\n",
      "Train Loss at iteration 12530: 0.04298815292200507\n",
      "Train Loss at iteration 12531: 0.04298810603452674\n",
      "Train Loss at iteration 12532: 0.04298805915196524\n",
      "Train Loss at iteration 12533: 0.04298801227431971\n",
      "Train Loss at iteration 12534: 0.04298796540158936\n",
      "Train Loss at iteration 12535: 0.04298791853377336\n",
      "Train Loss at iteration 12536: 0.042987871670870884\n",
      "Train Loss at iteration 12537: 0.042987824812881145\n",
      "Train Loss at iteration 12538: 0.0429877779598033\n",
      "Train Loss at iteration 12539: 0.042987731111636514\n",
      "Train Loss at iteration 12540: 0.042987684268380004\n",
      "Train Loss at iteration 12541: 0.04298763743003292\n",
      "Train Loss at iteration 12542: 0.04298759059659448\n",
      "Train Loss at iteration 12543: 0.042987543768063845\n",
      "Train Loss at iteration 12544: 0.04298749694444019\n",
      "Train Loss at iteration 12545: 0.04298745012572273\n",
      "Train Loss at iteration 12546: 0.0429874033119106\n",
      "Train Loss at iteration 12547: 0.04298735650300304\n",
      "Train Loss at iteration 12548: 0.04298730969899918\n",
      "Train Loss at iteration 12549: 0.042987262899898224\n",
      "Train Loss at iteration 12550: 0.04298721610569937\n",
      "Train Loss at iteration 12551: 0.04298716931640178\n",
      "Train Loss at iteration 12552: 0.04298712253200466\n",
      "Train Loss at iteration 12553: 0.04298707575250717\n",
      "Train Loss at iteration 12554: 0.04298702897790849\n",
      "Train Loss at iteration 12555: 0.04298698220820783\n",
      "Train Loss at iteration 12556: 0.04298693544340437\n",
      "Train Loss at iteration 12557: 0.04298688868349729\n",
      "Train Loss at iteration 12558: 0.04298684192848576\n",
      "Train Loss at iteration 12559: 0.04298679517836899\n",
      "Train Loss at iteration 12560: 0.042986748433146135\n",
      "Train Loss at iteration 12561: 0.042986701692816404\n",
      "Train Loss at iteration 12562: 0.042986654957378974\n",
      "Train Loss at iteration 12563: 0.04298660822683304\n",
      "Train Loss at iteration 12564: 0.04298656150117778\n",
      "Train Loss at iteration 12565: 0.042986514780412356\n",
      "Train Loss at iteration 12566: 0.04298646806453599\n",
      "Train Loss at iteration 12567: 0.04298642135354784\n",
      "Train Loss at iteration 12568: 0.04298637464744713\n",
      "Train Loss at iteration 12569: 0.042986327946233006\n",
      "Train Loss at iteration 12570: 0.04298628124990467\n",
      "Train Loss at iteration 12571: 0.0429862345584613\n",
      "Train Loss at iteration 12572: 0.042986187871902115\n",
      "Train Loss at iteration 12573: 0.04298614119022624\n",
      "Train Loss at iteration 12574: 0.04298609451343293\n",
      "Train Loss at iteration 12575: 0.04298604784152133\n",
      "Train Loss at iteration 12576: 0.04298600117449064\n",
      "Train Loss at iteration 12577: 0.04298595451234005\n",
      "Train Loss at iteration 12578: 0.042985907855068735\n",
      "Train Loss at iteration 12579: 0.04298586120267588\n",
      "Train Loss at iteration 12580: 0.042985814555160706\n",
      "Train Loss at iteration 12581: 0.042985767912522364\n",
      "Train Loss at iteration 12582: 0.04298572127476005\n",
      "Train Loss at iteration 12583: 0.042985674641872966\n",
      "Train Loss at iteration 12584: 0.04298562801386029\n",
      "Train Loss at iteration 12585: 0.04298558139072121\n",
      "Train Loss at iteration 12586: 0.04298553477245491\n",
      "Train Loss at iteration 12587: 0.042985488159060586\n",
      "Train Loss at iteration 12588: 0.04298544155053744\n",
      "Train Loss at iteration 12589: 0.04298539494688463\n",
      "Train Loss at iteration 12590: 0.04298534834810137\n",
      "Train Loss at iteration 12591: 0.042985301754186835\n",
      "Train Loss at iteration 12592: 0.042985255165140235\n",
      "Train Loss at iteration 12593: 0.04298520858096071\n",
      "Train Loss at iteration 12594: 0.04298516200164751\n",
      "Train Loss at iteration 12595: 0.042985115427199794\n",
      "Train Loss at iteration 12596: 0.04298506885761675\n",
      "Train Loss at iteration 12597: 0.04298502229289758\n",
      "Train Loss at iteration 12598: 0.04298497573304146\n",
      "Train Loss at iteration 12599: 0.0429849291780476\n",
      "Train Loss at iteration 12600: 0.042984882627915166\n",
      "Train Loss at iteration 12601: 0.04298483608264337\n",
      "Train Loss at iteration 12602: 0.042984789542231394\n",
      "Train Loss at iteration 12603: 0.042984743006678414\n",
      "Train Loss at iteration 12604: 0.04298469647598365\n",
      "Train Loss at iteration 12605: 0.042984649950146274\n",
      "Train Loss at iteration 12606: 0.04298460342916549\n",
      "Train Loss at iteration 12607: 0.04298455691304047\n",
      "Train Loss at iteration 12608: 0.04298451040177041\n",
      "Train Loss at iteration 12609: 0.04298446389535453\n",
      "Train Loss at iteration 12610: 0.04298441739379198\n",
      "Train Loss at iteration 12611: 0.042984370897081965\n",
      "Train Loss at iteration 12612: 0.0429843244052237\n",
      "Train Loss at iteration 12613: 0.04298427791821635\n",
      "Train Loss at iteration 12614: 0.042984231436059145\n",
      "Train Loss at iteration 12615: 0.04298418495875123\n",
      "Train Loss at iteration 12616: 0.04298413848629182\n",
      "Train Loss at iteration 12617: 0.0429840920186801\n",
      "Train Loss at iteration 12618: 0.04298404555591527\n",
      "Train Loss at iteration 12619: 0.04298399909799652\n",
      "Train Loss at iteration 12620: 0.04298395264492305\n",
      "Train Loss at iteration 12621: 0.042983906196694045\n",
      "Train Loss at iteration 12622: 0.042983859753308706\n",
      "Train Loss at iteration 12623: 0.04298381331476622\n",
      "Train Loss at iteration 12624: 0.04298376688106577\n",
      "Train Loss at iteration 12625: 0.042983720452206574\n",
      "Train Loss at iteration 12626: 0.04298367402818781\n",
      "Train Loss at iteration 12627: 0.04298362760900867\n",
      "Train Loss at iteration 12628: 0.042983581194668365\n",
      "Train Loss at iteration 12629: 0.04298353478516608\n",
      "Train Loss at iteration 12630: 0.042983488380501006\n",
      "Train Loss at iteration 12631: 0.04298344198067233\n",
      "Train Loss at iteration 12632: 0.04298339558567927\n",
      "Train Loss at iteration 12633: 0.04298334919552099\n",
      "Train Loss at iteration 12634: 0.04298330281019673\n",
      "Train Loss at iteration 12635: 0.042983256429705644\n",
      "Train Loss at iteration 12636: 0.04298321005404693\n",
      "Train Loss at iteration 12637: 0.0429831636832198\n",
      "Train Loss at iteration 12638: 0.04298311731722345\n",
      "Train Loss at iteration 12639: 0.04298307095605707\n",
      "Train Loss at iteration 12640: 0.042983024599719845\n",
      "Train Loss at iteration 12641: 0.04298297824821099\n",
      "Train Loss at iteration 12642: 0.042982931901529686\n",
      "Train Loss at iteration 12643: 0.04298288555967514\n",
      "Train Loss at iteration 12644: 0.04298283922264655\n",
      "Train Loss at iteration 12645: 0.04298279289044311\n",
      "Train Loss at iteration 12646: 0.04298274656306398\n",
      "Train Loss at iteration 12647: 0.04298270024050842\n",
      "Train Loss at iteration 12648: 0.042982653922775595\n",
      "Train Loss at iteration 12649: 0.04298260760986469\n",
      "Train Loss at iteration 12650: 0.042982561301774934\n",
      "Train Loss at iteration 12651: 0.04298251499850549\n",
      "Train Loss at iteration 12652: 0.042982468700055586\n",
      "Train Loss at iteration 12653: 0.04298242240642439\n",
      "Train Loss at iteration 12654: 0.042982376117611125\n",
      "Train Loss at iteration 12655: 0.04298232983361499\n",
      "Train Loss at iteration 12656: 0.04298228355443514\n",
      "Train Loss at iteration 12657: 0.042982237280070834\n",
      "Train Loss at iteration 12658: 0.04298219101052124\n",
      "Train Loss at iteration 12659: 0.042982144745785544\n",
      "Train Loss at iteration 12660: 0.042982098485862964\n",
      "Train Loss at iteration 12661: 0.0429820522307527\n",
      "Train Loss at iteration 12662: 0.042982005980453944\n",
      "Train Loss at iteration 12663: 0.04298195973496589\n",
      "Train Loss at iteration 12664: 0.04298191349428774\n",
      "Train Loss at iteration 12665: 0.042981867258418696\n",
      "Train Loss at iteration 12666: 0.042981821027357967\n",
      "Train Loss at iteration 12667: 0.04298177480110474\n",
      "Train Loss at iteration 12668: 0.04298172857965821\n",
      "Train Loss at iteration 12669: 0.04298168236301758\n",
      "Train Loss at iteration 12670: 0.04298163615118206\n",
      "Train Loss at iteration 12671: 0.042981589944150835\n",
      "Train Loss at iteration 12672: 0.042981543741923124\n",
      "Train Loss at iteration 12673: 0.042981497544498114\n",
      "Train Loss at iteration 12674: 0.04298145135187501\n",
      "Train Loss at iteration 12675: 0.042981405164053015\n",
      "Train Loss at iteration 12676: 0.04298135898103133\n",
      "Train Loss at iteration 12677: 0.04298131280280913\n",
      "Train Loss at iteration 12678: 0.04298126662938566\n",
      "Train Loss at iteration 12679: 0.0429812204607601\n",
      "Train Loss at iteration 12680: 0.04298117429693163\n",
      "Train Loss at iteration 12681: 0.042981128137899484\n",
      "Train Loss at iteration 12682: 0.04298108198366286\n",
      "Train Loss at iteration 12683: 0.042981035834220936\n",
      "Train Loss at iteration 12684: 0.04298098968957294\n",
      "Train Loss at iteration 12685: 0.042980943549718056\n",
      "Train Loss at iteration 12686: 0.0429808974146555\n",
      "Train Loss at iteration 12687: 0.04298085128438447\n",
      "Train Loss at iteration 12688: 0.04298080515890414\n",
      "Train Loss at iteration 12689: 0.042980759038213766\n",
      "Train Loss at iteration 12690: 0.04298071292231251\n",
      "Train Loss at iteration 12691: 0.04298066681119958\n",
      "Train Loss at iteration 12692: 0.042980620704874205\n",
      "Train Loss at iteration 12693: 0.04298057460333556\n",
      "Train Loss at iteration 12694: 0.04298052850658285\n",
      "Train Loss at iteration 12695: 0.04298048241461529\n",
      "Train Loss at iteration 12696: 0.04298043632743209\n",
      "Train Loss at iteration 12697: 0.042980390245032425\n",
      "Train Loss at iteration 12698: 0.042980344167415536\n",
      "Train Loss at iteration 12699: 0.042980298094580593\n",
      "Train Loss at iteration 12700: 0.04298025202652681\n",
      "Train Loss at iteration 12701: 0.04298020596325341\n",
      "Train Loss at iteration 12702: 0.04298015990475957\n",
      "Train Loss at iteration 12703: 0.04298011385104452\n",
      "Train Loss at iteration 12704: 0.042980067802107454\n",
      "Train Loss at iteration 12705: 0.04298002175794756\n",
      "Train Loss at iteration 12706: 0.042979975718564066\n",
      "Train Loss at iteration 12707: 0.04297992968395616\n",
      "Train Loss at iteration 12708: 0.042979883654123065\n",
      "Train Loss at iteration 12709: 0.042979837629063974\n",
      "Train Loss at iteration 12710: 0.04297979160877808\n",
      "Train Loss at iteration 12711: 0.04297974559326462\n",
      "Train Loss at iteration 12712: 0.042979699582522776\n",
      "Train Loss at iteration 12713: 0.04297965357655176\n",
      "Train Loss at iteration 12714: 0.04297960757535078\n",
      "Train Loss at iteration 12715: 0.042979561578919044\n",
      "Train Loss at iteration 12716: 0.04297951558725575\n",
      "Train Loss at iteration 12717: 0.04297946960036009\n",
      "Train Loss at iteration 12718: 0.04297942361823131\n",
      "Train Loss at iteration 12719: 0.04297937764086859\n",
      "Train Loss at iteration 12720: 0.04297933166827115\n",
      "Train Loss at iteration 12721: 0.042979285700438186\n",
      "Train Loss at iteration 12722: 0.04297923973736891\n",
      "Train Loss at iteration 12723: 0.04297919377906251\n",
      "Train Loss at iteration 12724: 0.042979147825518225\n",
      "Train Loss at iteration 12725: 0.042979101876735246\n",
      "Train Loss at iteration 12726: 0.04297905593271277\n",
      "Train Loss at iteration 12727: 0.042979009993450036\n",
      "Train Loss at iteration 12728: 0.042978964058946224\n",
      "Train Loss at iteration 12729: 0.04297891812920055\n",
      "Train Loss at iteration 12730: 0.042978872204212225\n",
      "Train Loss at iteration 12731: 0.04297882628398045\n",
      "Train Loss at iteration 12732: 0.042978780368504436\n",
      "Train Loss at iteration 12733: 0.0429787344577834\n",
      "Train Loss at iteration 12734: 0.04297868855181654\n",
      "Train Loss at iteration 12735: 0.04297864265060307\n",
      "Train Loss at iteration 12736: 0.04297859675414219\n",
      "Train Loss at iteration 12737: 0.042978550862433115\n",
      "Train Loss at iteration 12738: 0.042978504975475076\n",
      "Train Loss at iteration 12739: 0.042978459093267245\n",
      "Train Loss at iteration 12740: 0.04297841321580884\n",
      "Train Loss at iteration 12741: 0.042978367343099094\n",
      "Train Loss at iteration 12742: 0.04297832147513719\n",
      "Train Loss at iteration 12743: 0.04297827561192235\n",
      "Train Loss at iteration 12744: 0.04297822975345379\n",
      "Train Loss at iteration 12745: 0.04297818389973072\n",
      "Train Loss at iteration 12746: 0.042978138050752325\n",
      "Train Loss at iteration 12747: 0.04297809220651784\n",
      "Train Loss at iteration 12748: 0.042978046367026464\n",
      "Train Loss at iteration 12749: 0.04297800053227742\n",
      "Train Loss at iteration 12750: 0.042977954702269894\n",
      "Train Loss at iteration 12751: 0.04297790887700313\n",
      "Train Loss at iteration 12752: 0.04297786305647632\n",
      "Train Loss at iteration 12753: 0.04297781724068868\n",
      "Train Loss at iteration 12754: 0.04297777142963942\n",
      "Train Loss at iteration 12755: 0.04297772562332774\n",
      "Train Loss at iteration 12756: 0.04297767982175286\n",
      "Train Loss at iteration 12757: 0.042977634024914\n",
      "Train Loss at iteration 12758: 0.04297758823281037\n",
      "Train Loss at iteration 12759: 0.04297754244544117\n",
      "Train Loss at iteration 12760: 0.04297749666280562\n",
      "Train Loss at iteration 12761: 0.042977450884902936\n",
      "Train Loss at iteration 12762: 0.04297740511173233\n",
      "Train Loss at iteration 12763: 0.04297735934329299\n",
      "Train Loss at iteration 12764: 0.04297731357958417\n",
      "Train Loss at iteration 12765: 0.04297726782060504\n",
      "Train Loss at iteration 12766: 0.04297722206635485\n",
      "Train Loss at iteration 12767: 0.042977176316832795\n",
      "Train Loss at iteration 12768: 0.042977130572038086\n",
      "Train Loss at iteration 12769: 0.04297708483196994\n",
      "Train Loss at iteration 12770: 0.04297703909662756\n",
      "Train Loss at iteration 12771: 0.04297699336601019\n",
      "Train Loss at iteration 12772: 0.042976947640117016\n",
      "Train Loss at iteration 12773: 0.042976901918947256\n",
      "Train Loss at iteration 12774: 0.04297685620250012\n",
      "Train Loss at iteration 12775: 0.042976810490774855\n",
      "Train Loss at iteration 12776: 0.042976764783770625\n",
      "Train Loss at iteration 12777: 0.042976719081486696\n",
      "Train Loss at iteration 12778: 0.042976673383922234\n",
      "Train Loss at iteration 12779: 0.04297662769107648\n",
      "Train Loss at iteration 12780: 0.04297658200294864\n",
      "Train Loss at iteration 12781: 0.04297653631953794\n",
      "Train Loss at iteration 12782: 0.04297649064084358\n",
      "Train Loss at iteration 12783: 0.04297644496686478\n",
      "Train Loss at iteration 12784: 0.04297639929760076\n",
      "Train Loss at iteration 12785: 0.04297635363305073\n",
      "Train Loss at iteration 12786: 0.04297630797321392\n",
      "Train Loss at iteration 12787: 0.04297626231808953\n",
      "Train Loss at iteration 12788: 0.04297621666767678\n",
      "Train Loss at iteration 12789: 0.042976171021974895\n",
      "Train Loss at iteration 12790: 0.04297612538098306\n",
      "Train Loss at iteration 12791: 0.042976079744700524\n",
      "Train Loss at iteration 12792: 0.042976034113126504\n",
      "Train Loss at iteration 12793: 0.042975988486260185\n",
      "Train Loss at iteration 12794: 0.0429759428641008\n",
      "Train Loss at iteration 12795: 0.04297589724664758\n",
      "Train Loss at iteration 12796: 0.04297585163389974\n",
      "Train Loss at iteration 12797: 0.042975806025856475\n",
      "Train Loss at iteration 12798: 0.04297576042251702\n",
      "Train Loss at iteration 12799: 0.04297571482388059\n",
      "Train Loss at iteration 12800: 0.042975669229946396\n",
      "Train Loss at iteration 12801: 0.04297562364071366\n",
      "Train Loss at iteration 12802: 0.042975578056181594\n",
      "Train Loss at iteration 12803: 0.042975532476349415\n",
      "Train Loss at iteration 12804: 0.04297548690121636\n",
      "Train Loss at iteration 12805: 0.042975441330781634\n",
      "Train Loss at iteration 12806: 0.04297539576504443\n",
      "Train Loss at iteration 12807: 0.04297535020400402\n",
      "Train Loss at iteration 12808: 0.04297530464765958\n",
      "Train Loss at iteration 12809: 0.04297525909601033\n",
      "Train Loss at iteration 12810: 0.04297521354905553\n",
      "Train Loss at iteration 12811: 0.04297516800679435\n",
      "Train Loss at iteration 12812: 0.042975122469226024\n",
      "Train Loss at iteration 12813: 0.04297507693634978\n",
      "Train Loss at iteration 12814: 0.04297503140816483\n",
      "Train Loss at iteration 12815: 0.0429749858846704\n",
      "Train Loss at iteration 12816: 0.0429749403658657\n",
      "Train Loss at iteration 12817: 0.04297489485174996\n",
      "Train Loss at iteration 12818: 0.04297484934232239\n",
      "Train Loss at iteration 12819: 0.04297480383758222\n",
      "Train Loss at iteration 12820: 0.042974758337528667\n",
      "Train Loss at iteration 12821: 0.042974712842160936\n",
      "Train Loss at iteration 12822: 0.04297466735147827\n",
      "Train Loss at iteration 12823: 0.04297462186547987\n",
      "Train Loss at iteration 12824: 0.04297457638416498\n",
      "Train Loss at iteration 12825: 0.04297453090753278\n",
      "Train Loss at iteration 12826: 0.04297448543558254\n",
      "Train Loss at iteration 12827: 0.04297443996831345\n",
      "Train Loss at iteration 12828: 0.042974394505724746\n",
      "Train Loss at iteration 12829: 0.042974349047815635\n",
      "Train Loss at iteration 12830: 0.04297430359458535\n",
      "Train Loss at iteration 12831: 0.04297425814603311\n",
      "Train Loss at iteration 12832: 0.04297421270215812\n",
      "Train Loss at iteration 12833: 0.04297416726295963\n",
      "Train Loss at iteration 12834: 0.042974121828436844\n",
      "Train Loss at iteration 12835: 0.04297407639858899\n",
      "Train Loss at iteration 12836: 0.042974030973415275\n",
      "Train Loss at iteration 12837: 0.04297398555291494\n",
      "Train Loss at iteration 12838: 0.042973940137087205\n",
      "Train Loss at iteration 12839: 0.0429738947259313\n",
      "Train Loss at iteration 12840: 0.04297384931944642\n",
      "Train Loss at iteration 12841: 0.042973803917631816\n",
      "Train Loss at iteration 12842: 0.04297375852048669\n",
      "Train Loss at iteration 12843: 0.04297371312801029\n",
      "Train Loss at iteration 12844: 0.04297366774020181\n",
      "Train Loss at iteration 12845: 0.042973622357060495\n",
      "Train Loss at iteration 12846: 0.04297357697858555\n",
      "Train Loss at iteration 12847: 0.04297353160477622\n",
      "Train Loss at iteration 12848: 0.04297348623563171\n",
      "Train Loss at iteration 12849: 0.04297344087115124\n",
      "Train Loss at iteration 12850: 0.04297339551133408\n",
      "Train Loss at iteration 12851: 0.04297335015617939\n",
      "Train Loss at iteration 12852: 0.04297330480568643\n",
      "Train Loss at iteration 12853: 0.04297325945985442\n",
      "Train Loss at iteration 12854: 0.04297321411868259\n",
      "Train Loss at iteration 12855: 0.04297316878217015\n",
      "Train Loss at iteration 12856: 0.04297312345031633\n",
      "Train Loss at iteration 12857: 0.04297307812312035\n",
      "Train Loss at iteration 12858: 0.04297303280058146\n",
      "Train Loss at iteration 12859: 0.04297298748269885\n",
      "Train Loss at iteration 12860: 0.04297294216947176\n",
      "Train Loss at iteration 12861: 0.042972896860899445\n",
      "Train Loss at iteration 12862: 0.042972851556981086\n",
      "Train Loss at iteration 12863: 0.04297280625771592\n",
      "Train Loss at iteration 12864: 0.04297276096310317\n",
      "Train Loss at iteration 12865: 0.042972715673142085\n",
      "Train Loss at iteration 12866: 0.04297267038783187\n",
      "Train Loss at iteration 12867: 0.04297262510717177\n",
      "Train Loss at iteration 12868: 0.042972579831161\n",
      "Train Loss at iteration 12869: 0.04297253455979876\n",
      "Train Loss at iteration 12870: 0.04297248929308433\n",
      "Train Loss at iteration 12871: 0.042972444031016896\n",
      "Train Loss at iteration 12872: 0.0429723987735957\n",
      "Train Loss at iteration 12873: 0.042972353520819954\n",
      "Train Loss at iteration 12874: 0.042972308272688914\n",
      "Train Loss at iteration 12875: 0.04297226302920178\n",
      "Train Loss at iteration 12876: 0.04297221779035779\n",
      "Train Loss at iteration 12877: 0.04297217255615617\n",
      "Train Loss at iteration 12878: 0.04297212732659616\n",
      "Train Loss at iteration 12879: 0.042972082101676974\n",
      "Train Loss at iteration 12880: 0.042972036881397835\n",
      "Train Loss at iteration 12881: 0.042971991665757985\n",
      "Train Loss at iteration 12882: 0.04297194645475665\n",
      "Train Loss at iteration 12883: 0.04297190124839305\n",
      "Train Loss at iteration 12884: 0.04297185604666642\n",
      "Train Loss at iteration 12885: 0.04297181084957598\n",
      "Train Loss at iteration 12886: 0.04297176565712096\n",
      "Train Loss at iteration 12887: 0.04297172046930061\n",
      "Train Loss at iteration 12888: 0.042971675286114126\n",
      "Train Loss at iteration 12889: 0.04297163010756077\n",
      "Train Loss at iteration 12890: 0.042971584933639735\n",
      "Train Loss at iteration 12891: 0.04297153976435029\n",
      "Train Loss at iteration 12892: 0.04297149459969162\n",
      "Train Loss at iteration 12893: 0.042971449439663\n",
      "Train Loss at iteration 12894: 0.04297140428426364\n",
      "Train Loss at iteration 12895: 0.04297135913349276\n",
      "Train Loss at iteration 12896: 0.0429713139873496\n",
      "Train Loss at iteration 12897: 0.04297126884583339\n",
      "Train Loss at iteration 12898: 0.04297122370894336\n",
      "Train Loss at iteration 12899: 0.042971178576678724\n",
      "Train Loss at iteration 12900: 0.042971133449038745\n",
      "Train Loss at iteration 12901: 0.04297108832602263\n",
      "Train Loss at iteration 12902: 0.04297104320762962\n",
      "Train Loss at iteration 12903: 0.04297099809385894\n",
      "Train Loss at iteration 12904: 0.04297095298470982\n",
      "Train Loss at iteration 12905: 0.04297090788018149\n",
      "Train Loss at iteration 12906: 0.0429708627802732\n",
      "Train Loss at iteration 12907: 0.04297081768498415\n",
      "Train Loss at iteration 12908: 0.04297077259431359\n",
      "Train Loss at iteration 12909: 0.04297072750826076\n",
      "Train Loss at iteration 12910: 0.04297068242682488\n",
      "Train Loss at iteration 12911: 0.042970637350005184\n",
      "Train Loss at iteration 12912: 0.042970592277800895\n",
      "Train Loss at iteration 12913: 0.04297054721021126\n",
      "Train Loss at iteration 12914: 0.04297050214723551\n",
      "Train Loss at iteration 12915: 0.04297045708887286\n",
      "Train Loss at iteration 12916: 0.04297041203512257\n",
      "Train Loss at iteration 12917: 0.042970366985983846\n",
      "Train Loss at iteration 12918: 0.04297032194145593\n",
      "Train Loss at iteration 12919: 0.04297027690153807\n",
      "Train Loss at iteration 12920: 0.042970231866229475\n",
      "Train Loss at iteration 12921: 0.042970186835529396\n",
      "Train Loss at iteration 12922: 0.04297014180943706\n",
      "Train Loss at iteration 12923: 0.042970096787951696\n",
      "Train Loss at iteration 12924: 0.04297005177107255\n",
      "Train Loss at iteration 12925: 0.04297000675879884\n",
      "Train Loss at iteration 12926: 0.04296996175112981\n",
      "Train Loss at iteration 12927: 0.042969916748064704\n",
      "Train Loss at iteration 12928: 0.04296987174960273\n",
      "Train Loss at iteration 12929: 0.042969826755743144\n",
      "Train Loss at iteration 12930: 0.042969781766485166\n",
      "Train Loss at iteration 12931: 0.042969736781828034\n",
      "Train Loss at iteration 12932: 0.042969691801771\n",
      "Train Loss at iteration 12933: 0.04296964682631327\n",
      "Train Loss at iteration 12934: 0.04296960185545409\n",
      "Train Loss at iteration 12935: 0.04296955688919271\n",
      "Train Loss at iteration 12936: 0.042969511927528355\n",
      "Train Loss at iteration 12937: 0.042969466970460266\n",
      "Train Loss at iteration 12938: 0.04296942201798765\n",
      "Train Loss at iteration 12939: 0.042969377070109775\n",
      "Train Loss at iteration 12940: 0.04296933212682587\n",
      "Train Loss at iteration 12941: 0.042969287188135155\n",
      "Train Loss at iteration 12942: 0.04296924225403688\n",
      "Train Loss at iteration 12943: 0.04296919732453029\n",
      "Train Loss at iteration 12944: 0.0429691523996146\n",
      "Train Loss at iteration 12945: 0.04296910747928905\n",
      "Train Loss at iteration 12946: 0.04296906256355289\n",
      "Train Loss at iteration 12947: 0.04296901765240534\n",
      "Train Loss at iteration 12948: 0.04296897274584565\n",
      "Train Loss at iteration 12949: 0.04296892784387306\n",
      "Train Loss at iteration 12950: 0.0429688829464868\n",
      "Train Loss at iteration 12951: 0.0429688380536861\n",
      "Train Loss at iteration 12952: 0.042968793165470205\n",
      "Train Loss at iteration 12953: 0.042968748281838345\n",
      "Train Loss at iteration 12954: 0.042968703402789764\n",
      "Train Loss at iteration 12955: 0.042968658528323704\n",
      "Train Loss at iteration 12956: 0.0429686136584394\n",
      "Train Loss at iteration 12957: 0.04296856879313608\n",
      "Train Loss at iteration 12958: 0.042968523932412996\n",
      "Train Loss at iteration 12959: 0.042968479076269384\n",
      "Train Loss at iteration 12960: 0.04296843422470446\n",
      "Train Loss at iteration 12961: 0.04296838937771749\n",
      "Train Loss at iteration 12962: 0.042968344535307706\n",
      "Train Loss at iteration 12963: 0.04296829969747436\n",
      "Train Loss at iteration 12964: 0.04296825486421665\n",
      "Train Loss at iteration 12965: 0.04296821003553384\n",
      "Train Loss at iteration 12966: 0.04296816521142518\n",
      "Train Loss at iteration 12967: 0.042968120391889894\n",
      "Train Loss at iteration 12968: 0.04296807557692722\n",
      "Train Loss at iteration 12969: 0.0429680307665364\n",
      "Train Loss at iteration 12970: 0.04296798596071667\n",
      "Train Loss at iteration 12971: 0.04296794115946728\n",
      "Train Loss at iteration 12972: 0.042967896362787486\n",
      "Train Loss at iteration 12973: 0.04296785157067649\n",
      "Train Loss at iteration 12974: 0.04296780678313354\n",
      "Train Loss at iteration 12975: 0.042967762000157894\n",
      "Train Loss at iteration 12976: 0.04296771722174879\n",
      "Train Loss at iteration 12977: 0.042967672447905454\n",
      "Train Loss at iteration 12978: 0.04296762767862714\n",
      "Train Loss at iteration 12979: 0.042967582913913074\n",
      "Train Loss at iteration 12980: 0.04296753815376251\n",
      "Train Loss at iteration 12981: 0.04296749339817468\n",
      "Train Loss at iteration 12982: 0.04296744864714885\n",
      "Train Loss at iteration 12983: 0.04296740390068422\n",
      "Train Loss at iteration 12984: 0.04296735915878006\n",
      "Train Loss at iteration 12985: 0.04296731442143561\n",
      "Train Loss at iteration 12986: 0.042967269688650105\n",
      "Train Loss at iteration 12987: 0.04296722496042277\n",
      "Train Loss at iteration 12988: 0.04296718023675287\n",
      "Train Loss at iteration 12989: 0.04296713551763965\n",
      "Train Loss at iteration 12990: 0.04296709080308235\n",
      "Train Loss at iteration 12991: 0.04296704609308019\n",
      "Train Loss at iteration 12992: 0.04296700138763244\n",
      "Train Loss at iteration 12993: 0.04296695668673831\n",
      "Train Loss at iteration 12994: 0.04296691199039708\n",
      "Train Loss at iteration 12995: 0.04296686729860798\n",
      "Train Loss at iteration 12996: 0.04296682261137024\n",
      "Train Loss at iteration 12997: 0.04296677792868311\n",
      "Train Loss at iteration 12998: 0.04296673325054584\n",
      "Train Loss at iteration 12999: 0.04296668857695766\n",
      "Train Loss at iteration 13000: 0.04296664390791783\n",
      "Train Loss at iteration 13001: 0.04296659924342558\n",
      "Train Loss at iteration 13002: 0.04296655458348016\n",
      "Train Loss at iteration 13003: 0.042966509928080814\n",
      "Train Loss at iteration 13004: 0.04296646527722677\n",
      "Train Loss at iteration 13005: 0.04296642063091732\n",
      "Train Loss at iteration 13006: 0.04296637598915164\n",
      "Train Loss at iteration 13007: 0.04296633135192901\n",
      "Train Loss at iteration 13008: 0.042966286719248695\n",
      "Train Loss at iteration 13009: 0.0429662420911099\n",
      "Train Loss at iteration 13010: 0.042966197467511895\n",
      "Train Loss at iteration 13011: 0.04296615284845392\n",
      "Train Loss at iteration 13012: 0.04296610823393522\n",
      "Train Loss at iteration 13013: 0.04296606362395502\n",
      "Train Loss at iteration 13014: 0.04296601901851259\n",
      "Train Loss at iteration 13015: 0.042965974417607175\n",
      "Train Loss at iteration 13016: 0.042965929821238\n",
      "Train Loss at iteration 13017: 0.042965885229404324\n",
      "Train Loss at iteration 13018: 0.042965840642105395\n",
      "Train Loss at iteration 13019: 0.042965796059340464\n",
      "Train Loss at iteration 13020: 0.04296575148110877\n",
      "Train Loss at iteration 13021: 0.04296570690740954\n",
      "Train Loss at iteration 13022: 0.04296566233824205\n",
      "Train Loss at iteration 13023: 0.04296561777360554\n",
      "Train Loss at iteration 13024: 0.042965573213499246\n",
      "Train Loss at iteration 13025: 0.042965528657922414\n",
      "Train Loss at iteration 13026: 0.04296548410687431\n",
      "Train Loss at iteration 13027: 0.042965439560354146\n",
      "Train Loss at iteration 13028: 0.04296539501836119\n",
      "Train Loss at iteration 13029: 0.04296535048089472\n",
      "Train Loss at iteration 13030: 0.04296530594795392\n",
      "Train Loss at iteration 13031: 0.042965261419538076\n",
      "Train Loss at iteration 13032: 0.04296521689564643\n",
      "Train Loss at iteration 13033: 0.04296517237627823\n",
      "Train Loss at iteration 13034: 0.04296512786143273\n",
      "Train Loss at iteration 13035: 0.042965083351109164\n",
      "Train Loss at iteration 13036: 0.04296503884530678\n",
      "Train Loss at iteration 13037: 0.04296499434402484\n",
      "Train Loss at iteration 13038: 0.04296494984726258\n",
      "Train Loss at iteration 13039: 0.04296490535501926\n",
      "Train Loss at iteration 13040: 0.04296486086729412\n",
      "Train Loss at iteration 13041: 0.0429648163840864\n",
      "Train Loss at iteration 13042: 0.04296477190539536\n",
      "Train Loss at iteration 13043: 0.042964727431220255\n",
      "Train Loss at iteration 13044: 0.04296468296156032\n",
      "Train Loss at iteration 13045: 0.04296463849641482\n",
      "Train Loss at iteration 13046: 0.042964594035782976\n",
      "Train Loss at iteration 13047: 0.042964549579664064\n",
      "Train Loss at iteration 13048: 0.042964505128057336\n",
      "Train Loss at iteration 13049: 0.042964460680962026\n",
      "Train Loss at iteration 13050: 0.04296441623837738\n",
      "Train Loss at iteration 13051: 0.04296437180030267\n",
      "Train Loss at iteration 13052: 0.04296432736673713\n",
      "Train Loss at iteration 13053: 0.04296428293768001\n",
      "Train Loss at iteration 13054: 0.04296423851313057\n",
      "Train Loss at iteration 13055: 0.042964194093088034\n",
      "Train Loss at iteration 13056: 0.04296414967755168\n",
      "Train Loss at iteration 13057: 0.04296410526652076\n",
      "Train Loss at iteration 13058: 0.042964060859994505\n",
      "Train Loss at iteration 13059: 0.04296401645797219\n",
      "Train Loss at iteration 13060: 0.042963972060453035\n",
      "Train Loss at iteration 13061: 0.04296392766743633\n",
      "Train Loss at iteration 13062: 0.04296388327892128\n",
      "Train Loss at iteration 13063: 0.04296383889490717\n",
      "Train Loss at iteration 13064: 0.04296379451539325\n",
      "Train Loss at iteration 13065: 0.042963750140378766\n",
      "Train Loss at iteration 13066: 0.04296370576986295\n",
      "Train Loss at iteration 13067: 0.04296366140384508\n",
      "Train Loss at iteration 13068: 0.04296361704232442\n",
      "Train Loss at iteration 13069: 0.042963572685300176\n",
      "Train Loss at iteration 13070: 0.04296352833277164\n",
      "Train Loss at iteration 13071: 0.04296348398473804\n",
      "Train Loss at iteration 13072: 0.042963439641198656\n",
      "Train Loss at iteration 13073: 0.042963395302152714\n",
      "Train Loss at iteration 13074: 0.04296335096759947\n",
      "Train Loss at iteration 13075: 0.04296330663753819\n",
      "Train Loss at iteration 13076: 0.04296326231196812\n",
      "Train Loss at iteration 13077: 0.042963217990888515\n",
      "Train Loss at iteration 13078: 0.04296317367429863\n",
      "Train Loss at iteration 13079: 0.0429631293621977\n",
      "Train Loss at iteration 13080: 0.042963085054585015\n",
      "Train Loss at iteration 13081: 0.0429630407514598\n",
      "Train Loss at iteration 13082: 0.04296299645282132\n",
      "Train Loss at iteration 13083: 0.04296295215866881\n",
      "Train Loss at iteration 13084: 0.04296290786900156\n",
      "Train Loss at iteration 13085: 0.04296286358381879\n",
      "Train Loss at iteration 13086: 0.04296281930311977\n",
      "Train Loss at iteration 13087: 0.04296277502690375\n",
      "Train Loss at iteration 13088: 0.042962730755169994\n",
      "Train Loss at iteration 13089: 0.04296268648791774\n",
      "Train Loss at iteration 13090: 0.04296264222514626\n",
      "Train Loss at iteration 13091: 0.0429625979668548\n",
      "Train Loss at iteration 13092: 0.04296255371304261\n",
      "Train Loss at iteration 13093: 0.04296250946370897\n",
      "Train Loss at iteration 13094: 0.0429624652188531\n",
      "Train Loss at iteration 13095: 0.042962420978474275\n",
      "Train Loss at iteration 13096: 0.04296237674257175\n",
      "Train Loss at iteration 13097: 0.04296233251114477\n",
      "Train Loss at iteration 13098: 0.042962288284192615\n",
      "Train Loss at iteration 13099: 0.04296224406171452\n",
      "Train Loss at iteration 13100: 0.04296219984370974\n",
      "Train Loss at iteration 13101: 0.04296215563017752\n",
      "Train Loss at iteration 13102: 0.042962111421117166\n",
      "Train Loss at iteration 13103: 0.04296206721652789\n",
      "Train Loss at iteration 13104: 0.042962023016408966\n",
      "Train Loss at iteration 13105: 0.04296197882075963\n",
      "Train Loss at iteration 13106: 0.04296193462957917\n",
      "Train Loss at iteration 13107: 0.04296189044286682\n",
      "Train Loss at iteration 13108: 0.04296184626062184\n",
      "Train Loss at iteration 13109: 0.0429618020828435\n",
      "Train Loss at iteration 13110: 0.042961757909531044\n",
      "Train Loss at iteration 13111: 0.04296171374068374\n",
      "Train Loss at iteration 13112: 0.042961669576300834\n",
      "Train Loss at iteration 13113: 0.04296162541638159\n",
      "Train Loss at iteration 13114: 0.04296158126092525\n",
      "Train Loss at iteration 13115: 0.04296153710993111\n",
      "Train Loss at iteration 13116: 0.04296149296339839\n",
      "Train Loss at iteration 13117: 0.042961448821326374\n",
      "Train Loss at iteration 13118: 0.0429614046837143\n",
      "Train Loss at iteration 13119: 0.04296136055056145\n",
      "Train Loss at iteration 13120: 0.042961316421867055\n",
      "Train Loss at iteration 13121: 0.04296127229763039\n",
      "Train Loss at iteration 13122: 0.04296122817785072\n",
      "Train Loss at iteration 13123: 0.04296118406252728\n",
      "Train Loss at iteration 13124: 0.04296113995165936\n",
      "Train Loss at iteration 13125: 0.04296109584524618\n",
      "Train Loss at iteration 13126: 0.04296105174328705\n",
      "Train Loss at iteration 13127: 0.04296100764578119\n",
      "Train Loss at iteration 13128: 0.04296096355272787\n",
      "Train Loss at iteration 13129: 0.04296091946412635\n",
      "Train Loss at iteration 13130: 0.04296087537997589\n",
      "Train Loss at iteration 13131: 0.04296083130027576\n",
      "Train Loss at iteration 13132: 0.04296078722502521\n",
      "Train Loss at iteration 13133: 0.04296074315422349\n",
      "Train Loss at iteration 13134: 0.042960699087869876\n",
      "Train Loss at iteration 13135: 0.04296065502596363\n",
      "Train Loss at iteration 13136: 0.042960610968503996\n",
      "Train Loss at iteration 13137: 0.04296056691549025\n",
      "Train Loss at iteration 13138: 0.042960522866921654\n",
      "Train Loss at iteration 13139: 0.04296047882279745\n",
      "Train Loss at iteration 13140: 0.042960434783116924\n",
      "Train Loss at iteration 13141: 0.042960390747879315\n",
      "Train Loss at iteration 13142: 0.04296034671708389\n",
      "Train Loss at iteration 13143: 0.04296030269072994\n",
      "Train Loss at iteration 13144: 0.042960258668816684\n",
      "Train Loss at iteration 13145: 0.042960214651343406\n",
      "Train Loss at iteration 13146: 0.042960170638309356\n",
      "Train Loss at iteration 13147: 0.04296012662971381\n",
      "Train Loss at iteration 13148: 0.04296008262555601\n",
      "Train Loss at iteration 13149: 0.042960038625835244\n",
      "Train Loss at iteration 13150: 0.04295999463055076\n",
      "Train Loss at iteration 13151: 0.042959950639701816\n",
      "Train Loss at iteration 13152: 0.04295990665328768\n",
      "Train Loss at iteration 13153: 0.04295986267130762\n",
      "Train Loss at iteration 13154: 0.0429598186937609\n",
      "Train Loss at iteration 13155: 0.042959774720646764\n",
      "Train Loss at iteration 13156: 0.0429597307519645\n",
      "Train Loss at iteration 13157: 0.04295968678771335\n",
      "Train Loss at iteration 13158: 0.04295964282789259\n",
      "Train Loss at iteration 13159: 0.04295959887250147\n",
      "Train Loss at iteration 13160: 0.04295955492153927\n",
      "Train Loss at iteration 13161: 0.04295951097500525\n",
      "Train Loss at iteration 13162: 0.04295946703289867\n",
      "Train Loss at iteration 13163: 0.042959423095218795\n",
      "Train Loss at iteration 13164: 0.04295937916196488\n",
      "Train Loss at iteration 13165: 0.0429593352331362\n",
      "Train Loss at iteration 13166: 0.04295929130873203\n",
      "Train Loss at iteration 13167: 0.0429592473887516\n",
      "Train Loss at iteration 13168: 0.04295920347319422\n",
      "Train Loss at iteration 13169: 0.04295915956205912\n",
      "Train Loss at iteration 13170: 0.042959115655345576\n",
      "Train Loss at iteration 13171: 0.042959071753052844\n",
      "Train Loss at iteration 13172: 0.04295902785518021\n",
      "Train Loss at iteration 13173: 0.04295898396172691\n",
      "Train Loss at iteration 13174: 0.04295894007269224\n",
      "Train Loss at iteration 13175: 0.04295889618807544\n",
      "Train Loss at iteration 13176: 0.042958852307875814\n",
      "Train Loss at iteration 13177: 0.04295880843209257\n",
      "Train Loss at iteration 13178: 0.042958764560725025\n",
      "Train Loss at iteration 13179: 0.042958720693772406\n",
      "Train Loss at iteration 13180: 0.042958676831234006\n",
      "Train Loss at iteration 13181: 0.04295863297310909\n",
      "Train Loss at iteration 13182: 0.0429585891193969\n",
      "Train Loss at iteration 13183: 0.042958545270096724\n",
      "Train Loss at iteration 13184: 0.04295850142520782\n",
      "Train Loss at iteration 13185: 0.04295845758472947\n",
      "Train Loss at iteration 13186: 0.042958413748660926\n",
      "Train Loss at iteration 13187: 0.04295836991700146\n",
      "Train Loss at iteration 13188: 0.04295832608975032\n",
      "Train Loss at iteration 13189: 0.04295828226690681\n",
      "Train Loss at iteration 13190: 0.042958238448470173\n",
      "Train Loss at iteration 13191: 0.04295819463443968\n",
      "Train Loss at iteration 13192: 0.04295815082481459\n",
      "Train Loss at iteration 13193: 0.04295810701959419\n",
      "Train Loss at iteration 13194: 0.04295806321877773\n",
      "Train Loss at iteration 13195: 0.04295801942236449\n",
      "Train Loss at iteration 13196: 0.04295797563035373\n",
      "Train Loss at iteration 13197: 0.04295793184274472\n",
      "Train Loss at iteration 13198: 0.04295788805953673\n",
      "Train Loss at iteration 13199: 0.04295784428072903\n",
      "Train Loss at iteration 13200: 0.042957800506320885\n",
      "Train Loss at iteration 13201: 0.042957756736311574\n",
      "Train Loss at iteration 13202: 0.04295771297070034\n",
      "Train Loss at iteration 13203: 0.04295766920948649\n",
      "Train Loss at iteration 13204: 0.04295762545266925\n",
      "Train Loss at iteration 13205: 0.04295758170024793\n",
      "Train Loss at iteration 13206: 0.042957537952221764\n",
      "Train Loss at iteration 13207: 0.042957494208590054\n",
      "Train Loss at iteration 13208: 0.04295745046935204\n",
      "Train Loss at iteration 13209: 0.042957406734507\n",
      "Train Loss at iteration 13210: 0.04295736300405423\n",
      "Train Loss at iteration 13211: 0.042957319277992956\n",
      "Train Loss at iteration 13212: 0.04295727555632249\n",
      "Train Loss at iteration 13213: 0.042957231839042063\n",
      "Train Loss at iteration 13214: 0.04295718812615097\n",
      "Train Loss at iteration 13215: 0.04295714441764849\n",
      "Train Loss at iteration 13216: 0.04295710071353386\n",
      "Train Loss at iteration 13217: 0.04295705701380638\n",
      "Train Loss at iteration 13218: 0.042957013318465305\n",
      "Train Loss at iteration 13219: 0.04295696962750991\n",
      "Train Loss at iteration 13220: 0.04295692594093948\n",
      "Train Loss at iteration 13221: 0.042956882258753264\n",
      "Train Loss at iteration 13222: 0.04295683858095054\n",
      "Train Loss at iteration 13223: 0.04295679490753059\n",
      "Train Loss at iteration 13224: 0.04295675123849268\n",
      "Train Loss at iteration 13225: 0.042956707573836066\n",
      "Train Loss at iteration 13226: 0.04295666391356004\n",
      "Train Loss at iteration 13227: 0.04295662025766386\n",
      "Train Loss at iteration 13228: 0.04295657660614681\n",
      "Train Loss at iteration 13229: 0.04295653295900816\n",
      "Train Loss at iteration 13230: 0.042956489316247175\n",
      "Train Loss at iteration 13231: 0.042956445677863137\n",
      "Train Loss at iteration 13232: 0.04295640204385531\n",
      "Train Loss at iteration 13233: 0.042956358414222955\n",
      "Train Loss at iteration 13234: 0.04295631478896537\n",
      "Train Loss at iteration 13235: 0.04295627116808182\n",
      "Train Loss at iteration 13236: 0.04295622755157157\n",
      "Train Loss at iteration 13237: 0.042956183939433894\n",
      "Train Loss at iteration 13238: 0.042956140331668065\n",
      "Train Loss at iteration 13239: 0.04295609672827337\n",
      "Train Loss at iteration 13240: 0.04295605312924907\n",
      "Train Loss at iteration 13241: 0.04295600953459444\n",
      "Train Loss at iteration 13242: 0.04295596594430876\n",
      "Train Loss at iteration 13243: 0.04295592235839129\n",
      "Train Loss at iteration 13244: 0.042955878776841316\n",
      "Train Loss at iteration 13245: 0.04295583519965811\n",
      "Train Loss at iteration 13246: 0.04295579162684095\n",
      "Train Loss at iteration 13247: 0.04295574805838908\n",
      "Train Loss at iteration 13248: 0.042955704494301826\n",
      "Train Loss at iteration 13249: 0.042955660934578414\n",
      "Train Loss at iteration 13250: 0.04295561737921815\n",
      "Train Loss at iteration 13251: 0.0429555738282203\n",
      "Train Loss at iteration 13252: 0.04295553028158414\n",
      "Train Loss at iteration 13253: 0.042955486739308935\n",
      "Train Loss at iteration 13254: 0.04295544320139398\n",
      "Train Loss at iteration 13255: 0.04295539966783852\n",
      "Train Loss at iteration 13256: 0.04295535613864187\n",
      "Train Loss at iteration 13257: 0.04295531261380327\n",
      "Train Loss at iteration 13258: 0.042955269093322014\n",
      "Train Loss at iteration 13259: 0.04295522557719737\n",
      "Train Loss at iteration 13260: 0.042955182065428615\n",
      "Train Loss at iteration 13261: 0.042955138558015034\n",
      "Train Loss at iteration 13262: 0.0429550950549559\n",
      "Train Loss at iteration 13263: 0.04295505155625047\n",
      "Train Loss at iteration 13264: 0.04295500806189805\n",
      "Train Loss at iteration 13265: 0.042954964571897895\n",
      "Train Loss at iteration 13266: 0.0429549210862493\n",
      "Train Loss at iteration 13267: 0.04295487760495152\n",
      "Train Loss at iteration 13268: 0.042954834128003834\n",
      "Train Loss at iteration 13269: 0.04295479065540554\n",
      "Train Loss at iteration 13270: 0.0429547471871559\n",
      "Train Loss at iteration 13271: 0.042954703723254196\n",
      "Train Loss at iteration 13272: 0.0429546602636997\n",
      "Train Loss at iteration 13273: 0.042954616808491705\n",
      "Train Loss at iteration 13274: 0.04295457335762947\n",
      "Train Loss at iteration 13275: 0.04295452991111226\n",
      "Train Loss at iteration 13276: 0.042954486468939394\n",
      "Train Loss at iteration 13277: 0.042954443031110116\n",
      "Train Loss at iteration 13278: 0.04295439959762372\n",
      "Train Loss at iteration 13279: 0.04295435616847946\n",
      "Train Loss at iteration 13280: 0.04295431274367666\n",
      "Train Loss at iteration 13281: 0.04295426932321456\n",
      "Train Loss at iteration 13282: 0.042954225907092455\n",
      "Train Loss at iteration 13283: 0.04295418249530962\n",
      "Train Loss at iteration 13284: 0.04295413908786534\n",
      "Train Loss at iteration 13285: 0.04295409568475887\n",
      "Train Loss at iteration 13286: 0.042954052285989516\n",
      "Train Loss at iteration 13287: 0.042954008891556546\n",
      "Train Loss at iteration 13288: 0.042953965501459246\n",
      "Train Loss at iteration 13289: 0.04295392211569688\n",
      "Train Loss at iteration 13290: 0.04295387873426875\n",
      "Train Loss at iteration 13291: 0.042953835357174106\n",
      "Train Loss at iteration 13292: 0.042953791984412265\n",
      "Train Loss at iteration 13293: 0.04295374861598247\n",
      "Train Loss at iteration 13294: 0.04295370525188402\n",
      "Train Loss at iteration 13295: 0.042953661892116204\n",
      "Train Loss at iteration 13296: 0.0429536185366783\n",
      "Train Loss at iteration 13297: 0.042953575185569554\n",
      "Train Loss at iteration 13298: 0.042953531838789284\n",
      "Train Loss at iteration 13299: 0.042953488496336754\n",
      "Train Loss at iteration 13300: 0.04295344515821126\n",
      "Train Loss at iteration 13301: 0.04295340182441207\n",
      "Train Loss at iteration 13302: 0.04295335849493846\n",
      "Train Loss at iteration 13303: 0.04295331516978972\n",
      "Train Loss at iteration 13304: 0.04295327184896513\n",
      "Train Loss at iteration 13305: 0.04295322853246396\n",
      "Train Loss at iteration 13306: 0.04295318522028551\n",
      "Train Loss at iteration 13307: 0.042953141912429056\n",
      "Train Loss at iteration 13308: 0.04295309860889388\n",
      "Train Loss at iteration 13309: 0.04295305530967925\n",
      "Train Loss at iteration 13310: 0.042953012014784485\n",
      "Train Loss at iteration 13311: 0.04295296872420882\n",
      "Train Loss at iteration 13312: 0.04295292543795155\n",
      "Train Loss at iteration 13313: 0.04295288215601198\n",
      "Train Loss at iteration 13314: 0.04295283887838939\n",
      "Train Loss at iteration 13315: 0.042952795605083034\n",
      "Train Loss at iteration 13316: 0.04295275233609221\n",
      "Train Loss at iteration 13317: 0.042952709071416205\n",
      "Train Loss at iteration 13318: 0.0429526658110543\n",
      "Train Loss at iteration 13319: 0.04295262255500578\n",
      "Train Loss at iteration 13320: 0.04295257930326993\n",
      "Train Loss at iteration 13321: 0.042952536055846015\n",
      "Train Loss at iteration 13322: 0.04295249281273334\n",
      "Train Loss at iteration 13323: 0.04295244957393118\n",
      "Train Loss at iteration 13324: 0.042952406339438816\n",
      "Train Loss at iteration 13325: 0.04295236310925553\n",
      "Train Loss at iteration 13326: 0.04295231988338062\n",
      "Train Loss at iteration 13327: 0.04295227666181335\n",
      "Train Loss at iteration 13328: 0.04295223344455301\n",
      "Train Loss at iteration 13329: 0.04295219023159891\n",
      "Train Loss at iteration 13330: 0.042952147022950304\n",
      "Train Loss at iteration 13331: 0.04295210381860649\n",
      "Train Loss at iteration 13332: 0.04295206061856675\n",
      "Train Loss at iteration 13333: 0.04295201742283035\n",
      "Train Loss at iteration 13334: 0.04295197423139659\n",
      "Train Loss at iteration 13335: 0.042951931044264785\n",
      "Train Loss at iteration 13336: 0.04295188786143416\n",
      "Train Loss at iteration 13337: 0.04295184468290405\n",
      "Train Loss at iteration 13338: 0.04295180150867372\n",
      "Train Loss at iteration 13339: 0.04295175833874246\n",
      "Train Loss at iteration 13340: 0.04295171517310955\n",
      "Train Loss at iteration 13341: 0.04295167201177427\n",
      "Train Loss at iteration 13342: 0.04295162885473593\n",
      "Train Loss at iteration 13343: 0.0429515857019938\n",
      "Train Loss at iteration 13344: 0.04295154255354715\n",
      "Train Loss at iteration 13345: 0.042951499409395306\n",
      "Train Loss at iteration 13346: 0.04295145626953751\n",
      "Train Loss at iteration 13347: 0.042951413133973076\n",
      "Train Loss at iteration 13348: 0.04295137000270129\n",
      "Train Loss at iteration 13349: 0.04295132687572143\n",
      "Train Loss at iteration 13350: 0.04295128375303279\n",
      "Train Loss at iteration 13351: 0.042951240634634645\n",
      "Train Loss at iteration 13352: 0.04295119752052629\n",
      "Train Loss at iteration 13353: 0.04295115441070701\n",
      "Train Loss at iteration 13354: 0.04295111130517609\n",
      "Train Loss at iteration 13355: 0.04295106820393284\n",
      "Train Loss at iteration 13356: 0.04295102510697651\n",
      "Train Loss at iteration 13357: 0.04295098201430641\n",
      "Train Loss at iteration 13358: 0.042950938925921826\n",
      "Train Loss at iteration 13359: 0.042950895841822045\n",
      "Train Loss at iteration 13360: 0.04295085276200634\n",
      "Train Loss at iteration 13361: 0.04295080968647402\n",
      "Train Loss at iteration 13362: 0.04295076661522438\n",
      "Train Loss at iteration 13363: 0.04295072354825668\n",
      "Train Loss at iteration 13364: 0.04295068048557023\n",
      "Train Loss at iteration 13365: 0.0429506374271643\n",
      "Train Loss at iteration 13366: 0.042950594373038206\n",
      "Train Loss at iteration 13367: 0.042950551323191206\n",
      "Train Loss at iteration 13368: 0.042950508277622604\n",
      "Train Loss at iteration 13369: 0.042950465236331704\n",
      "Train Loss at iteration 13370: 0.042950422199317766\n",
      "Train Loss at iteration 13371: 0.042950379166580094\n",
      "Train Loss at iteration 13372: 0.04295033613811797\n",
      "Train Loss at iteration 13373: 0.0429502931139307\n",
      "Train Loss at iteration 13374: 0.04295025009401757\n",
      "Train Loss at iteration 13375: 0.04295020707837784\n",
      "Train Loss at iteration 13376: 0.04295016406701084\n",
      "Train Loss at iteration 13377: 0.042950121059915844\n",
      "Train Loss at iteration 13378: 0.04295007805709213\n",
      "Train Loss at iteration 13379: 0.04295003505853901\n",
      "Train Loss at iteration 13380: 0.04294999206425577\n",
      "Train Loss at iteration 13381: 0.042949949074241676\n",
      "Train Loss at iteration 13382: 0.04294990608849605\n",
      "Train Loss at iteration 13383: 0.04294986310701816\n",
      "Train Loss at iteration 13384: 0.0429498201298073\n",
      "Train Loss at iteration 13385: 0.04294977715686279\n",
      "Train Loss at iteration 13386: 0.04294973418818388\n",
      "Train Loss at iteration 13387: 0.04294969122376989\n",
      "Train Loss at iteration 13388: 0.04294964826362009\n",
      "Train Loss at iteration 13389: 0.042949605307733785\n",
      "Train Loss at iteration 13390: 0.04294956235611027\n",
      "Train Loss at iteration 13391: 0.04294951940874883\n",
      "Train Loss at iteration 13392: 0.04294947646564875\n",
      "Train Loss at iteration 13393: 0.04294943352680935\n",
      "Train Loss at iteration 13394: 0.04294939059222988\n",
      "Train Loss at iteration 13395: 0.04294934766190965\n",
      "Train Loss at iteration 13396: 0.04294930473584797\n",
      "Train Loss at iteration 13397: 0.042949261814044105\n",
      "Train Loss at iteration 13398: 0.04294921889649738\n",
      "Train Loss at iteration 13399: 0.04294917598320704\n",
      "Train Loss at iteration 13400: 0.04294913307417243\n",
      "Train Loss at iteration 13401: 0.04294909016939281\n",
      "Train Loss at iteration 13402: 0.042949047268867474\n",
      "Train Loss at iteration 13403: 0.04294900437259574\n",
      "Train Loss at iteration 13404: 0.04294896148057686\n",
      "Train Loss at iteration 13405: 0.042948918592810165\n",
      "Train Loss at iteration 13406: 0.04294887570929494\n",
      "Train Loss at iteration 13407: 0.04294883283003047\n",
      "Train Loss at iteration 13408: 0.04294878995501604\n",
      "Train Loss at iteration 13409: 0.04294874708425096\n",
      "Train Loss at iteration 13410: 0.04294870421773453\n",
      "Train Loss at iteration 13411: 0.04294866135546602\n",
      "Train Loss at iteration 13412: 0.042948618497444746\n",
      "Train Loss at iteration 13413: 0.042948575643669995\n",
      "Train Loss at iteration 13414: 0.042948532794141056\n",
      "Train Loss at iteration 13415: 0.04294848994885723\n",
      "Train Loss at iteration 13416: 0.042948447107817804\n",
      "Train Loss at iteration 13417: 0.04294840427102209\n",
      "Train Loss at iteration 13418: 0.04294836143846936\n",
      "Train Loss at iteration 13419: 0.04294831861015892\n",
      "Train Loss at iteration 13420: 0.042948275786090076\n",
      "Train Loss at iteration 13421: 0.04294823296626211\n",
      "Train Loss at iteration 13422: 0.04294819015067431\n",
      "Train Loss at iteration 13423: 0.04294814733932599\n",
      "Train Loss at iteration 13424: 0.04294810453221643\n",
      "Train Loss at iteration 13425: 0.04294806172934494\n",
      "Train Loss at iteration 13426: 0.042948018930710805\n",
      "Train Loss at iteration 13427: 0.04294797613631332\n",
      "Train Loss at iteration 13428: 0.0429479333461518\n",
      "Train Loss at iteration 13429: 0.042947890560225514\n",
      "Train Loss at iteration 13430: 0.04294784777853377\n",
      "Train Loss at iteration 13431: 0.04294780500107587\n",
      "Train Loss at iteration 13432: 0.042947762227851106\n",
      "Train Loss at iteration 13433: 0.04294771945885878\n",
      "Train Loss at iteration 13434: 0.042947676694098165\n",
      "Train Loss at iteration 13435: 0.04294763393356859\n",
      "Train Loss at iteration 13436: 0.042947591177269324\n",
      "Train Loss at iteration 13437: 0.042947548425199704\n",
      "Train Loss at iteration 13438: 0.04294750567735898\n",
      "Train Loss at iteration 13439: 0.042947462933746475\n",
      "Train Loss at iteration 13440: 0.04294742019436149\n",
      "Train Loss at iteration 13441: 0.04294737745920331\n",
      "Train Loss at iteration 13442: 0.04294733472827123\n",
      "Train Loss at iteration 13443: 0.04294729200156457\n",
      "Train Loss at iteration 13444: 0.042947249279082604\n",
      "Train Loss at iteration 13445: 0.04294720656082464\n",
      "Train Loss at iteration 13446: 0.04294716384678998\n",
      "Train Loss at iteration 13447: 0.042947121136977906\n",
      "Train Loss at iteration 13448: 0.042947078431387745\n",
      "Train Loss at iteration 13449: 0.042947035730018776\n",
      "Train Loss at iteration 13450: 0.042946993032870286\n",
      "Train Loss at iteration 13451: 0.04294695033994161\n",
      "Train Loss at iteration 13452: 0.04294690765123201\n",
      "Train Loss at iteration 13453: 0.0429468649667408\n",
      "Train Loss at iteration 13454: 0.04294682228646728\n",
      "Train Loss at iteration 13455: 0.04294677961041076\n",
      "Train Loss at iteration 13456: 0.042946736938570505\n",
      "Train Loss at iteration 13457: 0.042946694270945864\n",
      "Train Loss at iteration 13458: 0.04294665160753609\n",
      "Train Loss at iteration 13459: 0.04294660894834051\n",
      "Train Loss at iteration 13460: 0.04294656629335842\n",
      "Train Loss at iteration 13461: 0.04294652364258912\n",
      "Train Loss at iteration 13462: 0.0429464809960319\n",
      "Train Loss at iteration 13463: 0.04294643835368606\n",
      "Train Loss at iteration 13464: 0.042946395715550915\n",
      "Train Loss at iteration 13465: 0.042946353081625754\n",
      "Train Loss at iteration 13466: 0.042946310451909886\n",
      "Train Loss at iteration 13467: 0.042946267826402595\n",
      "Train Loss at iteration 13468: 0.04294622520510321\n",
      "Train Loss at iteration 13469: 0.042946182588011014\n",
      "Train Loss at iteration 13470: 0.042946139975125294\n",
      "Train Loss at iteration 13471: 0.04294609736644538\n",
      "Train Loss at iteration 13472: 0.04294605476197057\n",
      "Train Loss at iteration 13473: 0.04294601216170013\n",
      "Train Loss at iteration 13474: 0.042945969565633395\n",
      "Train Loss at iteration 13475: 0.042945926973769664\n",
      "Train Loss at iteration 13476: 0.04294588438610823\n",
      "Train Loss at iteration 13477: 0.04294584180264839\n",
      "Train Loss at iteration 13478: 0.042945799223389466\n",
      "Train Loss at iteration 13479: 0.04294575664833075\n",
      "Train Loss at iteration 13480: 0.04294571407747154\n",
      "Train Loss at iteration 13481: 0.042945671510811134\n",
      "Train Loss at iteration 13482: 0.04294562894834884\n",
      "Train Loss at iteration 13483: 0.042945586390083966\n",
      "Train Loss at iteration 13484: 0.0429455438360158\n",
      "Train Loss at iteration 13485: 0.04294550128614366\n",
      "Train Loss at iteration 13486: 0.04294545874046685\n",
      "Train Loss at iteration 13487: 0.04294541619898466\n",
      "Train Loss at iteration 13488: 0.04294537366169641\n",
      "Train Loss at iteration 13489: 0.042945331128601376\n",
      "Train Loss at iteration 13490: 0.042945288599698886\n",
      "Train Loss at iteration 13491: 0.04294524607498824\n",
      "Train Loss at iteration 13492: 0.04294520355446873\n",
      "Train Loss at iteration 13493: 0.04294516103813968\n",
      "Train Loss at iteration 13494: 0.042945118526000366\n",
      "Train Loss at iteration 13495: 0.04294507601805011\n",
      "Train Loss at iteration 13496: 0.042945033514288224\n",
      "Train Loss at iteration 13497: 0.042944991014713996\n",
      "Train Loss at iteration 13498: 0.042944948519326724\n",
      "Train Loss at iteration 13499: 0.04294490602812574\n",
      "Train Loss at iteration 13500: 0.04294486354111032\n",
      "Train Loss at iteration 13501: 0.04294482105827979\n",
      "Train Loss at iteration 13502: 0.04294477857963344\n",
      "Train Loss at iteration 13503: 0.04294473610517058\n",
      "Train Loss at iteration 13504: 0.042944693634890514\n",
      "Train Loss at iteration 13505: 0.042944651168792564\n",
      "Train Loss at iteration 13506: 0.042944608706876004\n",
      "Train Loss at iteration 13507: 0.04294456624914015\n",
      "Train Loss at iteration 13508: 0.042944523795584326\n",
      "Train Loss at iteration 13509: 0.042944481346207826\n",
      "Train Loss at iteration 13510: 0.042944438901009956\n",
      "Train Loss at iteration 13511: 0.04294439645999\n",
      "Train Loss at iteration 13512: 0.0429443540231473\n",
      "Train Loss at iteration 13513: 0.04294431159048115\n",
      "Train Loss at iteration 13514: 0.042944269161990845\n",
      "Train Loss at iteration 13515: 0.04294422673767569\n",
      "Train Loss at iteration 13516: 0.042944184317535015\n",
      "Train Loss at iteration 13517: 0.042944141901568096\n",
      "Train Loss at iteration 13518: 0.04294409948977427\n",
      "Train Loss at iteration 13519: 0.04294405708215282\n",
      "Train Loss at iteration 13520: 0.04294401467870306\n",
      "Train Loss at iteration 13521: 0.04294397227942431\n",
      "Train Loss at iteration 13522: 0.04294392988431585\n",
      "Train Loss at iteration 13523: 0.04294388749337701\n",
      "Train Loss at iteration 13524: 0.0429438451066071\n",
      "Train Loss at iteration 13525: 0.04294380272400541\n",
      "Train Loss at iteration 13526: 0.04294376034557125\n",
      "Train Loss at iteration 13527: 0.04294371797130393\n",
      "Train Loss at iteration 13528: 0.04294367560120277\n",
      "Train Loss at iteration 13529: 0.042943633235267065\n",
      "Train Loss at iteration 13530: 0.042943590873496126\n",
      "Train Loss at iteration 13531: 0.04294354851588927\n",
      "Train Loss at iteration 13532: 0.04294350616244578\n",
      "Train Loss at iteration 13533: 0.042943463813164995\n",
      "Train Loss at iteration 13534: 0.04294342146804621\n",
      "Train Loss at iteration 13535: 0.04294337912708872\n",
      "Train Loss at iteration 13536: 0.04294333679029185\n",
      "Train Loss at iteration 13537: 0.04294329445765493\n",
      "Train Loss at iteration 13538: 0.04294325212917722\n",
      "Train Loss at iteration 13539: 0.04294320980485805\n",
      "Train Loss at iteration 13540: 0.04294316748469676\n",
      "Train Loss at iteration 13541: 0.0429431251686926\n",
      "Train Loss at iteration 13542: 0.04294308285684494\n",
      "Train Loss at iteration 13543: 0.04294304054915303\n",
      "Train Loss at iteration 13544: 0.042942998245616246\n",
      "Train Loss at iteration 13545: 0.042942955946233846\n",
      "Train Loss at iteration 13546: 0.042942913651005166\n",
      "Train Loss at iteration 13547: 0.042942871359929496\n",
      "Train Loss at iteration 13548: 0.042942829073006165\n",
      "Train Loss at iteration 13549: 0.04294278679023446\n",
      "Train Loss at iteration 13550: 0.04294274451161373\n",
      "Train Loss at iteration 13551: 0.04294270223714325\n",
      "Train Loss at iteration 13552: 0.04294265996682233\n",
      "Train Loss at iteration 13553: 0.04294261770065032\n",
      "Train Loss at iteration 13554: 0.04294257543862648\n",
      "Train Loss at iteration 13555: 0.042942533180750156\n",
      "Train Loss at iteration 13556: 0.04294249092702065\n",
      "Train Loss at iteration 13557: 0.042942448677437274\n",
      "Train Loss at iteration 13558: 0.04294240643199933\n",
      "Train Loss at iteration 13559: 0.04294236419070613\n",
      "Train Loss at iteration 13560: 0.042942321953557\n",
      "Train Loss at iteration 13561: 0.04294227972055124\n",
      "Train Loss at iteration 13562: 0.04294223749168817\n",
      "Train Loss at iteration 13563: 0.04294219526696709\n",
      "Train Loss at iteration 13564: 0.04294215304638732\n",
      "Train Loss at iteration 13565: 0.04294211082994818\n",
      "Train Loss at iteration 13566: 0.04294206861764895\n",
      "Train Loss at iteration 13567: 0.04294202640948899\n",
      "Train Loss at iteration 13568: 0.04294198420546757\n",
      "Train Loss at iteration 13569: 0.04294194200558404\n",
      "Train Loss at iteration 13570: 0.04294189980983768\n",
      "Train Loss at iteration 13571: 0.0429418576182278\n",
      "Train Loss at iteration 13572: 0.04294181543075375\n",
      "Train Loss at iteration 13573: 0.04294177324741483\n",
      "Train Loss at iteration 13574: 0.04294173106821033\n",
      "Train Loss at iteration 13575: 0.04294168889313957\n",
      "Train Loss at iteration 13576: 0.04294164672220189\n",
      "Train Loss at iteration 13577: 0.04294160455539657\n",
      "Train Loss at iteration 13578: 0.04294156239272294\n",
      "Train Loss at iteration 13579: 0.042941520234180325\n",
      "Train Loss at iteration 13580: 0.04294147807976802\n",
      "Train Loss at iteration 13581: 0.04294143592948535\n",
      "Train Loss at iteration 13582: 0.042941393783331616\n",
      "Train Loss at iteration 13583: 0.04294135164130614\n",
      "Train Loss at iteration 13584: 0.04294130950340823\n",
      "Train Loss at iteration 13585: 0.0429412673696372\n",
      "Train Loss at iteration 13586: 0.042941225239992394\n",
      "Train Loss at iteration 13587: 0.04294118311447309\n",
      "Train Loss at iteration 13588: 0.04294114099307862\n",
      "Train Loss at iteration 13589: 0.0429410988758083\n",
      "Train Loss at iteration 13590: 0.042941056762661446\n",
      "Train Loss at iteration 13591: 0.04294101465363735\n",
      "Train Loss at iteration 13592: 0.04294097254873536\n",
      "Train Loss at iteration 13593: 0.042940930447954766\n",
      "Train Loss at iteration 13594: 0.04294088835129489\n",
      "Train Loss at iteration 13595: 0.042940846258755064\n",
      "Train Loss at iteration 13596: 0.04294080417033459\n",
      "Train Loss at iteration 13597: 0.04294076208603278\n",
      "Train Loss at iteration 13598: 0.04294072000584895\n",
      "Train Loss at iteration 13599: 0.04294067792978243\n",
      "Train Loss at iteration 13600: 0.04294063585783252\n",
      "Train Loss at iteration 13601: 0.042940593789998545\n",
      "Train Loss at iteration 13602: 0.04294055172627981\n",
      "Train Loss at iteration 13603: 0.04294050966667565\n",
      "Train Loss at iteration 13604: 0.04294046761118536\n",
      "Train Loss at iteration 13605: 0.04294042555980829\n",
      "Train Loss at iteration 13606: 0.042940383512543735\n",
      "Train Loss at iteration 13607: 0.042940341469391\n",
      "Train Loss at iteration 13608: 0.04294029943034942\n",
      "Train Loss at iteration 13609: 0.0429402573954183\n",
      "Train Loss at iteration 13610: 0.04294021536459696\n",
      "Train Loss at iteration 13611: 0.04294017333788473\n",
      "Train Loss at iteration 13612: 0.04294013131528092\n",
      "Train Loss at iteration 13613: 0.04294008929678486\n",
      "Train Loss at iteration 13614: 0.04294004728239583\n",
      "Train Loss at iteration 13615: 0.04294000527211319\n",
      "Train Loss at iteration 13616: 0.04293996326593622\n",
      "Train Loss at iteration 13617: 0.042939921263864284\n",
      "Train Loss at iteration 13618: 0.04293987926589666\n",
      "Train Loss at iteration 13619: 0.042939837272032676\n",
      "Train Loss at iteration 13620: 0.04293979528227166\n",
      "Train Loss at iteration 13621: 0.04293975329661294\n",
      "Train Loss at iteration 13622: 0.0429397113150558\n",
      "Train Loss at iteration 13623: 0.04293966933759959\n",
      "Train Loss at iteration 13624: 0.04293962736424361\n",
      "Train Loss at iteration 13625: 0.04293958539498719\n",
      "Train Loss at iteration 13626: 0.042939543429829634\n",
      "Train Loss at iteration 13627: 0.0429395014687703\n",
      "Train Loss at iteration 13628: 0.042939459511808456\n",
      "Train Loss at iteration 13629: 0.04293941755894347\n",
      "Train Loss at iteration 13630: 0.04293937561017462\n",
      "Train Loss at iteration 13631: 0.04293933366550125\n",
      "Train Loss at iteration 13632: 0.04293929172492267\n",
      "Train Loss at iteration 13633: 0.0429392497884382\n",
      "Train Loss at iteration 13634: 0.04293920785604718\n",
      "Train Loss at iteration 13635: 0.042939165927748886\n",
      "Train Loss at iteration 13636: 0.04293912400354268\n",
      "Train Loss at iteration 13637: 0.04293908208342788\n",
      "Train Loss at iteration 13638: 0.04293904016740377\n",
      "Train Loss at iteration 13639: 0.04293899825546971\n",
      "Train Loss at iteration 13640: 0.04293895634762499\n",
      "Train Loss at iteration 13641: 0.042938914443868964\n",
      "Train Loss at iteration 13642: 0.04293887254420093\n",
      "Train Loss at iteration 13643: 0.04293883064862021\n",
      "Train Loss at iteration 13644: 0.042938788757126145\n",
      "Train Loss at iteration 13645: 0.04293874686971802\n",
      "Train Loss at iteration 13646: 0.04293870498639518\n",
      "Train Loss at iteration 13647: 0.042938663107156945\n",
      "Train Loss at iteration 13648: 0.04293862123200264\n",
      "Train Loss at iteration 13649: 0.042938579360931595\n",
      "Train Loss at iteration 13650: 0.0429385374939431\n",
      "Train Loss at iteration 13651: 0.0429384956310365\n",
      "Train Loss at iteration 13652: 0.042938453772211116\n",
      "Train Loss at iteration 13653: 0.042938411917466265\n",
      "Train Loss at iteration 13654: 0.04293837006680127\n",
      "Train Loss at iteration 13655: 0.042938328220215455\n",
      "Train Loss at iteration 13656: 0.04293828637770815\n",
      "Train Loss at iteration 13657: 0.04293824453927866\n",
      "Train Loss at iteration 13658: 0.042938202704926307\n",
      "Train Loss at iteration 13659: 0.04293816087465045\n",
      "Train Loss at iteration 13660: 0.042938119048450386\n",
      "Train Loss at iteration 13661: 0.042938077226325415\n",
      "Train Loss at iteration 13662: 0.0429380354082749\n",
      "Train Loss at iteration 13663: 0.042937993594298146\n",
      "Train Loss at iteration 13664: 0.04293795178439448\n",
      "Train Loss at iteration 13665: 0.04293790997856322\n",
      "Train Loss at iteration 13666: 0.0429378681768037\n",
      "Train Loss at iteration 13667: 0.042937826379115224\n",
      "Train Loss at iteration 13668: 0.04293778458549714\n",
      "Train Loss at iteration 13669: 0.04293774279594876\n",
      "Train Loss at iteration 13670: 0.04293770101046941\n",
      "Train Loss at iteration 13671: 0.042937659229058414\n",
      "Train Loss at iteration 13672: 0.0429376174517151\n",
      "Train Loss at iteration 13673: 0.042937575678438776\n",
      "Train Loss at iteration 13674: 0.042937533909228796\n",
      "Train Loss at iteration 13675: 0.04293749214408446\n",
      "Train Loss at iteration 13676: 0.042937450383005095\n",
      "Train Loss at iteration 13677: 0.042937408625990034\n",
      "Train Loss at iteration 13678: 0.042937366873038604\n",
      "Train Loss at iteration 13679: 0.04293732512415012\n",
      "Train Loss at iteration 13680: 0.04293728337932392\n",
      "Train Loss at iteration 13681: 0.04293724163855932\n",
      "Train Loss at iteration 13682: 0.042937199901855655\n",
      "Train Loss at iteration 13683: 0.04293715816921222\n",
      "Train Loss at iteration 13684: 0.042937116440628385\n",
      "Train Loss at iteration 13685: 0.04293707471610345\n",
      "Train Loss at iteration 13686: 0.04293703299563675\n",
      "Train Loss at iteration 13687: 0.0429369912792276\n",
      "Train Loss at iteration 13688: 0.04293694956687533\n",
      "Train Loss at iteration 13689: 0.04293690785857929\n",
      "Train Loss at iteration 13690: 0.04293686615433876\n",
      "Train Loss at iteration 13691: 0.042936824454153096\n",
      "Train Loss at iteration 13692: 0.04293678275802164\n",
      "Train Loss at iteration 13693: 0.04293674106594369\n",
      "Train Loss at iteration 13694: 0.04293669937791858\n",
      "Train Loss at iteration 13695: 0.042936657693945626\n",
      "Train Loss at iteration 13696: 0.042936616014024195\n",
      "Train Loss at iteration 13697: 0.04293657433815357\n",
      "Train Loss at iteration 13698: 0.042936532666333106\n",
      "Train Loss at iteration 13699: 0.042936490998562116\n",
      "Train Loss at iteration 13700: 0.04293644933483993\n",
      "Train Loss at iteration 13701: 0.04293640767516587\n",
      "Train Loss at iteration 13702: 0.042936366019539284\n",
      "Train Loss at iteration 13703: 0.04293632436795949\n",
      "Train Loss at iteration 13704: 0.042936282720425795\n",
      "Train Loss at iteration 13705: 0.042936241076937554\n",
      "Train Loss at iteration 13706: 0.04293619943749409\n",
      "Train Loss at iteration 13707: 0.042936157802094733\n",
      "Train Loss at iteration 13708: 0.0429361161707388\n",
      "Train Loss at iteration 13709: 0.042936074543425615\n",
      "Train Loss at iteration 13710: 0.042936032920154535\n",
      "Train Loss at iteration 13711: 0.04293599130092486\n",
      "Train Loss at iteration 13712: 0.042935949685735934\n",
      "Train Loss at iteration 13713: 0.04293590807458708\n",
      "Train Loss at iteration 13714: 0.04293586646747762\n",
      "Train Loss at iteration 13715: 0.04293582486440692\n",
      "Train Loss at iteration 13716: 0.042935783265374255\n",
      "Train Loss at iteration 13717: 0.04293574167037899\n",
      "Train Loss at iteration 13718: 0.04293570007942044\n",
      "Train Loss at iteration 13719: 0.04293565849249794\n",
      "Train Loss at iteration 13720: 0.04293561690961082\n",
      "Train Loss at iteration 13721: 0.04293557533075842\n",
      "Train Loss at iteration 13722: 0.04293553375594006\n",
      "Train Loss at iteration 13723: 0.04293549218515506\n",
      "Train Loss at iteration 13724: 0.042935450618402766\n",
      "Train Loss at iteration 13725: 0.0429354090556825\n",
      "Train Loss at iteration 13726: 0.042935367496993594\n",
      "Train Loss at iteration 13727: 0.04293532594233537\n",
      "Train Loss at iteration 13728: 0.042935284391707185\n",
      "Train Loss at iteration 13729: 0.04293524284510835\n",
      "Train Loss at iteration 13730: 0.04293520130253819\n",
      "Train Loss at iteration 13731: 0.04293515976399605\n",
      "Train Loss at iteration 13732: 0.04293511822948125\n",
      "Train Loss at iteration 13733: 0.04293507669899313\n",
      "Train Loss at iteration 13734: 0.04293503517253103\n",
      "Train Loss at iteration 13735: 0.042934993650094254\n",
      "Train Loss at iteration 13736: 0.04293495213168216\n",
      "Train Loss at iteration 13737: 0.04293491061729406\n",
      "Train Loss at iteration 13738: 0.04293486910692929\n",
      "Train Loss at iteration 13739: 0.0429348276005872\n",
      "Train Loss at iteration 13740: 0.04293478609826712\n",
      "Train Loss at iteration 13741: 0.04293474459996835\n",
      "Train Loss at iteration 13742: 0.04293470310569025\n",
      "Train Loss at iteration 13743: 0.042934661615432135\n",
      "Train Loss at iteration 13744: 0.042934620129193364\n",
      "Train Loss at iteration 13745: 0.04293457864697324\n",
      "Train Loss at iteration 13746: 0.042934537168771124\n",
      "Train Loss at iteration 13747: 0.04293449569458633\n",
      "Train Loss at iteration 13748: 0.042934454224418196\n",
      "Train Loss at iteration 13749: 0.04293441275826605\n",
      "Train Loss at iteration 13750: 0.04293437129612923\n",
      "Train Loss at iteration 13751: 0.04293432983800706\n",
      "Train Loss at iteration 13752: 0.042934288383898886\n",
      "Train Loss at iteration 13753: 0.04293424693380404\n",
      "Train Loss at iteration 13754: 0.04293420548772185\n",
      "Train Loss at iteration 13755: 0.04293416404565166\n",
      "Train Loss at iteration 13756: 0.0429341226075928\n",
      "Train Loss at iteration 13757: 0.042934081173544594\n",
      "Train Loss at iteration 13758: 0.04293403974350637\n",
      "Train Loss at iteration 13759: 0.04293399831747748\n",
      "Train Loss at iteration 13760: 0.04293395689545726\n",
      "Train Loss at iteration 13761: 0.042933915477445045\n",
      "Train Loss at iteration 13762: 0.04293387406344014\n",
      "Train Loss at iteration 13763: 0.04293383265344192\n",
      "Train Loss at iteration 13764: 0.04293379124744969\n",
      "Train Loss at iteration 13765: 0.042933749845462804\n",
      "Train Loss at iteration 13766: 0.042933708447480574\n",
      "Train Loss at iteration 13767: 0.04293366705350236\n",
      "Train Loss at iteration 13768: 0.042933625663527486\n",
      "Train Loss at iteration 13769: 0.042933584277555295\n",
      "Train Loss at iteration 13770: 0.04293354289558511\n",
      "Train Loss at iteration 13771: 0.04293350151761626\n",
      "Train Loss at iteration 13772: 0.0429334601436481\n",
      "Train Loss at iteration 13773: 0.042933418773679954\n",
      "Train Loss at iteration 13774: 0.042933377407711186\n",
      "Train Loss at iteration 13775: 0.04293333604574108\n",
      "Train Loss at iteration 13776: 0.04293329468776902\n",
      "Train Loss at iteration 13777: 0.0429332533337943\n",
      "Train Loss at iteration 13778: 0.04293321198381629\n",
      "Train Loss at iteration 13779: 0.04293317063783432\n",
      "Train Loss at iteration 13780: 0.042933129295847715\n",
      "Train Loss at iteration 13781: 0.04293308795785583\n",
      "Train Loss at iteration 13782: 0.042933046623857975\n",
      "Train Loss at iteration 13783: 0.04293300529385351\n",
      "Train Loss at iteration 13784: 0.04293296396784174\n",
      "Train Loss at iteration 13785: 0.04293292264582205\n",
      "Train Loss at iteration 13786: 0.04293288132779375\n",
      "Train Loss at iteration 13787: 0.04293284001375617\n",
      "Train Loss at iteration 13788: 0.04293279870370866\n",
      "Train Loss at iteration 13789: 0.04293275739765056\n",
      "Train Loss at iteration 13790: 0.0429327160955812\n",
      "Train Loss at iteration 13791: 0.042932674797499924\n",
      "Train Loss at iteration 13792: 0.042932633503406065\n",
      "Train Loss at iteration 13793: 0.04293259221329895\n",
      "Train Loss at iteration 13794: 0.04293255092717794\n",
      "Train Loss at iteration 13795: 0.042932509645042354\n",
      "Train Loss at iteration 13796: 0.042932468366891546\n",
      "Train Loss at iteration 13797: 0.042932427092724844\n",
      "Train Loss at iteration 13798: 0.04293238582254158\n",
      "Train Loss at iteration 13799: 0.04293234455634111\n",
      "Train Loss at iteration 13800: 0.04293230329412276\n",
      "Train Loss at iteration 13801: 0.042932262035885876\n",
      "Train Loss at iteration 13802: 0.042932220781629796\n",
      "Train Loss at iteration 13803: 0.04293217953135385\n",
      "Train Loss at iteration 13804: 0.0429321382850574\n",
      "Train Loss at iteration 13805: 0.04293209704273975\n",
      "Train Loss at iteration 13806: 0.04293205580440027\n",
      "Train Loss at iteration 13807: 0.04293201457003829\n",
      "Train Loss at iteration 13808: 0.042931973339653144\n",
      "Train Loss at iteration 13809: 0.04293193211324418\n",
      "Train Loss at iteration 13810: 0.04293189089081072\n",
      "Train Loss at iteration 13811: 0.04293184967235213\n",
      "Train Loss at iteration 13812: 0.04293180845786773\n",
      "Train Loss at iteration 13813: 0.042931767247356874\n",
      "Train Loss at iteration 13814: 0.04293172604081889\n",
      "Train Loss at iteration 13815: 0.04293168483825313\n",
      "Train Loss at iteration 13816: 0.04293164363965893\n",
      "Train Loss at iteration 13817: 0.042931602445035626\n",
      "Train Loss at iteration 13818: 0.042931561254382564\n",
      "Train Loss at iteration 13819: 0.04293152006769908\n",
      "Train Loss at iteration 13820: 0.04293147888498452\n",
      "Train Loss at iteration 13821: 0.04293143770623822\n",
      "Train Loss at iteration 13822: 0.042931396531459526\n",
      "Train Loss at iteration 13823: 0.042931355360647766\n",
      "Train Loss at iteration 13824: 0.042931314193802306\n",
      "Train Loss at iteration 13825: 0.042931273030922465\n",
      "Train Loss at iteration 13826: 0.0429312318720076\n",
      "Train Loss at iteration 13827: 0.04293119071705704\n",
      "Train Loss at iteration 13828: 0.042931149566070144\n",
      "Train Loss at iteration 13829: 0.04293110841904624\n",
      "Train Loss at iteration 13830: 0.04293106727598467\n",
      "Train Loss at iteration 13831: 0.042931026136884765\n",
      "Train Loss at iteration 13832: 0.0429309850017459\n",
      "Train Loss at iteration 13833: 0.04293094387056741\n",
      "Train Loss at iteration 13834: 0.042930902743348605\n",
      "Train Loss at iteration 13835: 0.042930861620088856\n",
      "Train Loss at iteration 13836: 0.042930820500787496\n",
      "Train Loss at iteration 13837: 0.04293077938544386\n",
      "Train Loss at iteration 13838: 0.042930738274057315\n",
      "Train Loss at iteration 13839: 0.042930697166627195\n",
      "Train Loss at iteration 13840: 0.042930656063152835\n",
      "Train Loss at iteration 13841: 0.042930614963633575\n",
      "Train Loss at iteration 13842: 0.04293057386806877\n",
      "Train Loss at iteration 13843: 0.042930532776457746\n",
      "Train Loss at iteration 13844: 0.04293049168879988\n",
      "Train Loss at iteration 13845: 0.042930450605094476\n",
      "Train Loss at iteration 13846: 0.0429304095253409\n",
      "Train Loss at iteration 13847: 0.04293036844953851\n",
      "Train Loss at iteration 13848: 0.0429303273776866\n",
      "Train Loss at iteration 13849: 0.042930286309784585\n",
      "Train Loss at iteration 13850: 0.04293024524583174\n",
      "Train Loss at iteration 13851: 0.04293020418582746\n",
      "Train Loss at iteration 13852: 0.04293016312977104\n",
      "Train Loss at iteration 13853: 0.04293012207766188\n",
      "Train Loss at iteration 13854: 0.0429300810294993\n",
      "Train Loss at iteration 13855: 0.042930039985282636\n",
      "Train Loss at iteration 13856: 0.04292999894501125\n",
      "Train Loss at iteration 13857: 0.04292995790868446\n",
      "Train Loss at iteration 13858: 0.042929916876301634\n",
      "Train Loss at iteration 13859: 0.04292987584786212\n",
      "Train Loss at iteration 13860: 0.042929834823365244\n",
      "Train Loss at iteration 13861: 0.04292979380281037\n",
      "Train Loss at iteration 13862: 0.04292975278619683\n",
      "Train Loss at iteration 13863: 0.04292971177352398\n",
      "Train Loss at iteration 13864: 0.04292967076479118\n",
      "Train Loss at iteration 13865: 0.04292962975999773\n",
      "Train Loss at iteration 13866: 0.04292958875914302\n",
      "Train Loss at iteration 13867: 0.04292954776222636\n",
      "Train Loss at iteration 13868: 0.04292950676924713\n",
      "Train Loss at iteration 13869: 0.042929465780204665\n",
      "Train Loss at iteration 13870: 0.04292942479509831\n",
      "Train Loss at iteration 13871: 0.042929383813927385\n",
      "Train Loss at iteration 13872: 0.04292934283669129\n",
      "Train Loss at iteration 13873: 0.04292930186338933\n",
      "Train Loss at iteration 13874: 0.04292926089402087\n",
      "Train Loss at iteration 13875: 0.04292921992858526\n",
      "Train Loss at iteration 13876: 0.04292917896708183\n",
      "Train Loss at iteration 13877: 0.042929138009509926\n",
      "Train Loss at iteration 13878: 0.04292909705586893\n",
      "Train Loss at iteration 13879: 0.04292905610615815\n",
      "Train Loss at iteration 13880: 0.04292901516037695\n",
      "Train Loss at iteration 13881: 0.042928974218524665\n",
      "Train Loss at iteration 13882: 0.04292893328060068\n",
      "Train Loss at iteration 13883: 0.04292889234660429\n",
      "Train Loss at iteration 13884: 0.04292885141653489\n",
      "Train Loss at iteration 13885: 0.0429288104903918\n",
      "Train Loss at iteration 13886: 0.04292876956817438\n",
      "Train Loss at iteration 13887: 0.04292872864988197\n",
      "Train Loss at iteration 13888: 0.04292868773551392\n",
      "Train Loss at iteration 13889: 0.04292864682506959\n",
      "Train Loss at iteration 13890: 0.042928605918548315\n",
      "Train Loss at iteration 13891: 0.04292856501594945\n",
      "Train Loss at iteration 13892: 0.04292852411727235\n",
      "Train Loss at iteration 13893: 0.04292848322251634\n",
      "Train Loss at iteration 13894: 0.0429284423316808\n",
      "Train Loss at iteration 13895: 0.042928401444765056\n",
      "Train Loss at iteration 13896: 0.042928360561768464\n",
      "Train Loss at iteration 13897: 0.04292831968269038\n",
      "Train Loss at iteration 13898: 0.04292827880753017\n",
      "Train Loss at iteration 13899: 0.042928237936287125\n",
      "Train Loss at iteration 13900: 0.04292819706896066\n",
      "Train Loss at iteration 13901: 0.04292815620555007\n",
      "Train Loss at iteration 13902: 0.042928115346054764\n",
      "Train Loss at iteration 13903: 0.04292807449047403\n",
      "Train Loss at iteration 13904: 0.04292803363880726\n",
      "Train Loss at iteration 13905: 0.0429279927910538\n",
      "Train Loss at iteration 13906: 0.04292795194721298\n",
      "Train Loss at iteration 13907: 0.04292791110728418\n",
      "Train Loss at iteration 13908: 0.04292787027126673\n",
      "Train Loss at iteration 13909: 0.04292782943915997\n",
      "Train Loss at iteration 13910: 0.04292778861096328\n",
      "Train Loss at iteration 13911: 0.04292774778667599\n",
      "Train Loss at iteration 13912: 0.042927706966297455\n",
      "Train Loss at iteration 13913: 0.04292766614982703\n",
      "Train Loss at iteration 13914: 0.04292762533726406\n",
      "Train Loss at iteration 13915: 0.04292758452860791\n",
      "Train Loss at iteration 13916: 0.04292754372385791\n",
      "Train Loss at iteration 13917: 0.042927502923013436\n",
      "Train Loss at iteration 13918: 0.042927462126073825\n",
      "Train Loss at iteration 13919: 0.042927421333038424\n",
      "Train Loss at iteration 13920: 0.0429273805439066\n",
      "Train Loss at iteration 13921: 0.04292733975867769\n",
      "Train Loss at iteration 13922: 0.04292729897735107\n",
      "Train Loss at iteration 13923: 0.042927258199926065\n",
      "Train Loss at iteration 13924: 0.042927217426402035\n",
      "Train Loss at iteration 13925: 0.04292717665677833\n",
      "Train Loss at iteration 13926: 0.04292713589105432\n",
      "Train Loss at iteration 13927: 0.042927095129229334\n",
      "Train Loss at iteration 13928: 0.04292705437130275\n",
      "Train Loss at iteration 13929: 0.042927013617273906\n",
      "Train Loss at iteration 13930: 0.042926972867142144\n",
      "Train Loss at iteration 13931: 0.04292693212090683\n",
      "Train Loss at iteration 13932: 0.042926891378567325\n",
      "Train Loss at iteration 13933: 0.042926850640122964\n",
      "Train Loss at iteration 13934: 0.04292680990557312\n",
      "Train Loss at iteration 13935: 0.042926769174917126\n",
      "Train Loss at iteration 13936: 0.04292672844815436\n",
      "Train Loss at iteration 13937: 0.04292668772528416\n",
      "Train Loss at iteration 13938: 0.04292664700630588\n",
      "Train Loss at iteration 13939: 0.04292660629121887\n",
      "Train Loss at iteration 13940: 0.04292656558002248\n",
      "Train Loss at iteration 13941: 0.04292652487271608\n",
      "Train Loss at iteration 13942: 0.04292648416929902\n",
      "Train Loss at iteration 13943: 0.04292644346977066\n",
      "Train Loss at iteration 13944: 0.042926402774130346\n",
      "Train Loss at iteration 13945: 0.04292636208237743\n",
      "Train Loss at iteration 13946: 0.04292632139451126\n",
      "Train Loss at iteration 13947: 0.042926280710531214\n",
      "Train Loss at iteration 13948: 0.042926240030436644\n",
      "Train Loss at iteration 13949: 0.042926199354226875\n",
      "Train Loss at iteration 13950: 0.04292615868190129\n",
      "Train Loss at iteration 13951: 0.042926118013459226\n",
      "Train Loss at iteration 13952: 0.042926077348900075\n",
      "Train Loss at iteration 13953: 0.042926036688223146\n",
      "Train Loss at iteration 13954: 0.04292599603142782\n",
      "Train Loss at iteration 13955: 0.042925955378513445\n",
      "Train Loss at iteration 13956: 0.04292591472947937\n",
      "Train Loss at iteration 13957: 0.04292587408432498\n",
      "Train Loss at iteration 13958: 0.04292583344304961\n",
      "Train Loss at iteration 13959: 0.042925792805652604\n",
      "Train Loss at iteration 13960: 0.04292575217213333\n",
      "Train Loss at iteration 13961: 0.042925711542491155\n",
      "Train Loss at iteration 13962: 0.04292567091672544\n",
      "Train Loss at iteration 13963: 0.0429256302948355\n",
      "Train Loss at iteration 13964: 0.04292558967682073\n",
      "Train Loss at iteration 13965: 0.042925549062680486\n",
      "Train Loss at iteration 13966: 0.04292550845241409\n",
      "Train Loss at iteration 13967: 0.04292546784602095\n",
      "Train Loss at iteration 13968: 0.042925427243500396\n",
      "Train Loss at iteration 13969: 0.042925386644851765\n",
      "Train Loss at iteration 13970: 0.042925346050074455\n",
      "Train Loss at iteration 13971: 0.04292530545916779\n",
      "Train Loss at iteration 13972: 0.04292526487213116\n",
      "Train Loss at iteration 13973: 0.042925224288963885\n",
      "Train Loss at iteration 13974: 0.04292518370966534\n",
      "Train Loss at iteration 13975: 0.0429251431342349\n",
      "Train Loss at iteration 13976: 0.042925102562671916\n",
      "Train Loss at iteration 13977: 0.04292506199497571\n",
      "Train Loss at iteration 13978: 0.04292502143114568\n",
      "Train Loss at iteration 13979: 0.04292498087118118\n",
      "Train Loss at iteration 13980: 0.04292494031508156\n",
      "Train Loss at iteration 13981: 0.042924899762846164\n",
      "Train Loss at iteration 13982: 0.04292485921447437\n",
      "Train Loss at iteration 13983: 0.042924818669965545\n",
      "Train Loss at iteration 13984: 0.04292477812931903\n",
      "Train Loss at iteration 13985: 0.04292473759253418\n",
      "Train Loss at iteration 13986: 0.04292469705961037\n",
      "Train Loss at iteration 13987: 0.04292465653054695\n",
      "Train Loss at iteration 13988: 0.04292461600534328\n",
      "Train Loss at iteration 13989: 0.042924575483998736\n",
      "Train Loss at iteration 13990: 0.04292453496651264\n",
      "Train Loss at iteration 13991: 0.04292449445288438\n",
      "Train Loss at iteration 13992: 0.04292445394311332\n",
      "Train Loss at iteration 13993: 0.0429244134371988\n",
      "Train Loss at iteration 13994: 0.04292437293514019\n",
      "Train Loss at iteration 13995: 0.042924332436936836\n",
      "Train Loss at iteration 13996: 0.04292429194258813\n",
      "Train Loss at iteration 13997: 0.04292425145209341\n",
      "Train Loss at iteration 13998: 0.04292421096545203\n",
      "Train Loss at iteration 13999: 0.042924170482663365\n",
      "Train Loss at iteration 14000: 0.04292413000372677\n",
      "Train Loss at iteration 14001: 0.042924089528641615\n",
      "Train Loss at iteration 14002: 0.04292404905740724\n",
      "Train Loss at iteration 14003: 0.04292400859002302\n",
      "Train Loss at iteration 14004: 0.04292396812648831\n",
      "Train Loss at iteration 14005: 0.04292392766680249\n",
      "Train Loss at iteration 14006: 0.04292388721096489\n",
      "Train Loss at iteration 14007: 0.04292384675897489\n",
      "Train Loss at iteration 14008: 0.04292380631083185\n",
      "Train Loss at iteration 14009: 0.042923765866535145\n",
      "Train Loss at iteration 14010: 0.042923725426084094\n",
      "Train Loss at iteration 14011: 0.0429236849894781\n",
      "Train Loss at iteration 14012: 0.04292364455671651\n",
      "Train Loss at iteration 14013: 0.042923604127798685\n",
      "Train Loss at iteration 14014: 0.042923563702723995\n",
      "Train Loss at iteration 14015: 0.04292352328149179\n",
      "Train Loss at iteration 14016: 0.04292348286410144\n",
      "Train Loss at iteration 14017: 0.04292344245055232\n",
      "Train Loss at iteration 14018: 0.04292340204084375\n",
      "Train Loss at iteration 14019: 0.042923361634975135\n",
      "Train Loss at iteration 14020: 0.04292332123294583\n",
      "Train Loss at iteration 14021: 0.042923280834755176\n",
      "Train Loss at iteration 14022: 0.042923240440402556\n",
      "Train Loss at iteration 14023: 0.04292320004988733\n",
      "Train Loss at iteration 14024: 0.042923159663208865\n",
      "Train Loss at iteration 14025: 0.04292311928036652\n",
      "Train Loss at iteration 14026: 0.042923078901359636\n",
      "Train Loss at iteration 14027: 0.04292303852618762\n",
      "Train Loss at iteration 14028: 0.0429229981548498\n",
      "Train Loss at iteration 14029: 0.04292295778734556\n",
      "Train Loss at iteration 14030: 0.042922917423674246\n",
      "Train Loss at iteration 14031: 0.04292287706383524\n",
      "Train Loss at iteration 14032: 0.042922836707827886\n",
      "Train Loss at iteration 14033: 0.04292279635565157\n",
      "Train Loss at iteration 14034: 0.04292275600730565\n",
      "Train Loss at iteration 14035: 0.042922715662789476\n",
      "Train Loss at iteration 14036: 0.042922675322102415\n",
      "Train Loss at iteration 14037: 0.04292263498524385\n",
      "Train Loss at iteration 14038: 0.04292259465221313\n",
      "Train Loss at iteration 14039: 0.04292255432300963\n",
      "Train Loss at iteration 14040: 0.042922513997632694\n",
      "Train Loss at iteration 14041: 0.04292247367608173\n",
      "Train Loss at iteration 14042: 0.04292243335835605\n",
      "Train Loss at iteration 14043: 0.04292239304445504\n",
      "Train Loss at iteration 14044: 0.04292235273437808\n",
      "Train Loss at iteration 14045: 0.04292231242812453\n",
      "Train Loss at iteration 14046: 0.04292227212569374\n",
      "Train Loss at iteration 14047: 0.0429222318270851\n",
      "Train Loss at iteration 14048: 0.04292219153229795\n",
      "Train Loss at iteration 14049: 0.042922151241331655\n",
      "Train Loss at iteration 14050: 0.04292211095418561\n",
      "Train Loss at iteration 14051: 0.042922070670859166\n",
      "Train Loss at iteration 14052: 0.042922030391351666\n",
      "Train Loss at iteration 14053: 0.04292199011566251\n",
      "Train Loss at iteration 14054: 0.04292194984379105\n",
      "Train Loss at iteration 14055: 0.042921909575736654\n",
      "Train Loss at iteration 14056: 0.042921869311498696\n",
      "Train Loss at iteration 14057: 0.04292182905107653\n",
      "Train Loss at iteration 14058: 0.042921788794469506\n",
      "Train Loss at iteration 14059: 0.042921748541677046\n",
      "Train Loss at iteration 14060: 0.042921708292698466\n",
      "Train Loss at iteration 14061: 0.04292166804753314\n",
      "Train Loss at iteration 14062: 0.04292162780618045\n",
      "Train Loss at iteration 14063: 0.042921587568639764\n",
      "Train Loss at iteration 14064: 0.04292154733491045\n",
      "Train Loss at iteration 14065: 0.04292150710499186\n",
      "Train Loss at iteration 14066: 0.04292146687888337\n",
      "Train Loss at iteration 14067: 0.04292142665658434\n",
      "Train Loss at iteration 14068: 0.04292138643809416\n",
      "Train Loss at iteration 14069: 0.042921346223412184\n",
      "Train Loss at iteration 14070: 0.042921306012537776\n",
      "Train Loss at iteration 14071: 0.042921265805470316\n",
      "Train Loss at iteration 14072: 0.042921225602209144\n",
      "Train Loss at iteration 14073: 0.04292118540275366\n",
      "Train Loss at iteration 14074: 0.04292114520710323\n",
      "Train Loss at iteration 14075: 0.0429211050152572\n",
      "Train Loss at iteration 14076: 0.04292106482721496\n",
      "Train Loss at iteration 14077: 0.04292102464297586\n",
      "Train Loss at iteration 14078: 0.04292098446253929\n",
      "Train Loss at iteration 14079: 0.0429209442859046\n",
      "Train Loss at iteration 14080: 0.04292090411307117\n",
      "Train Loss at iteration 14081: 0.04292086394403838\n",
      "Train Loss at iteration 14082: 0.04292082377880557\n",
      "Train Loss at iteration 14083: 0.04292078361737212\n",
      "Train Loss at iteration 14084: 0.04292074345973742\n",
      "Train Loss at iteration 14085: 0.04292070330590081\n",
      "Train Loss at iteration 14086: 0.042920663155861696\n",
      "Train Loss at iteration 14087: 0.04292062300961941\n",
      "Train Loss at iteration 14088: 0.04292058286717333\n",
      "Train Loss at iteration 14089: 0.04292054272852284\n",
      "Train Loss at iteration 14090: 0.042920502593667316\n",
      "Train Loss at iteration 14091: 0.04292046246260609\n",
      "Train Loss at iteration 14092: 0.04292042233533858\n",
      "Train Loss at iteration 14093: 0.04292038221186413\n",
      "Train Loss at iteration 14094: 0.04292034209218211\n",
      "Train Loss at iteration 14095: 0.0429203019762919\n",
      "Train Loss at iteration 14096: 0.042920261864192864\n",
      "Train Loss at iteration 14097: 0.04292022175588437\n",
      "Train Loss at iteration 14098: 0.04292018165136581\n",
      "Train Loss at iteration 14099: 0.04292014155063652\n",
      "Train Loss at iteration 14100: 0.04292010145369589\n",
      "Train Loss at iteration 14101: 0.042920061360543306\n",
      "Train Loss at iteration 14102: 0.04292002127117811\n",
      "Train Loss at iteration 14103: 0.042919981185599705\n",
      "Train Loss at iteration 14104: 0.04291994110380744\n",
      "Train Loss at iteration 14105: 0.042919901025800684\n",
      "Train Loss at iteration 14106: 0.04291986095157883\n",
      "Train Loss at iteration 14107: 0.04291982088114123\n",
      "Train Loss at iteration 14108: 0.04291978081448726\n",
      "Train Loss at iteration 14109: 0.0429197407516163\n",
      "Train Loss at iteration 14110: 0.04291970069252771\n",
      "Train Loss at iteration 14111: 0.042919660637220865\n",
      "Train Loss at iteration 14112: 0.042919620585695165\n",
      "Train Loss at iteration 14113: 0.04291958053794995\n",
      "Train Loss at iteration 14114: 0.04291954049398459\n",
      "Train Loss at iteration 14115: 0.042919500453798484\n",
      "Train Loss at iteration 14116: 0.04291946041739099\n",
      "Train Loss at iteration 14117: 0.04291942038476147\n",
      "Train Loss at iteration 14118: 0.04291938035590932\n",
      "Train Loss at iteration 14119: 0.042919340330833895\n",
      "Train Loss at iteration 14120: 0.04291930030953458\n",
      "Train Loss at iteration 14121: 0.04291926029201074\n",
      "Train Loss at iteration 14122: 0.042919220278261755\n",
      "Train Loss at iteration 14123: 0.04291918026828699\n",
      "Train Loss at iteration 14124: 0.04291914026208583\n",
      "Train Loss at iteration 14125: 0.042919100259657644\n",
      "Train Loss at iteration 14126: 0.042919060261001805\n",
      "Train Loss at iteration 14127: 0.04291902026611767\n",
      "Train Loss at iteration 14128: 0.04291898027500465\n",
      "Train Loss at iteration 14129: 0.04291894028766209\n",
      "Train Loss at iteration 14130: 0.042918900304089375\n",
      "Train Loss at iteration 14131: 0.04291886032428588\n",
      "Train Loss at iteration 14132: 0.04291882034825097\n",
      "Train Loss at iteration 14133: 0.04291878037598403\n",
      "Train Loss at iteration 14134: 0.04291874040748444\n",
      "Train Loss at iteration 14135: 0.042918700442751555\n",
      "Train Loss at iteration 14136: 0.042918660481784755\n",
      "Train Loss at iteration 14137: 0.04291862052458345\n",
      "Train Loss at iteration 14138: 0.04291858057114696\n",
      "Train Loss at iteration 14139: 0.042918540621474696\n",
      "Train Loss at iteration 14140: 0.04291850067556601\n",
      "Train Loss at iteration 14141: 0.042918460733420316\n",
      "Train Loss at iteration 14142: 0.04291842079503694\n",
      "Train Loss at iteration 14143: 0.042918380860415294\n",
      "Train Loss at iteration 14144: 0.04291834092955475\n",
      "Train Loss at iteration 14145: 0.04291830100245467\n",
      "Train Loss at iteration 14146: 0.04291826107911444\n",
      "Train Loss at iteration 14147: 0.04291822115953342\n",
      "Train Loss at iteration 14148: 0.04291818124371101\n",
      "Train Loss at iteration 14149: 0.04291814133164657\n",
      "Train Loss at iteration 14150: 0.04291810142333948\n",
      "Train Loss at iteration 14151: 0.04291806151878913\n",
      "Train Loss at iteration 14152: 0.04291802161799486\n",
      "Train Loss at iteration 14153: 0.042917981720956094\n",
      "Train Loss at iteration 14154: 0.04291794182767217\n",
      "Train Loss at iteration 14155: 0.042917901938142475\n",
      "Train Loss at iteration 14156: 0.04291786205236641\n",
      "Train Loss at iteration 14157: 0.04291782217034334\n",
      "Train Loss at iteration 14158: 0.04291778229207261\n",
      "Train Loss at iteration 14159: 0.04291774241755364\n",
      "Train Loss at iteration 14160: 0.04291770254678578\n",
      "Train Loss at iteration 14161: 0.04291766267976843\n",
      "Train Loss at iteration 14162: 0.04291762281650094\n",
      "Train Loss at iteration 14163: 0.04291758295698273\n",
      "Train Loss at iteration 14164: 0.04291754310121312\n",
      "Train Loss at iteration 14165: 0.042917503249191545\n",
      "Train Loss at iteration 14166: 0.04291746340091735\n",
      "Train Loss at iteration 14167: 0.04291742355638991\n",
      "Train Loss at iteration 14168: 0.04291738371560862\n",
      "Train Loss at iteration 14169: 0.04291734387857286\n",
      "Train Loss at iteration 14170: 0.04291730404528199\n",
      "Train Loss at iteration 14171: 0.042917264215735405\n",
      "Train Loss at iteration 14172: 0.04291722438993247\n",
      "Train Loss at iteration 14173: 0.04291718456787258\n",
      "Train Loss at iteration 14174: 0.04291714474955511\n",
      "Train Loss at iteration 14175: 0.042917104934979425\n",
      "Train Loss at iteration 14176: 0.04291706512414491\n",
      "Train Loss at iteration 14177: 0.042917025317050946\n",
      "Train Loss at iteration 14178: 0.04291698551369694\n",
      "Train Loss at iteration 14179: 0.042916945714082215\n",
      "Train Loss at iteration 14180: 0.042916905918206204\n",
      "Train Loss at iteration 14181: 0.042916866126068244\n",
      "Train Loss at iteration 14182: 0.04291682633766776\n",
      "Train Loss at iteration 14183: 0.04291678655300408\n",
      "Train Loss at iteration 14184: 0.04291674677207661\n",
      "Train Loss at iteration 14185: 0.04291670699488475\n",
      "Train Loss at iteration 14186: 0.042916667221427854\n",
      "Train Loss at iteration 14187: 0.042916627451705304\n",
      "Train Loss at iteration 14188: 0.04291658768571648\n",
      "Train Loss at iteration 14189: 0.042916547923460766\n",
      "Train Loss at iteration 14190: 0.042916508164937556\n",
      "Train Loss at iteration 14191: 0.04291646841014622\n",
      "Train Loss at iteration 14192: 0.04291642865908612\n",
      "Train Loss at iteration 14193: 0.04291638891175666\n",
      "Train Loss at iteration 14194: 0.04291634916815723\n",
      "Train Loss at iteration 14195: 0.042916309428287176\n",
      "Train Loss at iteration 14196: 0.04291626969214591\n",
      "Train Loss at iteration 14197: 0.04291622995973281\n",
      "Train Loss at iteration 14198: 0.04291619023104724\n",
      "Train Loss at iteration 14199: 0.04291615050608859\n",
      "Train Loss at iteration 14200: 0.04291611078485624\n",
      "Train Loss at iteration 14201: 0.042916071067349584\n",
      "Train Loss at iteration 14202: 0.04291603135356799\n",
      "Train Loss at iteration 14203: 0.04291599164351084\n",
      "Train Loss at iteration 14204: 0.04291595193717753\n",
      "Train Loss at iteration 14205: 0.04291591223456742\n",
      "Train Loss at iteration 14206: 0.04291587253567992\n",
      "Train Loss at iteration 14207: 0.042915832840514374\n",
      "Train Loss at iteration 14208: 0.042915793149070215\n",
      "Train Loss at iteration 14209: 0.04291575346134679\n",
      "Train Loss at iteration 14210: 0.042915713777343475\n",
      "Train Loss at iteration 14211: 0.04291567409705967\n",
      "Train Loss at iteration 14212: 0.04291563442049477\n",
      "Train Loss at iteration 14213: 0.04291559474764813\n",
      "Train Loss at iteration 14214: 0.042915555078519164\n",
      "Train Loss at iteration 14215: 0.042915515413107216\n",
      "Train Loss at iteration 14216: 0.04291547575141169\n",
      "Train Loss at iteration 14217: 0.042915436093431986\n",
      "Train Loss at iteration 14218: 0.04291539643916746\n",
      "Train Loss at iteration 14219: 0.042915356788617506\n",
      "Train Loss at iteration 14220: 0.04291531714178151\n",
      "Train Loss at iteration 14221: 0.04291527749865885\n",
      "Train Loss at iteration 14222: 0.042915237859248924\n",
      "Train Loss at iteration 14223: 0.0429151982235511\n",
      "Train Loss at iteration 14224: 0.04291515859156478\n",
      "Train Loss at iteration 14225: 0.04291511896328931\n",
      "Train Loss at iteration 14226: 0.04291507933872412\n",
      "Train Loss at iteration 14227: 0.04291503971786858\n",
      "Train Loss at iteration 14228: 0.042915000100722044\n",
      "Train Loss at iteration 14229: 0.042914960487283944\n",
      "Train Loss at iteration 14230: 0.04291492087755363\n",
      "Train Loss at iteration 14231: 0.04291488127153051\n",
      "Train Loss at iteration 14232: 0.04291484166921395\n",
      "Train Loss at iteration 14233: 0.042914802070603345\n",
      "Train Loss at iteration 14234: 0.04291476247569808\n",
      "Train Loss at iteration 14235: 0.04291472288449753\n",
      "Train Loss at iteration 14236: 0.0429146832970011\n",
      "Train Loss at iteration 14237: 0.04291464371320816\n",
      "Train Loss at iteration 14238: 0.042914604133118095\n",
      "Train Loss at iteration 14239: 0.0429145645567303\n",
      "Train Loss at iteration 14240: 0.042914524984044154\n",
      "Train Loss at iteration 14241: 0.042914485415059045\n",
      "Train Loss at iteration 14242: 0.04291444584977436\n",
      "Train Loss at iteration 14243: 0.04291440628818949\n",
      "Train Loss at iteration 14244: 0.0429143667303038\n",
      "Train Loss at iteration 14245: 0.0429143271761167\n",
      "Train Loss at iteration 14246: 0.042914287625627566\n",
      "Train Loss at iteration 14247: 0.04291424807883579\n",
      "Train Loss at iteration 14248: 0.04291420853574074\n",
      "Train Loss at iteration 14249: 0.04291416899634184\n",
      "Train Loss at iteration 14250: 0.04291412946063843\n",
      "Train Loss at iteration 14251: 0.04291408992862993\n",
      "Train Loss at iteration 14252: 0.04291405040031573\n",
      "Train Loss at iteration 14253: 0.04291401087569518\n",
      "Train Loss at iteration 14254: 0.0429139713547677\n",
      "Train Loss at iteration 14255: 0.042913931837532684\n",
      "Train Loss at iteration 14256: 0.042913892323989476\n",
      "Train Loss at iteration 14257: 0.04291385281413752\n",
      "Train Loss at iteration 14258: 0.04291381330797616\n",
      "Train Loss at iteration 14259: 0.0429137738055048\n",
      "Train Loss at iteration 14260: 0.04291373430672284\n",
      "Train Loss at iteration 14261: 0.04291369481162963\n",
      "Train Loss at iteration 14262: 0.04291365532022459\n",
      "Train Loss at iteration 14263: 0.04291361583250713\n",
      "Train Loss at iteration 14264: 0.04291357634847656\n",
      "Train Loss at iteration 14265: 0.042913536868132346\n",
      "Train Loss at iteration 14266: 0.04291349739147384\n",
      "Train Loss at iteration 14267: 0.04291345791850044\n",
      "Train Loss at iteration 14268: 0.04291341844921153\n",
      "Train Loss at iteration 14269: 0.04291337898360651\n",
      "Train Loss at iteration 14270: 0.04291333952168476\n",
      "Train Loss at iteration 14271: 0.042913300063445646\n",
      "Train Loss at iteration 14272: 0.0429132606088886\n",
      "Train Loss at iteration 14273: 0.042913221158012986\n",
      "Train Loss at iteration 14274: 0.042913181710818205\n",
      "Train Loss at iteration 14275: 0.04291314226730362\n",
      "Train Loss at iteration 14276: 0.04291310282746866\n",
      "Train Loss at iteration 14277: 0.042913063391312685\n",
      "Train Loss at iteration 14278: 0.04291302395883509\n",
      "Train Loss at iteration 14279: 0.042912984530035274\n",
      "Train Loss at iteration 14280: 0.04291294510491261\n",
      "Train Loss at iteration 14281: 0.042912905683466514\n",
      "Train Loss at iteration 14282: 0.042912866265696355\n",
      "Train Loss at iteration 14283: 0.04291282685160153\n",
      "Train Loss at iteration 14284: 0.042912787441181446\n",
      "Train Loss at iteration 14285: 0.04291274803443545\n",
      "Train Loss at iteration 14286: 0.04291270863136297\n",
      "Train Loss at iteration 14287: 0.04291266923196337\n",
      "Train Loss at iteration 14288: 0.04291262983623607\n",
      "Train Loss at iteration 14289: 0.04291259044418044\n",
      "Train Loss at iteration 14290: 0.042912551055795875\n",
      "Train Loss at iteration 14291: 0.04291251167108178\n",
      "Train Loss at iteration 14292: 0.04291247229003753\n",
      "Train Loss at iteration 14293: 0.04291243291266251\n",
      "Train Loss at iteration 14294: 0.04291239353895613\n",
      "Train Loss at iteration 14295: 0.042912354168917775\n",
      "Train Loss at iteration 14296: 0.04291231480254682\n",
      "Train Loss at iteration 14297: 0.042912275439842676\n",
      "Train Loss at iteration 14298: 0.042912236080804744\n",
      "Train Loss at iteration 14299: 0.04291219672543238\n",
      "Train Loss at iteration 14300: 0.04291215737372501\n",
      "Train Loss at iteration 14301: 0.042912118025682004\n",
      "Train Loss at iteration 14302: 0.042912078681302764\n",
      "Train Loss at iteration 14303: 0.04291203934058669\n",
      "Train Loss at iteration 14304: 0.04291200000353315\n",
      "Train Loss at iteration 14305: 0.042911960670141566\n",
      "Train Loss at iteration 14306: 0.042911921340411316\n",
      "Train Loss at iteration 14307: 0.04291188201434178\n",
      "Train Loss at iteration 14308: 0.04291184269193238\n",
      "Train Loss at iteration 14309: 0.04291180337318247\n",
      "Train Loss at iteration 14310: 0.04291176405809149\n",
      "Train Loss at iteration 14311: 0.04291172474665878\n",
      "Train Loss at iteration 14312: 0.042911685438883775\n",
      "Train Loss at iteration 14313: 0.04291164613476586\n",
      "Train Loss at iteration 14314: 0.04291160683430442\n",
      "Train Loss at iteration 14315: 0.04291156753749884\n",
      "Train Loss at iteration 14316: 0.042911528244348525\n",
      "Train Loss at iteration 14317: 0.042911488954852875\n",
      "Train Loss at iteration 14318: 0.042911449669011274\n",
      "Train Loss at iteration 14319: 0.042911410386823126\n",
      "Train Loss at iteration 14320: 0.04291137110828781\n",
      "Train Loss at iteration 14321: 0.04291133183340472\n",
      "Train Loss at iteration 14322: 0.04291129256217326\n",
      "Train Loss at iteration 14323: 0.042911253294592826\n",
      "Train Loss at iteration 14324: 0.042911214030662795\n",
      "Train Loss at iteration 14325: 0.04291117477038258\n",
      "Train Loss at iteration 14326: 0.042911135513751586\n",
      "Train Loss at iteration 14327: 0.04291109626076917\n",
      "Train Loss at iteration 14328: 0.04291105701143476\n",
      "Train Loss at iteration 14329: 0.042911017765747725\n",
      "Train Loss at iteration 14330: 0.042910978523707496\n",
      "Train Loss at iteration 14331: 0.04291093928531342\n",
      "Train Loss at iteration 14332: 0.042910900050564936\n",
      "Train Loss at iteration 14333: 0.04291086081946142\n",
      "Train Loss at iteration 14334: 0.04291082159200224\n",
      "Train Loss at iteration 14335: 0.042910782368186844\n",
      "Train Loss at iteration 14336: 0.0429107431480146\n",
      "Train Loss at iteration 14337: 0.042910703931484905\n",
      "Train Loss at iteration 14338: 0.04291066471859715\n",
      "Train Loss at iteration 14339: 0.04291062550935075\n",
      "Train Loss at iteration 14340: 0.04291058630374507\n",
      "Train Loss at iteration 14341: 0.04291054710177955\n",
      "Train Loss at iteration 14342: 0.04291050790345353\n",
      "Train Loss at iteration 14343: 0.04291046870876644\n",
      "Train Loss at iteration 14344: 0.042910429517717695\n",
      "Train Loss at iteration 14345: 0.04291039033030665\n",
      "Train Loss at iteration 14346: 0.04291035114653273\n",
      "Train Loss at iteration 14347: 0.04291031196639531\n",
      "Train Loss at iteration 14348: 0.0429102727898938\n",
      "Train Loss at iteration 14349: 0.04291023361702761\n",
      "Train Loss at iteration 14350: 0.04291019444779611\n",
      "Train Loss at iteration 14351: 0.04291015528219871\n",
      "Train Loss at iteration 14352: 0.042910116120234806\n",
      "Train Loss at iteration 14353: 0.042910076961903784\n",
      "Train Loss at iteration 14354: 0.04291003780720507\n",
      "Train Loss at iteration 14355: 0.04290999865613804\n",
      "Train Loss at iteration 14356: 0.04290995950870209\n",
      "Train Loss at iteration 14357: 0.04290992036489664\n",
      "Train Loss at iteration 14358: 0.04290988122472105\n",
      "Train Loss at iteration 14359: 0.042909842088174756\n",
      "Train Loss at iteration 14360: 0.042909802955257134\n",
      "Train Loss at iteration 14361: 0.04290976382596758\n",
      "Train Loss at iteration 14362: 0.042909724700305504\n",
      "Train Loss at iteration 14363: 0.04290968557827029\n",
      "Train Loss at iteration 14364: 0.04290964645986135\n",
      "Train Loss at iteration 14365: 0.042909607345078093\n",
      "Train Loss at iteration 14366: 0.042909568233919886\n",
      "Train Loss at iteration 14367: 0.04290952912638616\n",
      "Train Loss at iteration 14368: 0.042909490022476265\n",
      "Train Loss at iteration 14369: 0.04290945092218966\n",
      "Train Loss at iteration 14370: 0.042909411825525705\n",
      "Train Loss at iteration 14371: 0.04290937273248381\n",
      "Train Loss at iteration 14372: 0.042909333643063395\n",
      "Train Loss at iteration 14373: 0.04290929455726382\n",
      "Train Loss at iteration 14374: 0.0429092554750845\n",
      "Train Loss at iteration 14375: 0.04290921639652485\n",
      "Train Loss at iteration 14376: 0.04290917732158426\n",
      "Train Loss at iteration 14377: 0.042909138250262095\n",
      "Train Loss at iteration 14378: 0.04290909918255782\n",
      "Train Loss at iteration 14379: 0.04290906011847079\n",
      "Train Loss at iteration 14380: 0.042909021058000416\n",
      "Train Loss at iteration 14381: 0.0429089820011461\n",
      "Train Loss at iteration 14382: 0.04290894294790724\n",
      "Train Loss at iteration 14383: 0.04290890389828324\n",
      "Train Loss at iteration 14384: 0.042908864852273486\n",
      "Train Loss at iteration 14385: 0.04290882580987739\n",
      "Train Loss at iteration 14386: 0.04290878677109436\n",
      "Train Loss at iteration 14387: 0.042908747735923784\n",
      "Train Loss at iteration 14388: 0.04290870870436506\n",
      "Train Loss at iteration 14389: 0.04290866967641761\n",
      "Train Loss at iteration 14390: 0.042908630652080816\n",
      "Train Loss at iteration 14391: 0.04290859163135408\n",
      "Train Loss at iteration 14392: 0.04290855261423682\n",
      "Train Loss at iteration 14393: 0.042908513600728414\n",
      "Train Loss at iteration 14394: 0.04290847459082827\n",
      "Train Loss at iteration 14395: 0.042908435584535796\n",
      "Train Loss at iteration 14396: 0.0429083965818504\n",
      "Train Loss at iteration 14397: 0.04290835758277146\n",
      "Train Loss at iteration 14398: 0.04290831858729839\n",
      "Train Loss at iteration 14399: 0.0429082795954306\n",
      "Train Loss at iteration 14400: 0.0429082406071675\n",
      "Train Loss at iteration 14401: 0.04290820162250846\n",
      "Train Loss at iteration 14402: 0.0429081626414529\n",
      "Train Loss at iteration 14403: 0.04290812366400023\n",
      "Train Loss at iteration 14404: 0.042908084690149846\n",
      "Train Loss at iteration 14405: 0.04290804571990114\n",
      "Train Loss at iteration 14406: 0.04290800675325353\n",
      "Train Loss at iteration 14407: 0.042907967790206406\n",
      "Train Loss at iteration 14408: 0.042907928830759175\n",
      "Train Loss at iteration 14409: 0.04290788987491125\n",
      "Train Loss at iteration 14410: 0.042907850922662014\n",
      "Train Loss at iteration 14411: 0.042907811974010904\n",
      "Train Loss at iteration 14412: 0.042907773028957284\n",
      "Train Loss at iteration 14413: 0.04290773408750057\n",
      "Train Loss at iteration 14414: 0.04290769514964017\n",
      "Train Loss at iteration 14415: 0.04290765621537549\n",
      "Train Loss at iteration 14416: 0.04290761728470594\n",
      "Train Loss at iteration 14417: 0.042907578357630906\n",
      "Train Loss at iteration 14418: 0.042907539434149794\n",
      "Train Loss at iteration 14419: 0.04290750051426203\n",
      "Train Loss at iteration 14420: 0.042907461597966975\n",
      "Train Loss at iteration 14421: 0.042907422685264075\n",
      "Train Loss at iteration 14422: 0.04290738377615271\n",
      "Train Loss at iteration 14423: 0.0429073448706323\n",
      "Train Loss at iteration 14424: 0.04290730596870225\n",
      "Train Loss at iteration 14425: 0.04290726707036195\n",
      "Train Loss at iteration 14426: 0.04290722817561079\n",
      "Train Loss at iteration 14427: 0.042907189284448234\n",
      "Train Loss at iteration 14428: 0.042907150396873615\n",
      "Train Loss at iteration 14429: 0.042907111512886384\n",
      "Train Loss at iteration 14430: 0.04290707263248594\n",
      "Train Loss at iteration 14431: 0.04290703375567167\n",
      "Train Loss at iteration 14432: 0.04290699488244299\n",
      "Train Loss at iteration 14433: 0.04290695601279931\n",
      "Train Loss at iteration 14434: 0.04290691714674003\n",
      "Train Loss at iteration 14435: 0.04290687828426455\n",
      "Train Loss at iteration 14436: 0.042906839425372295\n",
      "Train Loss at iteration 14437: 0.042906800570062645\n",
      "Train Loss at iteration 14438: 0.04290676171833501\n",
      "Train Loss at iteration 14439: 0.04290672287018883\n",
      "Train Loss at iteration 14440: 0.04290668402562347\n",
      "Train Loss at iteration 14441: 0.042906645184638346\n",
      "Train Loss at iteration 14442: 0.042906606347232874\n",
      "Train Loss at iteration 14443: 0.042906567513406456\n",
      "Train Loss at iteration 14444: 0.042906528683158494\n",
      "Train Loss at iteration 14445: 0.0429064898564884\n",
      "Train Loss at iteration 14446: 0.04290645103339557\n",
      "Train Loss at iteration 14447: 0.042906412213879426\n",
      "Train Loss at iteration 14448: 0.04290637339793937\n",
      "Train Loss at iteration 14449: 0.04290633458557481\n",
      "Train Loss at iteration 14450: 0.04290629577678513\n",
      "Train Loss at iteration 14451: 0.04290625697156978\n",
      "Train Loss at iteration 14452: 0.042906218169928116\n",
      "Train Loss at iteration 14453: 0.042906179371859594\n",
      "Train Loss at iteration 14454: 0.042906140577363604\n",
      "Train Loss at iteration 14455: 0.042906101786439534\n",
      "Train Loss at iteration 14456: 0.04290606299908682\n",
      "Train Loss at iteration 14457: 0.042906024215304844\n",
      "Train Loss at iteration 14458: 0.04290598543509303\n",
      "Train Loss at iteration 14459: 0.04290594665845077\n",
      "Train Loss at iteration 14460: 0.04290590788537751\n",
      "Train Loss at iteration 14461: 0.0429058691158726\n",
      "Train Loss at iteration 14462: 0.04290583034993551\n",
      "Train Loss at iteration 14463: 0.042905791587565606\n",
      "Train Loss at iteration 14464: 0.0429057528287623\n",
      "Train Loss at iteration 14465: 0.042905714073525024\n",
      "Train Loss at iteration 14466: 0.042905675321853146\n",
      "Train Loss at iteration 14467: 0.042905636573746124\n",
      "Train Loss at iteration 14468: 0.04290559782920334\n",
      "Train Loss at iteration 14469: 0.0429055590882242\n",
      "Train Loss at iteration 14470: 0.042905520350808114\n",
      "Train Loss at iteration 14471: 0.042905481616954504\n",
      "Train Loss at iteration 14472: 0.04290544288666276\n",
      "Train Loss at iteration 14473: 0.04290540415993231\n",
      "Train Loss at iteration 14474: 0.042905365436762544\n",
      "Train Loss at iteration 14475: 0.0429053267171529\n",
      "Train Loss at iteration 14476: 0.04290528800110276\n",
      "Train Loss at iteration 14477: 0.042905249288611544\n",
      "Train Loss at iteration 14478: 0.042905210579678645\n",
      "Train Loss at iteration 14479: 0.04290517187430351\n",
      "Train Loss at iteration 14480: 0.04290513317248552\n",
      "Train Loss at iteration 14481: 0.04290509447422409\n",
      "Train Loss at iteration 14482: 0.04290505577951862\n",
      "Train Loss at iteration 14483: 0.04290501708836856\n",
      "Train Loss at iteration 14484: 0.04290497840077327\n",
      "Train Loss at iteration 14485: 0.042904939716732204\n",
      "Train Loss at iteration 14486: 0.042904901036244725\n",
      "Train Loss at iteration 14487: 0.04290486235931029\n",
      "Train Loss at iteration 14488: 0.04290482368592828\n",
      "Train Loss at iteration 14489: 0.04290478501609813\n",
      "Train Loss at iteration 14490: 0.042904746349819216\n",
      "Train Loss at iteration 14491: 0.04290470768709099\n",
      "Train Loss at iteration 14492: 0.04290466902791283\n",
      "Train Loss at iteration 14493: 0.042904630372284154\n",
      "Train Loss at iteration 14494: 0.0429045917202044\n",
      "Train Loss at iteration 14495: 0.042904553071672936\n",
      "Train Loss at iteration 14496: 0.04290451442668921\n",
      "Train Loss at iteration 14497: 0.04290447578525262\n",
      "Train Loss at iteration 14498: 0.04290443714736257\n",
      "Train Loss at iteration 14499: 0.04290439851301848\n",
      "Train Loss at iteration 14500: 0.04290435988221975\n",
      "Train Loss at iteration 14501: 0.04290432125496582\n",
      "Train Loss at iteration 14502: 0.04290428263125608\n",
      "Train Loss at iteration 14503: 0.042904244011089936\n",
      "Train Loss at iteration 14504: 0.04290420539446683\n",
      "Train Loss at iteration 14505: 0.04290416678138612\n",
      "Train Loss at iteration 14506: 0.042904128171847286\n",
      "Train Loss at iteration 14507: 0.042904089565849714\n",
      "Train Loss at iteration 14508: 0.042904050963392795\n",
      "Train Loss at iteration 14509: 0.042904012364475955\n",
      "Train Loss at iteration 14510: 0.042903973769098616\n",
      "Train Loss at iteration 14511: 0.04290393517726016\n",
      "Train Loss at iteration 14512: 0.04290389658896006\n",
      "Train Loss at iteration 14513: 0.04290385800419767\n",
      "Train Loss at iteration 14514: 0.04290381942297244\n",
      "Train Loss at iteration 14515: 0.04290378084528377\n",
      "Train Loss at iteration 14516: 0.04290374227113106\n",
      "Train Loss at iteration 14517: 0.04290370370051375\n",
      "Train Loss at iteration 14518: 0.04290366513343123\n",
      "Train Loss at iteration 14519: 0.04290362656988293\n",
      "Train Loss at iteration 14520: 0.04290358800986826\n",
      "Train Loss at iteration 14521: 0.042903549453386626\n",
      "Train Loss at iteration 14522: 0.04290351090043745\n",
      "Train Loss at iteration 14523: 0.04290347235102014\n",
      "Train Loss at iteration 14524: 0.04290343380513412\n",
      "Train Loss at iteration 14525: 0.042903395262778796\n",
      "Train Loss at iteration 14526: 0.04290335672395359\n",
      "Train Loss at iteration 14527: 0.042903318188657905\n",
      "Train Loss at iteration 14528: 0.04290327965689117\n",
      "Train Loss at iteration 14529: 0.04290324112865278\n",
      "Train Loss at iteration 14530: 0.042903202603942175\n",
      "Train Loss at iteration 14531: 0.042903164082758745\n",
      "Train Loss at iteration 14532: 0.04290312556510191\n",
      "Train Loss at iteration 14533: 0.04290308705097111\n",
      "Train Loss at iteration 14534: 0.04290304854036573\n",
      "Train Loss at iteration 14535: 0.0429030100332852\n",
      "Train Loss at iteration 14536: 0.04290297152972892\n",
      "Train Loss at iteration 14537: 0.04290293302969633\n",
      "Train Loss at iteration 14538: 0.04290289453318683\n",
      "Train Loss at iteration 14539: 0.04290285604019985\n",
      "Train Loss at iteration 14540: 0.04290281755073477\n",
      "Train Loss at iteration 14541: 0.042902779064791056\n",
      "Train Loss at iteration 14542: 0.04290274058236808\n",
      "Train Loss at iteration 14543: 0.04290270210346528\n",
      "Train Loss at iteration 14544: 0.04290266362808208\n",
      "Train Loss at iteration 14545: 0.042902625156217865\n",
      "Train Loss at iteration 14546: 0.042902586687872085\n",
      "Train Loss at iteration 14547: 0.042902548223044144\n",
      "Train Loss at iteration 14548: 0.04290250976173345\n",
      "Train Loss at iteration 14549: 0.04290247130393944\n",
      "Train Loss at iteration 14550: 0.042902432849661505\n",
      "Train Loss at iteration 14551: 0.04290239439889906\n",
      "Train Loss at iteration 14552: 0.042902355951651566\n",
      "Train Loss at iteration 14553: 0.04290231750791838\n",
      "Train Loss at iteration 14554: 0.04290227906769897\n",
      "Train Loss at iteration 14555: 0.04290224063099274\n",
      "Train Loss at iteration 14556: 0.04290220219779908\n",
      "Train Loss at iteration 14557: 0.04290216376811744\n",
      "Train Loss at iteration 14558: 0.04290212534194722\n",
      "Train Loss at iteration 14559: 0.04290208691928785\n",
      "Train Loss at iteration 14560: 0.042902048500138734\n",
      "Train Loss at iteration 14561: 0.04290201008449929\n",
      "Train Loss at iteration 14562: 0.042901971672368935\n",
      "Train Loss at iteration 14563: 0.04290193326374711\n",
      "Train Loss at iteration 14564: 0.042901894858633216\n",
      "Train Loss at iteration 14565: 0.04290185645702666\n",
      "Train Loss at iteration 14566: 0.04290181805892689\n",
      "Train Loss at iteration 14567: 0.042901779664333305\n",
      "Train Loss at iteration 14568: 0.042901741273245315\n",
      "Train Loss at iteration 14569: 0.04290170288566235\n",
      "Train Loss at iteration 14570: 0.04290166450158382\n",
      "Train Loss at iteration 14571: 0.042901626121009166\n",
      "Train Loss at iteration 14572: 0.04290158774393779\n",
      "Train Loss at iteration 14573: 0.042901549370369114\n",
      "Train Loss at iteration 14574: 0.04290151100030255\n",
      "Train Loss at iteration 14575: 0.042901472633737524\n",
      "Train Loss at iteration 14576: 0.04290143427067346\n",
      "Train Loss at iteration 14577: 0.042901395911109766\n",
      "Train Loss at iteration 14578: 0.042901357555045865\n",
      "Train Loss at iteration 14579: 0.04290131920248119\n",
      "Train Loss at iteration 14580: 0.04290128085341514\n",
      "Train Loss at iteration 14581: 0.042901242507847145\n",
      "Train Loss at iteration 14582: 0.042901204165776634\n",
      "Train Loss at iteration 14583: 0.04290116582720301\n",
      "Train Loss at iteration 14584: 0.0429011274921257\n",
      "Train Loss at iteration 14585: 0.042901089160544116\n",
      "Train Loss at iteration 14586: 0.042901050832457696\n",
      "Train Loss at iteration 14587: 0.04290101250786586\n",
      "Train Loss at iteration 14588: 0.04290097418676801\n",
      "Train Loss at iteration 14589: 0.042900935869163576\n",
      "Train Loss at iteration 14590: 0.04290089755505198\n",
      "Train Loss at iteration 14591: 0.04290085924443263\n",
      "Train Loss at iteration 14592: 0.04290082093730497\n",
      "Train Loss at iteration 14593: 0.04290078263366841\n",
      "Train Loss at iteration 14594: 0.04290074433352237\n",
      "Train Loss at iteration 14595: 0.04290070603686627\n",
      "Train Loss at iteration 14596: 0.04290066774369953\n",
      "Train Loss at iteration 14597: 0.042900629454021566\n",
      "Train Loss at iteration 14598: 0.04290059116783182\n",
      "Train Loss at iteration 14599: 0.042900552885129686\n",
      "Train Loss at iteration 14600: 0.04290051460591461\n",
      "Train Loss at iteration 14601: 0.042900476330186\n",
      "Train Loss at iteration 14602: 0.04290043805794329\n",
      "Train Loss at iteration 14603: 0.04290039978918588\n",
      "Train Loss at iteration 14604: 0.04290036152391322\n",
      "Train Loss at iteration 14605: 0.04290032326212471\n",
      "Train Loss at iteration 14606: 0.04290028500381978\n",
      "Train Loss at iteration 14607: 0.042900246748997846\n",
      "Train Loss at iteration 14608: 0.04290020849765835\n",
      "Train Loss at iteration 14609: 0.042900170249800695\n",
      "Train Loss at iteration 14610: 0.0429001320054243\n",
      "Train Loss at iteration 14611: 0.0429000937645286\n",
      "Train Loss at iteration 14612: 0.04290005552711302\n",
      "Train Loss at iteration 14613: 0.04290001729317698\n",
      "Train Loss at iteration 14614: 0.042899979062719897\n",
      "Train Loss at iteration 14615: 0.04289994083574118\n",
      "Train Loss at iteration 14616: 0.04289990261224029\n",
      "Train Loss at iteration 14617: 0.04289986439221663\n",
      "Train Loss at iteration 14618: 0.04289982617566961\n",
      "Train Loss at iteration 14619: 0.042899787962598676\n",
      "Train Loss at iteration 14620: 0.04289974975300324\n",
      "Train Loss at iteration 14621: 0.04289971154688272\n",
      "Train Loss at iteration 14622: 0.04289967334423656\n",
      "Train Loss at iteration 14623: 0.04289963514506417\n",
      "Train Loss at iteration 14624: 0.042899596949364974\n",
      "Train Loss at iteration 14625: 0.0428995587571384\n",
      "Train Loss at iteration 14626: 0.04289952056838385\n",
      "Train Loss at iteration 14627: 0.04289948238310079\n",
      "Train Loss at iteration 14628: 0.04289944420128861\n",
      "Train Loss at iteration 14629: 0.04289940602294674\n",
      "Train Loss at iteration 14630: 0.04289936784807463\n",
      "Train Loss at iteration 14631: 0.04289932967667168\n",
      "Train Loss at iteration 14632: 0.0428992915087373\n",
      "Train Loss at iteration 14633: 0.04289925334427094\n",
      "Train Loss at iteration 14634: 0.042899215183272034\n",
      "Train Loss at iteration 14635: 0.04289917702573999\n",
      "Train Loss at iteration 14636: 0.04289913887167422\n",
      "Train Loss at iteration 14637: 0.04289910072107418\n",
      "Train Loss at iteration 14638: 0.04289906257393928\n",
      "Train Loss at iteration 14639: 0.04289902443026893\n",
      "Train Loss at iteration 14640: 0.04289898629006258\n",
      "Train Loss at iteration 14641: 0.04289894815331965\n",
      "Train Loss at iteration 14642: 0.04289891002003955\n",
      "Train Loss at iteration 14643: 0.042898871890221735\n",
      "Train Loss at iteration 14644: 0.04289883376386559\n",
      "Train Loss at iteration 14645: 0.042898795640970586\n",
      "Train Loss at iteration 14646: 0.042898757521536116\n",
      "Train Loss at iteration 14647: 0.042898719405561614\n",
      "Train Loss at iteration 14648: 0.04289868129304651\n",
      "Train Loss at iteration 14649: 0.04289864318399023\n",
      "Train Loss at iteration 14650: 0.042898605078392216\n",
      "Train Loss at iteration 14651: 0.04289856697625187\n",
      "Train Loss at iteration 14652: 0.04289852887756861\n",
      "Train Loss at iteration 14653: 0.0428984907823419\n",
      "Train Loss at iteration 14654: 0.04289845269057114\n",
      "Train Loss at iteration 14655: 0.04289841460225576\n",
      "Train Loss at iteration 14656: 0.042898376517395184\n",
      "Train Loss at iteration 14657: 0.04289833843598886\n",
      "Train Loss at iteration 14658: 0.0428983003580362\n",
      "Train Loss at iteration 14659: 0.04289826228353663\n",
      "Train Loss at iteration 14660: 0.04289822421248958\n",
      "Train Loss at iteration 14661: 0.04289818614489446\n",
      "Train Loss at iteration 14662: 0.04289814808075073\n",
      "Train Loss at iteration 14663: 0.042898110020057806\n",
      "Train Loss at iteration 14664: 0.0428980719628151\n",
      "Train Loss at iteration 14665: 0.042898033909022054\n",
      "Train Loss at iteration 14666: 0.04289799585867811\n",
      "Train Loss at iteration 14667: 0.04289795781178266\n",
      "Train Loss at iteration 14668: 0.04289791976833516\n",
      "Train Loss at iteration 14669: 0.042897881728335016\n",
      "Train Loss at iteration 14670: 0.04289784369178168\n",
      "Train Loss at iteration 14671: 0.04289780565867458\n",
      "Train Loss at iteration 14672: 0.04289776762901312\n",
      "Train Loss at iteration 14673: 0.04289772960279676\n",
      "Train Loss at iteration 14674: 0.0428976915800249\n",
      "Train Loss at iteration 14675: 0.042897653560696976\n",
      "Train Loss at iteration 14676: 0.04289761554481243\n",
      "Train Loss at iteration 14677: 0.04289757753237068\n",
      "Train Loss at iteration 14678: 0.042897539523371164\n",
      "Train Loss at iteration 14679: 0.042897501517813304\n",
      "Train Loss at iteration 14680: 0.042897463515696524\n",
      "Train Loss at iteration 14681: 0.04289742551702026\n",
      "Train Loss at iteration 14682: 0.042897387521783945\n",
      "Train Loss at iteration 14683: 0.04289734952998701\n",
      "Train Loss at iteration 14684: 0.04289731154162887\n",
      "Train Loss at iteration 14685: 0.04289727355670898\n",
      "Train Loss at iteration 14686: 0.04289723557522673\n",
      "Train Loss at iteration 14687: 0.04289719759718159\n",
      "Train Loss at iteration 14688: 0.04289715962257297\n",
      "Train Loss at iteration 14689: 0.04289712165140029\n",
      "Train Loss at iteration 14690: 0.04289708368366301\n",
      "Train Loss at iteration 14691: 0.04289704571936055\n",
      "Train Loss at iteration 14692: 0.042897007758492325\n",
      "Train Loss at iteration 14693: 0.042896969801057776\n",
      "Train Loss at iteration 14694: 0.04289693184705634\n",
      "Train Loss at iteration 14695: 0.042896893896487424\n",
      "Train Loss at iteration 14696: 0.04289685594935049\n",
      "Train Loss at iteration 14697: 0.04289681800564495\n",
      "Train Loss at iteration 14698: 0.04289678134609543\n",
      "Train Loss at iteration 14699: 0.042896743744343484\n",
      "Train Loss at iteration 14700: 0.04289670632007877\n",
      "Train Loss at iteration 14701: 0.04289666954986511\n",
      "Train Loss at iteration 14702: 0.04289663161543378\n",
      "Train Loss at iteration 14703: 0.04289659524811035\n",
      "Train Loss at iteration 14704: 0.042896557486475285\n",
      "Train Loss at iteration 14705: 0.04289652057134203\n",
      "Train Loss at iteration 14706: 0.042896483408444486\n",
      "Train Loss at iteration 14707: 0.04289644605082676\n",
      "Train Loss at iteration 14708: 0.04289640937621765\n",
      "Train Loss at iteration 14709: 0.04289637167435672\n",
      "Train Loss at iteration 14710: 0.042896335384246066\n",
      "Train Loss at iteration 14711: 0.04289629744355676\n",
      "Train Loss at iteration 14712: 0.0428962613921697\n",
      "Train Loss at iteration 14713: 0.04289622349247786\n",
      "Train Loss at iteration 14714: 0.04289618726030869\n",
      "Train Loss at iteration 14715: 0.0428961495720527\n",
      "Train Loss at iteration 14716: 0.042896113246147984\n",
      "Train Loss at iteration 14717: 0.042896075680051854\n",
      "Train Loss at iteration 14718: 0.04289603934137921\n",
      "Train Loss at iteration 14719: 0.0428960018134138\n",
      "Train Loss at iteration 14720: 0.04289596553888045\n",
      "Train Loss at iteration 14721: 0.04289592796947499\n",
      "Train Loss at iteration 14722: 0.042895891832219285\n",
      "Train Loss at iteration 14723: 0.04289585414596406\n",
      "Train Loss at iteration 14724: 0.04289581821556743\n",
      "Train Loss at iteration 14725: 0.04289578050108726\n",
      "Train Loss at iteration 14726: 0.04289574453764812\n",
      "Train Loss at iteration 14727: 0.0428957070721329\n",
      "Train Loss at iteration 14728: 0.042895670755232776\n",
      "Train Loss at iteration 14729: 0.042895633714871054\n",
      "Train Loss at iteration 14730: 0.0428955969871361\n",
      "Train Loss at iteration 14731: 0.042895560426957774\n",
      "Train Loss at iteration 14732: 0.04289552323204058\n",
      "Train Loss at iteration 14733: 0.0428954872050022\n",
      "Train Loss at iteration 14734: 0.04289544975687524\n",
      "Train Loss at iteration 14735: 0.042895413788554604\n",
      "Train Loss at iteration 14736: 0.04289537668316299\n",
      "Train Loss at iteration 14737: 0.042895340053940224\n",
      "Train Loss at iteration 14738: 0.042895303662818844\n",
      "Train Loss at iteration 14739: 0.042895266354215165\n",
      "Train Loss at iteration 14740: 0.04289523067961435\n",
      "Train Loss at iteration 14741: 0.04289519346311304\n",
      "Train Loss at iteration 14742: 0.042895156958923784\n",
      "Train Loss at iteration 14743: 0.04289512061684066\n",
      "Train Loss at iteration 14744: 0.04289508343001313\n",
      "Train Loss at iteration 14745: 0.042895047640784154\n",
      "Train Loss at iteration 14746: 0.04289501069796296\n",
      "Train Loss at iteration 14747: 0.04289497393013495\n",
      "Train Loss at iteration 14748: 0.04289493800410858\n",
      "Train Loss at iteration 14749: 0.04289490092467056\n",
      "Train Loss at iteration 14750: 0.042894864659729394\n",
      "Train Loss at iteration 14751: 0.04289482833300184\n",
      "Train Loss at iteration 14752: 0.04289479132678068\n",
      "Train Loss at iteration 14753: 0.04289475540878713\n",
      "Train Loss at iteration 14754: 0.04289471882757396\n",
      "Train Loss at iteration 14755: 0.042894681888946164\n",
      "Train Loss at iteration 14756: 0.04289464617497894\n",
      "Train Loss at iteration 14757: 0.042894609475097184\n",
      "Train Loss at iteration 14758: 0.04289457259916912\n",
      "Train Loss at iteration 14759: 0.04289453695635343\n",
      "Train Loss at iteration 14760: 0.04289450026452924\n",
      "Train Loss at iteration 14761: 0.04289446344697966\n",
      "Train Loss at iteration 14762: 0.04289442775160259\n",
      "Train Loss at iteration 14763: 0.04289439118620596\n",
      "Train Loss at iteration 14764: 0.04289435442320559\n",
      "Train Loss at iteration 14765: 0.042894318559923615\n",
      "Train Loss at iteration 14766: 0.04289428223163778\n",
      "Train Loss at iteration 14767: 0.042894245519781965\n",
      "Train Loss at iteration 14768: 0.04289420938090706\n",
      "Train Loss at iteration 14769: 0.04289417339334015\n",
      "Train Loss at iteration 14770: 0.04289413672959209\n",
      "Train Loss at iteration 14771: 0.04289410021444747\n",
      "Train Loss at iteration 14772: 0.04289406466469143\n",
      "Train Loss at iteration 14773: 0.04289402804633403\n",
      "Train Loss at iteration 14774: 0.04289399146448271\n",
      "Train Loss at iteration 14775: 0.04289395563307666\n",
      "Train Loss at iteration 14776: 0.04289391949533953\n",
      "Train Loss at iteration 14777: 0.04289388295231037\n",
      "Train Loss at iteration 14778: 0.04289384647413571\n",
      "Train Loss at iteration 14779: 0.042893811035995516\n",
      "Train Loss at iteration 14780: 0.042893774531378215\n",
      "Train Loss at iteration 14781: 0.04289373805992749\n",
      "Train Loss at iteration 14782: 0.04289370193138571\n",
      "Train Loss at iteration 14783: 0.04289366622717502\n",
      "Train Loss at iteration 14784: 0.042893629788684495\n",
      "Train Loss at iteration 14785: 0.0428935933813862\n",
      "Train Loss at iteration 14786: 0.042893557398024224\n",
      "Train Loss at iteration 14787: 0.042893521625511674\n",
      "Train Loss at iteration 14788: 0.04289348524783867\n",
      "Train Loss at iteration 14789: 0.042893448899661187\n",
      "Train Loss at iteration 14790: 0.04289341287488486\n",
      "Train Loss at iteration 14791: 0.04289337721516706\n",
      "Train Loss at iteration 14792: 0.04289334089378339\n",
      "Train Loss at iteration 14793: 0.04289330460042233\n",
      "Train Loss at iteration 14794: 0.042893268363406394\n",
      "Train Loss at iteration 14795: 0.0428932329827344\n",
      "Train Loss at iteration 14796: 0.04289319671374668\n",
      "Train Loss at iteration 14797: 0.042893160471496376\n",
      "Train Loss at iteration 14798: 0.04289312425358721\n",
      "Train Loss at iteration 14799: 0.04289308851979042\n",
      "Train Loss at iteration 14800: 0.04289305272219537\n",
      "Train Loss at iteration 14801: 0.04289301652516905\n",
      "Train Loss at iteration 14802: 0.04289298035295197\n",
      "Train Loss at iteration 14803: 0.04289294420324858\n",
      "Train Loss at iteration 14804: 0.042892908680173925\n",
      "Train Loss at iteration 14805: 0.042892872750797416\n",
      "Train Loss at iteration 14806: 0.04289283661920477\n",
      "Train Loss at iteration 14807: 0.04289280051081582\n",
      "Train Loss at iteration 14808: 0.04289276442341538\n",
      "Train Loss at iteration 14809: 0.042892728849019686\n",
      "Train Loss at iteration 14810: 0.042892693044147584\n",
      "Train Loss at iteration 14811: 0.0428926569726074\n",
      "Train Loss at iteration 14812: 0.042892620922921135\n",
      "Train Loss at iteration 14813: 0.04289258489293923\n",
      "Train Loss at iteration 14814: 0.04289254903149741\n",
      "Train Loss at iteration 14815: 0.042892513582289016\n",
      "Train Loss at iteration 14816: 0.042892477566318706\n",
      "Train Loss at iteration 14817: 0.04289244157105819\n",
      "Train Loss at iteration 14818: 0.042892405594411924\n",
      "Train Loss at iteration 14819: 0.04289236963575421\n",
      "Train Loss at iteration 14820: 0.042892333932819555\n",
      "Train Loss at iteration 14821: 0.04289229840584753\n",
      "Train Loss at iteration 14822: 0.04289226245895216\n",
      "Train Loss at iteration 14823: 0.04289222653126525\n",
      "Train Loss at iteration 14824: 0.042892190620764895\n",
      "Train Loss at iteration 14825: 0.04289215472689447\n",
      "Train Loss at iteration 14826: 0.04289211884915982\n",
      "Train Loss at iteration 14827: 0.04289208355161098\n",
      "Train Loss at iteration 14828: 0.04289204770818315\n",
      "Train Loss at iteration 14829: 0.042892011839935275\n",
      "Train Loss at iteration 14830: 0.04289197598922897\n",
      "Train Loss at iteration 14831: 0.04289194015412373\n",
      "Train Loss at iteration 14832: 0.042891904334139454\n",
      "Train Loss at iteration 14833: 0.04289186852885404\n",
      "Train Loss at iteration 14834: 0.04289183317328268\n",
      "Train Loss at iteration 14835: 0.04289179746999975\n",
      "Train Loss at iteration 14836: 0.04289176167254507\n",
      "Train Loss at iteration 14837: 0.04289172589132323\n",
      "Train Loss at iteration 14838: 0.04289169012445462\n",
      "Train Loss at iteration 14839: 0.0428916543715165\n",
      "Train Loss at iteration 14840: 0.042891618632140846\n",
      "Train Loss at iteration 14841: 0.042891582905979914\n",
      "Train Loss at iteration 14842: 0.04289154754396691\n",
      "Train Loss at iteration 14843: 0.04289151193527321\n",
      "Train Loss at iteration 14844: 0.042891476215423344\n",
      "Train Loss at iteration 14845: 0.04289144051044119\n",
      "Train Loss at iteration 14846: 0.04289140481851014\n",
      "Train Loss at iteration 14847: 0.04289136913926684\n",
      "Train Loss at iteration 14848: 0.04289133347239918\n",
      "Train Loss at iteration 14849: 0.04289129781761223\n",
      "Train Loss at iteration 14850: 0.042891262174626396\n",
      "Train Loss at iteration 14851: 0.0428912266754434\n",
      "Train Loss at iteration 14852: 0.04289119129647858\n",
      "Train Loss at iteration 14853: 0.04289115565855934\n",
      "Train Loss at iteration 14854: 0.04289112003419053\n",
      "Train Loss at iteration 14855: 0.042891084421614795\n",
      "Train Loss at iteration 14856: 0.04289104882052469\n",
      "Train Loss at iteration 14857: 0.04289101323066083\n",
      "Train Loss at iteration 14858: 0.04289097765177796\n",
      "Train Loss at iteration 14859: 0.042890942083643434\n",
      "Train Loss at iteration 14860: 0.042890906526036346\n",
      "Train Loss at iteration 14861: 0.04289087097874681\n",
      "Train Loss at iteration 14862: 0.042890835441575285\n",
      "Train Loss at iteration 14863: 0.042890800080447\n",
      "Train Loss at iteration 14864: 0.04289076467638441\n",
      "Train Loss at iteration 14865: 0.042890729143232\n",
      "Train Loss at iteration 14866: 0.04289069362203265\n",
      "Train Loss at iteration 14867: 0.042890658111100544\n",
      "Train Loss at iteration 14868: 0.0428906226101956\n",
      "Train Loss at iteration 14869: 0.042890587119121894\n",
      "Train Loss at iteration 14870: 0.042890551637694146\n",
      "Train Loss at iteration 14871: 0.042890516165736164\n",
      "Train Loss at iteration 14872: 0.04289048070308041\n",
      "Train Loss at iteration 14873: 0.042890445249567394\n",
      "Train Loss at iteration 14874: 0.0428904098050452\n",
      "Train Loss at iteration 14875: 0.04289037436936904\n",
      "Train Loss at iteration 14876: 0.04289033894240079\n",
      "Train Loss at iteration 14877: 0.04289030352400865\n",
      "Train Loss at iteration 14878: 0.04289026811406674\n",
      "Train Loss at iteration 14879: 0.042890232712454734\n",
      "Train Loss at iteration 14880: 0.042890197319057605\n",
      "Train Loss at iteration 14881: 0.04289016204384141\n",
      "Train Loss at iteration 14882: 0.04289012670491461\n",
      "Train Loss at iteration 14883: 0.04289009131513794\n",
      "Train Loss at iteration 14884: 0.04289005593544737\n",
      "Train Loss at iteration 14885: 0.04289002056423649\n",
      "Train Loss at iteration 14886: 0.0428899852013401\n",
      "Train Loss at iteration 14887: 0.04288994984663297\n",
      "Train Loss at iteration 14888: 0.042889914499996525\n",
      "Train Loss at iteration 14889: 0.042889879161317625\n",
      "Train Loss at iteration 14890: 0.0428898438304883\n",
      "Train Loss at iteration 14891: 0.04288980850740539\n",
      "Train Loss at iteration 14892: 0.042889773191970286\n",
      "Train Loss at iteration 14893: 0.04288973788408865\n",
      "Train Loss at iteration 14894: 0.04288970258367012\n",
      "Train Loss at iteration 14895: 0.04288966729062819\n",
      "Train Loss at iteration 14896: 0.042889632004879885\n",
      "Train Loss at iteration 14897: 0.04288959672634561\n",
      "Train Loss at iteration 14898: 0.04288956145494895\n",
      "Train Loss at iteration 14899: 0.042889526190616506\n",
      "Train Loss at iteration 14900: 0.04288949093327765\n",
      "Train Loss at iteration 14901: 0.04288945568286455\n",
      "Train Loss at iteration 14902: 0.04288942043931179\n",
      "Train Loss at iteration 14903: 0.04288938520255643\n",
      "Train Loss at iteration 14904: 0.04288934997253778\n",
      "Train Loss at iteration 14905: 0.0428893147491973\n",
      "Train Loss at iteration 14906: 0.04288927953247847\n",
      "Train Loss at iteration 14907: 0.042889244322326776\n",
      "Train Loss at iteration 14908: 0.04288920911868945\n",
      "Train Loss at iteration 14909: 0.04288917392151552\n",
      "Train Loss at iteration 14910: 0.04288913873075566\n",
      "Train Loss at iteration 14911: 0.042889103546362106\n",
      "Train Loss at iteration 14912: 0.0428890683682886\n",
      "Train Loss at iteration 14913: 0.04288903319649033\n",
      "Train Loss at iteration 14914: 0.042888998030923775\n",
      "Train Loss at iteration 14915: 0.042888962871546775\n",
      "Train Loss at iteration 14916: 0.042888927718318356\n",
      "Train Loss at iteration 14917: 0.042888892571198725\n",
      "Train Loss at iteration 14918: 0.042888857430149224\n",
      "Train Loss at iteration 14919: 0.04288882229513224\n",
      "Train Loss at iteration 14920: 0.04288878716611118\n",
      "Train Loss at iteration 14921: 0.04288875204305045\n",
      "Train Loss at iteration 14922: 0.04288871692591534\n",
      "Train Loss at iteration 14923: 0.04288868181467204\n",
      "Train Loss at iteration 14924: 0.04288864670928767\n",
      "Train Loss at iteration 14925: 0.04288861160973004\n",
      "Train Loss at iteration 14926: 0.04288857651596783\n",
      "Train Loss at iteration 14927: 0.042888541427970425\n",
      "Train Loss at iteration 14928: 0.042888506345707914\n",
      "Train Loss at iteration 14929: 0.042888471269151136\n",
      "Train Loss at iteration 14930: 0.042888436198271526\n",
      "Train Loss at iteration 14931: 0.042888401133041175\n",
      "Train Loss at iteration 14932: 0.04288836607343278\n",
      "Train Loss at iteration 14933: 0.04288833101941963\n",
      "Train Loss at iteration 14934: 0.042888295970975564\n",
      "Train Loss at iteration 14935: 0.042888260928074955\n",
      "Train Loss at iteration 14936: 0.042888225890692716\n",
      "Train Loss at iteration 14937: 0.042888190858804257\n",
      "Train Loss at iteration 14938: 0.04288815583238544\n",
      "Train Loss at iteration 14939: 0.04288812081141264\n",
      "Train Loss at iteration 14940: 0.04288808579586265\n",
      "Train Loss at iteration 14941: 0.042888050785712686\n",
      "Train Loss at iteration 14942: 0.042888015780940404\n",
      "Train Loss at iteration 14943: 0.04288798078152385\n",
      "Train Loss at iteration 14944: 0.04288794578744148\n",
      "Train Loss at iteration 14945: 0.04288791079867208\n",
      "Train Loss at iteration 14946: 0.04288787581519485\n",
      "Train Loss at iteration 14947: 0.042887840836989315\n",
      "Train Loss at iteration 14948: 0.04288780586403536\n",
      "Train Loss at iteration 14949: 0.042887770896313175\n",
      "Train Loss at iteration 14950: 0.042887735933803274\n",
      "Train Loss at iteration 14951: 0.042887700976486504\n",
      "Train Loss at iteration 14952: 0.04288766602434399\n",
      "Train Loss at iteration 14953: 0.04288763107735716\n",
      "Train Loss at iteration 14954: 0.04288759613550772\n",
      "Train Loss at iteration 14955: 0.04288756119877765\n",
      "Train Loss at iteration 14956: 0.0428875262671492\n",
      "Train Loss at iteration 14957: 0.04288749134060487\n",
      "Train Loss at iteration 14958: 0.04288745641912743\n",
      "Train Loss at iteration 14959: 0.0428874215026999\n",
      "Train Loss at iteration 14960: 0.04288738659130549\n",
      "Train Loss at iteration 14961: 0.0428873516849277\n",
      "Train Loss at iteration 14962: 0.042887316783550226\n",
      "Train Loss at iteration 14963: 0.042887281887157\n",
      "Train Loss at iteration 14964: 0.04288724699573216\n",
      "Train Loss at iteration 14965: 0.04288721210926004\n",
      "Train Loss at iteration 14966: 0.04288717722772521\n",
      "Train Loss at iteration 14967: 0.042887142351112406\n",
      "Train Loss at iteration 14968: 0.042887107479406605\n",
      "Train Loss at iteration 14969: 0.04288707261259291\n",
      "Train Loss at iteration 14970: 0.04288703775065667\n",
      "Train Loss at iteration 14971: 0.04288700289358338\n",
      "Train Loss at iteration 14972: 0.042886968041358726\n",
      "Train Loss at iteration 14973: 0.042886933193968566\n",
      "Train Loss at iteration 14974: 0.042886898351398924\n",
      "Train Loss at iteration 14975: 0.042886863513635985\n",
      "Train Loss at iteration 14976: 0.04288682868066613\n",
      "Train Loss at iteration 14977: 0.042886793852475846\n",
      "Train Loss at iteration 14978: 0.04288675902905181\n",
      "Train Loss at iteration 14979: 0.04288672421038086\n",
      "Train Loss at iteration 14980: 0.04288668939644997\n",
      "Train Loss at iteration 14981: 0.04288665458724625\n",
      "Train Loss at iteration 14982: 0.04288661978275694\n",
      "Train Loss at iteration 14983: 0.0428865849829695\n",
      "Train Loss at iteration 14984: 0.042886550187871464\n",
      "Train Loss at iteration 14985: 0.04288651539745048\n",
      "Train Loss at iteration 14986: 0.042886480611694394\n",
      "Train Loss at iteration 14987: 0.04288644583059114\n",
      "Train Loss at iteration 14988: 0.0428864110541288\n",
      "Train Loss at iteration 14989: 0.04288637628229557\n",
      "Train Loss at iteration 14990: 0.04288634151507978\n",
      "Train Loss at iteration 14991: 0.04288630675246988\n",
      "Train Loss at iteration 14992: 0.04288627199445445\n",
      "Train Loss at iteration 14993: 0.04288623724102215\n",
      "Train Loss at iteration 14994: 0.042886202492161817\n",
      "Train Loss at iteration 14995: 0.04288616774786235\n",
      "Train Loss at iteration 14996: 0.0428861330081128\n",
      "Train Loss at iteration 14997: 0.04288609827290228\n",
      "Train Loss at iteration 14998: 0.04288606354222008\n",
      "Train Loss at iteration 14999: 0.04288602881605553\n",
      "Train Loss at iteration 15000: 0.04288599409439811\n",
      "Train Loss at iteration 15001: 0.04288595937723738\n",
      "Train Loss at iteration 15002: 0.04288592466456302\n",
      "Train Loss at iteration 15003: 0.04288588995636481\n",
      "Train Loss at iteration 15004: 0.0428858552526326\n",
      "Train Loss at iteration 15005: 0.042885820553356394\n",
      "Train Loss at iteration 15006: 0.04288578585852623\n",
      "Train Loss at iteration 15007: 0.04288575116813231\n",
      "Train Loss at iteration 15008: 0.042885716482164854\n",
      "Train Loss at iteration 15009: 0.042885681800614237\n",
      "Train Loss at iteration 15010: 0.04288564712347088\n",
      "Train Loss at iteration 15011: 0.04288561245072535\n",
      "Train Loss at iteration 15012: 0.042885577782368234\n",
      "Train Loss at iteration 15013: 0.042885543118390275\n",
      "Train Loss at iteration 15014: 0.04288550845878226\n",
      "Train Loss at iteration 15015: 0.04288547380353506\n",
      "Train Loss at iteration 15016: 0.04288543915263965\n",
      "Train Loss at iteration 15017: 0.04288540450608709\n",
      "Train Loss at iteration 15018: 0.0428853698638685\n",
      "Train Loss at iteration 15019: 0.0428853352259751\n",
      "Train Loss at iteration 15020: 0.042885300592398194\n",
      "Train Loss at iteration 15021: 0.042885265963129145\n",
      "Train Loss at iteration 15022: 0.04288523133815943\n",
      "Train Loss at iteration 15023: 0.042885196717480534\n",
      "Train Loss at iteration 15024: 0.04288516210108412\n",
      "Train Loss at iteration 15025: 0.04288512748896182\n",
      "Train Loss at iteration 15026: 0.042885092881105424\n",
      "Train Loss at iteration 15027: 0.04288505827750676\n",
      "Train Loss at iteration 15028: 0.04288502367815773\n",
      "Train Loss at iteration 15029: 0.0428849890830503\n",
      "Train Loss at iteration 15030: 0.04288495449217654\n",
      "Train Loss at iteration 15031: 0.04288491990552853\n",
      "Train Loss at iteration 15032: 0.04288488532309849\n",
      "Train Loss at iteration 15033: 0.04288485074487865\n",
      "Train Loss at iteration 15034: 0.04288481617086137\n",
      "Train Loss at iteration 15035: 0.042884781601039\n",
      "Train Loss at iteration 15036: 0.042884747035404035\n",
      "Train Loss at iteration 15037: 0.04288471247394898\n",
      "Train Loss at iteration 15038: 0.04288467791666642\n",
      "Train Loss at iteration 15039: 0.04288464336354902\n",
      "Train Loss at iteration 15040: 0.04288460881458948\n",
      "Train Loss at iteration 15041: 0.04288457426978059\n",
      "Train Loss at iteration 15042: 0.042884539729115184\n",
      "Train Loss at iteration 15043: 0.04288450519258617\n",
      "Train Loss at iteration 15044: 0.04288447066018652\n",
      "Train Loss at iteration 15045: 0.04288443613190922\n",
      "Train Loss at iteration 15046: 0.042884401607747404\n",
      "Train Loss at iteration 15047: 0.04288436708769418\n",
      "Train Loss at iteration 15048: 0.042884332571742755\n",
      "Train Loss at iteration 15049: 0.042884298059886375\n",
      "Train Loss at iteration 15050: 0.04288426355211838\n",
      "Train Loss at iteration 15051: 0.04288422904843212\n",
      "Train Loss at iteration 15052: 0.04288419454882103\n",
      "Train Loss at iteration 15053: 0.042884160053278594\n",
      "Train Loss at iteration 15054: 0.04288412556179834\n",
      "Train Loss at iteration 15055: 0.04288409107437386\n",
      "Train Loss at iteration 15056: 0.042884056590998795\n",
      "Train Loss at iteration 15057: 0.04288402211166686\n",
      "Train Loss at iteration 15058: 0.04288398763637177\n",
      "Train Loss at iteration 15059: 0.042883953165107354\n",
      "Train Loss at iteration 15060: 0.042883918697867444\n",
      "Train Loss at iteration 15061: 0.042883884234645966\n",
      "Train Loss at iteration 15062: 0.04288384977543684\n",
      "Train Loss at iteration 15063: 0.04288381532023411\n",
      "Train Loss at iteration 15064: 0.0428837808690318\n",
      "Train Loss at iteration 15065: 0.04288374642182401\n",
      "Train Loss at iteration 15066: 0.042883711978604896\n",
      "Train Loss at iteration 15067: 0.04288367753936865\n",
      "Train Loss at iteration 15068: 0.042883643104109526\n",
      "Train Loss at iteration 15069: 0.042883608672821795\n",
      "Train Loss at iteration 15070: 0.04288357424549982\n",
      "Train Loss at iteration 15071: 0.04288353982213796\n",
      "Train Loss at iteration 15072: 0.04288350540273065\n",
      "Train Loss at iteration 15073: 0.04288347098727238\n",
      "Train Loss at iteration 15074: 0.042883436575757626\n",
      "Train Loss at iteration 15075: 0.042883402168181005\n",
      "Train Loss at iteration 15076: 0.04288336776453707\n",
      "Train Loss at iteration 15077: 0.04288333336482051\n",
      "Train Loss at iteration 15078: 0.042883298969026\n",
      "Train Loss at iteration 15079: 0.04288326457714826\n",
      "Train Loss at iteration 15080: 0.04288323018918207\n",
      "Train Loss at iteration 15081: 0.04288319580512226\n",
      "Train Loss at iteration 15082: 0.042883161424963685\n",
      "Train Loss at iteration 15083: 0.04288312704870124\n",
      "Train Loss at iteration 15084: 0.04288309267632986\n",
      "Train Loss at iteration 15085: 0.042883058307844515\n",
      "Train Loss at iteration 15086: 0.04288302394324025\n",
      "Train Loss at iteration 15087: 0.042882989582512104\n",
      "Train Loss at iteration 15088: 0.042882955225655175\n",
      "Train Loss at iteration 15089: 0.042882920872664584\n",
      "Train Loss at iteration 15090: 0.04288288652353555\n",
      "Train Loss at iteration 15091: 0.04288285217826322\n",
      "Train Loss at iteration 15092: 0.042882817836842876\n",
      "Train Loss at iteration 15093: 0.042882783499269814\n",
      "Train Loss at iteration 15094: 0.04288274916553934\n",
      "Train Loss at iteration 15095: 0.0428827148356468\n",
      "Train Loss at iteration 15096: 0.042882680509587594\n",
      "Train Loss at iteration 15097: 0.04288264618735715\n",
      "Train Loss at iteration 15098: 0.04288261186895093\n",
      "Train Loss at iteration 15099: 0.04288257755436445\n",
      "Train Loss at iteration 15100: 0.04288254324359323\n",
      "Train Loss at iteration 15101: 0.04288250893663283\n",
      "Train Loss at iteration 15102: 0.042882474633478856\n",
      "Train Loss at iteration 15103: 0.04288244033412694\n",
      "Train Loss at iteration 15104: 0.04288240603857277\n",
      "Train Loss at iteration 15105: 0.04288237174681202\n",
      "Train Loss at iteration 15106: 0.042882337458840424\n",
      "Train Loss at iteration 15107: 0.04288230317465376\n",
      "Train Loss at iteration 15108: 0.04288226889424782\n",
      "Train Loss at iteration 15109: 0.04288223461761843\n",
      "Train Loss at iteration 15110: 0.04288220034476146\n",
      "Train Loss at iteration 15111: 0.04288216607567279\n",
      "Train Loss at iteration 15112: 0.04288213181034835\n",
      "Train Loss at iteration 15113: 0.04288209754878408\n",
      "Train Loss at iteration 15114: 0.04288206329097598\n",
      "Train Loss at iteration 15115: 0.042882029036920045\n",
      "Train Loss at iteration 15116: 0.04288199478661234\n",
      "Train Loss at iteration 15117: 0.042881960540048905\n",
      "Train Loss at iteration 15118: 0.04288192629722586\n",
      "Train Loss at iteration 15119: 0.04288189205813935\n",
      "Train Loss at iteration 15120: 0.042881857822785495\n",
      "Train Loss at iteration 15121: 0.042881823591160516\n",
      "Train Loss at iteration 15122: 0.04288178936326061\n",
      "Train Loss at iteration 15123: 0.04288175513908202\n",
      "Train Loss at iteration 15124: 0.04288172091862102\n",
      "Train Loss at iteration 15125: 0.042881686701873895\n",
      "Train Loss at iteration 15126: 0.04288165248883698\n",
      "Train Loss at iteration 15127: 0.04288161827950662\n",
      "Train Loss at iteration 15128: 0.042881584073879195\n",
      "Train Loss at iteration 15129: 0.04288154987195111\n",
      "Train Loss at iteration 15130: 0.04288151567371879\n",
      "Train Loss at iteration 15131: 0.04288148147917869\n",
      "Train Loss at iteration 15132: 0.04288144728832728\n",
      "Train Loss at iteration 15133: 0.04288141310116109\n",
      "Train Loss at iteration 15134: 0.04288137891767663\n",
      "Train Loss at iteration 15135: 0.042881344737870454\n",
      "Train Loss at iteration 15136: 0.04288131056173916\n",
      "Train Loss at iteration 15137: 0.04288127638927934\n",
      "Train Loss at iteration 15138: 0.04288124222048761\n",
      "Train Loss at iteration 15139: 0.04288120805536064\n",
      "Train Loss at iteration 15140: 0.042881173893895105\n",
      "Train Loss at iteration 15141: 0.0428811397360877\n",
      "Train Loss at iteration 15142: 0.04288110558193514\n",
      "Train Loss at iteration 15143: 0.04288107143143418\n",
      "Train Loss at iteration 15144: 0.042881037284581584\n",
      "Train Loss at iteration 15145: 0.04288100314137416\n",
      "Train Loss at iteration 15146: 0.04288096900180867\n",
      "Train Loss at iteration 15147: 0.042880934865882014\n",
      "Train Loss at iteration 15148: 0.04288090073359102\n",
      "Train Loss at iteration 15149: 0.04288086660493256\n",
      "Train Loss at iteration 15150: 0.04288083247990354\n",
      "Train Loss at iteration 15151: 0.042880798358500886\n",
      "Train Loss at iteration 15152: 0.04288076424072154\n",
      "Train Loss at iteration 15153: 0.04288073012656247\n",
      "Train Loss at iteration 15154: 0.042880696016020646\n",
      "Train Loss at iteration 15155: 0.04288066190909311\n",
      "Train Loss at iteration 15156: 0.042880627805776825\n",
      "Train Loss at iteration 15157: 0.04288059370606889\n",
      "Train Loss at iteration 15158: 0.04288055960996637\n",
      "Train Loss at iteration 15159: 0.04288052551746632\n",
      "Train Loss at iteration 15160: 0.04288049142856588\n",
      "Train Loss at iteration 15161: 0.042880457343262145\n",
      "Train Loss at iteration 15162: 0.04288042326155228\n",
      "Train Loss at iteration 15163: 0.04288038918343343\n",
      "Train Loss at iteration 15164: 0.042880355108902805\n",
      "Train Loss at iteration 15165: 0.04288032103795759\n",
      "Train Loss at iteration 15166: 0.042880286970595\n",
      "Train Loss at iteration 15167: 0.042880252906812286\n",
      "Train Loss at iteration 15168: 0.0428802188466067\n",
      "Train Loss at iteration 15169: 0.04288018478997552\n",
      "Train Loss at iteration 15170: 0.042880150736916034\n",
      "Train Loss at iteration 15171: 0.04288011668742556\n",
      "Train Loss at iteration 15172: 0.04288008264150142\n",
      "Train Loss at iteration 15173: 0.04288004859914096\n",
      "Train Loss at iteration 15174: 0.042880014560341545\n",
      "Train Loss at iteration 15175: 0.04287998052510057\n",
      "Train Loss at iteration 15176: 0.042879946493415415\n",
      "Train Loss at iteration 15177: 0.0428799124652835\n",
      "Train Loss at iteration 15178: 0.042879878440702256\n",
      "Train Loss at iteration 15179: 0.042879844419669126\n",
      "Train Loss at iteration 15180: 0.04287981040218158\n",
      "Train Loss at iteration 15181: 0.0428797763882371\n",
      "Train Loss at iteration 15182: 0.04287974237783318\n",
      "Train Loss at iteration 15183: 0.04287970837096734\n",
      "Train Loss at iteration 15184: 0.042879674367637094\n",
      "Train Loss at iteration 15185: 0.04287964036784\n",
      "Train Loss at iteration 15186: 0.04287960637157361\n",
      "Train Loss at iteration 15187: 0.042879572378835495\n",
      "Train Loss at iteration 15188: 0.04287953838962326\n",
      "Train Loss at iteration 15189: 0.04287950440393451\n",
      "Train Loss at iteration 15190: 0.042879470421766826\n",
      "Train Loss at iteration 15191: 0.04287943644311792\n",
      "Train Loss at iteration 15192: 0.04287940246798537\n",
      "Train Loss at iteration 15193: 0.042879368496366865\n",
      "Train Loss at iteration 15194: 0.0428793345282601\n",
      "Train Loss at iteration 15195: 0.042879300563662764\n",
      "Train Loss at iteration 15196: 0.04287926660257255\n",
      "Train Loss at iteration 15197: 0.042879232644987185\n",
      "Train Loss at iteration 15198: 0.04287919869090441\n",
      "Train Loss at iteration 15199: 0.04287916474032197\n",
      "Train Loss at iteration 15200: 0.04287913079323764\n",
      "Train Loss at iteration 15201: 0.042879096849649176\n",
      "Train Loss at iteration 15202: 0.04287906290955439\n",
      "Train Loss at iteration 15203: 0.04287902897295108\n",
      "Train Loss at iteration 15204: 0.04287899503983706\n",
      "Train Loss at iteration 15205: 0.04287896111021015\n",
      "Train Loss at iteration 15206: 0.042878927184068216\n",
      "Train Loss at iteration 15207: 0.0428788932614091\n",
      "Train Loss at iteration 15208: 0.042878859342230666\n",
      "Train Loss at iteration 15209: 0.04287882542653081\n",
      "Train Loss at iteration 15210: 0.04287879151430743\n",
      "Train Loss at iteration 15211: 0.04287875760555841\n",
      "Train Loss at iteration 15212: 0.042878723700281694\n",
      "Train Loss at iteration 15213: 0.04287868979847519\n",
      "Train Loss at iteration 15214: 0.04287865590013687\n",
      "Train Loss at iteration 15215: 0.042878622005264655\n",
      "Train Loss at iteration 15216: 0.04287858811385654\n",
      "Train Loss at iteration 15217: 0.042878554225910484\n",
      "Train Loss at iteration 15218: 0.04287852034142449\n",
      "Train Loss at iteration 15219: 0.04287848646039656\n",
      "Train Loss at iteration 15220: 0.042878452582824716\n",
      "Train Loss at iteration 15221: 0.04287841870870697\n",
      "Train Loss at iteration 15222: 0.04287838483804136\n",
      "Train Loss at iteration 15223: 0.04287835097082595\n",
      "Train Loss at iteration 15224: 0.04287831710705877\n",
      "Train Loss at iteration 15225: 0.04287828324673792\n",
      "Train Loss at iteration 15226: 0.042878249389861456\n",
      "Train Loss at iteration 15227: 0.04287821553642747\n",
      "Train Loss at iteration 15228: 0.04287818168643409\n",
      "Train Loss at iteration 15229: 0.04287814783987942\n",
      "Train Loss at iteration 15230: 0.042878113996761565\n",
      "Train Loss at iteration 15231: 0.04287808015707868\n",
      "Train Loss at iteration 15232: 0.04287804632082891\n",
      "Train Loss at iteration 15233: 0.042878012488010396\n",
      "Train Loss at iteration 15234: 0.042877978658621294\n",
      "Train Loss at iteration 15235: 0.04287794483265982\n",
      "Train Loss at iteration 15236: 0.04287791101012411\n",
      "Train Loss at iteration 15237: 0.04287787719101239\n",
      "Train Loss at iteration 15238: 0.042877843375322855\n",
      "Train Loss at iteration 15239: 0.042877809563053725\n",
      "Train Loss at iteration 15240: 0.042877775754203214\n",
      "Train Loss at iteration 15241: 0.04287774194876957\n",
      "Train Loss at iteration 15242: 0.042877708146751015\n",
      "Train Loss at iteration 15243: 0.04287767434814583\n",
      "Train Loss at iteration 15244: 0.042877640552952255\n",
      "Train Loss at iteration 15245: 0.042877606761168564\n",
      "Train Loss at iteration 15246: 0.04287757297279304\n",
      "Train Loss at iteration 15247: 0.04287753918782397\n",
      "Train Loss at iteration 15248: 0.04287750540625967\n",
      "Train Loss at iteration 15249: 0.042877471628098435\n",
      "Train Loss at iteration 15250: 0.042877437853338564\n",
      "Train Loss at iteration 15251: 0.04287740408197841\n",
      "Train Loss at iteration 15252: 0.04287737031401629\n",
      "Train Loss at iteration 15253: 0.04287733654945055\n",
      "Train Loss at iteration 15254: 0.042877302788279524\n",
      "Train Loss at iteration 15255: 0.04287726903050163\n",
      "Train Loss at iteration 15256: 0.042877235276115165\n",
      "Train Loss at iteration 15257: 0.042877201525118556\n",
      "Train Loss at iteration 15258: 0.04287716777751015\n",
      "Train Loss at iteration 15259: 0.04287713403328835\n",
      "Train Loss at iteration 15260: 0.04287710029245159\n",
      "Train Loss at iteration 15261: 0.04287706655499822\n",
      "Train Loss at iteration 15262: 0.04287703282092671\n",
      "Train Loss at iteration 15263: 0.042876999090235464\n",
      "Train Loss at iteration 15264: 0.04287696536292291\n",
      "Train Loss at iteration 15265: 0.042876931638987495\n",
      "Train Loss at iteration 15266: 0.042876897918427664\n",
      "Train Loss at iteration 15267: 0.042876864201241864\n",
      "Train Loss at iteration 15268: 0.04287683048742859\n",
      "Train Loss at iteration 15269: 0.042876796776986276\n",
      "Train Loss at iteration 15270: 0.04287676306991341\n",
      "Train Loss at iteration 15271: 0.0428767293662085\n",
      "Train Loss at iteration 15272: 0.042876695665870004\n",
      "Train Loss at iteration 15273: 0.04287666196889645\n",
      "Train Loss at iteration 15274: 0.04287662827528634\n",
      "Train Loss at iteration 15275: 0.04287659458503818\n",
      "Train Loss at iteration 15276: 0.042876560898150506\n",
      "Train Loss at iteration 15277: 0.04287652721462183\n",
      "Train Loss at iteration 15278: 0.04287649353445071\n",
      "Train Loss at iteration 15279: 0.042876459857635656\n",
      "Train Loss at iteration 15280: 0.042876426184175254\n",
      "Train Loss at iteration 15281: 0.04287639251406803\n",
      "Train Loss at iteration 15282: 0.042876358847312564\n",
      "Train Loss at iteration 15283: 0.04287632518390743\n",
      "Train Loss at iteration 15284: 0.04287629152385118\n",
      "Train Loss at iteration 15285: 0.04287625786714243\n",
      "Train Loss at iteration 15286: 0.042876224213779734\n",
      "Train Loss at iteration 15287: 0.04287619056376174\n",
      "Train Loss at iteration 15288: 0.042876156917086995\n",
      "Train Loss at iteration 15289: 0.04287612327375414\n",
      "Train Loss at iteration 15290: 0.04287608963376177\n",
      "Train Loss at iteration 15291: 0.04287605599710853\n",
      "Train Loss at iteration 15292: 0.04287602236379304\n",
      "Train Loss at iteration 15293: 0.04287598873381393\n",
      "Train Loss at iteration 15294: 0.04287595510716982\n",
      "Train Loss at iteration 15295: 0.042875921483859394\n",
      "Train Loss at iteration 15296: 0.042875887863881273\n",
      "Train Loss at iteration 15297: 0.04287585424723414\n",
      "Train Loss at iteration 15298: 0.04287582063391663\n",
      "Train Loss at iteration 15299: 0.04287578702392745\n",
      "Train Loss at iteration 15300: 0.042875753417265246\n",
      "Train Loss at iteration 15301: 0.0428757198139287\n",
      "Train Loss at iteration 15302: 0.042875686213916506\n",
      "Train Loss at iteration 15303: 0.042875652617227374\n",
      "Train Loss at iteration 15304: 0.04287561902385998\n",
      "Train Loss at iteration 15305: 0.04287558543381303\n",
      "Train Loss at iteration 15306: 0.04287555184708524\n",
      "Train Loss at iteration 15307: 0.04287551826367532\n",
      "Train Loss at iteration 15308: 0.04287548468358199\n",
      "Train Loss at iteration 15309: 0.04287545110680396\n",
      "Train Loss at iteration 15310: 0.04287541753334001\n",
      "Train Loss at iteration 15311: 0.04287538396318882\n",
      "Train Loss at iteration 15312: 0.04287535039634919\n",
      "Train Loss at iteration 15313: 0.04287531683281981\n",
      "Train Loss at iteration 15314: 0.04287528327259946\n",
      "Train Loss at iteration 15315: 0.04287524971568688\n",
      "Train Loss at iteration 15316: 0.042875216162080856\n",
      "Train Loss at iteration 15317: 0.042875182611780145\n",
      "Train Loss at iteration 15318: 0.0428751490647835\n",
      "Train Loss at iteration 15319: 0.04287511552108973\n",
      "Train Loss at iteration 15320: 0.042875081980697616\n",
      "Train Loss at iteration 15321: 0.04287504844360593\n",
      "Train Loss at iteration 15322: 0.04287501490981344\n",
      "Train Loss at iteration 15323: 0.042874981379319\n",
      "Train Loss at iteration 15324: 0.042874947852121356\n",
      "Train Loss at iteration 15325: 0.042874914328219355\n",
      "Train Loss at iteration 15326: 0.04287488080761178\n",
      "Train Loss at iteration 15327: 0.04287484729029746\n",
      "Train Loss at iteration 15328: 0.042874813776275215\n",
      "Train Loss at iteration 15329: 0.04287478026554387\n",
      "Train Loss at iteration 15330: 0.04287474675810224\n",
      "Train Loss at iteration 15331: 0.04287471325394918\n",
      "Train Loss at iteration 15332: 0.04287467975308351\n",
      "Train Loss at iteration 15333: 0.0428746462555041\n",
      "Train Loss at iteration 15334: 0.04287461276120976\n",
      "Train Loss at iteration 15335: 0.04287457927019937\n",
      "Train Loss at iteration 15336: 0.04287454578247177\n",
      "Train Loss at iteration 15337: 0.042874512298025816\n",
      "Train Loss at iteration 15338: 0.04287447881686039\n",
      "Train Loss at iteration 15339: 0.04287444533897437\n",
      "Train Loss at iteration 15340: 0.04287441186436659\n",
      "Train Loss at iteration 15341: 0.042874378393035956\n",
      "Train Loss at iteration 15342: 0.04287434492498132\n",
      "Train Loss at iteration 15343: 0.04287431146020162\n",
      "Train Loss at iteration 15344: 0.042874277998695684\n",
      "Train Loss at iteration 15345: 0.04287424454046246\n",
      "Train Loss at iteration 15346: 0.042874211085500796\n",
      "Train Loss at iteration 15347: 0.04287417763380962\n",
      "Train Loss at iteration 15348: 0.04287414418538785\n",
      "Train Loss at iteration 15349: 0.042874110740234346\n",
      "Train Loss at iteration 15350: 0.04287407729834808\n",
      "Train Loss at iteration 15351: 0.042874043859727935\n",
      "Train Loss at iteration 15352: 0.042874010424372826\n",
      "Train Loss at iteration 15353: 0.04287397699228169\n",
      "Train Loss at iteration 15354: 0.04287394356345348\n",
      "Train Loss at iteration 15355: 0.04287391013788707\n",
      "Train Loss at iteration 15356: 0.04287387671558145\n",
      "Train Loss at iteration 15357: 0.042873843296535534\n",
      "Train Loss at iteration 15358: 0.04287380988074826\n",
      "Train Loss at iteration 15359: 0.04287377646821858\n",
      "Train Loss at iteration 15360: 0.04287374305894544\n",
      "Train Loss at iteration 15361: 0.042873709652927805\n",
      "Train Loss at iteration 15362: 0.042873676250164625\n",
      "Train Loss at iteration 15363: 0.04287364285065484\n",
      "Train Loss at iteration 15364: 0.042873609454397464\n",
      "Train Loss at iteration 15365: 0.04287357606139142\n",
      "Train Loss at iteration 15366: 0.042873542671635706\n",
      "Train Loss at iteration 15367: 0.04287350928512927\n",
      "Train Loss at iteration 15368: 0.04287347590187111\n",
      "Train Loss at iteration 15369: 0.042873442521860206\n",
      "Train Loss at iteration 15370: 0.04287340914509552\n",
      "Train Loss at iteration 15371: 0.042873375771576065\n",
      "Train Loss at iteration 15372: 0.042873342401300814\n",
      "Train Loss at iteration 15373: 0.042873309034268774\n",
      "Train Loss at iteration 15374: 0.04287327567047894\n",
      "Train Loss at iteration 15375: 0.0428732423099303\n",
      "Train Loss at iteration 15376: 0.04287320895262188\n",
      "Train Loss at iteration 15377: 0.04287317559855266\n",
      "Train Loss at iteration 15378: 0.04287314224772168\n",
      "Train Loss at iteration 15379: 0.04287310890012791\n",
      "Train Loss at iteration 15380: 0.04287307555577041\n",
      "Train Loss at iteration 15381: 0.04287304221464818\n",
      "Train Loss at iteration 15382: 0.04287300887676024\n",
      "Train Loss at iteration 15383: 0.04287297554210562\n",
      "Train Loss at iteration 15384: 0.04287294221068333\n",
      "Train Loss at iteration 15385: 0.042872908882492414\n",
      "Train Loss at iteration 15386: 0.04287287555753193\n",
      "Train Loss at iteration 15387: 0.04287284223580087\n",
      "Train Loss at iteration 15388: 0.042872808917298305\n",
      "Train Loss at iteration 15389: 0.04287277560202328\n",
      "Train Loss at iteration 15390: 0.04287274228997479\n",
      "Train Loss at iteration 15391: 0.04287270898115196\n",
      "Train Loss at iteration 15392: 0.04287267567555379\n",
      "Train Loss at iteration 15393: 0.04287264237317934\n",
      "Train Loss at iteration 15394: 0.04287260907402767\n",
      "Train Loss at iteration 15395: 0.042872575778097846\n",
      "Train Loss at iteration 15396: 0.04287254248538892\n",
      "Train Loss at iteration 15397: 0.042872509195899966\n",
      "Train Loss at iteration 15398: 0.04287247590963005\n",
      "Train Loss at iteration 15399: 0.042872442626578224\n",
      "Train Loss at iteration 15400: 0.04287240934674358\n",
      "Train Loss at iteration 15401: 0.04287237607012518\n",
      "Train Loss at iteration 15402: 0.042872342796722133\n",
      "Train Loss at iteration 15403: 0.042872309526533474\n",
      "Train Loss at iteration 15404: 0.0428722762595583\n",
      "Train Loss at iteration 15405: 0.042872242995795715\n",
      "Train Loss at iteration 15406: 0.04287220973524478\n",
      "Train Loss at iteration 15407: 0.04287217647790462\n",
      "Train Loss at iteration 15408: 0.04287214322377429\n",
      "Train Loss at iteration 15409: 0.04287210997285291\n",
      "Train Loss at iteration 15410: 0.04287207672513957\n",
      "Train Loss at iteration 15411: 0.04287204348063336\n",
      "Train Loss at iteration 15412: 0.04287201023933341\n",
      "Train Loss at iteration 15413: 0.04287197700123879\n",
      "Train Loss at iteration 15414: 0.04287194376634863\n",
      "Train Loss at iteration 15415: 0.04287191053466205\n",
      "Train Loss at iteration 15416: 0.04287187730617812\n",
      "Train Loss at iteration 15417: 0.042871844080895984\n",
      "Train Loss at iteration 15418: 0.042871810858814766\n",
      "Train Loss at iteration 15419: 0.04287177763993358\n",
      "Train Loss at iteration 15420: 0.04287174442425153\n",
      "Train Loss at iteration 15421: 0.04287171121176774\n",
      "Train Loss at iteration 15422: 0.04287167800248135\n",
      "Train Loss at iteration 15423: 0.042871644796391495\n",
      "Train Loss at iteration 15424: 0.04287161159349729\n",
      "Train Loss at iteration 15425: 0.04287157839379786\n",
      "Train Loss at iteration 15426: 0.04287154519729236\n",
      "Train Loss at iteration 15427: 0.042871512003979914\n",
      "Train Loss at iteration 15428: 0.04287147881385966\n",
      "Train Loss at iteration 15429: 0.04287144562693073\n",
      "Train Loss at iteration 15430: 0.0428714124431923\n",
      "Train Loss at iteration 15431: 0.04287137926264347\n",
      "Train Loss at iteration 15432: 0.04287134608528341\n",
      "Train Loss at iteration 15433: 0.04287131291111129\n",
      "Train Loss at iteration 15434: 0.04287127974012622\n",
      "Train Loss at iteration 15435: 0.04287124657232739\n",
      "Train Loss at iteration 15436: 0.04287121340771392\n",
      "Train Loss at iteration 15437: 0.04287118024628501\n",
      "Train Loss at iteration 15438: 0.04287114708803977\n",
      "Train Loss at iteration 15439: 0.0428711139329774\n",
      "Train Loss at iteration 15440: 0.04287108078109705\n",
      "Train Loss at iteration 15441: 0.04287104763239787\n",
      "Train Loss at iteration 15442: 0.04287101448687905\n",
      "Train Loss at iteration 15443: 0.04287098134453974\n",
      "Train Loss at iteration 15444: 0.04287094820537914\n",
      "Train Loss at iteration 15445: 0.042870915069396394\n",
      "Train Loss at iteration 15446: 0.04287088193659068\n",
      "Train Loss at iteration 15447: 0.04287084880696119\n",
      "Train Loss at iteration 15448: 0.04287081568050708\n",
      "Train Loss at iteration 15449: 0.042870782557227545\n",
      "Train Loss at iteration 15450: 0.04287074943712177\n",
      "Train Loss at iteration 15451: 0.04287071632018893\n",
      "Train Loss at iteration 15452: 0.04287068320642822\n",
      "Train Loss at iteration 15453: 0.0428706500958388\n",
      "Train Loss at iteration 15454: 0.04287061698841989\n",
      "Train Loss at iteration 15455: 0.04287058388417068\n",
      "Train Loss at iteration 15456: 0.04287055078309035\n",
      "Train Loss at iteration 15457: 0.042870517685178106\n",
      "Train Loss at iteration 15458: 0.04287048459043312\n",
      "Train Loss at iteration 15459: 0.04287045149885461\n",
      "Train Loss at iteration 15460: 0.04287041841044178\n",
      "Train Loss at iteration 15461: 0.042870385325193806\n",
      "Train Loss at iteration 15462: 0.04287035224310992\n",
      "Train Loss at iteration 15463: 0.04287031916418931\n",
      "Train Loss at iteration 15464: 0.04287028608843118\n",
      "Train Loss at iteration 15465: 0.042870253015834754\n",
      "Train Loss at iteration 15466: 0.042870219946399224\n",
      "Train Loss at iteration 15467: 0.04287018688012382\n",
      "Train Loss at iteration 15468: 0.04287015381700773\n",
      "Train Loss at iteration 15469: 0.04287012075705018\n",
      "Train Loss at iteration 15470: 0.04287008770025039\n",
      "Train Loss at iteration 15471: 0.04287005464660759\n",
      "Train Loss at iteration 15472: 0.04287002159612097\n",
      "Train Loss at iteration 15473: 0.04286998854878977\n",
      "Train Loss at iteration 15474: 0.04286995550461319\n",
      "Train Loss at iteration 15475: 0.042869922463590504\n",
      "Train Loss at iteration 15476: 0.04286988942572086\n",
      "Train Loss at iteration 15477: 0.042869856391003554\n",
      "Train Loss at iteration 15478: 0.042869823359437786\n",
      "Train Loss at iteration 15479: 0.042869790331022783\n",
      "Train Loss at iteration 15480: 0.042869757305757795\n",
      "Train Loss at iteration 15481: 0.042869724283642024\n",
      "Train Loss at iteration 15482: 0.04286969126467473\n",
      "Train Loss at iteration 15483: 0.04286965824885514\n",
      "Train Loss at iteration 15484: 0.04286962523618249\n",
      "Train Loss at iteration 15485: 0.042869592226656025\n",
      "Train Loss at iteration 15486: 0.042869559220274966\n",
      "Train Loss at iteration 15487: 0.042869526217038566\n",
      "Train Loss at iteration 15488: 0.042869493216946074\n",
      "Train Loss at iteration 15489: 0.042869460219996734\n",
      "Train Loss at iteration 15490: 0.04286942722618978\n",
      "Train Loss at iteration 15491: 0.04286939423552447\n",
      "Train Loss at iteration 15492: 0.04286936124800005\n",
      "Train Loss at iteration 15493: 0.04286932826361576\n",
      "Train Loss at iteration 15494: 0.042869295282370865\n",
      "Train Loss at iteration 15495: 0.04286926230426461\n",
      "Train Loss at iteration 15496: 0.042869229329296245\n",
      "Train Loss at iteration 15497: 0.04286919635746503\n",
      "Train Loss at iteration 15498: 0.042869163388770226\n",
      "Train Loss at iteration 15499: 0.04286913042321108\n",
      "Train Loss at iteration 15500: 0.042869097460786855\n",
      "Train Loss at iteration 15501: 0.04286906450149681\n",
      "Train Loss at iteration 15502: 0.04286903154534022\n",
      "Train Loss at iteration 15503: 0.04286899859231632\n",
      "Train Loss at iteration 15504: 0.0428689656424244\n",
      "Train Loss at iteration 15505: 0.042868932695663706\n",
      "Train Loss at iteration 15506: 0.04286889975203353\n",
      "Train Loss at iteration 15507: 0.04286886681153311\n",
      "Train Loss at iteration 15508: 0.04286883387416174\n",
      "Train Loss at iteration 15509: 0.042868800939918676\n",
      "Train Loss at iteration 15510: 0.042868768008803196\n",
      "Train Loss at iteration 15511: 0.042868735080814555\n",
      "Train Loss at iteration 15512: 0.04286870215595204\n",
      "Train Loss at iteration 15513: 0.042868669234214944\n",
      "Train Loss at iteration 15514: 0.04286863631560251\n",
      "Train Loss at iteration 15515: 0.04286860340011404\n",
      "Train Loss at iteration 15516: 0.042868570487748815\n",
      "Train Loss at iteration 15517: 0.04286853757850609\n",
      "Train Loss at iteration 15518: 0.042868504672385156\n",
      "Train Loss at iteration 15519: 0.042868471769385304\n",
      "Train Loss at iteration 15520: 0.0428684388695058\n",
      "Train Loss at iteration 15521: 0.04286840597274596\n",
      "Train Loss at iteration 15522: 0.04286837307910503\n",
      "Train Loss at iteration 15523: 0.042868340188582335\n",
      "Train Loss at iteration 15524: 0.04286830730117713\n",
      "Train Loss at iteration 15525: 0.04286827441688873\n",
      "Train Loss at iteration 15526: 0.04286824153571641\n",
      "Train Loss at iteration 15527: 0.042868208657659446\n",
      "Train Loss at iteration 15528: 0.042868175782717156\n",
      "Train Loss at iteration 15529: 0.04286814291088882\n",
      "Train Loss at iteration 15530: 0.04286811004217375\n",
      "Train Loss at iteration 15531: 0.04286807717657123\n",
      "Train Loss at iteration 15532: 0.04286804431408053\n",
      "Train Loss at iteration 15533: 0.04286801145470098\n",
      "Train Loss at iteration 15534: 0.042867978598431895\n",
      "Train Loss at iteration 15535: 0.04286794574527252\n",
      "Train Loss at iteration 15536: 0.0428679128952222\n",
      "Train Loss at iteration 15537: 0.04286788004828021\n",
      "Train Loss at iteration 15538: 0.04286784720444587\n",
      "Train Loss at iteration 15539: 0.04286781436371848\n",
      "Train Loss at iteration 15540: 0.04286778152609734\n",
      "Train Loss at iteration 15541: 0.04286774869158176\n",
      "Train Loss at iteration 15542: 0.04286771586017103\n",
      "Train Loss at iteration 15543: 0.042867683031864494\n",
      "Train Loss at iteration 15544: 0.042867650206661424\n",
      "Train Loss at iteration 15545: 0.042867617384561144\n",
      "Train Loss at iteration 15546: 0.04286758456556297\n",
      "Train Loss at iteration 15547: 0.042867551749666194\n",
      "Train Loss at iteration 15548: 0.04286751893687016\n",
      "Train Loss at iteration 15549: 0.04286748612717415\n",
      "Train Loss at iteration 15550: 0.042867453320577495\n",
      "Train Loss at iteration 15551: 0.04286742051707951\n",
      "Train Loss at iteration 15552: 0.04286738771667951\n",
      "Train Loss at iteration 15553: 0.0428673549193768\n",
      "Train Loss at iteration 15554: 0.04286732212517071\n",
      "Train Loss at iteration 15555: 0.04286728933406057\n",
      "Train Loss at iteration 15556: 0.04286725654604568\n",
      "Train Loss at iteration 15557: 0.04286722376112535\n",
      "Train Loss at iteration 15558: 0.04286719097929894\n",
      "Train Loss at iteration 15559: 0.04286715820056575\n",
      "Train Loss at iteration 15560: 0.042867125424925094\n",
      "Train Loss at iteration 15561: 0.042867092652376315\n",
      "Train Loss at iteration 15562: 0.042867059882918716\n",
      "Train Loss at iteration 15563: 0.042867027116551666\n",
      "Train Loss at iteration 15564: 0.04286699435327446\n",
      "Train Loss at iteration 15565: 0.042866961593086415\n",
      "Train Loss at iteration 15566: 0.04286692883598688\n",
      "Train Loss at iteration 15567: 0.04286689608197518\n",
      "Train Loss at iteration 15568: 0.04286686333105064\n",
      "Train Loss at iteration 15569: 0.0428668305832126\n",
      "Train Loss at iteration 15570: 0.04286679783846039\n",
      "Train Loss at iteration 15571: 0.04286676509679334\n",
      "Train Loss at iteration 15572: 0.04286673235821078\n",
      "Train Loss at iteration 15573: 0.04286669962271204\n",
      "Train Loss at iteration 15574: 0.042866666890296494\n",
      "Train Loss at iteration 15575: 0.04286663416096343\n",
      "Train Loss at iteration 15576: 0.04286660143471221\n",
      "Train Loss at iteration 15577: 0.04286656871154216\n",
      "Train Loss at iteration 15578: 0.042866535991452626\n",
      "Train Loss at iteration 15579: 0.04286650327444294\n",
      "Train Loss at iteration 15580: 0.04286647056051246\n",
      "Train Loss at iteration 15581: 0.04286643784966051\n",
      "Train Loss at iteration 15582: 0.042866405141886436\n",
      "Train Loss at iteration 15583: 0.04286637243718959\n",
      "Train Loss at iteration 15584: 0.04286633973556929\n",
      "Train Loss at iteration 15585: 0.04286630703702492\n",
      "Train Loss at iteration 15586: 0.04286627434155578\n",
      "Train Loss at iteration 15587: 0.04286624164916126\n",
      "Train Loss at iteration 15588: 0.042866208959840675\n",
      "Train Loss at iteration 15589: 0.04286617627359338\n",
      "Train Loss at iteration 15590: 0.04286614359041872\n",
      "Train Loss at iteration 15591: 0.04286611091031606\n",
      "Train Loss at iteration 15592: 0.04286607823328473\n",
      "Train Loss at iteration 15593: 0.0428660455593241\n",
      "Train Loss at iteration 15594: 0.04286601288843349\n",
      "Train Loss at iteration 15595: 0.04286598022061229\n",
      "Train Loss at iteration 15596: 0.042865947555859825\n",
      "Train Loss at iteration 15597: 0.04286591489417545\n",
      "Train Loss at iteration 15598: 0.04286588223555853\n",
      "Train Loss at iteration 15599: 0.04286584958000842\n",
      "Train Loss at iteration 15600: 0.04286581692752446\n",
      "Train Loss at iteration 15601: 0.04286578427810603\n",
      "Train Loss at iteration 15602: 0.04286575163175246\n",
      "Train Loss at iteration 15603: 0.042865718988463135\n",
      "Train Loss at iteration 15604: 0.04286568634823738\n",
      "Train Loss at iteration 15605: 0.042865653711074585\n",
      "Train Loss at iteration 15606: 0.042865621076974086\n",
      "Train Loss at iteration 15607: 0.04286558844593526\n",
      "Train Loss at iteration 15608: 0.042865555817957465\n",
      "Train Loss at iteration 15609: 0.04286552319304006\n",
      "Train Loss at iteration 15610: 0.042865490571182385\n",
      "Train Loss at iteration 15611: 0.04286545795238385\n",
      "Train Loss at iteration 15612: 0.04286542533664379\n",
      "Train Loss at iteration 15613: 0.04286539272396157\n",
      "Train Loss at iteration 15614: 0.042865360114336545\n",
      "Train Loss at iteration 15615: 0.0428653275077681\n",
      "Train Loss at iteration 15616: 0.042865294904255585\n",
      "Train Loss at iteration 15617: 0.042865262303798383\n",
      "Train Loss at iteration 15618: 0.042865229706395856\n",
      "Train Loss at iteration 15619: 0.04286519711204736\n",
      "Train Loss at iteration 15620: 0.042865164520752286\n",
      "Train Loss at iteration 15621: 0.04286513193250998\n",
      "Train Loss at iteration 15622: 0.04286509934731983\n",
      "Train Loss at iteration 15623: 0.04286506676518119\n",
      "Train Loss at iteration 15624: 0.04286503418609344\n",
      "Train Loss at iteration 15625: 0.042865001610055964\n",
      "Train Loss at iteration 15626: 0.042864969037068115\n",
      "Train Loss at iteration 15627: 0.04286493646712928\n",
      "Train Loss at iteration 15628: 0.04286490390023882\n",
      "Train Loss at iteration 15629: 0.04286487133639611\n",
      "Train Loss at iteration 15630: 0.04286483877560054\n",
      "Train Loss at iteration 15631: 0.04286480621785147\n",
      "Train Loss at iteration 15632: 0.04286477366314829\n",
      "Train Loss at iteration 15633: 0.04286474111149036\n",
      "Train Loss at iteration 15634: 0.04286470856287708\n",
      "Train Loss at iteration 15635: 0.042864676017307805\n",
      "Train Loss at iteration 15636: 0.04286464347478192\n",
      "Train Loss at iteration 15637: 0.042864610935298815\n",
      "Train Loss at iteration 15638: 0.04286457839885785\n",
      "Train Loss at iteration 15639: 0.042864545865458424\n",
      "Train Loss at iteration 15640: 0.04286451333509991\n",
      "Train Loss at iteration 15641: 0.04286448080778169\n",
      "Train Loss at iteration 15642: 0.04286444828350315\n",
      "Train Loss at iteration 15643: 0.042864415762263665\n",
      "Train Loss at iteration 15644: 0.042864383244062616\n",
      "Train Loss at iteration 15645: 0.042864350728899395\n",
      "Train Loss at iteration 15646: 0.04286431821677339\n",
      "Train Loss at iteration 15647: 0.042864285707683976\n",
      "Train Loss at iteration 15648: 0.04286425320163054\n",
      "Train Loss at iteration 15649: 0.04286422069861246\n",
      "Train Loss at iteration 15650: 0.04286418819862914\n",
      "Train Loss at iteration 15651: 0.04286415570167997\n",
      "Train Loss at iteration 15652: 0.04286412320776431\n",
      "Train Loss at iteration 15653: 0.042864090716881575\n",
      "Train Loss at iteration 15654: 0.04286405822903113\n",
      "Train Loss at iteration 15655: 0.042864025744212386\n",
      "Train Loss at iteration 15656: 0.04286399326242473\n",
      "Train Loss at iteration 15657: 0.04286396078366754\n",
      "Train Loss at iteration 15658: 0.042863928307940204\n",
      "Train Loss at iteration 15659: 0.042863895835242125\n",
      "Train Loss at iteration 15660: 0.0428638633655727\n",
      "Train Loss at iteration 15661: 0.04286383089893131\n",
      "Train Loss at iteration 15662: 0.042863798435317346\n",
      "Train Loss at iteration 15663: 0.042863765974730204\n",
      "Train Loss at iteration 15664: 0.04286373351716929\n",
      "Train Loss at iteration 15665: 0.04286370106263397\n",
      "Train Loss at iteration 15666: 0.04286366861112367\n",
      "Train Loss at iteration 15667: 0.042863636162637775\n",
      "Train Loss at iteration 15668: 0.04286360371717567\n",
      "Train Loss at iteration 15669: 0.042863571274736764\n",
      "Train Loss at iteration 15670: 0.042863538835320444\n",
      "Train Loss at iteration 15671: 0.042863506398926124\n",
      "Train Loss at iteration 15672: 0.04286347396555317\n",
      "Train Loss at iteration 15673: 0.04286344153520102\n",
      "Train Loss at iteration 15674: 0.04286340910786905\n",
      "Train Loss at iteration 15675: 0.04286337668355666\n",
      "Train Loss at iteration 15676: 0.04286334426226325\n",
      "Train Loss at iteration 15677: 0.042863311843988214\n",
      "Train Loss at iteration 15678: 0.04286327942873097\n",
      "Train Loss at iteration 15679: 0.04286324701649092\n",
      "Train Loss at iteration 15680: 0.04286321460726745\n",
      "Train Loss at iteration 15681: 0.042863182201059974\n",
      "Train Loss at iteration 15682: 0.04286314979786787\n",
      "Train Loss at iteration 15683: 0.04286311739769057\n",
      "Train Loss at iteration 15684: 0.042863085000527476\n",
      "Train Loss at iteration 15685: 0.04286305260637797\n",
      "Train Loss at iteration 15686: 0.042863020215241486\n",
      "Train Loss at iteration 15687: 0.0428629878271174\n",
      "Train Loss at iteration 15688: 0.042862955442005135\n",
      "Train Loss at iteration 15689: 0.04286292305990409\n",
      "Train Loss at iteration 15690: 0.042862890680813684\n",
      "Train Loss at iteration 15691: 0.0428628583047333\n",
      "Train Loss at iteration 15692: 0.042862825931662366\n",
      "Train Loss at iteration 15693: 0.04286279356160028\n",
      "Train Loss at iteration 15694: 0.042862761194546455\n",
      "Train Loss at iteration 15695: 0.042862728830500296\n",
      "Train Loss at iteration 15696: 0.04286269646946122\n",
      "Train Loss at iteration 15697: 0.04286266411142862\n",
      "Train Loss at iteration 15698: 0.04286263175640191\n",
      "Train Loss at iteration 15699: 0.0428625994043805\n",
      "Train Loss at iteration 15700: 0.042862567055363836\n",
      "Train Loss at iteration 15701: 0.04286253470935127\n",
      "Train Loss at iteration 15702: 0.042862502366342266\n",
      "Train Loss at iteration 15703: 0.042862470026336204\n",
      "Train Loss at iteration 15704: 0.042862437689332504\n",
      "Train Loss at iteration 15705: 0.042862405355330584\n",
      "Train Loss at iteration 15706: 0.042862373024329846\n",
      "Train Loss at iteration 15707: 0.042862340696329715\n",
      "Train Loss at iteration 15708: 0.04286230837132961\n",
      "Train Loss at iteration 15709: 0.04286227604932892\n",
      "Train Loss at iteration 15710: 0.04286224373032709\n",
      "Train Loss at iteration 15711: 0.042862211414323524\n",
      "Train Loss at iteration 15712: 0.04286217910131764\n",
      "Train Loss at iteration 15713: 0.04286214679130882\n",
      "Train Loss at iteration 15714: 0.04286211448429655\n",
      "Train Loss at iteration 15715: 0.0428620821802802\n",
      "Train Loss at iteration 15716: 0.04286204987925918\n",
      "Train Loss at iteration 15717: 0.04286201758123293\n",
      "Train Loss at iteration 15718: 0.04286198528620087\n",
      "Train Loss at iteration 15719: 0.04286195299416241\n",
      "Train Loss at iteration 15720: 0.042861920705116956\n",
      "Train Loss at iteration 15721: 0.042861888419063944\n",
      "Train Loss at iteration 15722: 0.0428618561360028\n",
      "Train Loss at iteration 15723: 0.04286182385593292\n",
      "Train Loss at iteration 15724: 0.04286179157885376\n",
      "Train Loss at iteration 15725: 0.0428617593047647\n",
      "Train Loss at iteration 15726: 0.042861727033665184\n",
      "Train Loss at iteration 15727: 0.042861694765554646\n",
      "Train Loss at iteration 15728: 0.04286166250043249\n",
      "Train Loss at iteration 15729: 0.042861630238298136\n",
      "Train Loss at iteration 15730: 0.04286159797915102\n",
      "Train Loss at iteration 15731: 0.04286156572299054\n",
      "Train Loss at iteration 15732: 0.042861533469816165\n",
      "Train Loss at iteration 15733: 0.04286150121962728\n",
      "Train Loss at iteration 15734: 0.04286146897242332\n",
      "Train Loss at iteration 15735: 0.0428614367282037\n",
      "Train Loss at iteration 15736: 0.04286140448696789\n",
      "Train Loss at iteration 15737: 0.042861372248715245\n",
      "Train Loss at iteration 15738: 0.04286134001344525\n",
      "Train Loss at iteration 15739: 0.04286130778115729\n",
      "Train Loss at iteration 15740: 0.04286127555185083\n",
      "Train Loss at iteration 15741: 0.04286124332552527\n",
      "Train Loss at iteration 15742: 0.04286121110218006\n",
      "Train Loss at iteration 15743: 0.04286117888181458\n",
      "Train Loss at iteration 15744: 0.04286114666442832\n",
      "Train Loss at iteration 15745: 0.04286111445002068\n",
      "Train Loss at iteration 15746: 0.042861082238591065\n",
      "Train Loss at iteration 15747: 0.042861050030138945\n",
      "Train Loss at iteration 15748: 0.04286101782466373\n",
      "Train Loss at iteration 15749: 0.04286098562216485\n",
      "Train Loss at iteration 15750: 0.04286095342264174\n",
      "Train Loss at iteration 15751: 0.04286092122609383\n",
      "Train Loss at iteration 15752: 0.04286088903252055\n",
      "Train Loss at iteration 15753: 0.04286085684192133\n",
      "Train Loss at iteration 15754: 0.04286082465429559\n",
      "Train Loss at iteration 15755: 0.04286079246964277\n",
      "Train Loss at iteration 15756: 0.042860760287962325\n",
      "Train Loss at iteration 15757: 0.04286072810925366\n",
      "Train Loss at iteration 15758: 0.04286069593351622\n",
      "Train Loss at iteration 15759: 0.04286066376074943\n",
      "Train Loss at iteration 15760: 0.04286063159095273\n",
      "Train Loss at iteration 15761: 0.04286059942412555\n",
      "Train Loss at iteration 15762: 0.04286056726026733\n",
      "Train Loss at iteration 15763: 0.042860535099377504\n",
      "Train Loss at iteration 15764: 0.0428605029414555\n",
      "Train Loss at iteration 15765: 0.042860470786500765\n",
      "Train Loss at iteration 15766: 0.04286043863451273\n",
      "Train Loss at iteration 15767: 0.04286040648549081\n",
      "Train Loss at iteration 15768: 0.042860374339434466\n",
      "Train Loss at iteration 15769: 0.042860342196343146\n",
      "Train Loss at iteration 15770: 0.04286031005621626\n",
      "Train Loss at iteration 15771: 0.04286027791905326\n",
      "Train Loss at iteration 15772: 0.04286024578485357\n",
      "Train Loss at iteration 15773: 0.042860213653616655\n",
      "Train Loss at iteration 15774: 0.0428601815253419\n",
      "Train Loss at iteration 15775: 0.0428601494000288\n",
      "Train Loss at iteration 15776: 0.042860117277676774\n",
      "Train Loss at iteration 15777: 0.04286008515828526\n",
      "Train Loss at iteration 15778: 0.042860053041853696\n",
      "Train Loss at iteration 15779: 0.04286002092838151\n",
      "Train Loss at iteration 15780: 0.04285998881786818\n",
      "Train Loss at iteration 15781: 0.042859956710313105\n",
      "Train Loss at iteration 15782: 0.04285992460571576\n",
      "Train Loss at iteration 15783: 0.04285989250407555\n",
      "Train Loss at iteration 15784: 0.042859860405391935\n",
      "Train Loss at iteration 15785: 0.04285982830966437\n",
      "Train Loss at iteration 15786: 0.042859796216892274\n",
      "Train Loss at iteration 15787: 0.04285976412707511\n",
      "Train Loss at iteration 15788: 0.0428597320402123\n",
      "Train Loss at iteration 15789: 0.0428596999563033\n",
      "Train Loss at iteration 15790: 0.042859667875347555\n",
      "Train Loss at iteration 15791: 0.04285963579734451\n",
      "Train Loss at iteration 15792: 0.042859603722293596\n",
      "Train Loss at iteration 15793: 0.04285957165019425\n",
      "Train Loss at iteration 15794: 0.04285953958104595\n",
      "Train Loss at iteration 15795: 0.04285950751484812\n",
      "Train Loss at iteration 15796: 0.04285947545160019\n",
      "Train Loss at iteration 15797: 0.042859443391301634\n",
      "Train Loss at iteration 15798: 0.04285941133395188\n",
      "Train Loss at iteration 15799: 0.042859379279550386\n",
      "Train Loss at iteration 15800: 0.042859347228096596\n",
      "Train Loss at iteration 15801: 0.04285931517958994\n",
      "Train Loss at iteration 15802: 0.042859283134029896\n",
      "Train Loss at iteration 15803: 0.042859251091415876\n",
      "Train Loss at iteration 15804: 0.04285921905174733\n",
      "Train Loss at iteration 15805: 0.042859187015023746\n",
      "Train Loss at iteration 15806: 0.042859154981244534\n",
      "Train Loss at iteration 15807: 0.042859122950409154\n",
      "Train Loss at iteration 15808: 0.04285909092251706\n",
      "Train Loss at iteration 15809: 0.04285905889756768\n",
      "Train Loss at iteration 15810: 0.0428590268755605\n",
      "Train Loss at iteration 15811: 0.042858994856494935\n",
      "Train Loss at iteration 15812: 0.042858962840370454\n",
      "Train Loss at iteration 15813: 0.04285893082718648\n",
      "Train Loss at iteration 15814: 0.0428588988169425\n",
      "Train Loss at iteration 15815: 0.04285886680963796\n",
      "Train Loss at iteration 15816: 0.04285883480527228\n",
      "Train Loss at iteration 15817: 0.042858802803844936\n",
      "Train Loss at iteration 15818: 0.04285877080535537\n",
      "Train Loss at iteration 15819: 0.04285873880980305\n",
      "Train Loss at iteration 15820: 0.04285870681718739\n",
      "Train Loss at iteration 15821: 0.04285867482750789\n",
      "Train Loss at iteration 15822: 0.04285864284076397\n",
      "Train Loss at iteration 15823: 0.04285861085695508\n",
      "Train Loss at iteration 15824: 0.042858578876080716\n",
      "Train Loss at iteration 15825: 0.042858546898140275\n",
      "Train Loss at iteration 15826: 0.04285851492313325\n",
      "Train Loss at iteration 15827: 0.04285848295105906\n",
      "Train Loss at iteration 15828: 0.0428584509819172\n",
      "Train Loss at iteration 15829: 0.0428584190157071\n",
      "Train Loss at iteration 15830: 0.04285838705242821\n",
      "Train Loss at iteration 15831: 0.042858355092080004\n",
      "Train Loss at iteration 15832: 0.042858323134661924\n",
      "Train Loss at iteration 15833: 0.042858291180173425\n",
      "Train Loss at iteration 15834: 0.04285825922861397\n",
      "Train Loss at iteration 15835: 0.042858227279983015\n",
      "Train Loss at iteration 15836: 0.042858195334279994\n",
      "Train Loss at iteration 15837: 0.042858163391504396\n",
      "Train Loss at iteration 15838: 0.04285813145165566\n",
      "Train Loss at iteration 15839: 0.04285809951473325\n",
      "Train Loss at iteration 15840: 0.04285806758073661\n",
      "Train Loss at iteration 15841: 0.04285803564966521\n",
      "Train Loss at iteration 15842: 0.04285800372151851\n",
      "Train Loss at iteration 15843: 0.04285797179629596\n",
      "Train Loss at iteration 15844: 0.04285793987399702\n",
      "Train Loss at iteration 15845: 0.04285790795462114\n",
      "Train Loss at iteration 15846: 0.0428578760381678\n",
      "Train Loss at iteration 15847: 0.04285784412463644\n",
      "Train Loss at iteration 15848: 0.04285781221402652\n",
      "Train Loss at iteration 15849: 0.04285778030633751\n",
      "Train Loss at iteration 15850: 0.04285774840156887\n",
      "Train Loss at iteration 15851: 0.04285771649972006\n",
      "Train Loss at iteration 15852: 0.042857684600790515\n",
      "Train Loss at iteration 15853: 0.04285765270477974\n",
      "Train Loss at iteration 15854: 0.042857620811687154\n",
      "Train Loss at iteration 15855: 0.04285758892151223\n",
      "Train Loss at iteration 15856: 0.04285755703425445\n",
      "Train Loss at iteration 15857: 0.042857525149913256\n",
      "Train Loss at iteration 15858: 0.04285749326848811\n",
      "Train Loss at iteration 15859: 0.04285746138997848\n",
      "Train Loss at iteration 15860: 0.04285742951438382\n",
      "Train Loss at iteration 15861: 0.04285739764170361\n",
      "Train Loss at iteration 15862: 0.04285736577193729\n",
      "Train Loss at iteration 15863: 0.04285733390508434\n",
      "Train Loss at iteration 15864: 0.04285730204114421\n",
      "Train Loss at iteration 15865: 0.042857270180116376\n",
      "Train Loss at iteration 15866: 0.04285723832200029\n",
      "Train Loss at iteration 15867: 0.04285720646679543\n",
      "Train Loss at iteration 15868: 0.042857174614501246\n",
      "Train Loss at iteration 15869: 0.042857142765117213\n",
      "Train Loss at iteration 15870: 0.042857110918642784\n",
      "Train Loss at iteration 15871: 0.04285707907507743\n",
      "Train Loss at iteration 15872: 0.04285704723442064\n",
      "Train Loss at iteration 15873: 0.04285701539667182\n",
      "Train Loss at iteration 15874: 0.042856983561830486\n",
      "Train Loss at iteration 15875: 0.04285695172989609\n",
      "Train Loss at iteration 15876: 0.04285691990086809\n",
      "Train Loss at iteration 15877: 0.04285688807474597\n",
      "Train Loss at iteration 15878: 0.04285685625152918\n",
      "Train Loss at iteration 15879: 0.04285682443121718\n",
      "Train Loss at iteration 15880: 0.042856792613809466\n",
      "Train Loss at iteration 15881: 0.042856760799305484\n",
      "Train Loss at iteration 15882: 0.042856728987704706\n",
      "Train Loss at iteration 15883: 0.042856697179006586\n",
      "Train Loss at iteration 15884: 0.04285666537321061\n",
      "Train Loss at iteration 15885: 0.04285663357031625\n",
      "Train Loss at iteration 15886: 0.04285660177032295\n",
      "Train Loss at iteration 15887: 0.0428565699732302\n",
      "Train Loss at iteration 15888: 0.04285653817903747\n",
      "Train Loss at iteration 15889: 0.04285650638774423\n",
      "Train Loss at iteration 15890: 0.04285647459934991\n",
      "Train Loss at iteration 15891: 0.042856442813854013\n",
      "Train Loss at iteration 15892: 0.04285641103125602\n",
      "Train Loss at iteration 15893: 0.042856379251555374\n",
      "Train Loss at iteration 15894: 0.04285634747475156\n",
      "Train Loss at iteration 15895: 0.04285631570084404\n",
      "Train Loss at iteration 15896: 0.04285628392983229\n",
      "Train Loss at iteration 15897: 0.042856252161715784\n",
      "Train Loss at iteration 15898: 0.04285622039649399\n",
      "Train Loss at iteration 15899: 0.04285618863416636\n",
      "Train Loss at iteration 15900: 0.042856156874732404\n",
      "Train Loss at iteration 15901: 0.04285612511819156\n",
      "Train Loss at iteration 15902: 0.04285609336454333\n",
      "Train Loss at iteration 15903: 0.04285606161378715\n",
      "Train Loss at iteration 15904: 0.04285602986592252\n",
      "Train Loss at iteration 15905: 0.04285599812094889\n",
      "Train Loss at iteration 15906: 0.04285596637886575\n",
      "Train Loss at iteration 15907: 0.042855934639672556\n",
      "Train Loss at iteration 15908: 0.042855902903368805\n",
      "Train Loss at iteration 15909: 0.04285587116995396\n",
      "Train Loss at iteration 15910: 0.04285583943942748\n",
      "Train Loss at iteration 15911: 0.04285580771178886\n",
      "Train Loss at iteration 15912: 0.04285577598703756\n",
      "Train Loss at iteration 15913: 0.04285574426517307\n",
      "Train Loss at iteration 15914: 0.042855712546194843\n",
      "Train Loss at iteration 15915: 0.04285568083010237\n",
      "Train Loss at iteration 15916: 0.0428556491168951\n",
      "Train Loss at iteration 15917: 0.04285561740657253\n",
      "Train Loss at iteration 15918: 0.042855585699134155\n",
      "Train Loss at iteration 15919: 0.0428555539945794\n",
      "Train Loss at iteration 15920: 0.042855522292907774\n",
      "Train Loss at iteration 15921: 0.042855490594118764\n",
      "Train Loss at iteration 15922: 0.04285545889821182\n",
      "Train Loss at iteration 15923: 0.042855427205186425\n",
      "Train Loss at iteration 15924: 0.042855395515042055\n",
      "Train Loss at iteration 15925: 0.04285536382777819\n",
      "Train Loss at iteration 15926: 0.04285533214339429\n",
      "Train Loss at iteration 15927: 0.042855300461889866\n",
      "Train Loss at iteration 15928: 0.04285526878326436\n",
      "Train Loss at iteration 15929: 0.04285523710751727\n",
      "Train Loss at iteration 15930: 0.042855205434648075\n",
      "Train Loss at iteration 15931: 0.04285517376465623\n",
      "Train Loss at iteration 15932: 0.042855142097541245\n",
      "Train Loss at iteration 15933: 0.04285511043330257\n",
      "Train Loss at iteration 15934: 0.0428550787719397\n",
      "Train Loss at iteration 15935: 0.0428550471134521\n",
      "Train Loss at iteration 15936: 0.04285501545783927\n",
      "Train Loss at iteration 15937: 0.04285498380510069\n",
      "Train Loss at iteration 15938: 0.0428549521552358\n",
      "Train Loss at iteration 15939: 0.042854920508244104\n",
      "Train Loss at iteration 15940: 0.0428548888641251\n",
      "Train Loss at iteration 15941: 0.042854857222878226\n",
      "Train Loss at iteration 15942: 0.042854825584502994\n",
      "Train Loss at iteration 15943: 0.04285479394899889\n",
      "Train Loss at iteration 15944: 0.04285476231636537\n",
      "Train Loss at iteration 15945: 0.04285473068660192\n",
      "Train Loss at iteration 15946: 0.04285469905970803\n",
      "Train Loss at iteration 15947: 0.042854667435683176\n",
      "Train Loss at iteration 15948: 0.04285463581452684\n",
      "Train Loss at iteration 15949: 0.042854604196238495\n",
      "Train Loss at iteration 15950: 0.042854572580817624\n",
      "Train Loss at iteration 15951: 0.042854540968263716\n",
      "Train Loss at iteration 15952: 0.042854509358576265\n",
      "Train Loss at iteration 15953: 0.042854477751754744\n",
      "Train Loss at iteration 15954: 0.0428544461477986\n",
      "Train Loss at iteration 15955: 0.04285441454670735\n",
      "Train Loss at iteration 15956: 0.042854382948480485\n",
      "Train Loss at iteration 15957: 0.042854351353117465\n",
      "Train Loss at iteration 15958: 0.042854319760617786\n",
      "Train Loss at iteration 15959: 0.04285428817098093\n",
      "Train Loss at iteration 15960: 0.04285425658420637\n",
      "Train Loss at iteration 15961: 0.042854225000293594\n",
      "Train Loss at iteration 15962: 0.042854193419242105\n",
      "Train Loss at iteration 15963: 0.042854161841051355\n",
      "Train Loss at iteration 15964: 0.04285413026572085\n",
      "Train Loss at iteration 15965: 0.04285409869325006\n",
      "Train Loss at iteration 15966: 0.04285406712363847\n",
      "Train Loss at iteration 15967: 0.042854035556885585\n",
      "Train Loss at iteration 15968: 0.04285400399299087\n",
      "Train Loss at iteration 15969: 0.042853972431953824\n",
      "Train Loss at iteration 15970: 0.04285394087377392\n",
      "Train Loss at iteration 15971: 0.042853909318450636\n",
      "Train Loss at iteration 15972: 0.04285387776598347\n",
      "Train Loss at iteration 15973: 0.042853846216371914\n",
      "Train Loss at iteration 15974: 0.042853814669615437\n",
      "Train Loss at iteration 15975: 0.04285378312571355\n",
      "Train Loss at iteration 15976: 0.04285375158466571\n",
      "Train Loss at iteration 15977: 0.042853720046471415\n",
      "Train Loss at iteration 15978: 0.042853688511130174\n",
      "Train Loss at iteration 15979: 0.04285365697864143\n",
      "Train Loss at iteration 15980: 0.04285362544900469\n",
      "Train Loss at iteration 15981: 0.042853593922219456\n",
      "Train Loss at iteration 15982: 0.042853562398285194\n",
      "Train Loss at iteration 15983: 0.042853530877201396\n",
      "Train Loss at iteration 15984: 0.04285349935896756\n",
      "Train Loss at iteration 15985: 0.04285346784358317\n",
      "Train Loss at iteration 15986: 0.04285343633104772\n",
      "Train Loss at iteration 15987: 0.04285340482136067\n",
      "Train Loss at iteration 15988: 0.04285337331452153\n",
      "Train Loss at iteration 15989: 0.04285334181052979\n",
      "Train Loss at iteration 15990: 0.04285331030938495\n",
      "Train Loss at iteration 15991: 0.042853278811086463\n",
      "Train Loss at iteration 15992: 0.04285324731563385\n",
      "Train Loss at iteration 15993: 0.042853215823026584\n",
      "Train Loss at iteration 15994: 0.04285318433326416\n",
      "Train Loss at iteration 15995: 0.04285315284634606\n",
      "Train Loss at iteration 15996: 0.04285312136227179\n",
      "Train Loss at iteration 15997: 0.04285308988104082\n",
      "Train Loss at iteration 15998: 0.04285305840265267\n",
      "Train Loss at iteration 15999: 0.04285302692710678\n",
      "Train Loss at iteration 16000: 0.042852995454402694\n",
      "Train Loss at iteration 16001: 0.04285296398453986\n",
      "Train Loss at iteration 16002: 0.04285293251751781\n",
      "Train Loss at iteration 16003: 0.042852901053335994\n",
      "Train Loss at iteration 16004: 0.042852869591993935\n",
      "Train Loss at iteration 16005: 0.042852838133491104\n",
      "Train Loss at iteration 16006: 0.04285280667782699\n",
      "Train Loss at iteration 16007: 0.0428527752250011\n",
      "Train Loss at iteration 16008: 0.04285274377501293\n",
      "Train Loss at iteration 16009: 0.042852712327861954\n",
      "Train Loss at iteration 16010: 0.04285268088354767\n",
      "Train Loss at iteration 16011: 0.04285264944206957\n",
      "Train Loss at iteration 16012: 0.04285261800342714\n",
      "Train Loss at iteration 16013: 0.042852586567619894\n",
      "Train Loss at iteration 16014: 0.0428525551346473\n",
      "Train Loss at iteration 16015: 0.04285252370450887\n",
      "Train Loss at iteration 16016: 0.04285249227720409\n",
      "Train Loss at iteration 16017: 0.04285246085273244\n",
      "Train Loss at iteration 16018: 0.04285242943109343\n",
      "Train Loss at iteration 16019: 0.04285239801228656\n",
      "Train Loss at iteration 16020: 0.042852366596311295\n",
      "Train Loss at iteration 16021: 0.04285233518316717\n",
      "Train Loss at iteration 16022: 0.04285230377285363\n",
      "Train Loss at iteration 16023: 0.0428522723653702\n",
      "Train Loss at iteration 16024: 0.04285224096071638\n",
      "Train Loss at iteration 16025: 0.04285220955889164\n",
      "Train Loss at iteration 16026: 0.0428521781598955\n",
      "Train Loss at iteration 16027: 0.042852146763727436\n",
      "Train Loss at iteration 16028: 0.04285211537038695\n",
      "Train Loss at iteration 16029: 0.04285208397987353\n",
      "Train Loss at iteration 16030: 0.04285205259218669\n",
      "Train Loss at iteration 16031: 0.0428520212073259\n",
      "Train Loss at iteration 16032: 0.04285198982529069\n",
      "Train Loss at iteration 16033: 0.04285195844608051\n",
      "Train Loss at iteration 16034: 0.0428519270696949\n",
      "Train Loss at iteration 16035: 0.04285189569613333\n",
      "Train Loss at iteration 16036: 0.04285186432539529\n",
      "Train Loss at iteration 16037: 0.0428518329574803\n",
      "Train Loss at iteration 16038: 0.04285180159238785\n",
      "Train Loss at iteration 16039: 0.042851770230117436\n",
      "Train Loss at iteration 16040: 0.042851738870668545\n",
      "Train Loss at iteration 16041: 0.04285170751404068\n",
      "Train Loss at iteration 16042: 0.042851676160233346\n",
      "Train Loss at iteration 16043: 0.04285164480924603\n",
      "Train Loss at iteration 16044: 0.04285161346107823\n",
      "Train Loss at iteration 16045: 0.04285158211572943\n",
      "Train Loss at iteration 16046: 0.04285155077319917\n",
      "Train Loss at iteration 16047: 0.04285151943348691\n",
      "Train Loss at iteration 16048: 0.04285148809659217\n",
      "Train Loss at iteration 16049: 0.04285145676251443\n",
      "Train Loss at iteration 16050: 0.042851425431253184\n",
      "Train Loss at iteration 16051: 0.042851394102807965\n",
      "Train Loss at iteration 16052: 0.042851362777178235\n",
      "Train Loss at iteration 16053: 0.04285133145436352\n",
      "Train Loss at iteration 16054: 0.04285130013436331\n",
      "Train Loss at iteration 16055: 0.04285126881717708\n",
      "Train Loss at iteration 16056: 0.04285123750280436\n",
      "Train Loss at iteration 16057: 0.04285120619124465\n",
      "Train Loss at iteration 16058: 0.04285117488249742\n",
      "Train Loss at iteration 16059: 0.04285114357656221\n",
      "Train Loss at iteration 16060: 0.0428511122734385\n",
      "Train Loss at iteration 16061: 0.04285108097312577\n",
      "Train Loss at iteration 16062: 0.04285104967562355\n",
      "Train Loss at iteration 16063: 0.04285101838093133\n",
      "Train Loss at iteration 16064: 0.04285098708904861\n",
      "Train Loss at iteration 16065: 0.0428509557999749\n",
      "Train Loss at iteration 16066: 0.04285092451370969\n",
      "Train Loss at iteration 16067: 0.04285089323025247\n",
      "Train Loss at iteration 16068: 0.04285086194960277\n",
      "Train Loss at iteration 16069: 0.04285083067176008\n",
      "Train Loss at iteration 16070: 0.04285079939672388\n",
      "Train Loss at iteration 16071: 0.042850768124493684\n",
      "Train Loss at iteration 16072: 0.042850736855069016\n",
      "Train Loss at iteration 16073: 0.042850705588449355\n",
      "Train Loss at iteration 16074: 0.04285067432463419\n",
      "Train Loss at iteration 16075: 0.04285064306362307\n",
      "Train Loss at iteration 16076: 0.04285061180541546\n",
      "Train Loss at iteration 16077: 0.04285058055001087\n",
      "Train Loss at iteration 16078: 0.04285054929740881\n",
      "Train Loss at iteration 16079: 0.042850518047608754\n",
      "Train Loss at iteration 16080: 0.04285048680061026\n",
      "Train Loss at iteration 16081: 0.042850455556412786\n",
      "Train Loss at iteration 16082: 0.04285042431501584\n",
      "Train Loss at iteration 16083: 0.04285039307641894\n",
      "Train Loss at iteration 16084: 0.042850361840621576\n",
      "Train Loss at iteration 16085: 0.04285033060762327\n",
      "Train Loss at iteration 16086: 0.04285029937742351\n",
      "Train Loss at iteration 16087: 0.0428502681500218\n",
      "Train Loss at iteration 16088: 0.042850236925417645\n",
      "Train Loss at iteration 16089: 0.042850205703610575\n",
      "Train Loss at iteration 16090: 0.042850174484600045\n",
      "Train Loss at iteration 16091: 0.0428501432683856\n",
      "Train Loss at iteration 16092: 0.04285011205496672\n",
      "Train Loss at iteration 16093: 0.04285008084434294\n",
      "Train Loss at iteration 16094: 0.04285004963651374\n",
      "Train Loss at iteration 16095: 0.04285001843147862\n",
      "Train Loss at iteration 16096: 0.0428499872292371\n",
      "Train Loss at iteration 16097: 0.04284995602978869\n",
      "Train Loss at iteration 16098: 0.042849924833132894\n",
      "Train Loss at iteration 16099: 0.042849893639269186\n",
      "Train Loss at iteration 16100: 0.042849862448197114\n",
      "Train Loss at iteration 16101: 0.04284983125991616\n",
      "Train Loss at iteration 16102: 0.042849800074425834\n",
      "Train Loss at iteration 16103: 0.04284976889172566\n",
      "Train Loss at iteration 16104: 0.04284973771181512\n",
      "Train Loss at iteration 16105: 0.04284970653469373\n",
      "Train Loss at iteration 16106: 0.04284967536036099\n",
      "Train Loss at iteration 16107: 0.04284964418881642\n",
      "Train Loss at iteration 16108: 0.04284961302005951\n",
      "Train Loss at iteration 16109: 0.0428495818540898\n",
      "Train Loss at iteration 16110: 0.042849550690906756\n",
      "Train Loss at iteration 16111: 0.04284951953050991\n",
      "Train Loss at iteration 16112: 0.042849488372898765\n",
      "Train Loss at iteration 16113: 0.04284945721807281\n",
      "Train Loss at iteration 16114: 0.042849426066031586\n",
      "Train Loss at iteration 16115: 0.04284939491677458\n",
      "Train Loss at iteration 16116: 0.04284936377030131\n",
      "Train Loss at iteration 16117: 0.042849332626611274\n",
      "Train Loss at iteration 16118: 0.04284930148570398\n",
      "Train Loss at iteration 16119: 0.042849270347578954\n",
      "Train Loss at iteration 16120: 0.042849239212235685\n",
      "Train Loss at iteration 16121: 0.04284920807967369\n",
      "Train Loss at iteration 16122: 0.04284917694989248\n",
      "Train Loss at iteration 16123: 0.042849145822891545\n",
      "Train Loss at iteration 16124: 0.042849114698670414\n",
      "Train Loss at iteration 16125: 0.04284908357722861\n",
      "Train Loss at iteration 16126: 0.04284905245856561\n",
      "Train Loss at iteration 16127: 0.04284902134268094\n",
      "Train Loss at iteration 16128: 0.042848990229574106\n",
      "Train Loss at iteration 16129: 0.04284895911924463\n",
      "Train Loss at iteration 16130: 0.042848928011691996\n",
      "Train Loss at iteration 16131: 0.042848896906915734\n",
      "Train Loss at iteration 16132: 0.042848865804915356\n",
      "Train Loss at iteration 16133: 0.04284883470569037\n",
      "Train Loss at iteration 16134: 0.04284880360924026\n",
      "Train Loss at iteration 16135: 0.042848772515564584\n",
      "Train Loss at iteration 16136: 0.04284874142466281\n",
      "Train Loss at iteration 16137: 0.042848710336534486\n",
      "Train Loss at iteration 16138: 0.04284867925117908\n",
      "Train Loss at iteration 16139: 0.04284864816859614\n",
      "Train Loss at iteration 16140: 0.04284861708878516\n",
      "Train Loss at iteration 16141: 0.04284858601174567\n",
      "Train Loss at iteration 16142: 0.04284855493747715\n",
      "Train Loss at iteration 16143: 0.04284852386597913\n",
      "Train Loss at iteration 16144: 0.04284849279725113\n",
      "Train Loss at iteration 16145: 0.04284846173129265\n",
      "Train Loss at iteration 16146: 0.042848430668103205\n",
      "Train Loss at iteration 16147: 0.042848399607682296\n",
      "Train Loss at iteration 16148: 0.04284836855002946\n",
      "Train Loss at iteration 16149: 0.04284833749514419\n",
      "Train Loss at iteration 16150: 0.042848306443026\n",
      "Train Loss at iteration 16151: 0.04284827539367441\n",
      "Train Loss at iteration 16152: 0.042848244347088935\n",
      "Train Loss at iteration 16153: 0.04284821330326907\n",
      "Train Loss at iteration 16154: 0.04284818226221436\n",
      "Train Loss at iteration 16155: 0.0428481512239243\n",
      "Train Loss at iteration 16156: 0.042848120188398384\n",
      "Train Loss at iteration 16157: 0.04284808915563616\n",
      "Train Loss at iteration 16158: 0.04284805812563711\n",
      "Train Loss at iteration 16159: 0.04284802709840078\n",
      "Train Loss at iteration 16160: 0.04284799607392665\n",
      "Train Loss at iteration 16161: 0.04284796505221427\n",
      "Train Loss at iteration 16162: 0.04284793403326311\n",
      "Train Loss at iteration 16163: 0.04284790301707274\n",
      "Train Loss at iteration 16164: 0.04284787200364263\n",
      "Train Loss at iteration 16165: 0.042847840992972314\n",
      "Train Loss at iteration 16166: 0.0428478099850613\n",
      "Train Loss at iteration 16167: 0.0428477789799091\n",
      "Train Loss at iteration 16168: 0.042847747977515234\n",
      "Train Loss at iteration 16169: 0.042847716977879216\n",
      "Train Loss at iteration 16170: 0.04284768598100056\n",
      "Train Loss at iteration 16171: 0.0428476549868788\n",
      "Train Loss at iteration 16172: 0.042847623995513416\n",
      "Train Loss at iteration 16173: 0.04284759300690396\n",
      "Train Loss at iteration 16174: 0.04284756202104992\n",
      "Train Loss at iteration 16175: 0.04284753103795081\n",
      "Train Loss at iteration 16176: 0.04284750005760617\n",
      "Train Loss at iteration 16177: 0.04284746908001549\n",
      "Train Loss at iteration 16178: 0.042847438105178326\n",
      "Train Loss at iteration 16179: 0.04284740713309415\n",
      "Train Loss at iteration 16180: 0.0428473761637625\n",
      "Train Loss at iteration 16181: 0.04284734519718289\n",
      "Train Loss at iteration 16182: 0.04284731423335483\n",
      "Train Loss at iteration 16183: 0.04284728327227785\n",
      "Train Loss at iteration 16184: 0.04284725231395146\n",
      "Train Loss at iteration 16185: 0.04284722135837517\n",
      "Train Loss at iteration 16186: 0.04284719040554852\n",
      "Train Loss at iteration 16187: 0.04284715945547099\n",
      "Train Loss at iteration 16188: 0.042847128508142124\n",
      "Train Loss at iteration 16189: 0.042847097563561434\n",
      "Train Loss at iteration 16190: 0.04284706662172845\n",
      "Train Loss at iteration 16191: 0.04284703568264267\n",
      "Train Loss at iteration 16192: 0.04284700474630363\n",
      "Train Loss at iteration 16193: 0.042846973812710826\n",
      "Train Loss at iteration 16194: 0.04284694288186379\n",
      "Train Loss at iteration 16195: 0.04284691195376204\n",
      "Train Loss at iteration 16196: 0.042846881028405094\n",
      "Train Loss at iteration 16197: 0.04284685010579247\n",
      "Train Loss at iteration 16198: 0.042846819185923694\n",
      "Train Loss at iteration 16199: 0.04284678826879826\n",
      "Train Loss at iteration 16200: 0.04284675735441571\n",
      "Train Loss at iteration 16201: 0.04284672644277556\n",
      "Train Loss at iteration 16202: 0.04284669553387732\n",
      "Train Loss at iteration 16203: 0.042846664627720535\n",
      "Train Loss at iteration 16204: 0.0428466337243047\n",
      "Train Loss at iteration 16205: 0.04284660282362932\n",
      "Train Loss at iteration 16206: 0.042846571925693945\n",
      "Train Loss at iteration 16207: 0.04284654103049809\n",
      "Train Loss at iteration 16208: 0.04284651013804126\n",
      "Train Loss at iteration 16209: 0.042846479248322994\n",
      "Train Loss at iteration 16210: 0.042846448361342784\n",
      "Train Loss at iteration 16211: 0.04284641747710019\n",
      "Train Loss at iteration 16212: 0.042846386595594706\n",
      "Train Loss at iteration 16213: 0.04284635571682586\n",
      "Train Loss at iteration 16214: 0.042846324840793155\n",
      "Train Loss at iteration 16215: 0.042846293967496146\n",
      "Train Loss at iteration 16216: 0.042846263096934324\n",
      "Train Loss at iteration 16217: 0.04284623222910722\n",
      "Train Loss at iteration 16218: 0.04284620136401436\n",
      "Train Loss at iteration 16219: 0.042846170501655255\n",
      "Train Loss at iteration 16220: 0.04284613964202944\n",
      "Train Loss at iteration 16221: 0.042846108785136436\n",
      "Train Loss at iteration 16222: 0.04284607793097574\n",
      "Train Loss at iteration 16223: 0.042846047079546895\n",
      "Train Loss at iteration 16224: 0.042846016230849424\n",
      "Train Loss at iteration 16225: 0.04284598538488286\n",
      "Train Loss at iteration 16226: 0.04284595454164668\n",
      "Train Loss at iteration 16227: 0.04284592370114046\n",
      "Train Loss at iteration 16228: 0.04284589286336367\n",
      "Train Loss at iteration 16229: 0.04284586202831589\n",
      "Train Loss at iteration 16230: 0.0428458311959966\n",
      "Train Loss at iteration 16231: 0.04284580036640534\n",
      "Train Loss at iteration 16232: 0.042845769539541625\n",
      "Train Loss at iteration 16233: 0.04284573871540498\n",
      "Train Loss at iteration 16234: 0.04284570789399493\n",
      "Train Loss at iteration 16235: 0.042845677075311006\n",
      "Train Loss at iteration 16236: 0.04284564625935272\n",
      "Train Loss at iteration 16237: 0.04284561544611958\n",
      "Train Loss at iteration 16238: 0.042845584635611146\n",
      "Train Loss at iteration 16239: 0.04284555382782692\n",
      "Train Loss at iteration 16240: 0.04284552302276643\n",
      "Train Loss at iteration 16241: 0.0428454922204292\n",
      "Train Loss at iteration 16242: 0.042845461420814746\n",
      "Train Loss at iteration 16243: 0.04284543062392261\n",
      "Train Loss at iteration 16244: 0.042845399829752294\n",
      "Train Loss at iteration 16245: 0.04284536903830334\n",
      "Train Loss at iteration 16246: 0.042845338249575274\n",
      "Train Loss at iteration 16247: 0.0428453074635676\n",
      "Train Loss at iteration 16248: 0.042845276680279855\n",
      "Train Loss at iteration 16249: 0.04284524589971157\n",
      "Train Loss at iteration 16250: 0.042845215121862255\n",
      "Train Loss at iteration 16251: 0.04284518434673146\n",
      "Train Loss at iteration 16252: 0.04284515357431869\n",
      "Train Loss at iteration 16253: 0.04284512280462347\n",
      "Train Loss at iteration 16254: 0.04284509203764532\n",
      "Train Loss at iteration 16255: 0.042845061273383785\n",
      "Train Loss at iteration 16256: 0.042845030511838376\n",
      "Train Loss at iteration 16257: 0.042844999753008636\n",
      "Train Loss at iteration 16258: 0.04284496899689407\n",
      "Train Loss at iteration 16259: 0.04284493824349422\n",
      "Train Loss at iteration 16260: 0.04284490749280861\n",
      "Train Loss at iteration 16261: 0.04284487674483673\n",
      "Train Loss at iteration 16262: 0.04284484599957816\n",
      "Train Loss at iteration 16263: 0.0428448152570324\n",
      "Train Loss at iteration 16264: 0.042844784517198975\n",
      "Train Loss at iteration 16265: 0.042844753780077426\n",
      "Train Loss at iteration 16266: 0.042844723045667256\n",
      "Train Loss at iteration 16267: 0.042844692313968015\n",
      "Train Loss at iteration 16268: 0.04284466158497923\n",
      "Train Loss at iteration 16269: 0.042844630858700404\n",
      "Train Loss at iteration 16270: 0.04284460013513109\n",
      "Train Loss at iteration 16271: 0.0428445694142708\n",
      "Train Loss at iteration 16272: 0.04284453869611907\n",
      "Train Loss at iteration 16273: 0.04284450798067542\n",
      "Train Loss at iteration 16274: 0.04284447726793938\n",
      "Train Loss at iteration 16275: 0.04284444655791048\n",
      "Train Loss at iteration 16276: 0.042844415850588256\n",
      "Train Loss at iteration 16277: 0.04284438514597221\n",
      "Train Loss at iteration 16278: 0.04284435444406189\n",
      "Train Loss at iteration 16279: 0.04284432374485684\n",
      "Train Loss at iteration 16280: 0.042844293048356547\n",
      "Train Loss at iteration 16281: 0.042844262354560576\n",
      "Train Loss at iteration 16282: 0.04284423166346843\n",
      "Train Loss at iteration 16283: 0.042844200975079655\n",
      "Train Loss at iteration 16284: 0.04284417028939378\n",
      "Train Loss at iteration 16285: 0.042844139606410335\n",
      "Train Loss at iteration 16286: 0.042844108926128825\n",
      "Train Loss at iteration 16287: 0.04284407824854879\n",
      "Train Loss at iteration 16288: 0.04284404757366978\n",
      "Train Loss at iteration 16289: 0.04284401690149132\n",
      "Train Loss at iteration 16290: 0.04284398623201291\n",
      "Train Loss at iteration 16291: 0.042843955565234106\n",
      "Train Loss at iteration 16292: 0.04284392490115443\n",
      "Train Loss at iteration 16293: 0.0428438942397734\n",
      "Train Loss at iteration 16294: 0.04284386358109058\n",
      "Train Loss at iteration 16295: 0.04284383292510546\n",
      "Train Loss at iteration 16296: 0.042843802271817596\n",
      "Train Loss at iteration 16297: 0.04284377162122651\n",
      "Train Loss at iteration 16298: 0.04284374097333174\n",
      "Train Loss at iteration 16299: 0.0428437103281328\n",
      "Train Loss at iteration 16300: 0.04284367968562923\n",
      "Train Loss at iteration 16301: 0.04284364904582056\n",
      "Train Loss at iteration 16302: 0.04284361840870633\n",
      "Train Loss at iteration 16303: 0.04284358777428606\n",
      "Train Loss at iteration 16304: 0.042843557142559274\n",
      "Train Loss at iteration 16305: 0.042843526513525514\n",
      "Train Loss at iteration 16306: 0.042843495887184314\n",
      "Train Loss at iteration 16307: 0.04284346526353519\n",
      "Train Loss at iteration 16308: 0.04284343464257769\n",
      "Train Loss at iteration 16309: 0.04284340402431134\n",
      "Train Loss at iteration 16310: 0.04284337340873568\n",
      "Train Loss at iteration 16311: 0.042843342795850226\n",
      "Train Loss at iteration 16312: 0.042843312185654514\n",
      "Train Loss at iteration 16313: 0.04284328157814808\n",
      "Train Loss at iteration 16314: 0.04284325097333046\n",
      "Train Loss at iteration 16315: 0.04284322037120116\n",
      "Train Loss at iteration 16316: 0.042843189771759764\n",
      "Train Loss at iteration 16317: 0.042843159175005766\n",
      "Train Loss at iteration 16318: 0.042843128580938694\n",
      "Train Loss at iteration 16319: 0.042843097989558106\n",
      "Train Loss at iteration 16320: 0.04284306740086351\n",
      "Train Loss at iteration 16321: 0.04284303681485447\n",
      "Train Loss at iteration 16322: 0.04284300623153048\n",
      "Train Loss at iteration 16323: 0.042842975650891096\n",
      "Train Loss at iteration 16324: 0.04284294507293584\n",
      "Train Loss at iteration 16325: 0.04284291449766427\n",
      "Train Loss at iteration 16326: 0.042842883925075896\n",
      "Train Loss at iteration 16327: 0.042842853355170255\n",
      "Train Loss at iteration 16328: 0.042842822787946884\n",
      "Train Loss at iteration 16329: 0.04284279222340532\n",
      "Train Loss at iteration 16330: 0.042842761661545097\n",
      "Train Loss at iteration 16331: 0.04284273110236573\n",
      "Train Loss at iteration 16332: 0.04284270054586677\n",
      "Train Loss at iteration 16333: 0.04284266999204776\n",
      "Train Loss at iteration 16334: 0.04284263944090823\n",
      "Train Loss at iteration 16335: 0.04284260889244769\n",
      "Train Loss at iteration 16336: 0.04284257834666571\n",
      "Train Loss at iteration 16337: 0.042842547803561795\n",
      "Train Loss at iteration 16338: 0.04284251726313548\n",
      "Train Loss at iteration 16339: 0.04284248672538633\n",
      "Train Loss at iteration 16340: 0.042842456190313856\n",
      "Train Loss at iteration 16341: 0.04284242565791759\n",
      "Train Loss at iteration 16342: 0.04284239512819707\n",
      "Train Loss at iteration 16343: 0.04284236460115185\n",
      "Train Loss at iteration 16344: 0.04284233407678145\n",
      "Train Loss at iteration 16345: 0.04284230355508541\n",
      "Train Loss at iteration 16346: 0.04284227303606324\n",
      "Train Loss at iteration 16347: 0.04284224251971453\n",
      "Train Loss at iteration 16348: 0.04284221200603876\n",
      "Train Loss at iteration 16349: 0.04284218149503551\n",
      "Train Loss at iteration 16350: 0.04284215098670428\n",
      "Train Loss at iteration 16351: 0.04284212048104463\n",
      "Train Loss at iteration 16352: 0.04284208997805609\n",
      "Train Loss at iteration 16353: 0.04284205947773818\n",
      "Train Loss at iteration 16354: 0.04284202898009046\n",
      "Train Loss at iteration 16355: 0.042841998485112456\n",
      "Train Loss at iteration 16356: 0.0428419679928037\n",
      "Train Loss at iteration 16357: 0.042841937503163736\n",
      "Train Loss at iteration 16358: 0.0428419070161921\n",
      "Train Loss at iteration 16359: 0.04284187653188833\n",
      "Train Loss at iteration 16360: 0.042841846050251965\n",
      "Train Loss at iteration 16361: 0.04284181557128252\n",
      "Train Loss at iteration 16362: 0.04284178509497956\n",
      "Train Loss at iteration 16363: 0.04284175462134262\n",
      "Train Loss at iteration 16364: 0.04284172415037122\n",
      "Train Loss at iteration 16365: 0.04284169368206492\n",
      "Train Loss at iteration 16366: 0.04284166321642322\n",
      "Train Loss at iteration 16367: 0.042841632753445706\n",
      "Train Loss at iteration 16368: 0.04284160229313188\n",
      "Train Loss at iteration 16369: 0.042841571835481304\n",
      "Train Loss at iteration 16370: 0.04284154138049349\n",
      "Train Loss at iteration 16371: 0.042841510928167986\n",
      "Train Loss at iteration 16372: 0.04284148047850434\n",
      "Train Loss at iteration 16373: 0.04284145003150209\n",
      "Train Loss at iteration 16374: 0.04284141958716077\n",
      "Train Loss at iteration 16375: 0.04284138914547991\n",
      "Train Loss at iteration 16376: 0.04284135870645908\n",
      "Train Loss at iteration 16377: 0.04284132827009777\n",
      "Train Loss at iteration 16378: 0.042841297836395544\n",
      "Train Loss at iteration 16379: 0.04284126740535195\n",
      "Train Loss at iteration 16380: 0.042841236976966526\n",
      "Train Loss at iteration 16381: 0.04284120655123878\n",
      "Train Loss at iteration 16382: 0.04284117612816829\n",
      "Train Loss at iteration 16383: 0.042841145707754574\n",
      "Train Loss at iteration 16384: 0.04284111528999717\n",
      "Train Loss at iteration 16385: 0.04284108487489564\n",
      "Train Loss at iteration 16386: 0.0428410544624495\n",
      "Train Loss at iteration 16387: 0.0428410240526583\n",
      "Train Loss at iteration 16388: 0.04284099364552158\n",
      "Train Loss at iteration 16389: 0.042840963241038865\n",
      "Train Loss at iteration 16390: 0.04284093283920973\n",
      "Train Loss at iteration 16391: 0.04284090244003368\n",
      "Train Loss at iteration 16392: 0.042840872043510274\n",
      "Train Loss at iteration 16393: 0.042840841649639035\n",
      "Train Loss at iteration 16394: 0.042840811258419535\n",
      "Train Loss at iteration 16395: 0.04284078086985127\n",
      "Train Loss at iteration 16396: 0.04284075048393382\n",
      "Train Loss at iteration 16397: 0.04284072010066672\n",
      "Train Loss at iteration 16398: 0.04284068972004948\n",
      "Train Loss at iteration 16399: 0.042840659342081686\n",
      "Train Loss at iteration 16400: 0.04284062896676284\n",
      "Train Loss at iteration 16401: 0.04284059859409251\n",
      "Train Loss at iteration 16402: 0.042840568224070216\n",
      "Train Loss at iteration 16403: 0.04284053785669552\n",
      "Train Loss at iteration 16404: 0.04284050749196795\n",
      "Train Loss at iteration 16405: 0.042840477129887054\n",
      "Train Loss at iteration 16406: 0.042840446770452366\n",
      "Train Loss at iteration 16407: 0.04284041641366343\n",
      "Train Loss at iteration 16408: 0.0428403860595198\n",
      "Train Loss at iteration 16409: 0.042840355708021\n",
      "Train Loss at iteration 16410: 0.04284032535916658\n",
      "Train Loss at iteration 16411: 0.04284029501295609\n",
      "Train Loss at iteration 16412: 0.04284026466938907\n",
      "Train Loss at iteration 16413: 0.04284023432846505\n",
      "Train Loss at iteration 16414: 0.04284020399018357\n",
      "Train Loss at iteration 16415: 0.04284017365454419\n",
      "Train Loss at iteration 16416: 0.04284014332154646\n",
      "Train Loss at iteration 16417: 0.04284011299118988\n",
      "Train Loss at iteration 16418: 0.04284008266347404\n",
      "Train Loss at iteration 16419: 0.04284005233839846\n",
      "Train Loss at iteration 16420: 0.042840022015962695\n",
      "Train Loss at iteration 16421: 0.04283999169616626\n",
      "Train Loss at iteration 16422: 0.04283996137900874\n",
      "Train Loss at iteration 16423: 0.04283993106448964\n",
      "Train Loss at iteration 16424: 0.042839900752608524\n",
      "Train Loss at iteration 16425: 0.042839870443364944\n",
      "Train Loss at iteration 16426: 0.04283984013675842\n",
      "Train Loss at iteration 16427: 0.042839809832788504\n",
      "Train Loss at iteration 16428: 0.04283977953145476\n",
      "Train Loss at iteration 16429: 0.0428397492327567\n",
      "Train Loss at iteration 16430: 0.04283971893669388\n",
      "Train Loss at iteration 16431: 0.04283968864326586\n",
      "Train Loss at iteration 16432: 0.04283965835247217\n",
      "Train Loss at iteration 16433: 0.04283962806431235\n",
      "Train Loss at iteration 16434: 0.04283959777878596\n",
      "Train Loss at iteration 16435: 0.042839567495892523\n",
      "Train Loss at iteration 16436: 0.04283953721563161\n",
      "Train Loss at iteration 16437: 0.04283950693800274\n",
      "Train Loss at iteration 16438: 0.04283947666300547\n",
      "Train Loss at iteration 16439: 0.04283944639063935\n",
      "Train Loss at iteration 16440: 0.04283941612090392\n",
      "Train Loss at iteration 16441: 0.04283938585379872\n",
      "Train Loss at iteration 16442: 0.042839355589323294\n",
      "Train Loss at iteration 16443: 0.04283932532747721\n",
      "Train Loss at iteration 16444: 0.04283929506825999\n",
      "Train Loss at iteration 16445: 0.042839264811671184\n",
      "Train Loss at iteration 16446: 0.04283923455771034\n",
      "Train Loss at iteration 16447: 0.04283920430637699\n",
      "Train Loss at iteration 16448: 0.042839174057670716\n",
      "Train Loss at iteration 16449: 0.042839143811591034\n",
      "Train Loss at iteration 16450: 0.042839113568137493\n",
      "Train Loss at iteration 16451: 0.04283908332730964\n",
      "Train Loss at iteration 16452: 0.042839053089107036\n",
      "Train Loss at iteration 16453: 0.04283902285352921\n",
      "Train Loss at iteration 16454: 0.04283899262057572\n",
      "Train Loss at iteration 16455: 0.04283896239024611\n",
      "Train Loss at iteration 16456: 0.04283893216253991\n",
      "Train Loss at iteration 16457: 0.04283890193745669\n",
      "Train Loss at iteration 16458: 0.04283887171499598\n",
      "Train Loss at iteration 16459: 0.04283884149515735\n",
      "Train Loss at iteration 16460: 0.042838811277940314\n",
      "Train Loss at iteration 16461: 0.04283878106334445\n",
      "Train Loss at iteration 16462: 0.042838750851369276\n",
      "Train Loss at iteration 16463: 0.04283872064201437\n",
      "Train Loss at iteration 16464: 0.04283869043527925\n",
      "Train Loss at iteration 16465: 0.042838660231163485\n",
      "Train Loss at iteration 16466: 0.04283863002966662\n",
      "Train Loss at iteration 16467: 0.042838599830788186\n",
      "Train Loss at iteration 16468: 0.04283856963452775\n",
      "Train Loss at iteration 16469: 0.042838539440884865\n",
      "Train Loss at iteration 16470: 0.04283850924985905\n",
      "Train Loss at iteration 16471: 0.04283847906144988\n",
      "Train Loss at iteration 16472: 0.04283844887565688\n",
      "Train Loss at iteration 16473: 0.04283841869247962\n",
      "Train Loss at iteration 16474: 0.04283838851191764\n",
      "Train Loss at iteration 16475: 0.04283835833397048\n",
      "Train Loss at iteration 16476: 0.0428383281586377\n",
      "Train Loss at iteration 16477: 0.04283829798591886\n",
      "Train Loss at iteration 16478: 0.042838267815813486\n",
      "Train Loss at iteration 16479: 0.04283823764832113\n",
      "Train Loss at iteration 16480: 0.04283820748344134\n",
      "Train Loss at iteration 16481: 0.0428381773211737\n",
      "Train Loss at iteration 16482: 0.04283814716151769\n",
      "Train Loss at iteration 16483: 0.0428381170044729\n",
      "Train Loss at iteration 16484: 0.0428380868500389\n",
      "Train Loss at iteration 16485: 0.04283805669821522\n",
      "Train Loss at iteration 16486: 0.0428380265490014\n",
      "Train Loss at iteration 16487: 0.04283799640239701\n",
      "Train Loss at iteration 16488: 0.042837966258401555\n",
      "Train Loss at iteration 16489: 0.042837936117014634\n",
      "Train Loss at iteration 16490: 0.042837905978235793\n",
      "Train Loss at iteration 16491: 0.042837875842064554\n",
      "Train Loss at iteration 16492: 0.04283784570850047\n",
      "Train Loss at iteration 16493: 0.04283781557754312\n",
      "Train Loss at iteration 16494: 0.04283778544919204\n",
      "Train Loss at iteration 16495: 0.04283775532344676\n",
      "Train Loss at iteration 16496: 0.042837725200306855\n",
      "Train Loss at iteration 16497: 0.04283769507977187\n",
      "Train Loss at iteration 16498: 0.04283766496184135\n",
      "Train Loss at iteration 16499: 0.042837634846514865\n",
      "Train Loss at iteration 16500: 0.04283760473379193\n",
      "Train Loss at iteration 16501: 0.042837574623672124\n",
      "Train Loss at iteration 16502: 0.04283754451615499\n",
      "Train Loss at iteration 16503: 0.04283751441124008\n",
      "Train Loss at iteration 16504: 0.042837484308926956\n",
      "Train Loss at iteration 16505: 0.04283745420921515\n",
      "Train Loss at iteration 16506: 0.04283742411210421\n",
      "Train Loss at iteration 16507: 0.0428373940175937\n",
      "Train Loss at iteration 16508: 0.042837363925683186\n",
      "Train Loss at iteration 16509: 0.042837333836372185\n",
      "Train Loss at iteration 16510: 0.042837303749660285\n",
      "Train Loss at iteration 16511: 0.042837273665547015\n",
      "Train Loss at iteration 16512: 0.04283724358403193\n",
      "Train Loss at iteration 16513: 0.04283721350511459\n",
      "Train Loss at iteration 16514: 0.042837183428794536\n",
      "Train Loss at iteration 16515: 0.04283715335507133\n",
      "Train Loss at iteration 16516: 0.04283712328394452\n",
      "Train Loss at iteration 16517: 0.042837093215413666\n",
      "Train Loss at iteration 16518: 0.04283706314947829\n",
      "Train Loss at iteration 16519: 0.04283703308613798\n",
      "Train Loss at iteration 16520: 0.042837003025392284\n",
      "Train Loss at iteration 16521: 0.04283697296724075\n",
      "Train Loss at iteration 16522: 0.04283694291168292\n",
      "Train Loss at iteration 16523: 0.042836912858718365\n",
      "Train Loss at iteration 16524: 0.04283688280834662\n",
      "Train Loss at iteration 16525: 0.04283685276056724\n",
      "Train Loss at iteration 16526: 0.042836822715379794\n",
      "Train Loss at iteration 16527: 0.04283679267278382\n",
      "Train Loss at iteration 16528: 0.04283676263277889\n",
      "Train Loss at iteration 16529: 0.04283673259536454\n",
      "Train Loss at iteration 16530: 0.04283670256054033\n",
      "Train Loss at iteration 16531: 0.042836672528305794\n",
      "Train Loss at iteration 16532: 0.042836642498660524\n",
      "Train Loss at iteration 16533: 0.04283661247160406\n",
      "Train Loss at iteration 16534: 0.04283658244713593\n",
      "Train Loss at iteration 16535: 0.042836552425255726\n",
      "Train Loss at iteration 16536: 0.04283652240596297\n",
      "Train Loss at iteration 16537: 0.04283649238925725\n",
      "Train Loss at iteration 16538: 0.04283646237513808\n",
      "Train Loss at iteration 16539: 0.04283643236360505\n",
      "Train Loss at iteration 16540: 0.0428364023546577\n",
      "Train Loss at iteration 16541: 0.04283637234829557\n",
      "Train Loss at iteration 16542: 0.042836342344518254\n",
      "Train Loss at iteration 16543: 0.042836312343325265\n",
      "Train Loss at iteration 16544: 0.04283628234471619\n",
      "Train Loss at iteration 16545: 0.04283625234869055\n",
      "Train Loss at iteration 16546: 0.04283622235524793\n",
      "Train Loss at iteration 16547: 0.042836192364387876\n",
      "Train Loss at iteration 16548: 0.04283616237610994\n",
      "Train Loss at iteration 16549: 0.04283613239041367\n",
      "Train Loss at iteration 16550: 0.042836102407298654\n",
      "Train Loss at iteration 16551: 0.042836072426764404\n",
      "Train Loss at iteration 16552: 0.04283604244881051\n",
      "Train Loss at iteration 16553: 0.042836012473436505\n",
      "Train Loss at iteration 16554: 0.04283598250064197\n",
      "Train Loss at iteration 16555: 0.04283595253042642\n",
      "Train Loss at iteration 16556: 0.042835922562789445\n",
      "Train Loss at iteration 16557: 0.0428358925977306\n",
      "Train Loss at iteration 16558: 0.042835862635249425\n",
      "Train Loss at iteration 16559: 0.04283583267534549\n",
      "Train Loss at iteration 16560: 0.04283580271801834\n",
      "Train Loss at iteration 16561: 0.04283577276326754\n",
      "Train Loss at iteration 16562: 0.042835742811092654\n",
      "Train Loss at iteration 16563: 0.042835712861493215\n",
      "Train Loss at iteration 16564: 0.042835682914468806\n",
      "Train Loss at iteration 16565: 0.042835652970018956\n",
      "Train Loss at iteration 16566: 0.04283562302814325\n",
      "Train Loss at iteration 16567: 0.042835593088841224\n",
      "Train Loss at iteration 16568: 0.04283556315211245\n",
      "Train Loss at iteration 16569: 0.04283553321795647\n",
      "Train Loss at iteration 16570: 0.042835503286372846\n",
      "Train Loss at iteration 16571: 0.042835473357361147\n",
      "Train Loss at iteration 16572: 0.04283544343092092\n",
      "Train Loss at iteration 16573: 0.04283541350705174\n",
      "Train Loss at iteration 16574: 0.042835383585753124\n",
      "Train Loss at iteration 16575: 0.04283535366702467\n",
      "Train Loss at iteration 16576: 0.042835323750865927\n",
      "Train Loss at iteration 16577: 0.04283529383727644\n",
      "Train Loss at iteration 16578: 0.04283526392625577\n",
      "Train Loss at iteration 16579: 0.0428352340178035\n",
      "Train Loss at iteration 16580: 0.04283520411191915\n",
      "Train Loss at iteration 16581: 0.0428351742086023\n",
      "Train Loss at iteration 16582: 0.0428351443078525\n",
      "Train Loss at iteration 16583: 0.04283511440966932\n",
      "Train Loss at iteration 16584: 0.04283508451405231\n",
      "Train Loss at iteration 16585: 0.04283505462100102\n",
      "Train Loss at iteration 16586: 0.04283502473051504\n",
      "Train Loss at iteration 16587: 0.04283499484259388\n",
      "Train Loss at iteration 16588: 0.04283496495723715\n",
      "Train Loss at iteration 16589: 0.04283493507444437\n",
      "Train Loss at iteration 16590: 0.04283490519421514\n",
      "Train Loss at iteration 16591: 0.04283487531654897\n",
      "Train Loss at iteration 16592: 0.04283484544144545\n",
      "Train Loss at iteration 16593: 0.04283481556890414\n",
      "Train Loss at iteration 16594: 0.04283478569892457\n",
      "Train Loss at iteration 16595: 0.042834755831506345\n",
      "Train Loss at iteration 16596: 0.042834725966648994\n",
      "Train Loss at iteration 16597: 0.042834696104352074\n",
      "Train Loss at iteration 16598: 0.04283466624461517\n",
      "Train Loss at iteration 16599: 0.04283463638743782\n",
      "Train Loss at iteration 16600: 0.04283460653281959\n",
      "Train Loss at iteration 16601: 0.042834576680760045\n",
      "Train Loss at iteration 16602: 0.042834546831258744\n",
      "Train Loss at iteration 16603: 0.042834516984315237\n",
      "Train Loss at iteration 16604: 0.042834487139929094\n",
      "Train Loss at iteration 16605: 0.042834457298099864\n",
      "Train Loss at iteration 16606: 0.04283442745882714\n",
      "Train Loss at iteration 16607: 0.042834397622110436\n",
      "Train Loss at iteration 16608: 0.04283436778794934\n",
      "Train Loss at iteration 16609: 0.042834337956343414\n",
      "Train Loss at iteration 16610: 0.04283430812729221\n",
      "Train Loss at iteration 16611: 0.04283427830079531\n",
      "Train Loss at iteration 16612: 0.04283424847685225\n",
      "Train Loss at iteration 16613: 0.04283421865546259\n",
      "Train Loss at iteration 16614: 0.042834188836625896\n",
      "Train Loss at iteration 16615: 0.04283415902034173\n",
      "Train Loss at iteration 16616: 0.04283412920660967\n",
      "Train Loss at iteration 16617: 0.04283409939542927\n",
      "Train Loss at iteration 16618: 0.04283406958680007\n",
      "Train Loss at iteration 16619: 0.04283403978072167\n",
      "Train Loss at iteration 16620: 0.04283400997719359\n",
      "Train Loss at iteration 16621: 0.04283398017621541\n",
      "Train Loss at iteration 16622: 0.04283395037778671\n",
      "Train Loss at iteration 16623: 0.04283392058190703\n",
      "Train Loss at iteration 16624: 0.04283389078857592\n",
      "Train Loss at iteration 16625: 0.04283386099779298\n",
      "Train Loss at iteration 16626: 0.04283383120955774\n",
      "Train Loss at iteration 16627: 0.04283380142386978\n",
      "Train Loss at iteration 16628: 0.04283377164072866\n",
      "Train Loss at iteration 16629: 0.04283374186013394\n",
      "Train Loss at iteration 16630: 0.04283371208208517\n",
      "Train Loss at iteration 16631: 0.04283368230658194\n",
      "Train Loss at iteration 16632: 0.042833652533623784\n",
      "Train Loss at iteration 16633: 0.04283362276321028\n",
      "Train Loss at iteration 16634: 0.04283359299534101\n",
      "Train Loss at iteration 16635: 0.042833563230015484\n",
      "Train Loss at iteration 16636: 0.04283353346723332\n",
      "Train Loss at iteration 16637: 0.042833503706994056\n",
      "Train Loss at iteration 16638: 0.04283347394929725\n",
      "Train Loss at iteration 16639: 0.042833444194142485\n",
      "Train Loss at iteration 16640: 0.0428334144415293\n",
      "Train Loss at iteration 16641: 0.042833384691457296\n",
      "Train Loss at iteration 16642: 0.04283335494392599\n",
      "Train Loss at iteration 16643: 0.04283332519893497\n",
      "Train Loss at iteration 16644: 0.04283329545648381\n",
      "Train Loss at iteration 16645: 0.04283326571657207\n",
      "Train Loss at iteration 16646: 0.04283323597919929\n",
      "Train Loss at iteration 16647: 0.04283320624436505\n",
      "Train Loss at iteration 16648: 0.04283317651206892\n",
      "Train Loss at iteration 16649: 0.04283314678231047\n",
      "Train Loss at iteration 16650: 0.04283311705508924\n",
      "Train Loss at iteration 16651: 0.042833087330404813\n",
      "Train Loss at iteration 16652: 0.042833057608256744\n",
      "Train Loss at iteration 16653: 0.0428330278886446\n",
      "Train Loss at iteration 16654: 0.04283299817156797\n",
      "Train Loss at iteration 16655: 0.04283296845702637\n",
      "Train Loss at iteration 16656: 0.042832938745019415\n",
      "Train Loss at iteration 16657: 0.04283290903554663\n",
      "Train Loss at iteration 16658: 0.042832879328607604\n",
      "Train Loss at iteration 16659: 0.04283284962420189\n",
      "Train Loss at iteration 16660: 0.04283281992232906\n",
      "Train Loss at iteration 16661: 0.04283279022298868\n",
      "Train Loss at iteration 16662: 0.04283276052618031\n",
      "Train Loss at iteration 16663: 0.04283273083190353\n",
      "Train Loss at iteration 16664: 0.04283270114015788\n",
      "Train Loss at iteration 16665: 0.04283267145094295\n",
      "Train Loss at iteration 16666: 0.042832641764258286\n",
      "Train Loss at iteration 16667: 0.042832612080103485\n",
      "Train Loss at iteration 16668: 0.042832582398478085\n",
      "Train Loss at iteration 16669: 0.04283255271938165\n",
      "Train Loss at iteration 16670: 0.04283252304281374\n",
      "Train Loss at iteration 16671: 0.04283249336877396\n",
      "Train Loss at iteration 16672: 0.04283246369726185\n",
      "Train Loss at iteration 16673: 0.042832434028276974\n",
      "Train Loss at iteration 16674: 0.042832404361818906\n",
      "Train Loss at iteration 16675: 0.042832374697887206\n",
      "Train Loss at iteration 16676: 0.042832345036481444\n",
      "Train Loss at iteration 16677: 0.042832315377601196\n",
      "Train Loss at iteration 16678: 0.04283228572124602\n",
      "Train Loss at iteration 16679: 0.042832256067415475\n",
      "Train Loss at iteration 16680: 0.042832226416109134\n",
      "Train Loss at iteration 16681: 0.04283219676732658\n",
      "Train Loss at iteration 16682: 0.042832167121067355\n",
      "Train Loss at iteration 16683: 0.04283213747733104\n",
      "Train Loss at iteration 16684: 0.04283210783611722\n",
      "Train Loss at iteration 16685: 0.04283207819742542\n",
      "Train Loss at iteration 16686: 0.04283204856125523\n",
      "Train Loss at iteration 16687: 0.04283201892760623\n",
      "Train Loss at iteration 16688: 0.04283198929647796\n",
      "Train Loss at iteration 16689: 0.04283195966787002\n",
      "Train Loss at iteration 16690: 0.04283193004178195\n",
      "Train Loss at iteration 16691: 0.04283190041821333\n",
      "Train Loss at iteration 16692: 0.04283187079716373\n",
      "Train Loss at iteration 16693: 0.042831841178632715\n",
      "Train Loss at iteration 16694: 0.04283181156261985\n",
      "Train Loss at iteration 16695: 0.04283178194912471\n",
      "Train Loss at iteration 16696: 0.04283175233814686\n",
      "Train Loss at iteration 16697: 0.042831722729685856\n",
      "Train Loss at iteration 16698: 0.04283169312374129\n",
      "Train Loss at iteration 16699: 0.04283166352031272\n",
      "Train Loss at iteration 16700: 0.042831633919399724\n",
      "Train Loss at iteration 16701: 0.042831604321001845\n",
      "Train Loss at iteration 16702: 0.04283157472511869\n",
      "Train Loss at iteration 16703: 0.04283154513174978\n",
      "Train Loss at iteration 16704: 0.04283151554089473\n",
      "Train Loss at iteration 16705: 0.042831485952553074\n",
      "Train Loss at iteration 16706: 0.042831456366724405\n",
      "Train Loss at iteration 16707: 0.042831426783408275\n",
      "Train Loss at iteration 16708: 0.042831397202604274\n",
      "Train Loss at iteration 16709: 0.04283136762431195\n",
      "Train Loss at iteration 16710: 0.042831338048530884\n",
      "Train Loss at iteration 16711: 0.04283130847526064\n",
      "Train Loss at iteration 16712: 0.0428312789045008\n",
      "Train Loss at iteration 16713: 0.04283124933625093\n",
      "Train Loss at iteration 16714: 0.0428312197705106\n",
      "Train Loss at iteration 16715: 0.04283119020727935\n",
      "Train Loss at iteration 16716: 0.042831160646556794\n",
      "Train Loss at iteration 16717: 0.04283113108834248\n",
      "Train Loss at iteration 16718: 0.04283110153263599\n",
      "Train Loss at iteration 16719: 0.04283107197943688\n",
      "Train Loss at iteration 16720: 0.04283104242874471\n",
      "Train Loss at iteration 16721: 0.04283101288055909\n",
      "Train Loss at iteration 16722: 0.04283098333487956\n",
      "Train Loss at iteration 16723: 0.042830953791705685\n",
      "Train Loss at iteration 16724: 0.04283092425103707\n",
      "Train Loss at iteration 16725: 0.04283089471287325\n",
      "Train Loss at iteration 16726: 0.042830865177213816\n",
      "Train Loss at iteration 16727: 0.04283083564405834\n",
      "Train Loss at iteration 16728: 0.04283080611340638\n",
      "Train Loss at iteration 16729: 0.0428307765852575\n",
      "Train Loss at iteration 16730: 0.04283074705961131\n",
      "Train Loss at iteration 16731: 0.04283071753646736\n",
      "Train Loss at iteration 16732: 0.042830688015825194\n",
      "Train Loss at iteration 16733: 0.04283065849768443\n",
      "Train Loss at iteration 16734: 0.0428306289820446\n",
      "Train Loss at iteration 16735: 0.0428305994689053\n",
      "Train Loss at iteration 16736: 0.04283056995826609\n",
      "Train Loss at iteration 16737: 0.042830540450126554\n",
      "Train Loss at iteration 16738: 0.042830510944486255\n",
      "Train Loss at iteration 16739: 0.04283048144134476\n",
      "Train Loss at iteration 16740: 0.042830451940701654\n",
      "Train Loss at iteration 16741: 0.042830422442556505\n",
      "Train Loss at iteration 16742: 0.04283039294690888\n",
      "Train Loss at iteration 16743: 0.04283036345375836\n",
      "Train Loss at iteration 16744: 0.0428303339631045\n",
      "Train Loss at iteration 16745: 0.04283030447494689\n",
      "Train Loss at iteration 16746: 0.0428302749892851\n",
      "Train Loss at iteration 16747: 0.04283024550611869\n",
      "Train Loss at iteration 16748: 0.04283021602544725\n",
      "Train Loss at iteration 16749: 0.042830186547270346\n",
      "Train Loss at iteration 16750: 0.04283015707158754\n",
      "Train Loss at iteration 16751: 0.04283012759839842\n",
      "Train Loss at iteration 16752: 0.04283009812770257\n",
      "Train Loss at iteration 16753: 0.04283006865949953\n",
      "Train Loss at iteration 16754: 0.042830039193788895\n",
      "Train Loss at iteration 16755: 0.042830009730570225\n",
      "Train Loss at iteration 16756: 0.04282998026984311\n",
      "Train Loss at iteration 16757: 0.04282995081160713\n",
      "Train Loss at iteration 16758: 0.04282992135586182\n",
      "Train Loss at iteration 16759: 0.042829891902606804\n",
      "Train Loss at iteration 16760: 0.042829862451841616\n",
      "Train Loss at iteration 16761: 0.04282983300356584\n",
      "Train Loss at iteration 16762: 0.04282980355777906\n",
      "Train Loss at iteration 16763: 0.04282977411448084\n",
      "Train Loss at iteration 16764: 0.042829744673670754\n",
      "Train Loss at iteration 16765: 0.04282971523534839\n",
      "Train Loss at iteration 16766: 0.04282968579951331\n",
      "Train Loss at iteration 16767: 0.0428296563661651\n",
      "Train Loss at iteration 16768: 0.042829626935303296\n",
      "Train Loss at iteration 16769: 0.04282959750692753\n",
      "Train Loss at iteration 16770: 0.04282956808103733\n",
      "Train Loss at iteration 16771: 0.0428295386576323\n",
      "Train Loss at iteration 16772: 0.04282950923671201\n",
      "Train Loss at iteration 16773: 0.04282947981827601\n",
      "Train Loss at iteration 16774: 0.04282945040232391\n",
      "Train Loss at iteration 16775: 0.04282942098885526\n",
      "Train Loss at iteration 16776: 0.04282939157786965\n",
      "Train Loss at iteration 16777: 0.04282936216936664\n",
      "Train Loss at iteration 16778: 0.04282933276334582\n",
      "Train Loss at iteration 16779: 0.04282930335980675\n",
      "Train Loss at iteration 16780: 0.042829273958749026\n",
      "Train Loss at iteration 16781: 0.04282924456017221\n",
      "Train Loss at iteration 16782: 0.042829215164075875\n",
      "Train Loss at iteration 16783: 0.04282918577045961\n",
      "Train Loss at iteration 16784: 0.04282915637932298\n",
      "Train Loss at iteration 16785: 0.04282912699066556\n",
      "Train Loss at iteration 16786: 0.04282909760448693\n",
      "Train Loss at iteration 16787: 0.042829068220786665\n",
      "Train Loss at iteration 16788: 0.04282903883956434\n",
      "Train Loss at iteration 16789: 0.042829009460819537\n",
      "Train Loss at iteration 16790: 0.04282898008455183\n",
      "Train Loss at iteration 16791: 0.04282895071076078\n",
      "Train Loss at iteration 16792: 0.04282892133944599\n",
      "Train Loss at iteration 16793: 0.04282889197060701\n",
      "Train Loss at iteration 16794: 0.04282886260424344\n",
      "Train Loss at iteration 16795: 0.04282883324035484\n",
      "Train Loss at iteration 16796: 0.04282880387894079\n",
      "Train Loss at iteration 16797: 0.042828774520000876\n",
      "Train Loss at iteration 16798: 0.042828745163534666\n",
      "Train Loss at iteration 16799: 0.042828715809541736\n",
      "Train Loss at iteration 16800: 0.042828686458021674\n",
      "Train Loss at iteration 16801: 0.04282865710897404\n",
      "Train Loss at iteration 16802: 0.04282862776239844\n",
      "Train Loss at iteration 16803: 0.042828598418294415\n",
      "Train Loss at iteration 16804: 0.04282856907666156\n",
      "Train Loss at iteration 16805: 0.042828539737499456\n",
      "Train Loss at iteration 16806: 0.04282851040080768\n",
      "Train Loss at iteration 16807: 0.04282848106658581\n",
      "Train Loss at iteration 16808: 0.04282845173483341\n",
      "Train Loss at iteration 16809: 0.04282842240555007\n",
      "Train Loss at iteration 16810: 0.042828393078735365\n",
      "Train Loss at iteration 16811: 0.04282836375438888\n",
      "Train Loss at iteration 16812: 0.04282833443251019\n",
      "Train Loss at iteration 16813: 0.042828305113098854\n",
      "Train Loss at iteration 16814: 0.04282827579615448\n",
      "Train Loss at iteration 16815: 0.04282824648167662\n",
      "Train Loss at iteration 16816: 0.042828217169664874\n",
      "Train Loss at iteration 16817: 0.04282818786011881\n",
      "Train Loss at iteration 16818: 0.042828158553038016\n",
      "Train Loss at iteration 16819: 0.042828129248422046\n",
      "Train Loss at iteration 16820: 0.04282809994627051\n",
      "Train Loss at iteration 16821: 0.042828070646582964\n",
      "Train Loss at iteration 16822: 0.04282804134935899\n",
      "Train Loss at iteration 16823: 0.04282801205459817\n",
      "Train Loss at iteration 16824: 0.04282798276230009\n",
      "Train Loss at iteration 16825: 0.042827953472464333\n",
      "Train Loss at iteration 16826: 0.04282792418509047\n",
      "Train Loss at iteration 16827: 0.04282789490017807\n",
      "Train Loss at iteration 16828: 0.04282786561772672\n",
      "Train Loss at iteration 16829: 0.04282783633773599\n",
      "Train Loss at iteration 16830: 0.042827807060205496\n",
      "Train Loss at iteration 16831: 0.04282777778513478\n",
      "Train Loss at iteration 16832: 0.042827748512523425\n",
      "Train Loss at iteration 16833: 0.04282771924237104\n",
      "Train Loss at iteration 16834: 0.04282768997467717\n",
      "Train Loss at iteration 16835: 0.04282766070944141\n",
      "Train Loss at iteration 16836: 0.04282763144666335\n",
      "Train Loss at iteration 16837: 0.04282760218634255\n",
      "Train Loss at iteration 16838: 0.0428275729284786\n",
      "Train Loss at iteration 16839: 0.04282754367307108\n",
      "Train Loss at iteration 16840: 0.04282751442011956\n",
      "Train Loss at iteration 16841: 0.04282748516962365\n",
      "Train Loss at iteration 16842: 0.04282745592158291\n",
      "Train Loss at iteration 16843: 0.04282742667599691\n",
      "Train Loss at iteration 16844: 0.04282739743286525\n",
      "Train Loss at iteration 16845: 0.04282736819218749\n",
      "Train Loss at iteration 16846: 0.04282733895396324\n",
      "Train Loss at iteration 16847: 0.04282730971819205\n",
      "Train Loss at iteration 16848: 0.04282728048487353\n",
      "Train Loss at iteration 16849: 0.04282725125400724\n",
      "Train Loss at iteration 16850: 0.042827222025592776\n",
      "Train Loss at iteration 16851: 0.042827192799629696\n",
      "Train Loss at iteration 16852: 0.042827163576117606\n",
      "Train Loss at iteration 16853: 0.042827134355056064\n",
      "Train Loss at iteration 16854: 0.04282710513644468\n",
      "Train Loss at iteration 16855: 0.042827075920283016\n",
      "Train Loss at iteration 16856: 0.042827046706570664\n",
      "Train Loss at iteration 16857: 0.0428270174953072\n",
      "Train Loss at iteration 16858: 0.042826988286492194\n",
      "Train Loss at iteration 16859: 0.04282695908012525\n",
      "Train Loss at iteration 16860: 0.04282692987620594\n",
      "Train Loss at iteration 16861: 0.04282690067473384\n",
      "Train Loss at iteration 16862: 0.04282687147570854\n",
      "Train Loss at iteration 16863: 0.04282684227912962\n",
      "Train Loss at iteration 16864: 0.042826813084996654\n",
      "Train Loss at iteration 16865: 0.04282678389330924\n",
      "Train Loss at iteration 16866: 0.04282675470406696\n",
      "Train Loss at iteration 16867: 0.04282672551726937\n",
      "Train Loss at iteration 16868: 0.042826696332916095\n",
      "Train Loss at iteration 16869: 0.04282666715100668\n",
      "Train Loss at iteration 16870: 0.042826637971540724\n",
      "Train Loss at iteration 16871: 0.04282660879451781\n",
      "Train Loss at iteration 16872: 0.042826579619937515\n",
      "Train Loss at iteration 16873: 0.04282655044779942\n",
      "Train Loss at iteration 16874: 0.04282652127810312\n",
      "Train Loss at iteration 16875: 0.042826492110848206\n",
      "Train Loss at iteration 16876: 0.04282646294603424\n",
      "Train Loss at iteration 16877: 0.0428264337836608\n",
      "Train Loss at iteration 16878: 0.0428264046237275\n",
      "Train Loss at iteration 16879: 0.0428263754662339\n",
      "Train Loss at iteration 16880: 0.04282634631117958\n",
      "Train Loss at iteration 16881: 0.04282631715856413\n",
      "Train Loss at iteration 16882: 0.04282628800838714\n",
      "Train Loss at iteration 16883: 0.0428262588606482\n",
      "Train Loss at iteration 16884: 0.04282622971534688\n",
      "Train Loss at iteration 16885: 0.04282620057248277\n",
      "Train Loss at iteration 16886: 0.042826171432055436\n",
      "Train Loss at iteration 16887: 0.0428261422940645\n",
      "Train Loss at iteration 16888: 0.04282611315850951\n",
      "Train Loss at iteration 16889: 0.04282608402539007\n",
      "Train Loss at iteration 16890: 0.042826054894705756\n",
      "Train Loss at iteration 16891: 0.04282602576645615\n",
      "Train Loss at iteration 16892: 0.04282599664064085\n",
      "Train Loss at iteration 16893: 0.04282596751725942\n",
      "Train Loss at iteration 16894: 0.04282593839631147\n",
      "Train Loss at iteration 16895: 0.042825909277796566\n",
      "Train Loss at iteration 16896: 0.0428258801617143\n",
      "Train Loss at iteration 16897: 0.04282585104806425\n",
      "Train Loss at iteration 16898: 0.042825821936846024\n",
      "Train Loss at iteration 16899: 0.042825792828059164\n",
      "Train Loss at iteration 16900: 0.04282576372170329\n",
      "Train Loss at iteration 16901: 0.04282573461777798\n",
      "Train Loss at iteration 16902: 0.042825705516282814\n",
      "Train Loss at iteration 16903: 0.042825676417217386\n",
      "Train Loss at iteration 16904: 0.04282564732058127\n",
      "Train Loss at iteration 16905: 0.04282561822637405\n",
      "Train Loss at iteration 16906: 0.04282558913459533\n",
      "Train Loss at iteration 16907: 0.04282556004524469\n",
      "Train Loss at iteration 16908: 0.042825530958321686\n",
      "Train Loss at iteration 16909: 0.042825501873825944\n",
      "Train Loss at iteration 16910: 0.04282547279175703\n",
      "Train Loss at iteration 16911: 0.04282544371211453\n",
      "Train Loss at iteration 16912: 0.04282541463489804\n",
      "Train Loss at iteration 16913: 0.04282538556010715\n",
      "Train Loss at iteration 16914: 0.04282535648774142\n",
      "Train Loss at iteration 16915: 0.04282532741780046\n",
      "Train Loss at iteration 16916: 0.04282529835028384\n",
      "Train Loss at iteration 16917: 0.042825269285191175\n",
      "Train Loss at iteration 16918: 0.04282524022252201\n",
      "Train Loss at iteration 16919: 0.04282521116227597\n",
      "Train Loss at iteration 16920: 0.042825182104452617\n",
      "Train Loss at iteration 16921: 0.04282515304905154\n",
      "Train Loss at iteration 16922: 0.04282512399607234\n",
      "Train Loss at iteration 16923: 0.042825094945514584\n",
      "Train Loss at iteration 16924: 0.042825065897377876\n",
      "Train Loss at iteration 16925: 0.04282503685166181\n",
      "Train Loss at iteration 16926: 0.042825007808365956\n",
      "Train Loss at iteration 16927: 0.0428249787674899\n",
      "Train Loss at iteration 16928: 0.042824949729033245\n",
      "Train Loss at iteration 16929: 0.04282492069299555\n",
      "Train Loss at iteration 16930: 0.04282489165937645\n",
      "Train Loss at iteration 16931: 0.042824862628175475\n",
      "Train Loss at iteration 16932: 0.04282483359939226\n",
      "Train Loss at iteration 16933: 0.042824804573026364\n",
      "Train Loss at iteration 16934: 0.04282477554907739\n",
      "Train Loss at iteration 16935: 0.04282474652754492\n",
      "Train Loss at iteration 16936: 0.04282471750842855\n",
      "Train Loss at iteration 16937: 0.04282468849172785\n",
      "Train Loss at iteration 16938: 0.04282465947744242\n",
      "Train Loss at iteration 16939: 0.042824630465571854\n",
      "Train Loss at iteration 16940: 0.042824601456115725\n",
      "Train Loss at iteration 16941: 0.04282457244907364\n",
      "Train Loss at iteration 16942: 0.04282454344444517\n",
      "Train Loss at iteration 16943: 0.042824514442229906\n",
      "Train Loss at iteration 16944: 0.04282448544242745\n",
      "Train Loss at iteration 16945: 0.04282445644503738\n",
      "Train Loss at iteration 16946: 0.04282442745005929\n",
      "Train Loss at iteration 16947: 0.04282439845749276\n",
      "Train Loss at iteration 16948: 0.042824369467337385\n",
      "Train Loss at iteration 16949: 0.04282434047959276\n",
      "Train Loss at iteration 16950: 0.04282431149425845\n",
      "Train Loss at iteration 16951: 0.04282428251133408\n",
      "Train Loss at iteration 16952: 0.04282425353081921\n",
      "Train Loss at iteration 16953: 0.04282422455271344\n",
      "Train Loss at iteration 16954: 0.04282419557701637\n",
      "Train Loss at iteration 16955: 0.04282416660372757\n",
      "Train Loss at iteration 16956: 0.04282413763284664\n",
      "Train Loss at iteration 16957: 0.04282410866437317\n",
      "Train Loss at iteration 16958: 0.04282407969830675\n",
      "Train Loss at iteration 16959: 0.04282405073464695\n",
      "Train Loss at iteration 16960: 0.04282402177339338\n",
      "Train Loss at iteration 16961: 0.04282399281454565\n",
      "Train Loss at iteration 16962: 0.04282396385810332\n",
      "Train Loss at iteration 16963: 0.042823934904065976\n",
      "Train Loss at iteration 16964: 0.04282390595243322\n",
      "Train Loss at iteration 16965: 0.042823877003204634\n",
      "Train Loss at iteration 16966: 0.04282384805637983\n",
      "Train Loss at iteration 16967: 0.042823819111958386\n",
      "Train Loss at iteration 16968: 0.042823790169939896\n",
      "Train Loss at iteration 16969: 0.04282376123032392\n",
      "Train Loss at iteration 16970: 0.04282373229311009\n",
      "Train Loss at iteration 16971: 0.04282370335829797\n",
      "Train Loss at iteration 16972: 0.04282367442588717\n",
      "Train Loss at iteration 16973: 0.04282364549587727\n",
      "Train Loss at iteration 16974: 0.042823616568267865\n",
      "Train Loss at iteration 16975: 0.04282358764305853\n",
      "Train Loss at iteration 16976: 0.042823558720248886\n",
      "Train Loss at iteration 16977: 0.0428235297998385\n",
      "Train Loss at iteration 16978: 0.04282350088182698\n",
      "Train Loss at iteration 16979: 0.042823471966213895\n",
      "Train Loss at iteration 16980: 0.04282344305299886\n",
      "Train Loss at iteration 16981: 0.04282341414218145\n",
      "Train Loss at iteration 16982: 0.042823385233761266\n",
      "Train Loss at iteration 16983: 0.04282335632773789\n",
      "Train Loss at iteration 16984: 0.042823327424110935\n",
      "Train Loss at iteration 16985: 0.04282329852287997\n",
      "Train Loss at iteration 16986: 0.04282326962404458\n",
      "Train Loss at iteration 16987: 0.042823240727604385\n",
      "Train Loss at iteration 16988: 0.04282321183355896\n",
      "Train Loss at iteration 16989: 0.04282318294190791\n",
      "Train Loss at iteration 16990: 0.04282315405265081\n",
      "Train Loss at iteration 16991: 0.04282312516578725\n",
      "Train Loss at iteration 16992: 0.04282309628131685\n",
      "Train Loss at iteration 16993: 0.04282306739923916\n",
      "Train Loss at iteration 16994: 0.042823038519553824\n",
      "Train Loss at iteration 16995: 0.04282300964226039\n",
      "Train Loss at iteration 16996: 0.042822980767358486\n",
      "Train Loss at iteration 16997: 0.042822951894847666\n",
      "Train Loss at iteration 16998: 0.042822923024727545\n",
      "Train Loss at iteration 16999: 0.04282289415699773\n",
      "Train Loss at iteration 17000: 0.04282286529165779\n",
      "Train Loss at iteration 17001: 0.04282283642870732\n",
      "Train Loss at iteration 17002: 0.04282280756814593\n",
      "Train Loss at iteration 17003: 0.042822778709973204\n",
      "Train Loss at iteration 17004: 0.04282274985418871\n",
      "Train Loss at iteration 17005: 0.04282272100079209\n",
      "Train Loss at iteration 17006: 0.04282269214978289\n",
      "Train Loss at iteration 17007: 0.04282266330116075\n",
      "Train Loss at iteration 17008: 0.04282263445492522\n",
      "Train Loss at iteration 17009: 0.04282260561107593\n",
      "Train Loss at iteration 17010: 0.042822576769612444\n",
      "Train Loss at iteration 17011: 0.042822547930534374\n",
      "Train Loss at iteration 17012: 0.0428225190938413\n",
      "Train Loss at iteration 17013: 0.04282249025953283\n",
      "Train Loss at iteration 17014: 0.04282246142760856\n",
      "Train Loss at iteration 17015: 0.04282243259806806\n",
      "Train Loss at iteration 17016: 0.04282240377091094\n",
      "Train Loss at iteration 17017: 0.0428223749461368\n",
      "Train Loss at iteration 17018: 0.04282234612374524\n",
      "Train Loss at iteration 17019: 0.04282231730373583\n",
      "Train Loss at iteration 17020: 0.042822288486108175\n",
      "Train Loss at iteration 17021: 0.04282225967086188\n",
      "Train Loss at iteration 17022: 0.04282223085799653\n",
      "Train Loss at iteration 17023: 0.042822202047511713\n",
      "Train Loss at iteration 17024: 0.04282217323940704\n",
      "Train Loss at iteration 17025: 0.04282214443368209\n",
      "Train Loss at iteration 17026: 0.042822115630336464\n",
      "Train Loss at iteration 17027: 0.04282208682936977\n",
      "Train Loss at iteration 17028: 0.04282205803078159\n",
      "Train Loss at iteration 17029: 0.042822029234571506\n",
      "Train Loss at iteration 17030: 0.04282200044073915\n",
      "Train Loss at iteration 17031: 0.04282197164928408\n",
      "Train Loss at iteration 17032: 0.04282194286020591\n",
      "Train Loss at iteration 17033: 0.04282191407350423\n",
      "Train Loss at iteration 17034: 0.04282188528917864\n",
      "Train Loss at iteration 17035: 0.04282185650722873\n",
      "Train Loss at iteration 17036: 0.04282182772765411\n",
      "Train Loss at iteration 17037: 0.04282179895045435\n",
      "Train Loss at iteration 17038: 0.04282177017562906\n",
      "Train Loss at iteration 17039: 0.04282174140317784\n",
      "Train Loss at iteration 17040: 0.042821712633100285\n",
      "Train Loss at iteration 17041: 0.04282168386539598\n",
      "Train Loss at iteration 17042: 0.04282165510006454\n",
      "Train Loss at iteration 17043: 0.04282162633710554\n",
      "Train Loss at iteration 17044: 0.0428215975765186\n",
      "Train Loss at iteration 17045: 0.04282156881830329\n",
      "Train Loss at iteration 17046: 0.04282154006245921\n",
      "Train Loss at iteration 17047: 0.04282151130898599\n",
      "Train Loss at iteration 17048: 0.042821482557883185\n",
      "Train Loss at iteration 17049: 0.04282145380915042\n",
      "Train Loss at iteration 17050: 0.042821425062787265\n",
      "Train Loss at iteration 17051: 0.04282139631879335\n",
      "Train Loss at iteration 17052: 0.04282136757716824\n",
      "Train Loss at iteration 17053: 0.04282133883791157\n",
      "Train Loss at iteration 17054: 0.04282131010102289\n",
      "Train Loss at iteration 17055: 0.04282128136650183\n",
      "Train Loss at iteration 17056: 0.04282125263434797\n",
      "Train Loss at iteration 17057: 0.04282122390456092\n",
      "Train Loss at iteration 17058: 0.04282119517714027\n",
      "Train Loss at iteration 17059: 0.04282116645208562\n",
      "Train Loss at iteration 17060: 0.04282113772939656\n",
      "Train Loss at iteration 17061: 0.04282110900907272\n",
      "Train Loss at iteration 17062: 0.042821080291113645\n",
      "Train Loss at iteration 17063: 0.04282105157551896\n",
      "Train Loss at iteration 17064: 0.04282102286228827\n",
      "Train Loss at iteration 17065: 0.04282099415142116\n",
      "Train Loss at iteration 17066: 0.04282096544291725\n",
      "Train Loss at iteration 17067: 0.0428209367367761\n",
      "Train Loss at iteration 17068: 0.04282090803299734\n",
      "Train Loss at iteration 17069: 0.042820879331580554\n",
      "Train Loss at iteration 17070: 0.04282085063252535\n",
      "Train Loss at iteration 17071: 0.04282082193583131\n",
      "Train Loss at iteration 17072: 0.042820793241498054\n",
      "Train Loss at iteration 17073: 0.04282076454952516\n",
      "Train Loss at iteration 17074: 0.04282073585991223\n",
      "Train Loss at iteration 17075: 0.04282070717265889\n",
      "Train Loss at iteration 17076: 0.04282067848776469\n",
      "Train Loss at iteration 17077: 0.04282064980522927\n",
      "Train Loss at iteration 17078: 0.04282062112505221\n",
      "Train Loss at iteration 17079: 0.042820592447233116\n",
      "Train Loss at iteration 17080: 0.04282056377177158\n",
      "Train Loss at iteration 17081: 0.042820535098667206\n",
      "Train Loss at iteration 17082: 0.0428205064279196\n",
      "Train Loss at iteration 17083: 0.04282047775952834\n",
      "Train Loss at iteration 17084: 0.04282044909349305\n",
      "Train Loss at iteration 17085: 0.04282042042981331\n",
      "Train Loss at iteration 17086: 0.042820391768488716\n",
      "Train Loss at iteration 17087: 0.0428203631095189\n",
      "Train Loss at iteration 17088: 0.04282033445290344\n",
      "Train Loss at iteration 17089: 0.04282030579864193\n",
      "Train Loss at iteration 17090: 0.04282027714673397\n",
      "Train Loss at iteration 17091: 0.042820248497179164\n",
      "Train Loss at iteration 17092: 0.042820219849977116\n",
      "Train Loss at iteration 17093: 0.04282019120512743\n",
      "Train Loss at iteration 17094: 0.04282016256262969\n",
      "Train Loss at iteration 17095: 0.042820133922483516\n",
      "Train Loss at iteration 17096: 0.04282010528468849\n",
      "Train Loss at iteration 17097: 0.04282007664924421\n",
      "Train Loss at iteration 17098: 0.0428200480161503\n",
      "Train Loss at iteration 17099: 0.042820019385406353\n",
      "Train Loss at iteration 17100: 0.042819990757011955\n",
      "Train Loss at iteration 17101: 0.04281996213096673\n",
      "Train Loss at iteration 17102: 0.04281993350727022\n",
      "Train Loss at iteration 17103: 0.04281990488592212\n",
      "Train Loss at iteration 17104: 0.04281987626692193\n",
      "Train Loss at iteration 17105: 0.042819847650269326\n",
      "Train Loss at iteration 17106: 0.042819819035963894\n",
      "Train Loss at iteration 17107: 0.04281979042400521\n",
      "Train Loss at iteration 17108: 0.042819761814392884\n",
      "Train Loss at iteration 17109: 0.04281973320712654\n",
      "Train Loss at iteration 17110: 0.04281970460220575\n",
      "Train Loss at iteration 17111: 0.04281967599963012\n",
      "Train Loss at iteration 17112: 0.042819647399399266\n",
      "Train Loss at iteration 17113: 0.042819618801512783\n",
      "Train Loss at iteration 17114: 0.04281959020597027\n",
      "Train Loss at iteration 17115: 0.04281956161277133\n",
      "Train Loss at iteration 17116: 0.04281953302191556\n",
      "Train Loss at iteration 17117: 0.04281950443340257\n",
      "Train Loss at iteration 17118: 0.04281947584723196\n",
      "Train Loss at iteration 17119: 0.042819447263403324\n",
      "Train Loss at iteration 17120: 0.04281941868191628\n",
      "Train Loss at iteration 17121: 0.04281939010277041\n",
      "Train Loss at iteration 17122: 0.04281936152596533\n",
      "Train Loss at iteration 17123: 0.04281933295150064\n",
      "Train Loss at iteration 17124: 0.04281930437937594\n",
      "Train Loss at iteration 17125: 0.04281927580959082\n",
      "Train Loss at iteration 17126: 0.042819247242144906\n",
      "Train Loss at iteration 17127: 0.0428192186770378\n",
      "Train Loss at iteration 17128: 0.042819190114269066\n",
      "Train Loss at iteration 17129: 0.04281916155383835\n",
      "Train Loss at iteration 17130: 0.042819132995745246\n",
      "Train Loss at iteration 17131: 0.04281910443998935\n",
      "Train Loss at iteration 17132: 0.04281907588657026\n",
      "Train Loss at iteration 17133: 0.042819047335487574\n",
      "Train Loss at iteration 17134: 0.042819018786740916\n",
      "Train Loss at iteration 17135: 0.042818990240329875\n",
      "Train Loss at iteration 17136: 0.04281896169625404\n",
      "Train Loss at iteration 17137: 0.04281893315451306\n",
      "Train Loss at iteration 17138: 0.0428189046151065\n",
      "Train Loss at iteration 17139: 0.04281887607803397\n",
      "Train Loss at iteration 17140: 0.042818847543295076\n",
      "Train Loss at iteration 17141: 0.04281881901088941\n",
      "Train Loss at iteration 17142: 0.042818790480816604\n",
      "Train Loss at iteration 17143: 0.04281876195307623\n",
      "Train Loss at iteration 17144: 0.04281873342766792\n",
      "Train Loss at iteration 17145: 0.04281870490459126\n",
      "Train Loss at iteration 17146: 0.04281867638384586\n",
      "Train Loss at iteration 17147: 0.04281864786543132\n",
      "Train Loss at iteration 17148: 0.04281861934934724\n",
      "Train Loss at iteration 17149: 0.04281859083559325\n",
      "Train Loss at iteration 17150: 0.042818562324168905\n",
      "Train Loss at iteration 17151: 0.04281853381507386\n",
      "Train Loss at iteration 17152: 0.0428185053083077\n",
      "Train Loss at iteration 17153: 0.042818476803870006\n",
      "Train Loss at iteration 17154: 0.042818448301760424\n",
      "Train Loss at iteration 17155: 0.04281841980197853\n",
      "Train Loss at iteration 17156: 0.04281839130452394\n",
      "Train Loss at iteration 17157: 0.04281836280939626\n",
      "Train Loss at iteration 17158: 0.04281833431659507\n",
      "Train Loss at iteration 17159: 0.04281830582612003\n",
      "Train Loss at iteration 17160: 0.04281827733797068\n",
      "Train Loss at iteration 17161: 0.04281824885214666\n",
      "Train Loss at iteration 17162: 0.04281822036864758\n",
      "Train Loss at iteration 17163: 0.04281819188747303\n",
      "Train Loss at iteration 17164: 0.042818163408622614\n",
      "Train Loss at iteration 17165: 0.04281813493209596\n",
      "Train Loss at iteration 17166: 0.04281810645789263\n",
      "Train Loss at iteration 17167: 0.042818077986012275\n",
      "Train Loss at iteration 17168: 0.04281804951645449\n",
      "Train Loss at iteration 17169: 0.042818021049218856\n",
      "Train Loss at iteration 17170: 0.042817992584305006\n",
      "Train Loss at iteration 17171: 0.04281796412171253\n",
      "Train Loss at iteration 17172: 0.04281793566144103\n",
      "Train Loss at iteration 17173: 0.04281790720349012\n",
      "Train Loss at iteration 17174: 0.042817878747859424\n",
      "Train Loss at iteration 17175: 0.04281785029454852\n",
      "Train Loss at iteration 17176: 0.04281782184355703\n",
      "Train Loss at iteration 17177: 0.04281779339488455\n",
      "Train Loss at iteration 17178: 0.04281776494853068\n",
      "Train Loss at iteration 17179: 0.042817736504495045\n",
      "Train Loss at iteration 17180: 0.04281770806277725\n",
      "Train Loss at iteration 17181: 0.04281767962337689\n",
      "Train Loss at iteration 17182: 0.04281765118629356\n",
      "Train Loss at iteration 17183: 0.04281762275152692\n",
      "Train Loss at iteration 17184: 0.04281759431907651\n",
      "Train Loss at iteration 17185: 0.042817565888941964\n",
      "Train Loss at iteration 17186: 0.042817537461122906\n",
      "Train Loss at iteration 17187: 0.04281750903561893\n",
      "Train Loss at iteration 17188: 0.042817480612429626\n",
      "Train Loss at iteration 17189: 0.042817452191554625\n",
      "Train Loss at iteration 17190: 0.04281742377299352\n",
      "Train Loss at iteration 17191: 0.04281739535674593\n",
      "Train Loss at iteration 17192: 0.04281736694281144\n",
      "Train Loss at iteration 17193: 0.04281733853118969\n",
      "Train Loss at iteration 17194: 0.04281731012188026\n",
      "Train Loss at iteration 17195: 0.04281728171488277\n",
      "Train Loss at iteration 17196: 0.042817253310196826\n",
      "Train Loss at iteration 17197: 0.04281722490782204\n",
      "Train Loss at iteration 17198: 0.042817196507758\n",
      "Train Loss at iteration 17199: 0.042817168110004335\n",
      "Train Loss at iteration 17200: 0.04281713971456065\n",
      "Train Loss at iteration 17201: 0.042817111321426536\n",
      "Train Loss at iteration 17202: 0.04281708293060162\n",
      "Train Loss at iteration 17203: 0.0428170545420855\n",
      "Train Loss at iteration 17204: 0.0428170261558778\n",
      "Train Loss at iteration 17205: 0.04281699777197811\n",
      "Train Loss at iteration 17206: 0.04281696939038604\n",
      "Train Loss at iteration 17207: 0.04281694101110121\n",
      "Train Loss at iteration 17208: 0.042816912634123204\n",
      "Train Loss at iteration 17209: 0.04281688425945166\n",
      "Train Loss at iteration 17210: 0.04281685588708617\n",
      "Train Loss at iteration 17211: 0.04281682751702635\n",
      "Train Loss at iteration 17212: 0.042816799149271806\n",
      "Train Loss at iteration 17213: 0.04281677078382214\n",
      "Train Loss at iteration 17214: 0.04281674242067696\n",
      "Train Loss at iteration 17215: 0.042816714059835885\n",
      "Train Loss at iteration 17216: 0.04281668570129853\n",
      "Train Loss at iteration 17217: 0.0428166573450645\n",
      "Train Loss at iteration 17218: 0.04281662899113338\n",
      "Train Loss at iteration 17219: 0.0428166006395048\n",
      "Train Loss at iteration 17220: 0.04281657229017838\n",
      "Train Loss at iteration 17221: 0.042816543943153706\n",
      "Train Loss at iteration 17222: 0.042816515598430395\n",
      "Train Loss at iteration 17223: 0.04281648725600807\n",
      "Train Loss at iteration 17224: 0.04281645891588632\n",
      "Train Loss at iteration 17225: 0.042816430578064765\n",
      "Train Loss at iteration 17226: 0.042816402242543024\n",
      "Train Loss at iteration 17227: 0.04281637390932068\n",
      "Train Loss at iteration 17228: 0.04281634557839738\n",
      "Train Loss at iteration 17229: 0.042816317249772695\n",
      "Train Loss at iteration 17230: 0.042816288923446265\n",
      "Train Loss at iteration 17231: 0.042816260599417684\n",
      "Train Loss at iteration 17232: 0.042816232277686564\n",
      "Train Loss at iteration 17233: 0.04281620395825252\n",
      "Train Loss at iteration 17234: 0.042816175641115166\n",
      "Train Loss at iteration 17235: 0.042816147326274104\n",
      "Train Loss at iteration 17236: 0.042816119013728934\n",
      "Train Loss at iteration 17237: 0.0428160907034793\n",
      "Train Loss at iteration 17238: 0.042816062395524775\n",
      "Train Loss at iteration 17239: 0.042816034089865\n",
      "Train Loss at iteration 17240: 0.04281600578649956\n",
      "Train Loss at iteration 17241: 0.04281597748542808\n",
      "Train Loss at iteration 17242: 0.04281594918665017\n",
      "Train Loss at iteration 17243: 0.042815920890165435\n",
      "Train Loss at iteration 17244: 0.04281589259597351\n",
      "Train Loss at iteration 17245: 0.04281586430407397\n",
      "Train Loss at iteration 17246: 0.04281583601446643\n",
      "Train Loss at iteration 17247: 0.04281580772715053\n",
      "Train Loss at iteration 17248: 0.04281577944212588\n",
      "Train Loss at iteration 17249: 0.04281575115939206\n",
      "Train Loss at iteration 17250: 0.04281572287894869\n",
      "Train Loss at iteration 17251: 0.04281569460079541\n",
      "Train Loss at iteration 17252: 0.042815666324931785\n",
      "Train Loss at iteration 17253: 0.04281563805135748\n",
      "Train Loss at iteration 17254: 0.04281560978007207\n",
      "Train Loss at iteration 17255: 0.04281558151107518\n",
      "Train Loss at iteration 17256: 0.042815553244366394\n",
      "Train Loss at iteration 17257: 0.04281552497994536\n",
      "Train Loss at iteration 17258: 0.04281549671781169\n",
      "Train Loss at iteration 17259: 0.04281546845796497\n",
      "Train Loss at iteration 17260: 0.042815440200404835\n",
      "Train Loss at iteration 17261: 0.04281541194513088\n",
      "Train Loss at iteration 17262: 0.04281538369214274\n",
      "Train Loss at iteration 17263: 0.042815355441439995\n",
      "Train Loss at iteration 17264: 0.04281532719302229\n",
      "Train Loss at iteration 17265: 0.042815298946889215\n",
      "Train Loss at iteration 17266: 0.042815270703040385\n",
      "Train Loss at iteration 17267: 0.042815242461475424\n",
      "Train Loss at iteration 17268: 0.04281521422219394\n",
      "Train Loss at iteration 17269: 0.04281518598519554\n",
      "Train Loss at iteration 17270: 0.04281515775047985\n",
      "Train Loss at iteration 17271: 0.042815129518046474\n",
      "Train Loss at iteration 17272: 0.042815101287895015\n",
      "Train Loss at iteration 17273: 0.0428150730600251\n",
      "Train Loss at iteration 17274: 0.04281504483443633\n",
      "Train Loss at iteration 17275: 0.04281501661112834\n",
      "Train Loss at iteration 17276: 0.04281498839010072\n",
      "Train Loss at iteration 17277: 0.042814960171353104\n",
      "Train Loss at iteration 17278: 0.04281493195488508\n",
      "Train Loss at iteration 17279: 0.04281490374069628\n",
      "Train Loss at iteration 17280: 0.04281487552878633\n",
      "Train Loss at iteration 17281: 0.04281484731915482\n",
      "Train Loss at iteration 17282: 0.042814819111801364\n",
      "Train Loss at iteration 17283: 0.04281479090672557\n",
      "Train Loss at iteration 17284: 0.04281476270392709\n",
      "Train Loss at iteration 17285: 0.0428147345034055\n",
      "Train Loss at iteration 17286: 0.04281470630516042\n",
      "Train Loss at iteration 17287: 0.04281467810919148\n",
      "Train Loss at iteration 17288: 0.04281464991549827\n",
      "Train Loss at iteration 17289: 0.04281462172408043\n",
      "Train Loss at iteration 17290: 0.04281459353493757\n",
      "Train Loss at iteration 17291: 0.04281456534806928\n",
      "Train Loss at iteration 17292: 0.042814537163475205\n",
      "Train Loss at iteration 17293: 0.04281450898115494\n",
      "Train Loss at iteration 17294: 0.042814480801108105\n",
      "Train Loss at iteration 17295: 0.04281445262333432\n",
      "Train Loss at iteration 17296: 0.04281442444783319\n",
      "Train Loss at iteration 17297: 0.04281439627460433\n",
      "Train Loss at iteration 17298: 0.042814368103647374\n",
      "Train Loss at iteration 17299: 0.04281433993496191\n",
      "Train Loss at iteration 17300: 0.04281431176854757\n",
      "Train Loss at iteration 17301: 0.042814283604403966\n",
      "Train Loss at iteration 17302: 0.0428142554425307\n",
      "Train Loss at iteration 17303: 0.0428142272829274\n",
      "Train Loss at iteration 17304: 0.042814199125593706\n",
      "Train Loss at iteration 17305: 0.04281417097052918\n",
      "Train Loss at iteration 17306: 0.04281414281773348\n",
      "Train Loss at iteration 17307: 0.0428141146672062\n",
      "Train Loss at iteration 17308: 0.042814086518946956\n",
      "Train Loss at iteration 17309: 0.04281405837295536\n",
      "Train Loss at iteration 17310: 0.04281403022923107\n",
      "Train Loss at iteration 17311: 0.04281400208777363\n",
      "Train Loss at iteration 17312: 0.042813973948582724\n",
      "Train Loss at iteration 17313: 0.04281394581165793\n",
      "Train Loss at iteration 17314: 0.04281391767699887\n",
      "Train Loss at iteration 17315: 0.04281388954460515\n",
      "Train Loss at iteration 17316: 0.04281386141447641\n",
      "Train Loss at iteration 17317: 0.042813833286612264\n",
      "Train Loss at iteration 17318: 0.042813805161012304\n",
      "Train Loss at iteration 17319: 0.042813777037676165\n",
      "Train Loss at iteration 17320: 0.04281374891660346\n",
      "Train Loss at iteration 17321: 0.0428137207977938\n",
      "Train Loss at iteration 17322: 0.0428136926812468\n",
      "Train Loss at iteration 17323: 0.042813664566962104\n",
      "Train Loss at iteration 17324: 0.042813636454939304\n",
      "Train Loss at iteration 17325: 0.042813608345178014\n",
      "Train Loss at iteration 17326: 0.042813580237677853\n",
      "Train Loss at iteration 17327: 0.04281355213243844\n",
      "Train Loss at iteration 17328: 0.042813524029459406\n",
      "Train Loss at iteration 17329: 0.04281349592874035\n",
      "Train Loss at iteration 17330: 0.042813467830280894\n",
      "Train Loss at iteration 17331: 0.042813439734080645\n",
      "Train Loss at iteration 17332: 0.04281341164013924\n",
      "Train Loss at iteration 17333: 0.042813383548456305\n",
      "Train Loss at iteration 17334: 0.042813355459031414\n",
      "Train Loss at iteration 17335: 0.04281332737186424\n",
      "Train Loss at iteration 17336: 0.04281329928695434\n",
      "Train Loss at iteration 17337: 0.042813271204301374\n",
      "Train Loss at iteration 17338: 0.04281324312390496\n",
      "Train Loss at iteration 17339: 0.04281321504576468\n",
      "Train Loss at iteration 17340: 0.042813186969880186\n",
      "Train Loss at iteration 17341: 0.04281315889625109\n",
      "Train Loss at iteration 17342: 0.042813130824877005\n",
      "Train Loss at iteration 17343: 0.04281310275575754\n",
      "Train Loss at iteration 17344: 0.042813074688892325\n",
      "Train Loss at iteration 17345: 0.04281304662428099\n",
      "Train Loss at iteration 17346: 0.04281301856192312\n",
      "Train Loss at iteration 17347: 0.04281299050181835\n",
      "Train Loss at iteration 17348: 0.04281296244396632\n",
      "Train Loss at iteration 17349: 0.04281293438836662\n",
      "Train Loss at iteration 17350: 0.04281290633501887\n",
      "Train Loss at iteration 17351: 0.04281287828392271\n",
      "Train Loss at iteration 17352: 0.04281285023507773\n",
      "Train Loss at iteration 17353: 0.042812822188483576\n",
      "Train Loss at iteration 17354: 0.04281279414413983\n",
      "Train Loss at iteration 17355: 0.04281276610204616\n",
      "Train Loss at iteration 17356: 0.04281273806220215\n",
      "Train Loss at iteration 17357: 0.04281271002460743\n",
      "Train Loss at iteration 17358: 0.042812681989261615\n",
      "Train Loss at iteration 17359: 0.04281265395616434\n",
      "Train Loss at iteration 17360: 0.04281262592531519\n",
      "Train Loss at iteration 17361: 0.042812597896713815\n",
      "Train Loss at iteration 17362: 0.04281256987035983\n",
      "Train Loss at iteration 17363: 0.04281254184625283\n",
      "Train Loss at iteration 17364: 0.042812513824392474\n",
      "Train Loss at iteration 17365: 0.04281248580477833\n",
      "Train Loss at iteration 17366: 0.04281245778741008\n",
      "Train Loss at iteration 17367: 0.042812429772287296\n",
      "Train Loss at iteration 17368: 0.042812401759409625\n",
      "Train Loss at iteration 17369: 0.042812373748776666\n",
      "Train Loss at iteration 17370: 0.04281234574038806\n",
      "Train Loss at iteration 17371: 0.0428123177342434\n",
      "Train Loss at iteration 17372: 0.042812289730342325\n",
      "Train Loss at iteration 17373: 0.04281226172868445\n",
      "Train Loss at iteration 17374: 0.042812233729269414\n",
      "Train Loss at iteration 17375: 0.0428122057320968\n",
      "Train Loss at iteration 17376: 0.042812177737166245\n",
      "Train Loss at iteration 17377: 0.04281214974447738\n",
      "Train Loss at iteration 17378: 0.04281212175402982\n",
      "Train Loss at iteration 17379: 0.04281209376582318\n",
      "Train Loss at iteration 17380: 0.04281206577985709\n",
      "Train Loss at iteration 17381: 0.042812037796131165\n",
      "Train Loss at iteration 17382: 0.042812009814645016\n",
      "Train Loss at iteration 17383: 0.042811981835398284\n",
      "Train Loss at iteration 17384: 0.04281195385839056\n",
      "Train Loss at iteration 17385: 0.0428119258836215\n",
      "Train Loss at iteration 17386: 0.0428118979110907\n",
      "Train Loss at iteration 17387: 0.04281186994079779\n",
      "Train Loss at iteration 17388: 0.0428118419727424\n",
      "Train Loss at iteration 17389: 0.042811814006924125\n",
      "Train Loss at iteration 17390: 0.04281178604334262\n",
      "Train Loss at iteration 17391: 0.042811758081997484\n",
      "Train Loss at iteration 17392: 0.04281173012288834\n",
      "Train Loss at iteration 17393: 0.04281170216601482\n",
      "Train Loss at iteration 17394: 0.042811674211376535\n",
      "Train Loss at iteration 17395: 0.042811646258973116\n",
      "Train Loss at iteration 17396: 0.042811618308804165\n",
      "Train Loss at iteration 17397: 0.04281159036086933\n",
      "Train Loss at iteration 17398: 0.04281156241516821\n",
      "Train Loss at iteration 17399: 0.042811534471700455\n",
      "Train Loss at iteration 17400: 0.04281150653046565\n",
      "Train Loss at iteration 17401: 0.042811478591463455\n",
      "Train Loss at iteration 17402: 0.04281145065469345\n",
      "Train Loss at iteration 17403: 0.0428114227201553\n",
      "Train Loss at iteration 17404: 0.04281139478784861\n",
      "Train Loss at iteration 17405: 0.04281136685777299\n",
      "Train Loss at iteration 17406: 0.042811338929928076\n",
      "Train Loss at iteration 17407: 0.042811311004313485\n",
      "Train Loss at iteration 17408: 0.04281128308092884\n",
      "Train Loss at iteration 17409: 0.04281125515977377\n",
      "Train Loss at iteration 17410: 0.042811227240847884\n",
      "Train Loss at iteration 17411: 0.042811199324150825\n",
      "Train Loss at iteration 17412: 0.042811171409682186\n",
      "Train Loss at iteration 17413: 0.04281114349744163\n",
      "Train Loss at iteration 17414: 0.04281111558742875\n",
      "Train Loss at iteration 17415: 0.042811087679643166\n",
      "Train Loss at iteration 17416: 0.04281105977408453\n",
      "Train Loss at iteration 17417: 0.042811031870752435\n",
      "Train Loss at iteration 17418: 0.04281100396964652\n",
      "Train Loss at iteration 17419: 0.0428109760707664\n",
      "Train Loss at iteration 17420: 0.04281094817411172\n",
      "Train Loss at iteration 17421: 0.04281092027968206\n",
      "Train Loss at iteration 17422: 0.04281089238747708\n",
      "Train Loss at iteration 17423: 0.0428108644974964\n",
      "Train Loss at iteration 17424: 0.04281083660973963\n",
      "Train Loss at iteration 17425: 0.04281080872420641\n",
      "Train Loss at iteration 17426: 0.04281078084089633\n",
      "Train Loss at iteration 17427: 0.04281075295980906\n",
      "Train Loss at iteration 17428: 0.0428107250809442\n",
      "Train Loss at iteration 17429: 0.042810697204301376\n",
      "Train Loss at iteration 17430: 0.042810669329880195\n",
      "Train Loss at iteration 17431: 0.04281064145768032\n",
      "Train Loss at iteration 17432: 0.042810613587701336\n",
      "Train Loss at iteration 17433: 0.04281058571994291\n",
      "Train Loss at iteration 17434: 0.04281055785440461\n",
      "Train Loss at iteration 17435: 0.042810529991086095\n",
      "Train Loss at iteration 17436: 0.042810502129987\n",
      "Train Loss at iteration 17437: 0.042810474271106916\n",
      "Train Loss at iteration 17438: 0.04281044641444551\n",
      "Train Loss at iteration 17439: 0.04281041856000236\n",
      "Train Loss at iteration 17440: 0.04281039070777711\n",
      "Train Loss at iteration 17441: 0.042810362857769416\n",
      "Train Loss at iteration 17442: 0.042810335009978856\n",
      "Train Loss at iteration 17443: 0.042810307164405086\n",
      "Train Loss at iteration 17444: 0.04281027932104771\n",
      "Train Loss at iteration 17445: 0.04281025147990636\n",
      "Train Loss at iteration 17446: 0.042810223640980666\n",
      "Train Loss at iteration 17447: 0.04281019580427025\n",
      "Train Loss at iteration 17448: 0.042810167969774736\n",
      "Train Loss at iteration 17449: 0.042810140137493756\n",
      "Train Loss at iteration 17450: 0.04281011230742692\n",
      "Train Loss at iteration 17451: 0.04281008447957387\n",
      "Train Loss at iteration 17452: 0.042810056653934225\n",
      "Train Loss at iteration 17453: 0.04281002883050761\n",
      "Train Loss at iteration 17454: 0.04281000100929365\n",
      "Train Loss at iteration 17455: 0.04280997319029197\n",
      "Train Loss at iteration 17456: 0.0428099453735022\n",
      "Train Loss at iteration 17457: 0.04280991755892397\n",
      "Train Loss at iteration 17458: 0.042809889746556896\n",
      "Train Loss at iteration 17459: 0.042809861936400614\n",
      "Train Loss at iteration 17460: 0.042809834128454724\n",
      "Train Loss at iteration 17461: 0.04280980632271888\n",
      "Train Loss at iteration 17462: 0.042809778519192696\n",
      "Train Loss at iteration 17463: 0.042809750717875815\n",
      "Train Loss at iteration 17464: 0.04280972291876784\n",
      "Train Loss at iteration 17465: 0.04280969512186841\n",
      "Train Loss at iteration 17466: 0.04280966732717715\n",
      "Train Loss at iteration 17467: 0.04280963953469369\n",
      "Train Loss at iteration 17468: 0.04280961174441764\n",
      "Train Loss at iteration 17469: 0.04280958395634865\n",
      "Train Loss at iteration 17470: 0.04280955617048634\n",
      "Train Loss at iteration 17471: 0.042809528386830324\n",
      "Train Loss at iteration 17472: 0.04280950060538024\n",
      "Train Loss at iteration 17473: 0.04280947282613571\n",
      "Train Loss at iteration 17474: 0.04280944504909637\n",
      "Train Loss at iteration 17475: 0.04280941727426183\n",
      "Train Loss at iteration 17476: 0.042809389501631726\n",
      "Train Loss at iteration 17477: 0.0428093617312057\n",
      "Train Loss at iteration 17478: 0.04280933396298336\n",
      "Train Loss at iteration 17479: 0.04280930619696433\n",
      "Train Loss at iteration 17480: 0.04280927843314825\n",
      "Train Loss at iteration 17481: 0.04280925067153476\n",
      "Train Loss at iteration 17482: 0.04280922291212345\n",
      "Train Loss at iteration 17483: 0.042809195154914\n",
      "Train Loss at iteration 17484: 0.04280916739990598\n",
      "Train Loss at iteration 17485: 0.04280913964709904\n",
      "Train Loss at iteration 17486: 0.042809111896492814\n",
      "Train Loss at iteration 17487: 0.04280908414808696\n",
      "Train Loss at iteration 17488: 0.04280905640188105\n",
      "Train Loss at iteration 17489: 0.04280902865787473\n",
      "Train Loss at iteration 17490: 0.04280900091606765\n",
      "Train Loss at iteration 17491: 0.04280897317645942\n",
      "Train Loss at iteration 17492: 0.04280894543904965\n",
      "Train Loss at iteration 17493: 0.04280891770383801\n",
      "Train Loss at iteration 17494: 0.042808889970824104\n",
      "Train Loss at iteration 17495: 0.04280886224000756\n",
      "Train Loss at iteration 17496: 0.04280883451138802\n",
      "Train Loss at iteration 17497: 0.04280880678496509\n",
      "Train Loss at iteration 17498: 0.042808779060738426\n",
      "Train Loss at iteration 17499: 0.04280875133870763\n",
      "Train Loss at iteration 17500: 0.04280872361887234\n",
      "Train Loss at iteration 17501: 0.0428086959012322\n",
      "Train Loss at iteration 17502: 0.04280866818578681\n",
      "Train Loss at iteration 17503: 0.04280864047253583\n",
      "Train Loss at iteration 17504: 0.042808612761478876\n",
      "Train Loss at iteration 17505: 0.04280858505261557\n",
      "Train Loss at iteration 17506: 0.042808557345945554\n",
      "Train Loss at iteration 17507: 0.04280852964146844\n",
      "Train Loss at iteration 17508: 0.042808501939183875\n",
      "Train Loss at iteration 17509: 0.042808474239091475\n",
      "Train Loss at iteration 17510: 0.04280844654119088\n",
      "Train Loss at iteration 17511: 0.042808418845481706\n",
      "Train Loss at iteration 17512: 0.04280839115196359\n",
      "Train Loss at iteration 17513: 0.04280836346063617\n",
      "Train Loss at iteration 17514: 0.04280833577149906\n",
      "Train Loss at iteration 17515: 0.04280830808455192\n",
      "Train Loss at iteration 17516: 0.04280828039979434\n",
      "Train Loss at iteration 17517: 0.04280825271722598\n",
      "Train Loss at iteration 17518: 0.04280822503684645\n",
      "Train Loss at iteration 17519: 0.04280819735865537\n",
      "Train Loss at iteration 17520: 0.04280816968265242\n",
      "Train Loss at iteration 17521: 0.042808142008837174\n",
      "Train Loss at iteration 17522: 0.042808114337209296\n",
      "Train Loss at iteration 17523: 0.04280808666776841\n",
      "Train Loss at iteration 17524: 0.04280805900051414\n",
      "Train Loss at iteration 17525: 0.04280803133544612\n",
      "Train Loss at iteration 17526: 0.042808003672563974\n",
      "Train Loss at iteration 17527: 0.042807976011867345\n",
      "Train Loss at iteration 17528: 0.04280794835335585\n",
      "Train Loss at iteration 17529: 0.04280792069702914\n",
      "Train Loss at iteration 17530: 0.04280789304288683\n",
      "Train Loss at iteration 17531: 0.042807865390928554\n",
      "Train Loss at iteration 17532: 0.04280783774115394\n",
      "Train Loss at iteration 17533: 0.04280781009356262\n",
      "Train Loss at iteration 17534: 0.04280778244815421\n",
      "Train Loss at iteration 17535: 0.042807754804928384\n",
      "Train Loss at iteration 17536: 0.04280772716388473\n",
      "Train Loss at iteration 17537: 0.042807699525022905\n",
      "Train Loss at iteration 17538: 0.04280767188834254\n",
      "Train Loss at iteration 17539: 0.04280764425384324\n",
      "Train Loss at iteration 17540: 0.04280761662152465\n",
      "Train Loss at iteration 17541: 0.04280758899138641\n",
      "Train Loss at iteration 17542: 0.042807561363428163\n",
      "Train Loss at iteration 17543: 0.04280753373764951\n",
      "Train Loss at iteration 17544: 0.0428075061140501\n",
      "Train Loss at iteration 17545: 0.042807478492629565\n",
      "Train Loss at iteration 17546: 0.04280745087338754\n",
      "Train Loss at iteration 17547: 0.04280742325632365\n",
      "Train Loss at iteration 17548: 0.04280739564143751\n",
      "Train Loss at iteration 17549: 0.042807368028728775\n",
      "Train Loss at iteration 17550: 0.04280734041819709\n",
      "Train Loss at iteration 17551: 0.04280731280984205\n",
      "Train Loss at iteration 17552: 0.0428072852036633\n",
      "Train Loss at iteration 17553: 0.0428072575996605\n",
      "Train Loss at iteration 17554: 0.04280722999783326\n",
      "Train Loss at iteration 17555: 0.042807202398181206\n",
      "Train Loss at iteration 17556: 0.04280717480070398\n",
      "Train Loss at iteration 17557: 0.04280714720540121\n",
      "Train Loss at iteration 17558: 0.04280711961227253\n",
      "Train Loss at iteration 17559: 0.04280709202131757\n",
      "Train Loss at iteration 17560: 0.04280706443253598\n",
      "Train Loss at iteration 17561: 0.042807036845927376\n",
      "Train Loss at iteration 17562: 0.04280700926149139\n",
      "Train Loss at iteration 17563: 0.042806981679227654\n",
      "Train Loss at iteration 17564: 0.04280695409913581\n",
      "Train Loss at iteration 17565: 0.04280692652121548\n",
      "Train Loss at iteration 17566: 0.042806898945466326\n",
      "Train Loss at iteration 17567: 0.042806871371887954\n",
      "Train Loss at iteration 17568: 0.04280684380047999\n",
      "Train Loss at iteration 17569: 0.042806816231242086\n",
      "Train Loss at iteration 17570: 0.042806788664173874\n",
      "Train Loss at iteration 17571: 0.042806761099274976\n",
      "Train Loss at iteration 17572: 0.04280673353654504\n",
      "Train Loss at iteration 17573: 0.04280670597598368\n",
      "Train Loss at iteration 17574: 0.04280667841759056\n",
      "Train Loss at iteration 17575: 0.04280665086136528\n",
      "Train Loss at iteration 17576: 0.04280662330730751\n",
      "Train Loss at iteration 17577: 0.042806595755416836\n",
      "Train Loss at iteration 17578: 0.04280656820569295\n",
      "Train Loss at iteration 17579: 0.04280654065813543\n",
      "Train Loss at iteration 17580: 0.04280651311274395\n",
      "Train Loss at iteration 17581: 0.042806485569518125\n",
      "Train Loss at iteration 17582: 0.04280645802845761\n",
      "Train Loss at iteration 17583: 0.042806430489562\n",
      "Train Loss at iteration 17584: 0.04280640295283095\n",
      "Train Loss at iteration 17585: 0.042806375418264105\n",
      "Train Loss at iteration 17586: 0.04280634788586111\n",
      "Train Loss at iteration 17587: 0.042806320355621555\n",
      "Train Loss at iteration 17588: 0.04280629282754512\n",
      "Train Loss at iteration 17589: 0.04280626530163141\n",
      "Train Loss at iteration 17590: 0.04280623777788007\n",
      "Train Loss at iteration 17591: 0.04280621025629075\n",
      "Train Loss at iteration 17592: 0.042806182736863055\n",
      "Train Loss at iteration 17593: 0.04280615521959663\n",
      "Train Loss at iteration 17594: 0.04280612770449114\n",
      "Train Loss at iteration 17595: 0.04280610019154617\n",
      "Train Loss at iteration 17596: 0.042806072680761385\n",
      "Train Loss at iteration 17597: 0.04280604517213641\n",
      "Train Loss at iteration 17598: 0.0428060176656709\n",
      "Train Loss at iteration 17599: 0.04280599016136447\n",
      "Train Loss at iteration 17600: 0.04280596265921676\n",
      "Train Loss at iteration 17601: 0.04280593515922741\n",
      "Train Loss at iteration 17602: 0.042805907661396046\n",
      "Train Loss at iteration 17603: 0.042805880165722315\n",
      "Train Loss at iteration 17604: 0.04280585267220584\n",
      "Train Loss at iteration 17605: 0.042805825180846274\n",
      "Train Loss at iteration 17606: 0.04280579769164324\n",
      "Train Loss at iteration 17607: 0.04280577020459638\n",
      "Train Loss at iteration 17608: 0.04280574271970533\n",
      "Train Loss at iteration 17609: 0.04280571523696971\n",
      "Train Loss at iteration 17610: 0.04280568775638917\n",
      "Train Loss at iteration 17611: 0.04280566027796335\n",
      "Train Loss at iteration 17612: 0.04280563280169188\n",
      "Train Loss at iteration 17613: 0.04280560532757441\n",
      "Train Loss at iteration 17614: 0.042805577855610545\n",
      "Train Loss at iteration 17615: 0.04280555038579996\n",
      "Train Loss at iteration 17616: 0.04280552291814225\n",
      "Train Loss at iteration 17617: 0.04280549545263709\n",
      "Train Loss at iteration 17618: 0.04280546798928409\n",
      "Train Loss at iteration 17619: 0.04280544052808289\n",
      "Train Loss at iteration 17620: 0.04280541306903314\n",
      "Train Loss at iteration 17621: 0.04280538561213448\n",
      "Train Loss at iteration 17622: 0.04280535815738652\n",
      "Train Loss at iteration 17623: 0.042805330704788924\n",
      "Train Loss at iteration 17624: 0.04280530325434133\n",
      "Train Loss at iteration 17625: 0.04280527580604335\n",
      "Train Loss at iteration 17626: 0.042805248359894625\n",
      "Train Loss at iteration 17627: 0.04280522091589481\n",
      "Train Loss at iteration 17628: 0.04280519347404354\n",
      "Train Loss at iteration 17629: 0.042805166034340446\n",
      "Train Loss at iteration 17630: 0.04280513859678516\n",
      "Train Loss at iteration 17631: 0.04280511116137734\n",
      "Train Loss at iteration 17632: 0.042805083728116596\n",
      "Train Loss at iteration 17633: 0.042805056297002576\n",
      "Train Loss at iteration 17634: 0.042805028868034924\n",
      "Train Loss at iteration 17635: 0.04280500144121328\n",
      "Train Loss at iteration 17636: 0.04280497401653726\n",
      "Train Loss at iteration 17637: 0.042804946594006535\n",
      "Train Loss at iteration 17638: 0.04280491917362072\n",
      "Train Loss at iteration 17639: 0.04280489175537946\n",
      "Train Loss at iteration 17640: 0.04280486433928237\n",
      "Train Loss at iteration 17641: 0.04280483692532913\n",
      "Train Loss at iteration 17642: 0.04280480951351935\n",
      "Train Loss at iteration 17643: 0.04280478210385268\n",
      "Train Loss at iteration 17644: 0.04280475469632875\n",
      "Train Loss at iteration 17645: 0.042804727290947205\n",
      "Train Loss at iteration 17646: 0.04280469988770768\n",
      "Train Loss at iteration 17647: 0.04280467248660983\n",
      "Train Loss at iteration 17648: 0.042804645087653255\n",
      "Train Loss at iteration 17649: 0.04280461769083761\n",
      "Train Loss at iteration 17650: 0.042804590296162556\n",
      "Train Loss at iteration 17651: 0.04280456290362772\n",
      "Train Loss at iteration 17652: 0.04280453551323273\n",
      "Train Loss at iteration 17653: 0.04280450812497723\n",
      "Train Loss at iteration 17654: 0.04280448073886086\n",
      "Train Loss at iteration 17655: 0.04280445335488326\n",
      "Train Loss at iteration 17656: 0.042804425973044076\n",
      "Train Loss at iteration 17657: 0.04280439859334292\n",
      "Train Loss at iteration 17658: 0.04280437121577947\n",
      "Train Loss at iteration 17659: 0.04280434384035335\n",
      "Train Loss at iteration 17660: 0.042804316467064174\n",
      "Train Loss at iteration 17661: 0.042804289095911614\n",
      "Train Loss at iteration 17662: 0.04280426172689529\n",
      "Train Loss at iteration 17663: 0.042804234360014855\n",
      "Train Loss at iteration 17664: 0.042804206995269954\n",
      "Train Loss at iteration 17665: 0.04280417963266021\n",
      "Train Loss at iteration 17666: 0.04280415227218525\n",
      "Train Loss at iteration 17667: 0.042804124913844756\n",
      "Train Loss at iteration 17668: 0.042804097557638345\n",
      "Train Loss at iteration 17669: 0.04280407020356564\n",
      "Train Loss at iteration 17670: 0.042804042851626305\n",
      "Train Loss at iteration 17671: 0.04280401550181997\n",
      "Train Loss at iteration 17672: 0.04280398815414628\n",
      "Train Loss at iteration 17673: 0.04280396080860487\n",
      "Train Loss at iteration 17674: 0.04280393346519538\n",
      "Train Loss at iteration 17675: 0.04280390612391746\n",
      "Train Loss at iteration 17676: 0.04280387878477073\n",
      "Train Loss at iteration 17677: 0.04280385144775486\n",
      "Train Loss at iteration 17678: 0.04280382411286947\n",
      "Train Loss at iteration 17679: 0.042803796780114194\n",
      "Train Loss at iteration 17680: 0.04280376944948869\n",
      "Train Loss at iteration 17681: 0.04280374212099259\n",
      "Train Loss at iteration 17682: 0.042803714794625546\n",
      "Train Loss at iteration 17683: 0.04280368747038718\n",
      "Train Loss at iteration 17684: 0.042803660148277155\n",
      "Train Loss at iteration 17685: 0.04280363282829508\n",
      "Train Loss at iteration 17686: 0.04280360551044063\n",
      "Train Loss at iteration 17687: 0.04280357819471342\n",
      "Train Loss at iteration 17688: 0.042803550881113116\n",
      "Train Loss at iteration 17689: 0.04280352356963934\n",
      "Train Loss at iteration 17690: 0.042803496260291736\n",
      "Train Loss at iteration 17691: 0.04280346895306994\n",
      "Train Loss at iteration 17692: 0.042803441647973624\n",
      "Train Loss at iteration 17693: 0.04280341434500239\n",
      "Train Loss at iteration 17694: 0.0428033870441559\n",
      "Train Loss at iteration 17695: 0.04280335974543379\n",
      "Train Loss at iteration 17696: 0.04280333244883572\n",
      "Train Loss at iteration 17697: 0.04280330515436129\n",
      "Train Loss at iteration 17698: 0.04280327786201017\n",
      "Train Loss at iteration 17699: 0.04280325057178201\n",
      "Train Loss at iteration 17700: 0.042803223283676445\n",
      "Train Loss at iteration 17701: 0.04280319599769311\n",
      "Train Loss at iteration 17702: 0.04280316871383165\n",
      "Train Loss at iteration 17703: 0.04280314143209171\n",
      "Train Loss at iteration 17704: 0.042803114152472926\n",
      "Train Loss at iteration 17705: 0.042803086874974944\n",
      "Train Loss at iteration 17706: 0.042803059599597404\n",
      "Train Loss at iteration 17707: 0.04280303232633995\n",
      "Train Loss at iteration 17708: 0.04280300505520223\n",
      "Train Loss at iteration 17709: 0.042802977786183874\n",
      "Train Loss at iteration 17710: 0.04280295051928453\n",
      "Train Loss at iteration 17711: 0.04280292325450385\n",
      "Train Loss at iteration 17712: 0.04280289599184146\n",
      "Train Loss at iteration 17713: 0.042802868731297024\n",
      "Train Loss at iteration 17714: 0.04280284147287016\n",
      "Train Loss at iteration 17715: 0.042802814216560535\n",
      "Train Loss at iteration 17716: 0.042802786962367764\n",
      "Train Loss at iteration 17717: 0.04280275971029152\n",
      "Train Loss at iteration 17718: 0.04280273246033143\n",
      "Train Loss at iteration 17719: 0.04280270521248715\n",
      "Train Loss at iteration 17720: 0.042802677966758296\n",
      "Train Loss at iteration 17721: 0.04280265072314453\n",
      "Train Loss at iteration 17722: 0.04280262348164549\n",
      "Train Loss at iteration 17723: 0.04280259624226084\n",
      "Train Loss at iteration 17724: 0.04280256900499019\n",
      "Train Loss at iteration 17725: 0.04280254176983322\n",
      "Train Loss at iteration 17726: 0.04280251453678953\n",
      "Train Loss at iteration 17727: 0.04280248730585879\n",
      "Train Loss at iteration 17728: 0.042802460077040645\n",
      "Train Loss at iteration 17729: 0.042802432850334735\n",
      "Train Loss at iteration 17730: 0.0428024056257407\n",
      "Train Loss at iteration 17731: 0.042802378403258186\n",
      "Train Loss at iteration 17732: 0.04280235118288684\n",
      "Train Loss at iteration 17733: 0.042802323964626306\n",
      "Train Loss at iteration 17734: 0.04280229674847623\n",
      "Train Loss at iteration 17735: 0.04280226953443625\n",
      "Train Loss at iteration 17736: 0.042802242322506\n",
      "Train Loss at iteration 17737: 0.04280221511268516\n",
      "Train Loss at iteration 17738: 0.04280218790497333\n",
      "Train Loss at iteration 17739: 0.042802160699370186\n",
      "Train Loss at iteration 17740: 0.04280213349587535\n",
      "Train Loss at iteration 17741: 0.04280210629448848\n",
      "Train Loss at iteration 17742: 0.04280207909520923\n",
      "Train Loss at iteration 17743: 0.042802051898037236\n",
      "Train Loss at iteration 17744: 0.042802024702972126\n",
      "Train Loss at iteration 17745: 0.04280199751001357\n",
      "Train Loss at iteration 17746: 0.04280197031916119\n",
      "Train Loss at iteration 17747: 0.04280194313041465\n",
      "Train Loss at iteration 17748: 0.04280191594377359\n",
      "Train Loss at iteration 17749: 0.04280188875923765\n",
      "Train Loss at iteration 17750: 0.04280186157680648\n",
      "Train Loss at iteration 17751: 0.04280183439647973\n",
      "Train Loss at iteration 17752: 0.04280180721825703\n",
      "Train Loss at iteration 17753: 0.04280178004213803\n",
      "Train Loss at iteration 17754: 0.042801752868122385\n",
      "Train Loss at iteration 17755: 0.042801725696209714\n",
      "Train Loss at iteration 17756: 0.042801698526399715\n",
      "Train Loss at iteration 17757: 0.042801671358691985\n",
      "Train Loss at iteration 17758: 0.04280164419308619\n",
      "Train Loss at iteration 17759: 0.04280161702958197\n",
      "Train Loss at iteration 17760: 0.04280158986817897\n",
      "Train Loss at iteration 17761: 0.042801562708876834\n",
      "Train Loss at iteration 17762: 0.042801535551675235\n",
      "Train Loss at iteration 17763: 0.04280150839657377\n",
      "Train Loss at iteration 17764: 0.042801481243572116\n",
      "Train Loss at iteration 17765: 0.04280145409266992\n",
      "Train Loss at iteration 17766: 0.04280142694386683\n",
      "Train Loss at iteration 17767: 0.04280139979716248\n",
      "Train Loss at iteration 17768: 0.042801372652556506\n",
      "Train Loss at iteration 17769: 0.04280134551004857\n",
      "Train Loss at iteration 17770: 0.04280131836963833\n",
      "Train Loss at iteration 17771: 0.04280129123132542\n",
      "Train Loss at iteration 17772: 0.042801264095109474\n",
      "Train Loss at iteration 17773: 0.04280123696099016\n",
      "Train Loss at iteration 17774: 0.0428012098289671\n",
      "Train Loss at iteration 17775: 0.042801182699039986\n",
      "Train Loss at iteration 17776: 0.0428011555712084\n",
      "Train Loss at iteration 17777: 0.042801128445472056\n",
      "Train Loss at iteration 17778: 0.042801101321830556\n",
      "Train Loss at iteration 17779: 0.04280107420028354\n",
      "Train Loss at iteration 17780: 0.042801047080830705\n",
      "Train Loss at iteration 17781: 0.04280101996347165\n",
      "Train Loss at iteration 17782: 0.04280099284820605\n",
      "Train Loss at iteration 17783: 0.04280096573503353\n",
      "Train Loss at iteration 17784: 0.04280093862395376\n",
      "Train Loss at iteration 17785: 0.04280091151496636\n",
      "Train Loss at iteration 17786: 0.04280088440807101\n",
      "Train Loss at iteration 17787: 0.042800857303267326\n",
      "Train Loss at iteration 17788: 0.04280083020055497\n",
      "Train Loss at iteration 17789: 0.04280080309993361\n",
      "Train Loss at iteration 17790: 0.04280077600140287\n",
      "Train Loss at iteration 17791: 0.04280074890496239\n",
      "Train Loss at iteration 17792: 0.042800721810611835\n",
      "Train Loss at iteration 17793: 0.042800694718350844\n",
      "Train Loss at iteration 17794: 0.042800667628179065\n",
      "Train Loss at iteration 17795: 0.042800640540096156\n",
      "Train Loss at iteration 17796: 0.04280061345410175\n",
      "Train Loss at iteration 17797: 0.042800586370195515\n",
      "Train Loss at iteration 17798: 0.04280055928837709\n",
      "Train Loss at iteration 17799: 0.0428005322086461\n",
      "Train Loss at iteration 17800: 0.04280050513100224\n",
      "Train Loss at iteration 17801: 0.04280047805544511\n",
      "Train Loss at iteration 17802: 0.042800450981974374\n",
      "Train Loss at iteration 17803: 0.042800423910589705\n",
      "Train Loss at iteration 17804: 0.04280039684129073\n",
      "Train Loss at iteration 17805: 0.04280036977407709\n",
      "Train Loss at iteration 17806: 0.04280034270894846\n",
      "Train Loss at iteration 17807: 0.04280031564590446\n",
      "Train Loss at iteration 17808: 0.04280028858494476\n",
      "Train Loss at iteration 17809: 0.04280026152606899\n",
      "Train Loss at iteration 17810: 0.04280023446927683\n",
      "Train Loss at iteration 17811: 0.04280020741456789\n",
      "Train Loss at iteration 17812: 0.04280018036194184\n",
      "Train Loss at iteration 17813: 0.04280015331139833\n",
      "Train Loss at iteration 17814: 0.04280012626293701\n",
      "Train Loss at iteration 17815: 0.04280009921655753\n",
      "Train Loss at iteration 17816: 0.042800072172259514\n",
      "Train Loss at iteration 17817: 0.04280004513004265\n",
      "Train Loss at iteration 17818: 0.042800018089906564\n",
      "Train Loss at iteration 17819: 0.04279999105185089\n",
      "Train Loss at iteration 17820: 0.042799964015875325\n",
      "Train Loss at iteration 17821: 0.04279993698197949\n",
      "Train Loss at iteration 17822: 0.042799909950163015\n",
      "Train Loss at iteration 17823: 0.04279988292042559\n",
      "Train Loss at iteration 17824: 0.04279985589276684\n",
      "Train Loss at iteration 17825: 0.04279982886718642\n",
      "Train Loss at iteration 17826: 0.042799801843683975\n",
      "Train Loss at iteration 17827: 0.04279977482225916\n",
      "Train Loss at iteration 17828: 0.04279974780291164\n",
      "Train Loss at iteration 17829: 0.04279972078564104\n",
      "Train Loss at iteration 17830: 0.04279969377044702\n",
      "Train Loss at iteration 17831: 0.042799666757329234\n",
      "Train Loss at iteration 17832: 0.042799639746287324\n",
      "Train Loss at iteration 17833: 0.04279961273732094\n",
      "Train Loss at iteration 17834: 0.04279958573042975\n",
      "Train Loss at iteration 17835: 0.04279955872561339\n",
      "Train Loss at iteration 17836: 0.0427995317228715\n",
      "Train Loss at iteration 17837: 0.042799504722203764\n",
      "Train Loss at iteration 17838: 0.042799477723609806\n",
      "Train Loss at iteration 17839: 0.04279945072708927\n",
      "Train Loss at iteration 17840: 0.04279942373264183\n",
      "Train Loss at iteration 17841: 0.04279939674026712\n",
      "Train Loss at iteration 17842: 0.042799369749964795\n",
      "Train Loss at iteration 17843: 0.04279934276173451\n",
      "Train Loss at iteration 17844: 0.04279931577557593\n",
      "Train Loss at iteration 17845: 0.04279928879148868\n",
      "Train Loss at iteration 17846: 0.042799261809472404\n",
      "Train Loss at iteration 17847: 0.04279923482952679\n",
      "Train Loss at iteration 17848: 0.042799207851651457\n",
      "Train Loss at iteration 17849: 0.042799180875846075\n",
      "Train Loss at iteration 17850: 0.04279915390211029\n",
      "Train Loss at iteration 17851: 0.04279912693044375\n",
      "Train Loss at iteration 17852: 0.04279909996084611\n",
      "Train Loss at iteration 17853: 0.042799072993317026\n",
      "Train Loss at iteration 17854: 0.04279904602785615\n",
      "Train Loss at iteration 17855: 0.04279901906446311\n",
      "Train Loss at iteration 17856: 0.04279899210313759\n",
      "Train Loss at iteration 17857: 0.042798965143879215\n",
      "Train Loss at iteration 17858: 0.042798938186687646\n",
      "Train Loss at iteration 17859: 0.042798911231562556\n",
      "Train Loss at iteration 17860: 0.04279888427850358\n",
      "Train Loss at iteration 17861: 0.04279885732751035\n",
      "Train Loss at iteration 17862: 0.04279883037858255\n",
      "Train Loss at iteration 17863: 0.04279880343171983\n",
      "Train Loss at iteration 17864: 0.04279877648692182\n",
      "Train Loss at iteration 17865: 0.04279874954418818\n",
      "Train Loss at iteration 17866: 0.04279872260351858\n",
      "Train Loss at iteration 17867: 0.04279869566491265\n",
      "Train Loss at iteration 17868: 0.04279866872837006\n",
      "Train Loss at iteration 17869: 0.042798641793890445\n",
      "Train Loss at iteration 17870: 0.04279861486147346\n",
      "Train Loss at iteration 17871: 0.04279858793111877\n",
      "Train Loss at iteration 17872: 0.04279856100282604\n",
      "Train Loss at iteration 17873: 0.04279853407659489\n",
      "Train Loss at iteration 17874: 0.04279850715242499\n",
      "Train Loss at iteration 17875: 0.04279848023031599\n",
      "Train Loss at iteration 17876: 0.04279845331026754\n",
      "Train Loss at iteration 17877: 0.04279842639227931\n",
      "Train Loss at iteration 17878: 0.042798399476350925\n",
      "Train Loss at iteration 17879: 0.04279837256248207\n",
      "Train Loss at iteration 17880: 0.04279834565067237\n",
      "Train Loss at iteration 17881: 0.042798318740921486\n",
      "Train Loss at iteration 17882: 0.042798291833229085\n",
      "Train Loss at iteration 17883: 0.0427982649275948\n",
      "Train Loss at iteration 17884: 0.042798238024018304\n",
      "Train Loss at iteration 17885: 0.04279821112249924\n",
      "Train Loss at iteration 17886: 0.04279818422303727\n",
      "Train Loss at iteration 17887: 0.04279815732563204\n",
      "Train Loss at iteration 17888: 0.042798130430283195\n",
      "Train Loss at iteration 17889: 0.042798103536990394\n",
      "Train Loss at iteration 17890: 0.0427980766457533\n",
      "Train Loss at iteration 17891: 0.04279804975657157\n",
      "Train Loss at iteration 17892: 0.04279802286944485\n",
      "Train Loss at iteration 17893: 0.04279799598437278\n",
      "Train Loss at iteration 17894: 0.042797969101355054\n",
      "Train Loss at iteration 17895: 0.04279794222039129\n",
      "Train Loss at iteration 17896: 0.04279791534148114\n",
      "Train Loss at iteration 17897: 0.04279788846462427\n",
      "Train Loss at iteration 17898: 0.042797861589820345\n",
      "Train Loss at iteration 17899: 0.042797834717069015\n",
      "Train Loss at iteration 17900: 0.042797807846369926\n",
      "Train Loss at iteration 17901: 0.04279778097772273\n",
      "Train Loss at iteration 17902: 0.042797754111127094\n",
      "Train Loss at iteration 17903: 0.04279772724658267\n",
      "Train Loss at iteration 17904: 0.04279770038408909\n",
      "Train Loss at iteration 17905: 0.04279767352364604\n",
      "Train Loss at iteration 17906: 0.04279764666525316\n",
      "Train Loss at iteration 17907: 0.042797619808910115\n",
      "Train Loss at iteration 17908: 0.04279759295461654\n",
      "Train Loss at iteration 17909: 0.04279756610237211\n",
      "Train Loss at iteration 17910: 0.04279753925217649\n",
      "Train Loss at iteration 17911: 0.04279751240402928\n",
      "Train Loss at iteration 17912: 0.04279748555793019\n",
      "Train Loss at iteration 17913: 0.04279745871387887\n",
      "Train Loss at iteration 17914: 0.042797431871874944\n",
      "Train Loss at iteration 17915: 0.04279740503191811\n",
      "Train Loss at iteration 17916: 0.04279737819400799\n",
      "Train Loss at iteration 17917: 0.042797351358144234\n",
      "Train Loss at iteration 17918: 0.04279732452432653\n",
      "Train Loss at iteration 17919: 0.04279729769255451\n",
      "Train Loss at iteration 17920: 0.04279727086282784\n",
      "Train Loss at iteration 17921: 0.042797244035146176\n",
      "Train Loss at iteration 17922: 0.04279721720950916\n",
      "Train Loss at iteration 17923: 0.04279719038591646\n",
      "Train Loss at iteration 17924: 0.042797163564367746\n",
      "Train Loss at iteration 17925: 0.04279713674486264\n",
      "Train Loss at iteration 17926: 0.042797109927400816\n",
      "Train Loss at iteration 17927: 0.04279708311198195\n",
      "Train Loss at iteration 17928: 0.042797056298605654\n",
      "Train Loss at iteration 17929: 0.04279702948727162\n",
      "Train Loss at iteration 17930: 0.0427970026779795\n",
      "Train Loss at iteration 17931: 0.042796975870728934\n",
      "Train Loss at iteration 17932: 0.042796949065519584\n",
      "Train Loss at iteration 17933: 0.04279692226235113\n",
      "Train Loss at iteration 17934: 0.04279689546122319\n",
      "Train Loss at iteration 17935: 0.042796868662135445\n",
      "Train Loss at iteration 17936: 0.04279684186508754\n",
      "Train Loss at iteration 17937: 0.04279681507007915\n",
      "Train Loss at iteration 17938: 0.042796788277109914\n",
      "Train Loss at iteration 17939: 0.042796761486179495\n",
      "Train Loss at iteration 17940: 0.04279673469728756\n",
      "Train Loss at iteration 17941: 0.04279670791043373\n",
      "Train Loss at iteration 17942: 0.04279668112561771\n",
      "Train Loss at iteration 17943: 0.042796654342839115\n",
      "Train Loss at iteration 17944: 0.042796627562097636\n",
      "Train Loss at iteration 17945: 0.04279660078339292\n",
      "Train Loss at iteration 17946: 0.042796574006724604\n",
      "Train Loss at iteration 17947: 0.042796547232092384\n",
      "Train Loss at iteration 17948: 0.04279652045949588\n",
      "Train Loss at iteration 17949: 0.04279649368893477\n",
      "Train Loss at iteration 17950: 0.0427964669204087\n",
      "Train Loss at iteration 17951: 0.04279644015391733\n",
      "Train Loss at iteration 17952: 0.04279641338946033\n",
      "Train Loss at iteration 17953: 0.042796386627037344\n",
      "Train Loss at iteration 17954: 0.04279635986664805\n",
      "Train Loss at iteration 17955: 0.04279633310829208\n",
      "Train Loss at iteration 17956: 0.04279630635196909\n",
      "Train Loss at iteration 17957: 0.04279627959767877\n",
      "Train Loss at iteration 17958: 0.042796252845420754\n",
      "Train Loss at iteration 17959: 0.04279622609519469\n",
      "Train Loss at iteration 17960: 0.04279619934700027\n",
      "Train Loss at iteration 17961: 0.042796172600837123\n",
      "Train Loss at iteration 17962: 0.042796145856704916\n",
      "Train Loss at iteration 17963: 0.042796119114603313\n",
      "Train Loss at iteration 17964: 0.04279609237453196\n",
      "Train Loss at iteration 17965: 0.042796065636490535\n",
      "Train Loss at iteration 17966: 0.04279603890047868\n",
      "Train Loss at iteration 17967: 0.042796012166496054\n",
      "Train Loss at iteration 17968: 0.04279598543454232\n",
      "Train Loss at iteration 17969: 0.04279595870461714\n",
      "Train Loss at iteration 17970: 0.042795931976720165\n",
      "Train Loss at iteration 17971: 0.04279590525085107\n",
      "Train Loss at iteration 17972: 0.042795878527009494\n",
      "Train Loss at iteration 17973: 0.042795851805195115\n",
      "Train Loss at iteration 17974: 0.04279582508540756\n",
      "Train Loss at iteration 17975: 0.04279579836764653\n",
      "Train Loss at iteration 17976: 0.04279577165191165\n",
      "Train Loss at iteration 17977: 0.042795744938202596\n",
      "Train Loss at iteration 17978: 0.04279571822651901\n",
      "Train Loss at iteration 17979: 0.042795691516860586\n",
      "Train Loss at iteration 17980: 0.04279566480922696\n",
      "Train Loss at iteration 17981: 0.04279563810361779\n",
      "Train Loss at iteration 17982: 0.04279561140003273\n",
      "Train Loss at iteration 17983: 0.042795584698471445\n",
      "Train Loss at iteration 17984: 0.04279555799893363\n",
      "Train Loss at iteration 17985: 0.04279553130141889\n",
      "Train Loss at iteration 17986: 0.0427955046059269\n",
      "Train Loss at iteration 17987: 0.04279547791245734\n",
      "Train Loss at iteration 17988: 0.04279545122100985\n",
      "Train Loss at iteration 17989: 0.0427954245315841\n",
      "Train Loss at iteration 17990: 0.04279539784417975\n",
      "Train Loss at iteration 17991: 0.04279537115879646\n",
      "Train Loss at iteration 17992: 0.042795344475433886\n",
      "Train Loss at iteration 17993: 0.042795317794091674\n",
      "Train Loss at iteration 17994: 0.04279529111476951\n",
      "Train Loss at iteration 17995: 0.04279526443746704\n",
      "Train Loss at iteration 17996: 0.04279523776218394\n",
      "Train Loss at iteration 17997: 0.04279521108891984\n",
      "Train Loss at iteration 17998: 0.04279518441767443\n",
      "Train Loss at iteration 17999: 0.042795157748447374\n",
      "Train Loss at iteration 18000: 0.04279513108123829\n",
      "Train Loss at iteration 18001: 0.042795104416046885\n",
      "Train Loss at iteration 18002: 0.04279507775287279\n",
      "Train Loss at iteration 18003: 0.04279505109171569\n",
      "Train Loss at iteration 18004: 0.04279502443257522\n",
      "Train Loss at iteration 18005: 0.04279499777545105\n",
      "Train Loss at iteration 18006: 0.042794971120342855\n",
      "Train Loss at iteration 18007: 0.04279494446725029\n",
      "Train Loss at iteration 18008: 0.04279491781617301\n",
      "Train Loss at iteration 18009: 0.04279489116711067\n",
      "Train Loss at iteration 18010: 0.04279486452006293\n",
      "Train Loss at iteration 18011: 0.042794837875029475\n",
      "Train Loss at iteration 18012: 0.042794811232009955\n",
      "Train Loss at iteration 18013: 0.04279478459100402\n",
      "Train Loss at iteration 18014: 0.04279475795201133\n",
      "Train Loss at iteration 18015: 0.042794731315031566\n",
      "Train Loss at iteration 18016: 0.04279470468006437\n",
      "Train Loss at iteration 18017: 0.04279467804710943\n",
      "Train Loss at iteration 18018: 0.042794651416166375\n",
      "Train Loss at iteration 18019: 0.042794624787234885\n",
      "Train Loss at iteration 18020: 0.042794598160314605\n",
      "Train Loss at iteration 18021: 0.04279457153540524\n",
      "Train Loss at iteration 18022: 0.042794544912506394\n",
      "Train Loss at iteration 18023: 0.04279451829161778\n",
      "Train Loss at iteration 18024: 0.04279449167273902\n",
      "Train Loss at iteration 18025: 0.042794465055869806\n",
      "Train Loss at iteration 18026: 0.04279443844100979\n",
      "Train Loss at iteration 18027: 0.04279441182815862\n",
      "Train Loss at iteration 18028: 0.042794385217315975\n",
      "Train Loss at iteration 18029: 0.042794358608481506\n",
      "Train Loss at iteration 18030: 0.04279433200165488\n",
      "Train Loss at iteration 18031: 0.04279430539683576\n",
      "Train Loss at iteration 18032: 0.042794278794023824\n",
      "Train Loss at iteration 18033: 0.04279425219321871\n",
      "Train Loss at iteration 18034: 0.042794225594420095\n",
      "Train Loss at iteration 18035: 0.04279419899762763\n",
      "Train Loss at iteration 18036: 0.042794172402840995\n",
      "Train Loss at iteration 18037: 0.04279414581005983\n",
      "Train Loss at iteration 18038: 0.042794119219283805\n",
      "Train Loss at iteration 18039: 0.04279409263051262\n",
      "Train Loss at iteration 18040: 0.04279406604374588\n",
      "Train Loss at iteration 18041: 0.042794039458983295\n",
      "Train Loss at iteration 18042: 0.042794012876224474\n",
      "Train Loss at iteration 18043: 0.042793986295469134\n",
      "Train Loss at iteration 18044: 0.042793959716716914\n",
      "Train Loss at iteration 18045: 0.04279393313996749\n",
      "Train Loss at iteration 18046: 0.0427939065652205\n",
      "Train Loss at iteration 18047: 0.04279387999247564\n",
      "Train Loss at iteration 18048: 0.04279385342173254\n",
      "Train Loss at iteration 18049: 0.04279382685299088\n",
      "Train Loss at iteration 18050: 0.04279380028625035\n",
      "Train Loss at iteration 18051: 0.042793773721510574\n",
      "Train Loss at iteration 18052: 0.04279374715877123\n",
      "Train Loss at iteration 18053: 0.042793720598031965\n",
      "Train Loss at iteration 18054: 0.04279369403929247\n",
      "Train Loss at iteration 18055: 0.042793667482552394\n",
      "Train Loss at iteration 18056: 0.042793640927811426\n",
      "Train Loss at iteration 18057: 0.042793614375069185\n",
      "Train Loss at iteration 18058: 0.042793587824325366\n",
      "Train Loss at iteration 18059: 0.04279356127557962\n",
      "Train Loss at iteration 18060: 0.04279353472883162\n",
      "Train Loss at iteration 18061: 0.042793508184081036\n",
      "Train Loss at iteration 18062: 0.042793481641327506\n",
      "Train Loss at iteration 18063: 0.04279345510057072\n",
      "Train Loss at iteration 18064: 0.04279342856181034\n",
      "Train Loss at iteration 18065: 0.042793402025046015\n",
      "Train Loss at iteration 18066: 0.04279337549027743\n",
      "Train Loss at iteration 18067: 0.042793348957504224\n",
      "Train Loss at iteration 18068: 0.042793322426726094\n",
      "Train Loss at iteration 18069: 0.04279329589794267\n",
      "Train Loss at iteration 18070: 0.04279326937115364\n",
      "Train Loss at iteration 18071: 0.04279324284635866\n",
      "Train Loss at iteration 18072: 0.0427932163235574\n",
      "Train Loss at iteration 18073: 0.04279318980274952\n",
      "Train Loss at iteration 18074: 0.04279316328393469\n",
      "Train Loss at iteration 18075: 0.04279313676711257\n",
      "Train Loss at iteration 18076: 0.04279311025228282\n",
      "Train Loss at iteration 18077: 0.042793083739445126\n",
      "Train Loss at iteration 18078: 0.04279305722859912\n",
      "Train Loss at iteration 18079: 0.04279303071974451\n",
      "Train Loss at iteration 18080: 0.042793004212880924\n",
      "Train Loss at iteration 18081: 0.04279297770800806\n",
      "Train Loss at iteration 18082: 0.042792951205125544\n",
      "Train Loss at iteration 18083: 0.042792924704233064\n",
      "Train Loss at iteration 18084: 0.04279289820533028\n",
      "Train Loss at iteration 18085: 0.04279287170841687\n",
      "Train Loss at iteration 18086: 0.042792845213492484\n",
      "Train Loss at iteration 18087: 0.0427928187205568\n",
      "Train Loss at iteration 18088: 0.04279279222960948\n",
      "Train Loss at iteration 18089: 0.04279276574065019\n",
      "Train Loss at iteration 18090: 0.0427927392536786\n",
      "Train Loss at iteration 18091: 0.04279271276869435\n",
      "Train Loss at iteration 18092: 0.042792686285697164\n",
      "Train Loss at iteration 18093: 0.04279265980468664\n",
      "Train Loss at iteration 18094: 0.04279263332566247\n",
      "Train Loss at iteration 18095: 0.04279260684862435\n",
      "Train Loss at iteration 18096: 0.0427925803735719\n",
      "Train Loss at iteration 18097: 0.04279255390050481\n",
      "Train Loss at iteration 18098: 0.04279252742942275\n",
      "Train Loss at iteration 18099: 0.042792500960325386\n",
      "Train Loss at iteration 18100: 0.04279247449321236\n",
      "Train Loss at iteration 18101: 0.04279244802808337\n",
      "Train Loss at iteration 18102: 0.04279242156493808\n",
      "Train Loss at iteration 18103: 0.04279239510377613\n",
      "Train Loss at iteration 18104: 0.042792368644597206\n",
      "Train Loss at iteration 18105: 0.042792342187400974\n",
      "Train Loss at iteration 18106: 0.04279231573218711\n",
      "Train Loss at iteration 18107: 0.04279228927895526\n",
      "Train Loss at iteration 18108: 0.042792262827705096\n",
      "Train Loss at iteration 18109: 0.04279223637843631\n",
      "Train Loss at iteration 18110: 0.04279220993114853\n",
      "Train Loss at iteration 18111: 0.04279218348584145\n",
      "Train Loss at iteration 18112: 0.04279215704251473\n",
      "Train Loss at iteration 18113: 0.04279213060116804\n",
      "Train Loss at iteration 18114: 0.042792104161801035\n",
      "Train Loss at iteration 18115: 0.042792077724413395\n",
      "Train Loss at iteration 18116: 0.04279205128900477\n",
      "Train Loss at iteration 18117: 0.042792024855574866\n",
      "Train Loss at iteration 18118: 0.04279199842412332\n",
      "Train Loss at iteration 18119: 0.042791971994649805\n",
      "Train Loss at iteration 18120: 0.04279194556715398\n",
      "Train Loss at iteration 18121: 0.04279191914163553\n",
      "Train Loss at iteration 18122: 0.04279189271809412\n",
      "Train Loss at iteration 18123: 0.0427918662965294\n",
      "Train Loss at iteration 18124: 0.04279183987694106\n",
      "Train Loss at iteration 18125: 0.04279181345932875\n",
      "Train Loss at iteration 18126: 0.04279178704369215\n",
      "Train Loss at iteration 18127: 0.042791760630030935\n",
      "Train Loss at iteration 18128: 0.04279173421834475\n",
      "Train Loss at iteration 18129: 0.04279170780863327\n",
      "Train Loss at iteration 18130: 0.04279168140089618\n",
      "Train Loss at iteration 18131: 0.042791654995133155\n",
      "Train Loss at iteration 18132: 0.04279162859134383\n",
      "Train Loss at iteration 18133: 0.042791602189527884\n",
      "Train Loss at iteration 18134: 0.042791575789684996\n",
      "Train Loss at iteration 18135: 0.04279154939181483\n",
      "Train Loss at iteration 18136: 0.042791522995917056\n",
      "Train Loss at iteration 18137: 0.042791496601991344\n",
      "Train Loss at iteration 18138: 0.04279147021003734\n",
      "Train Loss at iteration 18139: 0.04279144382005476\n",
      "Train Loss at iteration 18140: 0.042791417432043226\n",
      "Train Loss at iteration 18141: 0.04279139104600244\n",
      "Train Loss at iteration 18142: 0.042791364661932035\n",
      "Train Loss at iteration 18143: 0.04279133827983173\n",
      "Train Loss at iteration 18144: 0.04279131189970115\n",
      "Train Loss at iteration 18145: 0.042791285521539976\n",
      "Train Loss at iteration 18146: 0.0427912591453479\n",
      "Train Loss at iteration 18147: 0.042791232771124565\n",
      "Train Loss at iteration 18148: 0.04279120639886964\n",
      "Train Loss at iteration 18149: 0.042791180028582815\n",
      "Train Loss at iteration 18150: 0.04279115366026374\n",
      "Train Loss at iteration 18151: 0.042791127293912094\n",
      "Train Loss at iteration 18152: 0.04279110092952754\n",
      "Train Loss at iteration 18153: 0.042791074567109746\n",
      "Train Loss at iteration 18154: 0.0427910482066584\n",
      "Train Loss at iteration 18155: 0.04279102184817315\n",
      "Train Loss at iteration 18156: 0.042790995491653674\n",
      "Train Loss at iteration 18157: 0.04279096913709965\n",
      "Train Loss at iteration 18158: 0.042790942784510747\n",
      "Train Loss at iteration 18159: 0.04279091643388662\n",
      "Train Loss at iteration 18160: 0.04279089008522694\n",
      "Train Loss at iteration 18161: 0.0427908637385314\n",
      "Train Loss at iteration 18162: 0.042790837393799644\n",
      "Train Loss at iteration 18163: 0.04279081105103136\n",
      "Train Loss at iteration 18164: 0.0427907847102262\n",
      "Train Loss at iteration 18165: 0.042790758371383845\n",
      "Train Loss at iteration 18166: 0.04279073203450399\n",
      "Train Loss at iteration 18167: 0.04279070569958626\n",
      "Train Loss at iteration 18168: 0.042790679366630356\n",
      "Train Loss at iteration 18169: 0.04279065303563593\n",
      "Train Loss at iteration 18170: 0.04279062670660267\n",
      "Train Loss at iteration 18171: 0.042790600379530246\n",
      "Train Loss at iteration 18172: 0.042790574054418294\n",
      "Train Loss at iteration 18173: 0.04279054773126653\n",
      "Train Loss at iteration 18174: 0.04279052141007461\n",
      "Train Loss at iteration 18175: 0.0427904950908422\n",
      "Train Loss at iteration 18176: 0.042790468773568964\n",
      "Train Loss at iteration 18177: 0.04279044245825459\n",
      "Train Loss at iteration 18178: 0.04279041614489874\n",
      "Train Loss at iteration 18179: 0.04279038983350109\n",
      "Train Loss at iteration 18180: 0.0427903635240613\n",
      "Train Loss at iteration 18181: 0.04279033721657905\n",
      "Train Loss at iteration 18182: 0.04279031091105401\n",
      "Train Loss at iteration 18183: 0.04279028460748586\n",
      "Train Loss at iteration 18184: 0.042790258305874254\n",
      "Train Loss at iteration 18185: 0.04279023200621887\n",
      "Train Loss at iteration 18186: 0.04279020570851938\n",
      "Train Loss at iteration 18187: 0.042790179412775464\n",
      "Train Loss at iteration 18188: 0.042790153118986776\n",
      "Train Loss at iteration 18189: 0.04279012682715301\n",
      "Train Loss at iteration 18190: 0.04279010053727382\n",
      "Train Loss at iteration 18191: 0.04279007424934888\n",
      "Train Loss at iteration 18192: 0.042790047963377875\n",
      "Train Loss at iteration 18193: 0.042790021679360475\n",
      "Train Loss at iteration 18194: 0.04278999539729633\n",
      "Train Loss at iteration 18195: 0.04278996911718512\n",
      "Train Loss at iteration 18196: 0.04278994283902654\n",
      "Train Loss at iteration 18197: 0.042789916562820246\n",
      "Train Loss at iteration 18198: 0.0427898902885659\n",
      "Train Loss at iteration 18199: 0.04278986401626318\n",
      "Train Loss at iteration 18200: 0.04278983774591178\n",
      "Train Loss at iteration 18201: 0.04278981147751134\n",
      "Train Loss at iteration 18202: 0.04278978521106156\n",
      "Train Loss at iteration 18203: 0.0427897589465621\n",
      "Train Loss at iteration 18204: 0.04278973268401263\n",
      "Train Loss at iteration 18205: 0.04278970642341281\n",
      "Train Loss at iteration 18206: 0.04278968016476235\n",
      "Train Loss at iteration 18207: 0.042789653908060894\n",
      "Train Loss at iteration 18208: 0.04278962765330812\n",
      "Train Loss at iteration 18209: 0.042789601400503696\n",
      "Train Loss at iteration 18210: 0.04278957514964731\n",
      "Train Loss at iteration 18211: 0.04278954890073862\n",
      "Train Loss at iteration 18212: 0.04278952265377731\n",
      "Train Loss at iteration 18213: 0.042789496408763054\n",
      "Train Loss at iteration 18214: 0.0427894701656955\n",
      "Train Loss at iteration 18215: 0.04278944392457437\n",
      "Train Loss at iteration 18216: 0.04278941768539928\n",
      "Train Loss at iteration 18217: 0.04278939144816994\n",
      "Train Loss at iteration 18218: 0.042789365212886024\n",
      "Train Loss at iteration 18219: 0.04278933897954719\n",
      "Train Loss at iteration 18220: 0.042789312748153105\n",
      "Train Loss at iteration 18221: 0.04278928651870348\n",
      "Train Loss at iteration 18222: 0.04278926029119796\n",
      "Train Loss at iteration 18223: 0.04278923406563619\n",
      "Train Loss at iteration 18224: 0.042789207842017916\n",
      "Train Loss at iteration 18225: 0.042789181620342745\n",
      "Train Loss at iteration 18226: 0.0427891554006104\n",
      "Train Loss at iteration 18227: 0.04278912918282052\n",
      "Train Loss at iteration 18228: 0.042789102966972796\n",
      "Train Loss at iteration 18229: 0.04278907675306689\n",
      "Train Loss at iteration 18230: 0.042789050541102484\n",
      "Train Loss at iteration 18231: 0.042789024331079256\n",
      "Train Loss at iteration 18232: 0.04278899812299687\n",
      "Train Loss at iteration 18233: 0.04278897191685501\n",
      "Train Loss at iteration 18234: 0.04278894571265335\n",
      "Train Loss at iteration 18235: 0.04278891951039156\n",
      "Train Loss at iteration 18236: 0.04278889331006931\n",
      "Train Loss at iteration 18237: 0.04278886711168628\n",
      "Train Loss at iteration 18238: 0.04278884091524216\n",
      "Train Loss at iteration 18239: 0.04278881472073658\n",
      "Train Loss at iteration 18240: 0.04278878852816926\n",
      "Train Loss at iteration 18241: 0.04278876233753986\n",
      "Train Loss at iteration 18242: 0.042788736148848046\n",
      "Train Loss at iteration 18243: 0.04278870996209351\n",
      "Train Loss at iteration 18244: 0.04278868377727591\n",
      "Train Loss at iteration 18245: 0.04278865759439492\n",
      "Train Loss at iteration 18246: 0.042788631413450226\n",
      "Train Loss at iteration 18247: 0.04278860523444152\n",
      "Train Loss at iteration 18248: 0.04278857905736843\n",
      "Train Loss at iteration 18249: 0.04278855288223067\n",
      "Train Loss at iteration 18250: 0.0427885267090279\n",
      "Train Loss at iteration 18251: 0.042788500537759795\n",
      "Train Loss at iteration 18252: 0.04278847436842603\n",
      "Train Loss at iteration 18253: 0.0427884482010263\n",
      "Train Loss at iteration 18254: 0.042788422035560256\n",
      "Train Loss at iteration 18255: 0.04278839587202757\n",
      "Train Loss at iteration 18256: 0.04278836971042793\n",
      "Train Loss at iteration 18257: 0.04278834355076103\n",
      "Train Loss at iteration 18258: 0.042788317393026506\n",
      "Train Loss at iteration 18259: 0.04278829123722407\n",
      "Train Loss at iteration 18260: 0.04278826508335337\n",
      "Train Loss at iteration 18261: 0.04278823893141412\n",
      "Train Loss at iteration 18262: 0.04278821278140594\n",
      "Train Loss at iteration 18263: 0.04278818663332854\n",
      "Train Loss at iteration 18264: 0.04278816048718159\n",
      "Train Loss at iteration 18265: 0.04278813434296478\n",
      "Train Loss at iteration 18266: 0.04278810820067776\n",
      "Train Loss at iteration 18267: 0.04278808206032023\n",
      "Train Loss at iteration 18268: 0.04278805592189185\n",
      "Train Loss at iteration 18269: 0.042788029785392326\n",
      "Train Loss at iteration 18270: 0.04278800365082127\n",
      "Train Loss at iteration 18271: 0.04278797751817843\n",
      "Train Loss at iteration 18272: 0.042787951387463434\n",
      "Train Loss at iteration 18273: 0.04278792525867598\n",
      "Train Loss at iteration 18274: 0.04278789913181575\n",
      "Train Loss at iteration 18275: 0.04278787300688241\n",
      "Train Loss at iteration 18276: 0.04278784688387563\n",
      "Train Loss at iteration 18277: 0.0427878207627951\n",
      "Train Loss at iteration 18278: 0.04278779464364049\n",
      "Train Loss at iteration 18279: 0.042787768526411474\n",
      "Train Loss at iteration 18280: 0.042787742411107735\n",
      "Train Loss at iteration 18281: 0.042787716297728956\n",
      "Train Loss at iteration 18282: 0.0427876901862748\n",
      "Train Loss at iteration 18283: 0.04278766407674495\n",
      "Train Loss at iteration 18284: 0.04278763796913908\n",
      "Train Loss at iteration 18285: 0.04278761186345687\n",
      "Train Loss at iteration 18286: 0.042787585759698016\n",
      "Train Loss at iteration 18287: 0.04278755965786216\n",
      "Train Loss at iteration 18288: 0.042787533557948995\n",
      "Train Loss at iteration 18289: 0.04278750745995821\n",
      "Train Loss at iteration 18290: 0.04278748136388946\n",
      "Train Loss at iteration 18291: 0.042787455269742446\n",
      "Train Loss at iteration 18292: 0.04278742917751684\n",
      "Train Loss at iteration 18293: 0.0427874030872123\n",
      "Train Loss at iteration 18294: 0.04278737699882852\n",
      "Train Loss at iteration 18295: 0.04278735091236519\n",
      "Train Loss at iteration 18296: 0.04278732482782196\n",
      "Train Loss at iteration 18297: 0.04278729874519852\n",
      "Train Loss at iteration 18298: 0.04278727266449456\n",
      "Train Loss at iteration 18299: 0.04278724658570974\n",
      "Train Loss at iteration 18300: 0.04278722050884374\n",
      "Train Loss at iteration 18301: 0.04278719443389625\n",
      "Train Loss at iteration 18302: 0.042787168360866934\n",
      "Train Loss at iteration 18303: 0.042787142289755505\n",
      "Train Loss at iteration 18304: 0.04278711622056158\n",
      "Train Loss at iteration 18305: 0.042787090153284885\n",
      "Train Loss at iteration 18306: 0.04278706408792509\n",
      "Train Loss at iteration 18307: 0.04278703802448187\n",
      "Train Loss at iteration 18308: 0.042787011962954896\n",
      "Train Loss at iteration 18309: 0.042786985903343844\n",
      "Train Loss at iteration 18310: 0.042786959845648405\n",
      "Train Loss at iteration 18311: 0.04278693378986826\n",
      "Train Loss at iteration 18312: 0.042786907736003084\n",
      "Train Loss at iteration 18313: 0.04278688168405254\n",
      "Train Loss at iteration 18314: 0.042786855634016324\n",
      "Train Loss at iteration 18315: 0.04278682958589411\n",
      "Train Loss at iteration 18316: 0.04278680353968558\n",
      "Train Loss at iteration 18317: 0.04278677749539042\n",
      "Train Loss at iteration 18318: 0.04278675145300829\n",
      "Train Loss at iteration 18319: 0.042786725412538876\n",
      "Train Loss at iteration 18320: 0.042786699373981864\n",
      "Train Loss at iteration 18321: 0.04278667333733693\n",
      "Train Loss at iteration 18322: 0.04278664730260375\n",
      "Train Loss at iteration 18323: 0.042786621269782006\n",
      "Train Loss at iteration 18324: 0.04278659523887138\n",
      "Train Loss at iteration 18325: 0.04278656920987154\n",
      "Train Loss at iteration 18326: 0.04278654318278219\n",
      "Train Loss at iteration 18327: 0.04278651715760298\n",
      "Train Loss at iteration 18328: 0.04278649113433361\n",
      "Train Loss at iteration 18329: 0.04278646511297375\n",
      "Train Loss at iteration 18330: 0.04278643909352308\n",
      "Train Loss at iteration 18331: 0.04278641307598129\n",
      "Train Loss at iteration 18332: 0.04278638706034805\n",
      "Train Loss at iteration 18333: 0.04278636104662304\n",
      "Train Loss at iteration 18334: 0.04278633503480596\n",
      "Train Loss at iteration 18335: 0.042786309024896434\n",
      "Train Loss at iteration 18336: 0.042786283016894196\n",
      "Train Loss at iteration 18337: 0.042786257010798925\n",
      "Train Loss at iteration 18338: 0.04278623100661028\n",
      "Train Loss at iteration 18339: 0.04278620500432794\n",
      "Train Loss at iteration 18340: 0.0427861790039516\n",
      "Train Loss at iteration 18341: 0.042786153005480936\n",
      "Train Loss at iteration 18342: 0.04278612700891562\n",
      "Train Loss at iteration 18343: 0.04278610101425534\n",
      "Train Loss at iteration 18344: 0.04278607502149977\n",
      "Train Loss at iteration 18345: 0.0427860490306486\n",
      "Train Loss at iteration 18346: 0.0427860230417015\n",
      "Train Loss at iteration 18347: 0.042785997054658166\n",
      "Train Loss at iteration 18348: 0.04278597106951826\n",
      "Train Loss at iteration 18349: 0.04278594508628149\n",
      "Train Loss at iteration 18350: 0.0427859191049475\n",
      "Train Loss at iteration 18351: 0.042785893125516006\n",
      "Train Loss at iteration 18352: 0.04278586714798666\n",
      "Train Loss at iteration 18353: 0.04278584117235916\n",
      "Train Loss at iteration 18354: 0.042785815198633184\n",
      "Train Loss at iteration 18355: 0.04278578922680841\n",
      "Train Loss at iteration 18356: 0.04278576325688451\n",
      "Train Loss at iteration 18357: 0.042785737288861196\n",
      "Train Loss at iteration 18358: 0.042785711322738125\n",
      "Train Loss at iteration 18359: 0.042785685358514985\n",
      "Train Loss at iteration 18360: 0.04278565939619145\n",
      "Train Loss at iteration 18361: 0.04278563343576721\n",
      "Train Loss at iteration 18362: 0.04278560747724194\n",
      "Train Loss at iteration 18363: 0.042785581520615325\n",
      "Train Loss at iteration 18364: 0.04278555556588705\n",
      "Train Loss at iteration 18365: 0.04278552961305679\n",
      "Train Loss at iteration 18366: 0.04278550366212424\n",
      "Train Loss at iteration 18367: 0.042785477713089055\n",
      "Train Loss at iteration 18368: 0.042785451765950944\n",
      "Train Loss at iteration 18369: 0.042785425820709586\n",
      "Train Loss at iteration 18370: 0.042785399877364634\n",
      "Train Loss at iteration 18371: 0.04278537393591582\n",
      "Train Loss at iteration 18372: 0.04278534799636277\n",
      "Train Loss at iteration 18373: 0.042785322058705215\n",
      "Train Loss at iteration 18374: 0.04278529612294281\n",
      "Train Loss at iteration 18375: 0.04278527018907524\n",
      "Train Loss at iteration 18376: 0.042785244257102194\n",
      "Train Loss at iteration 18377: 0.042785218327023346\n",
      "Train Loss at iteration 18378: 0.04278519239883839\n",
      "Train Loss at iteration 18379: 0.042785166472547\n",
      "Train Loss at iteration 18380: 0.04278514054814886\n",
      "Train Loss at iteration 18381: 0.04278511462564365\n",
      "Train Loss at iteration 18382: 0.04278508870503105\n",
      "Train Loss at iteration 18383: 0.04278506278631076\n",
      "Train Loss at iteration 18384: 0.04278503686948245\n",
      "Train Loss at iteration 18385: 0.042785010954545795\n",
      "Train Loss at iteration 18386: 0.0427849850415005\n",
      "Train Loss at iteration 18387: 0.042784959130346224\n",
      "Train Loss at iteration 18388: 0.04278493322108267\n",
      "Train Loss at iteration 18389: 0.04278490731370951\n",
      "Train Loss at iteration 18390: 0.042784881408226415\n",
      "Train Loss at iteration 18391: 0.04278485550463309\n",
      "Train Loss at iteration 18392: 0.04278482960292922\n",
      "Train Loss at iteration 18393: 0.042784803703114466\n",
      "Train Loss at iteration 18394: 0.042784777805188516\n",
      "Train Loss at iteration 18395: 0.04278475190915108\n",
      "Train Loss at iteration 18396: 0.042784726015001814\n",
      "Train Loss at iteration 18397: 0.04278470012274041\n",
      "Train Loss at iteration 18398: 0.04278467423236656\n",
      "Train Loss at iteration 18399: 0.04278464834387992\n",
      "Train Loss at iteration 18400: 0.0427846224572802\n",
      "Train Loss at iteration 18401: 0.04278459657256708\n",
      "Train Loss at iteration 18402: 0.04278457068974024\n",
      "Train Loss at iteration 18403: 0.04278454480879936\n",
      "Train Loss at iteration 18404: 0.042784518929744116\n",
      "Train Loss at iteration 18405: 0.04278449305257423\n",
      "Train Loss at iteration 18406: 0.04278446717728934\n",
      "Train Loss at iteration 18407: 0.04278444130388916\n",
      "Train Loss at iteration 18408: 0.042784415432373366\n",
      "Train Loss at iteration 18409: 0.04278438956274162\n",
      "Train Loss at iteration 18410: 0.04278436369499366\n",
      "Train Loss at iteration 18411: 0.04278433782912911\n",
      "Train Loss at iteration 18412: 0.04278431196514768\n",
      "Train Loss at iteration 18413: 0.04278428610304907\n",
      "Train Loss at iteration 18414: 0.04278426024283293\n",
      "Train Loss at iteration 18415: 0.04278423438449898\n",
      "Train Loss at iteration 18416: 0.042784208528046874\n",
      "Train Loss at iteration 18417: 0.04278418267347631\n",
      "Train Loss at iteration 18418: 0.042784156820787\n",
      "Train Loss at iteration 18419: 0.04278413096997857\n",
      "Train Loss at iteration 18420: 0.042784105121050756\n",
      "Train Loss at iteration 18421: 0.042784079274003205\n",
      "Train Loss at iteration 18422: 0.04278405342883564\n",
      "Train Loss at iteration 18423: 0.04278402758554771\n",
      "Train Loss at iteration 18424: 0.042784001744139134\n",
      "Train Loss at iteration 18425: 0.04278397590460956\n",
      "Train Loss at iteration 18426: 0.0427839500669587\n",
      "Train Loss at iteration 18427: 0.04278392423118624\n",
      "Train Loss at iteration 18428: 0.042783898397291846\n",
      "Train Loss at iteration 18429: 0.042783872565275216\n",
      "Train Loss at iteration 18430: 0.042783846735136036\n",
      "Train Loss at iteration 18431: 0.04278382090687398\n",
      "Train Loss at iteration 18432: 0.042783795080488754\n",
      "Train Loss at iteration 18433: 0.04278376925598002\n",
      "Train Loss at iteration 18434: 0.04278374343334748\n",
      "Train Loss at iteration 18435: 0.04278371761259081\n",
      "Train Loss at iteration 18436: 0.042783691793709715\n",
      "Train Loss at iteration 18437: 0.042783665976703855\n",
      "Train Loss at iteration 18438: 0.04278364016157292\n",
      "Train Loss at iteration 18439: 0.0427836143483166\n",
      "Train Loss at iteration 18440: 0.0427835885369346\n",
      "Train Loss at iteration 18441: 0.042783562727426575\n",
      "Train Loss at iteration 18442: 0.042783536919792226\n",
      "Train Loss at iteration 18443: 0.042783511114031246\n",
      "Train Loss at iteration 18444: 0.0427834853101433\n",
      "Train Loss at iteration 18445: 0.042783459508128105\n",
      "Train Loss at iteration 18446: 0.04278343370798532\n",
      "Train Loss at iteration 18447: 0.04278340790971464\n",
      "Train Loss at iteration 18448: 0.04278338211331574\n",
      "Train Loss at iteration 18449: 0.04278335631878833\n",
      "Train Loss at iteration 18450: 0.04278333052613209\n",
      "Train Loss at iteration 18451: 0.042783304735346694\n",
      "Train Loss at iteration 18452: 0.042783278946431816\n",
      "Train Loss at iteration 18453: 0.04278325315938719\n",
      "Train Loss at iteration 18454: 0.04278322737421247\n",
      "Train Loss at iteration 18455: 0.04278320159090733\n",
      "Train Loss at iteration 18456: 0.04278317580947148\n",
      "Train Loss at iteration 18457: 0.04278315002990461\n",
      "Train Loss at iteration 18458: 0.04278312425220639\n",
      "Train Loss at iteration 18459: 0.04278309847637652\n",
      "Train Loss at iteration 18460: 0.04278307270241467\n",
      "Train Loss at iteration 18461: 0.042783046930320555\n",
      "Train Loss at iteration 18462: 0.04278302116009383\n",
      "Train Loss at iteration 18463: 0.042782995391734205\n",
      "Train Loss at iteration 18464: 0.042782969625241356\n",
      "Train Loss at iteration 18465: 0.04278294386061497\n",
      "Train Loss at iteration 18466: 0.04278291809785475\n",
      "Train Loss at iteration 18467: 0.04278289233696035\n",
      "Train Loss at iteration 18468: 0.0427828665779315\n",
      "Train Loss at iteration 18469: 0.04278284082076786\n",
      "Train Loss at iteration 18470: 0.04278281506546911\n",
      "Train Loss at iteration 18471: 0.04278278931203497\n",
      "Train Loss at iteration 18472: 0.04278276356046509\n",
      "Train Loss at iteration 18473: 0.04278273781075919\n",
      "Train Loss at iteration 18474: 0.042782712062916936\n",
      "Train Loss at iteration 18475: 0.04278268631693803\n",
      "Train Loss at iteration 18476: 0.042782660572822144\n",
      "Train Loss at iteration 18477: 0.04278263483056898\n",
      "Train Loss at iteration 18478: 0.042782609090178225\n",
      "Train Loss at iteration 18479: 0.04278258335164956\n",
      "Train Loss at iteration 18480: 0.04278255761498268\n",
      "Train Loss at iteration 18481: 0.04278253188017726\n",
      "Train Loss at iteration 18482: 0.042782506147233\n",
      "Train Loss at iteration 18483: 0.04278248041614959\n",
      "Train Loss at iteration 18484: 0.042782454686926714\n",
      "Train Loss at iteration 18485: 0.04278242895956406\n",
      "Train Loss at iteration 18486: 0.042782403234061316\n",
      "Train Loss at iteration 18487: 0.04278237751041816\n",
      "Train Loss at iteration 18488: 0.04278235178863431\n",
      "Train Loss at iteration 18489: 0.042782326068709416\n",
      "Train Loss at iteration 18490: 0.0427823003506432\n",
      "Train Loss at iteration 18491: 0.04278227463443533\n",
      "Train Loss at iteration 18492: 0.0427822489200855\n",
      "Train Loss at iteration 18493: 0.042782223207593395\n",
      "Train Loss at iteration 18494: 0.042782197496958725\n",
      "Train Loss at iteration 18495: 0.04278217178818115\n",
      "Train Loss at iteration 18496: 0.04278214608126038\n",
      "Train Loss at iteration 18497: 0.042782120376196095\n",
      "Train Loss at iteration 18498: 0.04278209467298798\n",
      "Train Loss at iteration 18499: 0.04278206897163574\n",
      "Train Loss at iteration 18500: 0.04278204327213903\n",
      "Train Loss at iteration 18501: 0.04278201757449759\n",
      "Train Loss at iteration 18502: 0.04278199187871105\n",
      "Train Loss at iteration 18503: 0.04278196618477916\n",
      "Train Loss at iteration 18504: 0.042781940492701565\n",
      "Train Loss at iteration 18505: 0.04278191480247796\n",
      "Train Loss at iteration 18506: 0.04278188911410807\n",
      "Train Loss at iteration 18507: 0.04278186342759154\n",
      "Train Loss at iteration 18508: 0.04278183774292808\n",
      "Train Loss at iteration 18509: 0.04278181206011738\n",
      "Train Loss at iteration 18510: 0.04278178637915912\n",
      "Train Loss at iteration 18511: 0.042781760700053004\n",
      "Train Loss at iteration 18512: 0.04278173502279871\n",
      "Train Loss at iteration 18513: 0.04278170934739593\n",
      "Train Loss at iteration 18514: 0.042781683673844356\n",
      "Train Loss at iteration 18515: 0.04278165800214369\n",
      "Train Loss at iteration 18516: 0.042781632332293595\n",
      "Train Loss at iteration 18517: 0.042781606664293786\n",
      "Train Loss at iteration 18518: 0.042781580998143935\n",
      "Train Loss at iteration 18519: 0.04278155533384374\n",
      "Train Loss at iteration 18520: 0.0427815296713929\n",
      "Train Loss at iteration 18521: 0.042781504010791095\n",
      "Train Loss at iteration 18522: 0.04278147835203801\n",
      "Train Loss at iteration 18523: 0.042781452695133344\n",
      "Train Loss at iteration 18524: 0.04278142704007679\n",
      "Train Loss at iteration 18525: 0.04278140138686803\n",
      "Train Loss at iteration 18526: 0.042781375735506756\n",
      "Train Loss at iteration 18527: 0.042781350085992674\n",
      "Train Loss at iteration 18528: 0.04278132443832544\n",
      "Train Loss at iteration 18529: 0.042781298792504786\n",
      "Train Loss at iteration 18530: 0.04278127314853036\n",
      "Train Loss at iteration 18531: 0.0427812475064019\n",
      "Train Loss at iteration 18532: 0.042781221866119074\n",
      "Train Loss at iteration 18533: 0.04278119622768156\n",
      "Train Loss at iteration 18534: 0.04278117059108906\n",
      "Train Loss at iteration 18535: 0.04278114495634127\n",
      "Train Loss at iteration 18536: 0.04278111932343787\n",
      "Train Loss at iteration 18537: 0.04278109369237857\n",
      "Train Loss at iteration 18538: 0.04278106806316303\n",
      "Train Loss at iteration 18539: 0.04278104243579098\n",
      "Train Loss at iteration 18540: 0.04278101681026208\n",
      "Train Loss at iteration 18541: 0.04278099118657604\n",
      "Train Loss at iteration 18542: 0.042780965564732536\n",
      "Train Loss at iteration 18543: 0.04278093994473127\n",
      "Train Loss at iteration 18544: 0.04278091432657193\n",
      "Train Loss at iteration 18545: 0.04278088871025421\n",
      "Train Loss at iteration 18546: 0.04278086309577781\n",
      "Train Loss at iteration 18547: 0.04278083748314239\n",
      "Train Loss at iteration 18548: 0.04278081187234768\n",
      "Train Loss at iteration 18549: 0.04278078626339335\n",
      "Train Loss at iteration 18550: 0.0427807606562791\n",
      "Train Loss at iteration 18551: 0.04278073505100461\n",
      "Train Loss at iteration 18552: 0.04278070944756958\n",
      "Train Loss at iteration 18553: 0.04278068384597372\n",
      "Train Loss at iteration 18554: 0.04278065824621669\n",
      "Train Loss at iteration 18555: 0.04278063264829819\n",
      "Train Loss at iteration 18556: 0.04278060705221794\n",
      "Train Loss at iteration 18557: 0.04278058145797559\n",
      "Train Loss at iteration 18558: 0.04278055586557087\n",
      "Train Loss at iteration 18559: 0.04278053027500344\n",
      "Train Loss at iteration 18560: 0.04278050468627303\n",
      "Train Loss at iteration 18561: 0.04278047909937929\n",
      "Train Loss at iteration 18562: 0.04278045351432194\n",
      "Train Loss at iteration 18563: 0.04278042793110067\n",
      "Train Loss at iteration 18564: 0.04278040234971516\n",
      "Train Loss at iteration 18565: 0.042780376770165114\n",
      "Train Loss at iteration 18566: 0.04278035119245022\n",
      "Train Loss at iteration 18567: 0.04278032561657017\n",
      "Train Loss at iteration 18568: 0.04278030004252466\n",
      "Train Loss at iteration 18569: 0.042780274470313386\n",
      "Train Loss at iteration 18570: 0.04278024889993602\n",
      "Train Loss at iteration 18571: 0.04278022333139228\n",
      "Train Loss at iteration 18572: 0.04278019776468185\n",
      "Train Loss at iteration 18573: 0.042780172199804435\n",
      "Train Loss at iteration 18574: 0.04278014663675971\n",
      "Train Loss at iteration 18575: 0.042780121075547364\n",
      "Train Loss at iteration 18576: 0.04278009551616711\n",
      "Train Loss at iteration 18577: 0.042780069958618626\n",
      "Train Loss at iteration 18578: 0.04278004440290163\n",
      "Train Loss at iteration 18579: 0.04278001884901577\n",
      "Train Loss at iteration 18580: 0.04277999329696077\n",
      "Train Loss at iteration 18581: 0.04277996774673633\n",
      "Train Loss at iteration 18582: 0.042779942198342136\n",
      "Train Loss at iteration 18583: 0.042779916651777876\n",
      "Train Loss at iteration 18584: 0.042779891107043236\n",
      "Train Loss at iteration 18585: 0.042779865564137924\n",
      "Train Loss at iteration 18586: 0.042779840023061635\n",
      "Train Loss at iteration 18587: 0.04277981448381406\n",
      "Train Loss at iteration 18588: 0.04277978894639488\n",
      "Train Loss at iteration 18589: 0.042779763410803785\n",
      "Train Loss at iteration 18590: 0.042779737877040515\n",
      "Train Loss at iteration 18591: 0.042779712345104706\n",
      "Train Loss at iteration 18592: 0.04277968681499608\n",
      "Train Loss at iteration 18593: 0.04277966128671435\n",
      "Train Loss at iteration 18594: 0.04277963576025918\n",
      "Train Loss at iteration 18595: 0.04277961023563026\n",
      "Train Loss at iteration 18596: 0.04277958471282731\n",
      "Train Loss at iteration 18597: 0.04277955919185002\n",
      "Train Loss at iteration 18598: 0.04277953367269805\n",
      "Train Loss at iteration 18599: 0.04277950815537115\n",
      "Train Loss at iteration 18600: 0.04277948263986895\n",
      "Train Loss at iteration 18601: 0.0427794571261912\n",
      "Train Loss at iteration 18602: 0.04277943161433759\n",
      "Train Loss at iteration 18603: 0.042779406104307784\n",
      "Train Loss at iteration 18604: 0.04277938059610148\n",
      "Train Loss at iteration 18605: 0.042779355089718404\n",
      "Train Loss at iteration 18606: 0.04277932958515822\n",
      "Train Loss at iteration 18607: 0.04277930408242064\n",
      "Train Loss at iteration 18608: 0.04277927858150535\n",
      "Train Loss at iteration 18609: 0.04277925308241204\n",
      "Train Loss at iteration 18610: 0.04277922758514042\n",
      "Train Loss at iteration 18611: 0.04277920208969019\n",
      "Train Loss at iteration 18612: 0.04277917659606101\n",
      "Train Loss at iteration 18613: 0.042779151104252615\n",
      "Train Loss at iteration 18614: 0.04277912561426467\n",
      "Train Loss at iteration 18615: 0.04277910012609689\n",
      "Train Loss at iteration 18616: 0.04277907463974896\n",
      "Train Loss at iteration 18617: 0.04277904915522059\n",
      "Train Loss at iteration 18618: 0.042779023672511456\n",
      "Train Loss at iteration 18619: 0.04277899819162126\n",
      "Train Loss at iteration 18620: 0.04277897271254971\n",
      "Train Loss at iteration 18621: 0.04277894723529649\n",
      "Train Loss at iteration 18622: 0.042778921759861296\n",
      "Train Loss at iteration 18623: 0.04277889628624382\n",
      "Train Loss at iteration 18624: 0.04277887081444375\n",
      "Train Loss at iteration 18625: 0.042778845344460824\n",
      "Train Loss at iteration 18626: 0.04277881987629468\n",
      "Train Loss at iteration 18627: 0.04277879440994505\n",
      "Train Loss at iteration 18628: 0.042778768945411626\n",
      "Train Loss at iteration 18629: 0.042778743482694115\n",
      "Train Loss at iteration 18630: 0.042778718021792166\n",
      "Train Loss at iteration 18631: 0.04277869256270552\n",
      "Train Loss at iteration 18632: 0.042778667105433855\n",
      "Train Loss at iteration 18633: 0.04277864164997689\n",
      "Train Loss at iteration 18634: 0.042778616196334295\n",
      "Train Loss at iteration 18635: 0.04277859074450578\n",
      "Train Loss at iteration 18636: 0.042778565294491026\n",
      "Train Loss at iteration 18637: 0.042778539846289744\n",
      "Train Loss at iteration 18638: 0.042778514399901615\n",
      "Train Loss at iteration 18639: 0.04277848895532637\n",
      "Train Loss at iteration 18640: 0.04277846351256367\n",
      "Train Loss at iteration 18641: 0.04277843807161322\n",
      "Train Loss at iteration 18642: 0.04277841263247473\n",
      "Train Loss at iteration 18643: 0.04277838719514787\n",
      "Train Loss at iteration 18644: 0.042778361759632386\n",
      "Train Loss at iteration 18645: 0.042778336325927924\n",
      "Train Loss at iteration 18646: 0.042778310894034194\n",
      "Train Loss at iteration 18647: 0.0427782854639509\n",
      "Train Loss at iteration 18648: 0.04277826003567776\n",
      "Train Loss at iteration 18649: 0.04277823460921443\n",
      "Train Loss at iteration 18650: 0.042778209184560614\n",
      "Train Loss at iteration 18651: 0.04277818376171605\n",
      "Train Loss at iteration 18652: 0.04277815834068039\n",
      "Train Loss at iteration 18653: 0.04277813292145335\n",
      "Train Loss at iteration 18654: 0.042778107504034636\n",
      "Train Loss at iteration 18655: 0.04277808208842391\n",
      "Train Loss at iteration 18656: 0.04277805667462091\n",
      "Train Loss at iteration 18657: 0.04277803126262532\n",
      "Train Loss at iteration 18658: 0.04277800585243683\n",
      "Train Loss at iteration 18659: 0.04277798044405513\n",
      "Train Loss at iteration 18660: 0.04277795503747995\n",
      "Train Loss at iteration 18661: 0.04277792963271096\n",
      "Train Loss at iteration 18662: 0.042777904229747865\n",
      "Train Loss at iteration 18663: 0.04277787882859036\n",
      "Train Loss at iteration 18664: 0.04277785342923816\n",
      "Train Loss at iteration 18665: 0.04277782803169094\n",
      "Train Loss at iteration 18666: 0.0427778026359484\n",
      "Train Loss at iteration 18667: 0.042777777242010255\n",
      "Train Loss at iteration 18668: 0.04277775184987619\n",
      "Train Loss at iteration 18669: 0.0427777264595459\n",
      "Train Loss at iteration 18670: 0.04277770107101911\n",
      "Train Loss at iteration 18671: 0.04277767568429548\n",
      "Train Loss at iteration 18672: 0.04277765029937474\n",
      "Train Loss at iteration 18673: 0.04277762491625655\n",
      "Train Loss at iteration 18674: 0.042777599534940654\n",
      "Train Loss at iteration 18675: 0.04277757415542672\n",
      "Train Loss at iteration 18676: 0.04277754877771447\n",
      "Train Loss at iteration 18677: 0.042777523401803554\n",
      "Train Loss at iteration 18678: 0.04277749802769374\n",
      "Train Loss at iteration 18679: 0.04277747265538467\n",
      "Train Loss at iteration 18680: 0.04277744728487606\n",
      "Train Loss at iteration 18681: 0.04277742191616764\n",
      "Train Loss at iteration 18682: 0.042777396549259064\n",
      "Train Loss at iteration 18683: 0.04277737118415005\n",
      "Train Loss at iteration 18684: 0.0427773458208403\n",
      "Train Loss at iteration 18685: 0.04277732045932949\n",
      "Train Loss at iteration 18686: 0.04277729509961736\n",
      "Train Loss at iteration 18687: 0.04277726974170358\n",
      "Train Loss at iteration 18688: 0.04277724438558785\n",
      "Train Loss at iteration 18689: 0.04277721903126987\n",
      "Train Loss at iteration 18690: 0.04277719367874936\n",
      "Train Loss at iteration 18691: 0.042777168328026\n",
      "Train Loss at iteration 18692: 0.04277714297909948\n",
      "Train Loss at iteration 18693: 0.04277711763196951\n",
      "Train Loss at iteration 18694: 0.042777092286635794\n",
      "Train Loss at iteration 18695: 0.04277706694309805\n",
      "Train Loss at iteration 18696: 0.04277704160135593\n",
      "Train Loss at iteration 18697: 0.04277701626140918\n",
      "Train Loss at iteration 18698: 0.04277699092325746\n",
      "Train Loss at iteration 18699: 0.042776965586900494\n",
      "Train Loss at iteration 18700: 0.042776940252337976\n",
      "Train Loss at iteration 18701: 0.04277691491956961\n",
      "Train Loss at iteration 18702: 0.042776889588595095\n",
      "Train Loss at iteration 18703: 0.04277686425941412\n",
      "Train Loss at iteration 18704: 0.042776838932026405\n",
      "Train Loss at iteration 18705: 0.04277681360643164\n",
      "Train Loss at iteration 18706: 0.042776788282629506\n",
      "Train Loss at iteration 18707: 0.042776762960619726\n",
      "Train Loss at iteration 18708: 0.042776737640402\n",
      "Train Loss at iteration 18709: 0.04277671232197602\n",
      "Train Loss at iteration 18710: 0.04277668700534149\n",
      "Train Loss at iteration 18711: 0.042776661690498104\n",
      "Train Loss at iteration 18712: 0.042776636377445575\n",
      "Train Loss at iteration 18713: 0.042776611066183585\n",
      "Train Loss at iteration 18714: 0.04277658575671185\n",
      "Train Loss at iteration 18715: 0.04277656044903007\n",
      "Train Loss at iteration 18716: 0.04277653514313794\n",
      "Train Loss at iteration 18717: 0.042776509839035165\n",
      "Train Loss at iteration 18718: 0.042776484536721435\n",
      "Train Loss at iteration 18719: 0.04277645923619646\n",
      "Train Loss at iteration 18720: 0.042776433937459946\n",
      "Train Loss at iteration 18721: 0.04277640864051158\n",
      "Train Loss at iteration 18722: 0.042776383345351075\n",
      "Train Loss at iteration 18723: 0.04277635805197812\n",
      "Train Loss at iteration 18724: 0.04277633276039244\n",
      "Train Loss at iteration 18725: 0.0427763074705937\n",
      "Train Loss at iteration 18726: 0.04277628218258163\n",
      "Train Loss at iteration 18727: 0.04277625689635592\n",
      "Train Loss at iteration 18728: 0.04277623161191627\n",
      "Train Loss at iteration 18729: 0.04277620632926238\n",
      "Train Loss at iteration 18730: 0.04277618104839395\n",
      "Train Loss at iteration 18731: 0.0427761557693107\n",
      "Train Loss at iteration 18732: 0.04277613049201231\n",
      "Train Loss at iteration 18733: 0.04277610521649848\n",
      "Train Loss at iteration 18734: 0.04277607994276894\n",
      "Train Loss at iteration 18735: 0.042776054670823356\n",
      "Train Loss at iteration 18736: 0.042776029400661456\n",
      "Train Loss at iteration 18737: 0.042776004132282934\n",
      "Train Loss at iteration 18738: 0.04277597886568747\n",
      "Train Loss at iteration 18739: 0.04277595360087479\n",
      "Train Loss at iteration 18740: 0.04277592833784461\n",
      "Train Loss at iteration 18741: 0.04277590307659659\n",
      "Train Loss at iteration 18742: 0.042775877817130464\n",
      "Train Loss at iteration 18743: 0.04277585255944592\n",
      "Train Loss at iteration 18744: 0.04277582730354268\n",
      "Train Loss at iteration 18745: 0.042775802049420414\n",
      "Train Loss at iteration 18746: 0.04277577679707885\n",
      "Train Loss at iteration 18747: 0.04277575154651767\n",
      "Train Loss at iteration 18748: 0.042775726297736585\n",
      "Train Loss at iteration 18749: 0.04277570105073532\n",
      "Train Loss at iteration 18750: 0.04277567580551353\n",
      "Train Loss at iteration 18751: 0.042775650562070965\n",
      "Train Loss at iteration 18752: 0.042775625320407296\n",
      "Train Loss at iteration 18753: 0.04277560008052223\n",
      "Train Loss at iteration 18754: 0.04277557484241548\n",
      "Train Loss at iteration 18755: 0.04277554960608674\n",
      "Train Loss at iteration 18756: 0.04277552437153573\n",
      "Train Loss at iteration 18757: 0.042775499138762135\n",
      "Train Loss at iteration 18758: 0.04277547390776564\n",
      "Train Loss at iteration 18759: 0.042775448678546\n",
      "Train Loss at iteration 18760: 0.04277542345110287\n",
      "Train Loss at iteration 18761: 0.042775398225435975\n",
      "Train Loss at iteration 18762: 0.04277537300154502\n",
      "Train Loss at iteration 18763: 0.04277534777942969\n",
      "Train Loss at iteration 18764: 0.0427753225590897\n",
      "Train Loss at iteration 18765: 0.04277529734052475\n",
      "Train Loss at iteration 18766: 0.04277527212373455\n",
      "Train Loss at iteration 18767: 0.0427752469087188\n",
      "Train Loss at iteration 18768: 0.0427752216954772\n",
      "Train Loss at iteration 18769: 0.04277519648400946\n",
      "Train Loss at iteration 18770: 0.04277517127431528\n",
      "Train Loss at iteration 18771: 0.042775146066394355\n",
      "Train Loss at iteration 18772: 0.042775120860246395\n",
      "Train Loss at iteration 18773: 0.04277509565587109\n",
      "Train Loss at iteration 18774: 0.042775070453268185\n",
      "Train Loss at iteration 18775: 0.04277504525243735\n",
      "Train Loss at iteration 18776: 0.04277502005337829\n",
      "Train Loss at iteration 18777: 0.0427749948560907\n",
      "Train Loss at iteration 18778: 0.04277496966057433\n",
      "Train Loss at iteration 18779: 0.04277494446682882\n",
      "Train Loss at iteration 18780: 0.04277491927485391\n",
      "Train Loss at iteration 18781: 0.04277489408464931\n",
      "Train Loss at iteration 18782: 0.04277486889621471\n",
      "Train Loss at iteration 18783: 0.0427748437095498\n",
      "Train Loss at iteration 18784: 0.04277481852465431\n",
      "Train Loss at iteration 18785: 0.04277479334152796\n",
      "Train Loss at iteration 18786: 0.0427747681601704\n",
      "Train Loss at iteration 18787: 0.04277474298058138\n",
      "Train Loss at iteration 18788: 0.042774717802760584\n",
      "Train Loss at iteration 18789: 0.04277469262670771\n",
      "Train Loss at iteration 18790: 0.04277466745242248\n",
      "Train Loss at iteration 18791: 0.04277464227990459\n",
      "Train Loss at iteration 18792: 0.04277461710915375\n",
      "Train Loss at iteration 18793: 0.04277459194016965\n",
      "Train Loss at iteration 18794: 0.042774566772952\n",
      "Train Loss at iteration 18795: 0.042774541607500514\n",
      "Train Loss at iteration 18796: 0.04277451644381489\n",
      "Train Loss at iteration 18797: 0.04277449128189485\n",
      "Train Loss at iteration 18798: 0.04277446612174007\n",
      "Train Loss at iteration 18799: 0.04277444096335027\n",
      "Train Loss at iteration 18800: 0.04277441580672514\n",
      "Train Loss at iteration 18801: 0.042774390651864426\n",
      "Train Loss at iteration 18802: 0.042774365498767786\n",
      "Train Loss at iteration 18803: 0.04277434034743495\n",
      "Train Loss at iteration 18804: 0.04277431519786562\n",
      "Train Loss at iteration 18805: 0.04277429005005949\n",
      "Train Loss at iteration 18806: 0.042774264904016264\n",
      "Train Loss at iteration 18807: 0.042774239759735676\n",
      "Train Loss at iteration 18808: 0.04277421461721739\n",
      "Train Loss at iteration 18809: 0.04277418947646116\n",
      "Train Loss at iteration 18810: 0.04277416433746665\n",
      "Train Loss at iteration 18811: 0.04277413920023358\n",
      "Train Loss at iteration 18812: 0.04277411406476166\n",
      "Train Loss at iteration 18813: 0.042774088931050586\n",
      "Train Loss at iteration 18814: 0.042774063799100065\n",
      "Train Loss at iteration 18815: 0.04277403866890982\n",
      "Train Loss at iteration 18816: 0.04277401354047953\n",
      "Train Loss at iteration 18817: 0.04277398841380892\n",
      "Train Loss at iteration 18818: 0.042773963288897696\n",
      "Train Loss at iteration 18819: 0.04277393816574555\n",
      "Train Loss at iteration 18820: 0.042773913044352185\n",
      "Train Loss at iteration 18821: 0.04277388792471732\n",
      "Train Loss at iteration 18822: 0.04277386280684067\n",
      "Train Loss at iteration 18823: 0.04277383769072193\n",
      "Train Loss at iteration 18824: 0.042773812576360794\n",
      "Train Loss at iteration 18825: 0.04277378746375699\n",
      "Train Loss at iteration 18826: 0.04277376235291021\n",
      "Train Loss at iteration 18827: 0.04277373724382018\n",
      "Train Loss at iteration 18828: 0.04277371213648656\n",
      "Train Loss at iteration 18829: 0.0427736870309091\n",
      "Train Loss at iteration 18830: 0.04277366192708749\n",
      "Train Loss at iteration 18831: 0.04277363682502145\n",
      "Train Loss at iteration 18832: 0.04277361172471067\n",
      "Train Loss at iteration 18833: 0.04277358662615485\n",
      "Train Loss at iteration 18834: 0.04277356152935373\n",
      "Train Loss at iteration 18835: 0.04277353643430699\n",
      "Train Loss at iteration 18836: 0.04277351134101434\n",
      "Train Loss at iteration 18837: 0.04277348624947549\n",
      "Train Loss at iteration 18838: 0.042773461159690135\n",
      "Train Loss at iteration 18839: 0.042773436071658\n",
      "Train Loss at iteration 18840: 0.04277341098537878\n",
      "Train Loss at iteration 18841: 0.0427733859008522\n",
      "Train Loss at iteration 18842: 0.04277336081807796\n",
      "Train Loss at iteration 18843: 0.04277333573705575\n",
      "Train Loss at iteration 18844: 0.042773310657785286\n",
      "Train Loss at iteration 18845: 0.04277328558026628\n",
      "Train Loss at iteration 18846: 0.042773260504498437\n",
      "Train Loss at iteration 18847: 0.04277323543048147\n",
      "Train Loss at iteration 18848: 0.04277321035821507\n",
      "Train Loss at iteration 18849: 0.042773185287698946\n",
      "Train Loss at iteration 18850: 0.042773160218932835\n",
      "Train Loss at iteration 18851: 0.04277313515191642\n",
      "Train Loss at iteration 18852: 0.04277311008664941\n",
      "Train Loss at iteration 18853: 0.042773085023131506\n",
      "Train Loss at iteration 18854: 0.04277305996136242\n",
      "Train Loss at iteration 18855: 0.04277303490134189\n",
      "Train Loss at iteration 18856: 0.042773009843069566\n",
      "Train Loss at iteration 18857: 0.042772984786545204\n",
      "Train Loss at iteration 18858: 0.0427729597317685\n",
      "Train Loss at iteration 18859: 0.04277293467873915\n",
      "Train Loss at iteration 18860: 0.042772909627456876\n",
      "Train Loss at iteration 18861: 0.04277288457792136\n",
      "Train Loss at iteration 18862: 0.04277285953013235\n",
      "Train Loss at iteration 18863: 0.042772834484089535\n",
      "Train Loss at iteration 18864: 0.04277280943979261\n",
      "Train Loss at iteration 18865: 0.04277278439724129\n",
      "Train Loss at iteration 18866: 0.0427727593564353\n",
      "Train Loss at iteration 18867: 0.04277273431737432\n",
      "Train Loss at iteration 18868: 0.042772709280058094\n",
      "Train Loss at iteration 18869: 0.0427726842444863\n",
      "Train Loss at iteration 18870: 0.04277265921065865\n",
      "Train Loss at iteration 18871: 0.04277263417857488\n",
      "Train Loss at iteration 18872: 0.04277260914823465\n",
      "Train Loss at iteration 18873: 0.04277258411963772\n",
      "Train Loss at iteration 18874: 0.04277255909278377\n",
      "Train Loss at iteration 18875: 0.042772534067672514\n",
      "Train Loss at iteration 18876: 0.04277250904430366\n",
      "Train Loss at iteration 18877: 0.04277248402267691\n",
      "Train Loss at iteration 18878: 0.04277245900279198\n",
      "Train Loss at iteration 18879: 0.04277243398464859\n",
      "Train Loss at iteration 18880: 0.04277240896824644\n",
      "Train Loss at iteration 18881: 0.042772383953585236\n",
      "Train Loss at iteration 18882: 0.042772358940664675\n",
      "Train Loss at iteration 18883: 0.04277233392948449\n",
      "Train Loss at iteration 18884: 0.042772308920044376\n",
      "Train Loss at iteration 18885: 0.042772283912344056\n",
      "Train Loss at iteration 18886: 0.04277225890638322\n",
      "Train Loss at iteration 18887: 0.042772233902161574\n",
      "Train Loss at iteration 18888: 0.042772208899678865\n",
      "Train Loss at iteration 18889: 0.042772183898934765\n",
      "Train Loss at iteration 18890: 0.042772158899928976\n",
      "Train Loss at iteration 18891: 0.04277213390266126\n",
      "Train Loss at iteration 18892: 0.04277210890713127\n",
      "Train Loss at iteration 18893: 0.042772083913338754\n",
      "Train Loss at iteration 18894: 0.0427720589212834\n",
      "Train Loss at iteration 18895: 0.042772033930964924\n",
      "Train Loss at iteration 18896: 0.04277200894238304\n",
      "Train Loss at iteration 18897: 0.04277198395553745\n",
      "Train Loss at iteration 18898: 0.04277195897042786\n",
      "Train Loss at iteration 18899: 0.04277193398705399\n",
      "Train Loss at iteration 18900: 0.04277190900541556\n",
      "Train Loss at iteration 18901: 0.04277188402551226\n",
      "Train Loss at iteration 18902: 0.04277185904734382\n",
      "Train Loss at iteration 18903: 0.04277183407090992\n",
      "Train Loss at iteration 18904: 0.0427718090962103\n",
      "Train Loss at iteration 18905: 0.04277178412324465\n",
      "Train Loss at iteration 18906: 0.042771759152012694\n",
      "Train Loss at iteration 18907: 0.04277173418251412\n",
      "Train Loss at iteration 18908: 0.042771709214748675\n",
      "Train Loss at iteration 18909: 0.042771684248716045\n",
      "Train Loss at iteration 18910: 0.04277165928441593\n",
      "Train Loss at iteration 18911: 0.042771634321848065\n",
      "Train Loss at iteration 18912: 0.04277160936101216\n",
      "Train Loss at iteration 18913: 0.04277158440190792\n",
      "Train Loss at iteration 18914: 0.04277155944453503\n",
      "Train Loss at iteration 18915: 0.042771534488893224\n",
      "Train Loss at iteration 18916: 0.04277150953498223\n",
      "Train Loss at iteration 18917: 0.04277148458280172\n",
      "Train Loss at iteration 18918: 0.04277145963235145\n",
      "Train Loss at iteration 18919: 0.042771434683631086\n",
      "Train Loss at iteration 18920: 0.04277140973664036\n",
      "Train Loss at iteration 18921: 0.04277138479137899\n",
      "Train Loss at iteration 18922: 0.04277135984784667\n",
      "Train Loss at iteration 18923: 0.04277133490604314\n",
      "Train Loss at iteration 18924: 0.04277130996596806\n",
      "Train Loss at iteration 18925: 0.0427712850276212\n",
      "Train Loss at iteration 18926: 0.042771260091002225\n",
      "Train Loss at iteration 18927: 0.042771235156110886\n",
      "Train Loss at iteration 18928: 0.042771210222946845\n",
      "Train Loss at iteration 18929: 0.04277118529150986\n",
      "Train Loss at iteration 18930: 0.04277116036179962\n",
      "Train Loss at iteration 18931: 0.042771135433815846\n",
      "Train Loss at iteration 18932: 0.04277111050755823\n",
      "Train Loss at iteration 18933: 0.042771085583026516\n",
      "Train Loss at iteration 18934: 0.042771060660220385\n",
      "Train Loss at iteration 18935: 0.04277103573913956\n",
      "Train Loss at iteration 18936: 0.04277101081978375\n",
      "Train Loss at iteration 18937: 0.04277098590215268\n",
      "Train Loss at iteration 18938: 0.04277096098624606\n",
      "Train Loss at iteration 18939: 0.04277093607206357\n",
      "Train Loss at iteration 18940: 0.04277091115960496\n",
      "Train Loss at iteration 18941: 0.04277088624886993\n",
      "Train Loss at iteration 18942: 0.04277086133985819\n",
      "Train Loss at iteration 18943: 0.04277083643256946\n",
      "Train Loss at iteration 18944: 0.04277081152700343\n",
      "Train Loss at iteration 18945: 0.04277078662315984\n",
      "Train Loss at iteration 18946: 0.04277076172103837\n",
      "Train Loss at iteration 18947: 0.042770736820638755\n",
      "Train Loss at iteration 18948: 0.04277071192196072\n",
      "Train Loss at iteration 18949: 0.042770687025003945\n",
      "Train Loss at iteration 18950: 0.04277066212976816\n",
      "Train Loss at iteration 18951: 0.04277063723625308\n",
      "Train Loss at iteration 18952: 0.042770612344458406\n",
      "Train Loss at iteration 18953: 0.04277058745438388\n",
      "Train Loss at iteration 18954: 0.04277056256602917\n",
      "Train Loss at iteration 18955: 0.04277053767939401\n",
      "Train Loss at iteration 18956: 0.04277051279447813\n",
      "Train Loss at iteration 18957: 0.04277048791128121\n",
      "Train Loss at iteration 18958: 0.04277046302980299\n",
      "Train Loss at iteration 18959: 0.042770438150043166\n",
      "Train Loss at iteration 18960: 0.04277041327200146\n",
      "Train Loss at iteration 18961: 0.04277038839567759\n",
      "Train Loss at iteration 18962: 0.04277036352107125\n",
      "Train Loss at iteration 18963: 0.04277033864818218\n",
      "Train Loss at iteration 18964: 0.04277031377701006\n",
      "Train Loss at iteration 18965: 0.042770288907554625\n",
      "Train Loss at iteration 18966: 0.042770264039815584\n",
      "Train Loss at iteration 18967: 0.04277023917379265\n",
      "Train Loss at iteration 18968: 0.042770214309485535\n",
      "Train Loss at iteration 18969: 0.04277018944689395\n",
      "Train Loss at iteration 18970: 0.04277016458601764\n",
      "Train Loss at iteration 18971: 0.04277013972685627\n",
      "Train Loss at iteration 18972: 0.04277011486940956\n",
      "Train Loss at iteration 18973: 0.04277009001367725\n",
      "Train Loss at iteration 18974: 0.04277006515965904\n",
      "Train Loss at iteration 18975: 0.042770040307354665\n",
      "Train Loss at iteration 18976: 0.042770015456763795\n",
      "Train Loss at iteration 18977: 0.04276999060788618\n",
      "Train Loss at iteration 18978: 0.042769965760721514\n",
      "Train Loss at iteration 18979: 0.04276994091526952\n",
      "Train Loss at iteration 18980: 0.042769916071529905\n",
      "Train Loss at iteration 18981: 0.042769891229502394\n",
      "Train Loss at iteration 18982: 0.0427698663891867\n",
      "Train Loss at iteration 18983: 0.04276984155058252\n",
      "Train Loss at iteration 18984: 0.04276981671368959\n",
      "Train Loss at iteration 18985: 0.04276979187850761\n",
      "Train Loss at iteration 18986: 0.042769767045036294\n",
      "Train Loss at iteration 18987: 0.042769742213275376\n",
      "Train Loss at iteration 18988: 0.042769717383224534\n",
      "Train Loss at iteration 18989: 0.04276969255488352\n",
      "Train Loss at iteration 18990: 0.04276966772825203\n",
      "Train Loss at iteration 18991: 0.04276964290332978\n",
      "Train Loss at iteration 18992: 0.042769618080116485\n",
      "Train Loss at iteration 18993: 0.042769593258611856\n",
      "Train Loss at iteration 18994: 0.04276956843881562\n",
      "Train Loss at iteration 18995: 0.042769543620727456\n",
      "Train Loss at iteration 18996: 0.04276951880434713\n",
      "Train Loss at iteration 18997: 0.04276949398967433\n",
      "Train Loss at iteration 18998: 0.04276946917670877\n",
      "Train Loss at iteration 18999: 0.042769444365450174\n",
      "Train Loss at iteration 19000: 0.04276941955589823\n",
      "Train Loss at iteration 19001: 0.04276939474805271\n",
      "Train Loss at iteration 19002: 0.04276936994191326\n",
      "Train Loss at iteration 19003: 0.04276934513747963\n",
      "Train Loss at iteration 19004: 0.04276932033475155\n",
      "Train Loss at iteration 19005: 0.04276929553372871\n",
      "Train Loss at iteration 19006: 0.042769270734410836\n",
      "Train Loss at iteration 19007: 0.04276924593679764\n",
      "Train Loss at iteration 19008: 0.04276922114088883\n",
      "Train Loss at iteration 19009: 0.042769196346684135\n",
      "Train Loss at iteration 19010: 0.042769171554183257\n",
      "Train Loss at iteration 19011: 0.04276914676338593\n",
      "Train Loss at iteration 19012: 0.04276912197429186\n",
      "Train Loss at iteration 19013: 0.04276909718690075\n",
      "Train Loss at iteration 19014: 0.04276907240121233\n",
      "Train Loss at iteration 19015: 0.042769047617226316\n",
      "Train Loss at iteration 19016: 0.04276902283494242\n",
      "Train Loss at iteration 19017: 0.04276899805436036\n",
      "Train Loss at iteration 19018: 0.042768973275479835\n",
      "Train Loss at iteration 19019: 0.04276894849830058\n",
      "Train Loss at iteration 19020: 0.04276892372282231\n",
      "Train Loss at iteration 19021: 0.04276889894904474\n",
      "Train Loss at iteration 19022: 0.042768874176967585\n",
      "Train Loss at iteration 19023: 0.04276884940659055\n",
      "Train Loss at iteration 19024: 0.042768824637913365\n",
      "Train Loss at iteration 19025: 0.042768799870935736\n",
      "Train Loss at iteration 19026: 0.042768775105657394\n",
      "Train Loss at iteration 19027: 0.04276875034207805\n",
      "Train Loss at iteration 19028: 0.0427687255801974\n",
      "Train Loss at iteration 19029: 0.042768700820015174\n",
      "Train Loss at iteration 19030: 0.042768676061531105\n",
      "Train Loss at iteration 19031: 0.042768651304744894\n",
      "Train Loss at iteration 19032: 0.04276862654965625\n",
      "Train Loss at iteration 19033: 0.04276860179626489\n",
      "Train Loss at iteration 19034: 0.04276857704457055\n",
      "Train Loss at iteration 19035: 0.04276855229457293\n",
      "Train Loss at iteration 19036: 0.042768527546271765\n",
      "Train Loss at iteration 19037: 0.04276850279966675\n",
      "Train Loss at iteration 19038: 0.0427684780547576\n",
      "Train Loss at iteration 19039: 0.04276845331154405\n",
      "Train Loss at iteration 19040: 0.042768428570025815\n",
      "Train Loss at iteration 19041: 0.042768403830202605\n",
      "Train Loss at iteration 19042: 0.04276837909207412\n",
      "Train Loss at iteration 19043: 0.04276835435564012\n",
      "Train Loss at iteration 19044: 0.04276832962090028\n",
      "Train Loss at iteration 19045: 0.04276830488785436\n",
      "Train Loss at iteration 19046: 0.04276828015650202\n",
      "Train Loss at iteration 19047: 0.04276825542684301\n",
      "Train Loss at iteration 19048: 0.042768230698877065\n",
      "Train Loss at iteration 19049: 0.04276820597260387\n",
      "Train Loss at iteration 19050: 0.04276818124802315\n",
      "Train Loss at iteration 19051: 0.04276815652513464\n",
      "Train Loss at iteration 19052: 0.04276813180393803\n",
      "Train Loss at iteration 19053: 0.042768107084433064\n",
      "Train Loss at iteration 19054: 0.04276808236661944\n",
      "Train Loss at iteration 19055: 0.042768057650496905\n",
      "Train Loss at iteration 19056: 0.042768032936065126\n",
      "Train Loss at iteration 19057: 0.04276800822332386\n",
      "Train Loss at iteration 19058: 0.04276798351227282\n",
      "Train Loss at iteration 19059: 0.042767958802911725\n",
      "Train Loss at iteration 19060: 0.04276793409524026\n",
      "Train Loss at iteration 19061: 0.0427679093892582\n",
      "Train Loss at iteration 19062: 0.042767884684965216\n",
      "Train Loss at iteration 19063: 0.04276785998236103\n",
      "Train Loss at iteration 19064: 0.04276783528144539\n",
      "Train Loss at iteration 19065: 0.04276781058221799\n",
      "Train Loss at iteration 19066: 0.04276778588467854\n",
      "Train Loss at iteration 19067: 0.0427677611888268\n",
      "Train Loss at iteration 19068: 0.04276773649466246\n",
      "Train Loss at iteration 19069: 0.04276771180218523\n",
      "Train Loss at iteration 19070: 0.04276768711139483\n",
      "Train Loss at iteration 19071: 0.04276766242229099\n",
      "Train Loss at iteration 19072: 0.04276763773487342\n",
      "Train Loss at iteration 19073: 0.04276761304914184\n",
      "Train Loss at iteration 19074: 0.04276758836509597\n",
      "Train Loss at iteration 19075: 0.04276756368273554\n",
      "Train Loss at iteration 19076: 0.04276753900206025\n",
      "Train Loss at iteration 19077: 0.04276751432306984\n",
      "Train Loss at iteration 19078: 0.042767489645764004\n",
      "Train Loss at iteration 19079: 0.04276746497014247\n",
      "Train Loss at iteration 19080: 0.04276744029620497\n",
      "Train Loss at iteration 19081: 0.042767415623951206\n",
      "Train Loss at iteration 19082: 0.0427673909533809\n",
      "Train Loss at iteration 19083: 0.04276736628449377\n",
      "Train Loss at iteration 19084: 0.04276734161728953\n",
      "Train Loss at iteration 19085: 0.042767316951767935\n",
      "Train Loss at iteration 19086: 0.04276729228792866\n",
      "Train Loss at iteration 19087: 0.042767267625771446\n",
      "Train Loss at iteration 19088: 0.04276724296529601\n",
      "Train Loss at iteration 19089: 0.04276721830650206\n",
      "Train Loss at iteration 19090: 0.042767193649389315\n",
      "Train Loss at iteration 19091: 0.042767168993957515\n",
      "Train Loss at iteration 19092: 0.04276714434020636\n",
      "Train Loss at iteration 19093: 0.04276711968813559\n",
      "Train Loss at iteration 19094: 0.04276709503774489\n",
      "Train Loss at iteration 19095: 0.04276707038903403\n",
      "Train Loss at iteration 19096: 0.04276704574200268\n",
      "Train Loss at iteration 19097: 0.04276702109665058\n",
      "Train Loss at iteration 19098: 0.042766996452977456\n",
      "Train Loss at iteration 19099: 0.04276697181098301\n",
      "Train Loss at iteration 19100: 0.042766947170666994\n",
      "Train Loss at iteration 19101: 0.04276692253202908\n",
      "Train Loss at iteration 19102: 0.04276689789506903\n",
      "Train Loss at iteration 19103: 0.04276687325978654\n",
      "Train Loss at iteration 19104: 0.04276684862618137\n",
      "Train Loss at iteration 19105: 0.04276682399425317\n",
      "Train Loss at iteration 19106: 0.04276679936400171\n",
      "Train Loss at iteration 19107: 0.0427667747354267\n",
      "Train Loss at iteration 19108: 0.04276675010852785\n",
      "Train Loss at iteration 19109: 0.0427667254833049\n",
      "Train Loss at iteration 19110: 0.04276670085975757\n",
      "Train Loss at iteration 19111: 0.042766676237885555\n",
      "Train Loss at iteration 19112: 0.04276665161768859\n",
      "Train Loss at iteration 19113: 0.0427666269991664\n",
      "Train Loss at iteration 19114: 0.04276660238231869\n",
      "Train Loss at iteration 19115: 0.042766577767145195\n",
      "Train Loss at iteration 19116: 0.042766553153645644\n",
      "Train Loss at iteration 19117: 0.04276652854181974\n",
      "Train Loss at iteration 19118: 0.0427665039316672\n",
      "Train Loss at iteration 19119: 0.04276647932318776\n",
      "Train Loss at iteration 19120: 0.04276645471638112\n",
      "Train Loss at iteration 19121: 0.04276643011124704\n",
      "Train Loss at iteration 19122: 0.042766405507785196\n",
      "Train Loss at iteration 19123: 0.04276638090599535\n",
      "Train Loss at iteration 19124: 0.042766356305877176\n",
      "Train Loss at iteration 19125: 0.04276633170743044\n",
      "Train Loss at iteration 19126: 0.042766307110654836\n",
      "Train Loss at iteration 19127: 0.042766282515550084\n",
      "Train Loss at iteration 19128: 0.04276625792211591\n",
      "Train Loss at iteration 19129: 0.042766233330352066\n",
      "Train Loss at iteration 19130: 0.04276620874025823\n",
      "Train Loss at iteration 19131: 0.04276618415183414\n",
      "Train Loss at iteration 19132: 0.04276615956507951\n",
      "Train Loss at iteration 19133: 0.042766134979994076\n",
      "Train Loss at iteration 19134: 0.04276611039657756\n",
      "Train Loss at iteration 19135: 0.04276608581482966\n",
      "Train Loss at iteration 19136: 0.04276606123475011\n",
      "Train Loss at iteration 19137: 0.042766036656338643\n",
      "Train Loss at iteration 19138: 0.04276601207959498\n",
      "Train Loss at iteration 19139: 0.042765987504518814\n",
      "Train Loss at iteration 19140: 0.042765962931109895\n",
      "Train Loss at iteration 19141: 0.04276593835936794\n",
      "Train Loss at iteration 19142: 0.04276591378929266\n",
      "Train Loss at iteration 19143: 0.04276588922088379\n",
      "Train Loss at iteration 19144: 0.042765864654141054\n",
      "Train Loss at iteration 19145: 0.042765840089064144\n",
      "Train Loss at iteration 19146: 0.04276581552565282\n",
      "Train Loss at iteration 19147: 0.04276579096390678\n",
      "Train Loss at iteration 19148: 0.042765766403825754\n",
      "Train Loss at iteration 19149: 0.042765741845409475\n",
      "Train Loss at iteration 19150: 0.04276571728865764\n",
      "Train Loss at iteration 19151: 0.04276569273356999\n",
      "Train Loss at iteration 19152: 0.04276566818014625\n",
      "Train Loss at iteration 19153: 0.04276564362838613\n",
      "Train Loss at iteration 19154: 0.04276561907828936\n",
      "Train Loss at iteration 19155: 0.042765594529855656\n",
      "Train Loss at iteration 19156: 0.04276556998308474\n",
      "Train Loss at iteration 19157: 0.04276554543797635\n",
      "Train Loss at iteration 19158: 0.04276552089453018\n",
      "Train Loss at iteration 19159: 0.04276549635274597\n",
      "Train Loss at iteration 19160: 0.04276547181262346\n",
      "Train Loss at iteration 19161: 0.042765447274162344\n",
      "Train Loss at iteration 19162: 0.04276542273736236\n",
      "Train Loss at iteration 19163: 0.042765398202223234\n",
      "Train Loss at iteration 19164: 0.04276537366874467\n",
      "Train Loss at iteration 19165: 0.0427653491369264\n",
      "Train Loss at iteration 19166: 0.04276532460676816\n",
      "Train Loss at iteration 19167: 0.04276530007826965\n",
      "Train Loss at iteration 19168: 0.042765275551430615\n",
      "Train Loss at iteration 19169: 0.04276525102625077\n",
      "Train Loss at iteration 19170: 0.04276522650272982\n",
      "Train Loss at iteration 19171: 0.042765201980867515\n",
      "Train Loss at iteration 19172: 0.04276517746066357\n",
      "Train Loss at iteration 19173: 0.042765152942117704\n",
      "Train Loss at iteration 19174: 0.042765128425229636\n",
      "Train Loss at iteration 19175: 0.0427651039099991\n",
      "Train Loss at iteration 19176: 0.04276507939642583\n",
      "Train Loss at iteration 19177: 0.042765054884509504\n",
      "Train Loss at iteration 19178: 0.04276503037424989\n",
      "Train Loss at iteration 19179: 0.04276500586564671\n",
      "Train Loss at iteration 19180: 0.042764981358699675\n",
      "Train Loss at iteration 19181: 0.042764956853408476\n",
      "Train Loss at iteration 19182: 0.0427649323497729\n",
      "Train Loss at iteration 19183: 0.042764907847792626\n",
      "Train Loss at iteration 19184: 0.0427648833474674\n",
      "Train Loss at iteration 19185: 0.04276485884879692\n",
      "Train Loss at iteration 19186: 0.04276483435178094\n",
      "Train Loss at iteration 19187: 0.04276480985641918\n",
      "Train Loss at iteration 19188: 0.04276478536271135\n",
      "Train Loss at iteration 19189: 0.04276476087065718\n",
      "Train Loss at iteration 19190: 0.04276473638025639\n",
      "Train Loss at iteration 19191: 0.0427647118915087\n",
      "Train Loss at iteration 19192: 0.042764687404413845\n",
      "Train Loss at iteration 19193: 0.042764662918971554\n",
      "Train Loss at iteration 19194: 0.04276463843518155\n",
      "Train Loss at iteration 19195: 0.04276461395304354\n",
      "Train Loss at iteration 19196: 0.042764589958751736\n",
      "Train Loss at iteration 19197: 0.042764566242909974\n",
      "Train Loss at iteration 19198: 0.042764541440407824\n",
      "Train Loss at iteration 19199: 0.04276451698393161\n",
      "Train Loss at iteration 19200: 0.04276449377895503\n",
      "Train Loss at iteration 19201: 0.042764468975003335\n",
      "Train Loss at iteration 19202: 0.042764444491544\n",
      "Train Loss at iteration 19203: 0.04276442074693513\n",
      "Train Loss at iteration 19204: 0.042764396882401225\n",
      "Train Loss at iteration 19205: 0.04276437208176043\n",
      "Train Loss at iteration 19206: 0.04276434813522323\n",
      "Train Loss at iteration 19207: 0.04276432450604886\n",
      "Train Loss at iteration 19208: 0.04276429970150112\n",
      "Train Loss at iteration 19209: 0.042764275692922106\n",
      "Train Loss at iteration 19210: 0.0427642521592351\n",
      "Train Loss at iteration 19211: 0.04276422735200779\n",
      "Train Loss at iteration 19212: 0.04276420338867526\n",
      "Train Loss at iteration 19213: 0.04276417983881324\n",
      "Train Loss at iteration 19214: 0.04276415502848567\n",
      "Train Loss at iteration 19215: 0.042764131208783494\n",
      "Train Loss at iteration 19216: 0.04276410754083294\n",
      "Train Loss at iteration 19217: 0.04276408272721318\n",
      "Train Loss at iteration 19218: 0.042764059141558855\n",
      "Train Loss at iteration 19219: 0.04276403526226795\n",
      "Train Loss at iteration 19220: 0.04276401044533458\n",
      "Train Loss at iteration 19221: 0.042763987176932215\n",
      "Train Loss at iteration 19222: 0.042763963000811825\n",
      "Train Loss at iteration 19223: 0.04276393848789309\n",
      "Train Loss at iteration 19224: 0.04276391526050005\n",
      "Train Loss at iteration 19225: 0.0427638904186317\n",
      "Train Loss at iteration 19226: 0.04276386681755041\n",
      "Train Loss at iteration 19227: 0.04276384300731504\n",
      "Train Loss at iteration 19228: 0.04276381826990557\n",
      "Train Loss at iteration 19229: 0.042763795296372\n",
      "Train Loss at iteration 19230: 0.04276377044302937\n",
      "Train Loss at iteration 19231: 0.04276374676972364\n",
      "Train Loss at iteration 19232: 0.04276372305880008\n",
      "Train Loss at iteration 19233: 0.04276369834310602\n",
      "Train Loss at iteration 19234: 0.04276367537201449\n",
      "Train Loss at iteration 19235: 0.042763650508091076\n",
      "Train Loss at iteration 19236: 0.04276362698962918\n",
      "Train Loss at iteration 19237: 0.04276360314666313\n",
      "Train Loss at iteration 19238: 0.042763578668157755\n",
      "Train Loss at iteration 19239: 0.04276355548053652\n",
      "Train Loss at iteration 19240: 0.042763530607211594\n",
      "Train Loss at iteration 19241: 0.04276350744281507\n",
      "Train Loss at iteration 19242: 0.04276348326571515\n",
      "Train Loss at iteration 19243: 0.04276345921372825\n",
      "Train Loss at iteration 19244: 0.0427634356179349\n",
      "Train Loss at iteration 19245: 0.04276341113045835\n",
      "Train Loss at iteration 19246: 0.04276338796237451\n",
      "Train Loss at iteration 19247: 0.04276336309961891\n",
      "Train Loss at iteration 19248: 0.04276334031111456\n",
      "Train Loss at iteration 19249: 0.04276331541261601\n",
      "Train Loss at iteration 19250: 0.042763292113916475\n",
      "Train Loss at iteration 19251: 0.04276326809944892\n",
      "Train Loss at iteration 19252: 0.042763244664281615\n",
      "Train Loss at iteration 19253: 0.042763221323913365\n",
      "Train Loss at iteration 19254: 0.04276319698205953\n",
      "Train Loss at iteration 19255: 0.04276317368699821\n",
      "Train Loss at iteration 19256: 0.0427631490971185\n",
      "Train Loss at iteration 19257: 0.042763126893111535\n",
      "Train Loss at iteration 19258: 0.04276310209594844\n",
      "Train Loss at iteration 19259: 0.04276307927300823\n",
      "Train Loss at iteration 19260: 0.042763054351316415\n",
      "Train Loss at iteration 19261: 0.04276303154906638\n",
      "Train Loss at iteration 19262: 0.04276300791394734\n",
      "Train Loss at iteration 19263: 0.04276298427578861\n",
      "Train Loss at iteration 19264: 0.04276296031144727\n",
      "Train Loss at iteration 19265: 0.04276293649590518\n",
      "Train Loss at iteration 19266: 0.04276291317251641\n",
      "Train Loss at iteration 19267: 0.042762889601158954\n",
      "Train Loss at iteration 19268: 0.0427628659424914\n",
      "Train Loss at iteration 19269: 0.04276284188609896\n",
      "Train Loss at iteration 19270: 0.04276281833726627\n",
      "Train Loss at iteration 19271: 0.04276279492050047\n",
      "Train Loss at iteration 19272: 0.042762771585877976\n",
      "Train Loss at iteration 19273: 0.04276274739004826\n",
      "Train Loss at iteration 19274: 0.04276272398099036\n",
      "Train Loss at iteration 19275: 0.042762699879873745\n",
      "Train Loss at iteration 19276: 0.04276267723581032\n",
      "Train Loss at iteration 19277: 0.042762652995346935\n",
      "Train Loss at iteration 19278: 0.04276262963552896\n",
      "Train Loss at iteration 19279: 0.04276260540524974\n",
      "Train Loss at iteration 19280: 0.0427625824310411\n",
      "Train Loss at iteration 19281: 0.0427625586926952\n",
      "Train Loss at iteration 19282: 0.042762535301140284\n",
      "Train Loss at iteration 19283: 0.04276251114907298\n",
      "Train Loss at iteration 19284: 0.04276248771246873\n",
      "Train Loss at iteration 19285: 0.04276246428260984\n",
      "Train Loss at iteration 19286: 0.04276244098159754\n",
      "Train Loss at iteration 19287: 0.042762416978096796\n",
      "Train Loss at iteration 19288: 0.042762393394482484\n",
      "Train Loss at iteration 19289: 0.04276236956869838\n",
      "Train Loss at iteration 19290: 0.042762346671488864\n",
      "Train Loss at iteration 19291: 0.042762322884224005\n",
      "Train Loss at iteration 19292: 0.04276229909040934\n",
      "Train Loss at iteration 19293: 0.04276227543450368\n",
      "Train Loss at iteration 19294: 0.04276225187489037\n",
      "Train Loss at iteration 19295: 0.04276222886151207\n",
      "Train Loss at iteration 19296: 0.042762204800672186\n",
      "Train Loss at iteration 19297: 0.042762181448833037\n",
      "Train Loss at iteration 19298: 0.042762157233417836\n",
      "Train Loss at iteration 19299: 0.04276213469939582\n",
      "Train Loss at iteration 19300: 0.042762110529140133\n",
      "Train Loss at iteration 19301: 0.04276208753159591\n",
      "Train Loss at iteration 19302: 0.042762062965040815\n",
      "Train Loss at iteration 19303: 0.04276204024963541\n",
      "Train Loss at iteration 19304: 0.042762016270434196\n",
      "Train Loss at iteration 19305: 0.04276199367692654\n",
      "Train Loss at iteration 19306: 0.042761968909857215\n",
      "Train Loss at iteration 19307: 0.042761945903370124\n",
      "Train Loss at iteration 19308: 0.04276192232921703\n",
      "Train Loss at iteration 19309: 0.04276189910419621\n",
      "Train Loss at iteration 19310: 0.042761875530607396\n",
      "Train Loss at iteration 19311: 0.042761851537572795\n",
      "Train Loss at iteration 19312: 0.04276182825833221\n",
      "Train Loss at iteration 19313: 0.04276180467399693\n",
      "Train Loss at iteration 19314: 0.04276178186098238\n",
      "Train Loss at iteration 19315: 0.04276175729102276\n",
      "Train Loss at iteration 19316: 0.04276173461476706\n",
      "Train Loss at iteration 19317: 0.042761710062251455\n",
      "Train Loss at iteration 19318: 0.04276168782511443\n",
      "Train Loss at iteration 19319: 0.042761663910648076\n",
      "Train Loss at iteration 19320: 0.042761640168652686\n",
      "Train Loss at iteration 19321: 0.042761616731956616\n",
      "Train Loss at iteration 19322: 0.042761593071783954\n",
      "Train Loss at iteration 19323: 0.042761570415147984\n",
      "Train Loss at iteration 19324: 0.042761545931931905\n",
      "Train Loss at iteration 19325: 0.042761523245016295\n",
      "Train Loss at iteration 19326: 0.0427614986060101\n",
      "Train Loss at iteration 19327: 0.04276147640714481\n",
      "Train Loss at iteration 19328: 0.04276145266771115\n",
      "Train Loss at iteration 19329: 0.042761428844868554\n",
      "Train Loss at iteration 19330: 0.04276140556272177\n",
      "Train Loss at iteration 19331: 0.04276138153361815\n",
      "Train Loss at iteration 19332: 0.042761359316549834\n",
      "Train Loss at iteration 19333: 0.04276133472111856\n",
      "Train Loss at iteration 19334: 0.04276131187708303\n",
      "Train Loss at iteration 19335: 0.04276128797250885\n",
      "Train Loss at iteration 19336: 0.04276126479994071\n",
      "Train Loss at iteration 19337: 0.04276124178614557\n",
      "Train Loss at iteration 19338: 0.0427612175591892\n",
      "Train Loss at iteration 19339: 0.04276119472887199\n",
      "Train Loss at iteration 19340: 0.04276117019932167\n",
      "Train Loss at iteration 19341: 0.042761148162085684\n",
      "Train Loss at iteration 19342: 0.042761124340886685\n",
      "Train Loss at iteration 19343: 0.04276110052309472\n",
      "Train Loss at iteration 19344: 0.04276107733622564\n",
      "Train Loss at iteration 19345: 0.0427610533601285\n",
      "Train Loss at iteration 19346: 0.04276103119466567\n",
      "Train Loss at iteration 19347: 0.04276100667057175\n",
      "Train Loss at iteration 19348: 0.04276098361260927\n",
      "Train Loss at iteration 19349: 0.0427609600192584\n",
      "Train Loss at iteration 19350: 0.0427609366996902\n",
      "Train Loss at iteration 19351: 0.04276091393130006\n",
      "Train Loss at iteration 19352: 0.04276088943725263\n",
      "Train Loss at iteration 19353: 0.04276086661563755\n",
      "Train Loss at iteration 19354: 0.04276084301478515\n",
      "Train Loss at iteration 19355: 0.04276081986042571\n",
      "Train Loss at iteration 19356: 0.04276079677491353\n",
      "Train Loss at iteration 19357: 0.04276077233535424\n",
      "Train Loss at iteration 19358: 0.04276074984462901\n",
      "Train Loss at iteration 19359: 0.0427607259174948\n",
      "Train Loss at iteration 19360: 0.04276070299955163\n",
      "Train Loss at iteration 19361: 0.04276067967608883\n",
      "Train Loss at iteration 19362: 0.042760655383059336\n",
      "Train Loss at iteration 19363: 0.04276063282279437\n",
      "Train Loss at iteration 19364: 0.04276060922621923\n",
      "Train Loss at iteration 19365: 0.042760586053584224\n",
      "Train Loss at iteration 19366: 0.04276056268445715\n",
      "Train Loss at iteration 19367: 0.04276053844447977\n",
      "Train Loss at iteration 19368: 0.04276051622792502\n",
      "Train Loss at iteration 19369: 0.042760492292114434\n",
      "Train Loss at iteration 19370: 0.042760469128101386\n",
      "Train Loss at iteration 19371: 0.04276044578290006\n",
      "Train Loss at iteration 19372: 0.04276042152330723\n",
      "Train Loss at iteration 19373: 0.04276039972319809\n",
      "Train Loss at iteration 19374: 0.04276037544531854\n",
      "Train Loss at iteration 19375: 0.042760352221351494\n",
      "Train Loss at iteration 19376: 0.04276032896703487\n",
      "Train Loss at iteration 19377: 0.042760304872129215\n",
      "Train Loss at iteration 19378: 0.04276028305529949\n",
      "Train Loss at iteration 19379: 0.04276025867878249\n",
      "Train Loss at iteration 19380: 0.04276023533676557\n",
      "Train Loss at iteration 19381: 0.04276021222962129\n",
      "Train Loss at iteration 19382: 0.04276018851176555\n",
      "Train Loss at iteration 19383: 0.0427601659580128\n",
      "Train Loss at iteration 19384: 0.042760142339671244\n",
      "Train Loss at iteration 19385: 0.042760118354423\n",
      "Train Loss at iteration 19386: 0.04276009573089645\n",
      "Train Loss at iteration 19387: 0.042760072145118413\n",
      "Train Loss at iteration 19388: 0.04276004908573495\n",
      "Train Loss at iteration 19389: 0.04276002574531771\n",
      "Train Loss at iteration 19390: 0.042760001496912836\n",
      "Train Loss at iteration 19391: 0.04275997954429428\n",
      "Train Loss at iteration 19392: 0.04275995559446667\n",
      "Train Loss at iteration 19393: 0.04275993224814121\n",
      "Train Loss at iteration 19394: 0.04275990922071377\n",
      "Train Loss at iteration 19395: 0.042759884945027915\n",
      "Train Loss at iteration 19396: 0.042759862907808824\n",
      "Train Loss at iteration 19397: 0.042759839458579446\n",
      "Train Loss at iteration 19398: 0.04275981531980194\n",
      "Train Loss at iteration 19399: 0.042759792796876105\n",
      "Train Loss at iteration 19400: 0.042759768912700974\n",
      "Train Loss at iteration 19401: 0.04275974609669925\n",
      "Train Loss at iteration 19402: 0.04275972306104061\n",
      "Train Loss at iteration 19403: 0.04275969880115944\n",
      "Train Loss at iteration 19404: 0.04275967589531526\n",
      "Train Loss at iteration 19405: 0.04275965329546999\n",
      "Train Loss at iteration 19406: 0.04275962919834997\n",
      "Train Loss at iteration 19407: 0.042759606761447494\n",
      "Train Loss at iteration 19408: 0.04275958250164871\n",
      "Train Loss at iteration 19409: 0.042759559429124676\n",
      "Train Loss at iteration 19410: 0.042759537098106176\n",
      "Train Loss at iteration 19411: 0.042759512869669884\n",
      "Train Loss at iteration 19412: 0.0427594898181533\n",
      "Train Loss at iteration 19413: 0.042759466605742376\n",
      "Train Loss at iteration 19414: 0.042759443104884456\n",
      "Train Loss at iteration 19415: 0.042759420525212734\n",
      "Train Loss at iteration 19416: 0.04275939702962036\n",
      "Train Loss at iteration 19417: 0.042759372954952336\n",
      "Train Loss at iteration 19418: 0.04275935070685909\n",
      "Train Loss at iteration 19419: 0.04275932714063558\n",
      "Train Loss at iteration 19420: 0.04275930378244524\n",
      "Train Loss at iteration 19421: 0.0427592809140751\n",
      "Train Loss at iteration 19422: 0.042759256724363426\n",
      "Train Loss at iteration 19423: 0.04275923428174278\n",
      "Train Loss at iteration 19424: 0.042759211400525354\n",
      "Train Loss at iteration 19425: 0.04275918722747427\n",
      "Train Loss at iteration 19426: 0.04275916437490344\n",
      "Train Loss at iteration 19427: 0.042759141208932566\n",
      "Train Loss at iteration 19428: 0.042759117762469975\n",
      "Train Loss at iteration 19429: 0.04275909511466429\n",
      "Train Loss at iteration 19430: 0.042759071589979165\n",
      "Train Loss at iteration 19431: 0.04275904756340177\n",
      "Train Loss at iteration 19432: 0.042759025688639894\n",
      "Train Loss at iteration 19433: 0.04275900182649726\n",
      "Train Loss at iteration 19434: 0.04275897843127092\n",
      "Train Loss at iteration 19435: 0.042758955672146166\n",
      "Train Loss at iteration 19436: 0.04275893164681046\n",
      "Train Loss at iteration 19437: 0.042758909213038813\n",
      "Train Loss at iteration 19438: 0.04275888628095326\n",
      "Train Loss at iteration 19439: 0.04275886215159328\n",
      "Train Loss at iteration 19440: 0.0427588391095705\n",
      "Train Loss at iteration 19441: 0.04275881657258629\n",
      "Train Loss at iteration 19442: 0.042758792803917645\n",
      "Train Loss at iteration 19443: 0.042758769894802526\n",
      "Train Loss at iteration 19444: 0.042758746699542494\n",
      "Train Loss at iteration 19445: 0.042758722672311275\n",
      "Train Loss at iteration 19446: 0.04275870069019182\n",
      "Train Loss at iteration 19447: 0.042758677394109\n",
      "Train Loss at iteration 19448: 0.042758653299850194\n",
      "Train Loss at iteration 19449: 0.04275863060212023\n",
      "Train Loss at iteration 19450: 0.04275860773376542\n",
      "Train Loss at iteration 19451: 0.042758584033997446\n",
      "Train Loss at iteration 19452: 0.042758561405680566\n",
      "Train Loss at iteration 19453: 0.04275853797567587\n",
      "Train Loss at iteration 19454: 0.04275851395080184\n",
      "Train Loss at iteration 19455: 0.04275849222134895\n",
      "Train Loss at iteration 19456: 0.042758468748957876\n",
      "Train Loss at iteration 19457: 0.04275844469821055\n",
      "Train Loss at iteration 19458: 0.04275842239317899\n",
      "Train Loss at iteration 19459: 0.04275839879334138\n",
      "Train Loss at iteration 19460: 0.0427583756506104\n",
      "Train Loss at iteration 19461: 0.04275835317422356\n",
      "Train Loss at iteration 19462: 0.042758329116338464\n",
      "Train Loss at iteration 19463: 0.04275830565771522\n",
      "Train Loss at iteration 19464: 0.042758283992323856\n",
      "Train Loss at iteration 19465: 0.04275825994690432\n",
      "Train Loss at iteration 19466: 0.04275823648953145\n",
      "Train Loss at iteration 19467: 0.042758213947913896\n",
      "Train Loss at iteration 19468: 0.04275819039202405\n",
      "Train Loss at iteration 19469: 0.042758167368153854\n",
      "Train Loss at iteration 19470: 0.042758144818805716\n",
      "Train Loss at iteration 19471: 0.04275812079145217\n",
      "Train Loss at iteration 19472: 0.04275809739978878\n",
      "Train Loss at iteration 19473: 0.04275807571158328\n",
      "Train Loss at iteration 19474: 0.04275805169570251\n",
      "Train Loss at iteration 19475: 0.042758028251556696\n",
      "Train Loss at iteration 19476: 0.04275800573890121\n",
      "Train Loss at iteration 19477: 0.04275798222175381\n",
      "Train Loss at iteration 19478: 0.04275795915553074\n",
      "Train Loss at iteration 19479: 0.042757936681246354\n",
      "Train Loss at iteration 19480: 0.042757912681758124\n",
      "Train Loss at iteration 19481: 0.042757889227982106\n",
      "Train Loss at iteration 19482: 0.04275786764413326\n",
      "Train Loss at iteration 19483: 0.04275784365540881\n",
      "Train Loss at iteration 19484: 0.042757820092322624\n",
      "Train Loss at iteration 19485: 0.04275779773891156\n",
      "Train Loss at iteration 19486: 0.04275777427069027\n",
      "Train Loss at iteration 19487: 0.0427577510254365\n",
      "Train Loss at iteration 19488: 0.0427577287489707\n",
      "Train Loss at iteration 19489: 0.04275770477536765\n",
      "Train Loss at iteration 19490: 0.04275768121967286\n",
      "Train Loss at iteration 19491: 0.04275765938855685\n",
      "Train Loss at iteration 19492: 0.04275763615805845\n",
      "Train Loss at iteration 19493: 0.04275761220897674\n",
      "Train Loss at iteration 19494: 0.04275758940459599\n",
      "Train Loss at iteration 19495: 0.042757566888757786\n",
      "Train Loss at iteration 19496: 0.04275754329637067\n",
      "Train Loss at iteration 19497: 0.04275752034111431\n",
      "Train Loss at iteration 19498: 0.04275749744238982\n",
      "Train Loss at iteration 19499: 0.04275747361917361\n",
      "Train Loss at iteration 19500: 0.0427574512962777\n",
      "Train Loss at iteration 19501: 0.04275742856009506\n",
      "Train Loss at iteration 19502: 0.042757404629964316\n",
      "Train Loss at iteration 19503: 0.04275738136728163\n",
      "Train Loss at iteration 19504: 0.04275735938400631\n",
      "Train Loss at iteration 19505: 0.042757335775962246\n",
      "Train Loss at iteration 19506: 0.04275731234498775\n",
      "Train Loss at iteration 19507: 0.04275728995776635\n",
      "Train Loss at iteration 19508: 0.04275726620110615\n",
      "Train Loss at iteration 19509: 0.04275724335761172\n",
      "Train Loss at iteration 19510: 0.0427572207874974\n",
      "Train Loss at iteration 19511: 0.04275719756705759\n",
      "Train Loss at iteration 19512: 0.04275717367435447\n",
      "Train Loss at iteration 19513: 0.042757151493838665\n",
      "Train Loss at iteration 19514: 0.04275712878632741\n",
      "Train Loss at iteration 19515: 0.04275710489999523\n",
      "Train Loss at iteration 19516: 0.042757081854521074\n",
      "Train Loss at iteration 19517: 0.04275705934094235\n",
      "Train Loss at iteration 19518: 0.04275703614384083\n",
      "Train Loss at iteration 19519: 0.04275701286621773\n",
      "Train Loss at iteration 19520: 0.042756990381828716\n",
      "Train Loss at iteration 19521: 0.04275696650465647\n",
      "Train Loss at iteration 19522: 0.04275694369457991\n",
      "Train Loss at iteration 19523: 0.042756921652257245\n",
      "Train Loss at iteration 19524: 0.04275689778358732\n",
      "Train Loss at iteration 19525: 0.04275687424482559\n",
      "Train Loss at iteration 19526: 0.04275685179866057\n",
      "Train Loss at iteration 19527: 0.04275682941948236\n",
      "Train Loss at iteration 19528: 0.04275680557177017\n",
      "Train Loss at iteration 19529: 0.042756782533615104\n",
      "Train Loss at iteration 19530: 0.04275675985466204\n",
      "Train Loss at iteration 19531: 0.04275673677288452\n",
      "Train Loss at iteration 19532: 0.04275671358736234\n",
      "Train Loss at iteration 19533: 0.042756691202196884\n",
      "Train Loss at iteration 19534: 0.04275666736066699\n",
      "Train Loss at iteration 19535: 0.04275664420754634\n",
      "Train Loss at iteration 19536: 0.04275662214200573\n",
      "Train Loss at iteration 19537: 0.04275659907145685\n",
      "Train Loss at iteration 19538: 0.04275657524975056\n",
      "Train Loss at iteration 19539: 0.0427565522870817\n",
      "Train Loss at iteration 19540: 0.04275653042090466\n",
      "Train Loss at iteration 19541: 0.042756506658288\n",
      "Train Loss at iteration 19542: 0.04275648336443023\n",
      "Train Loss at iteration 19543: 0.042756460993165095\n",
      "Train Loss at iteration 19544: 0.04275643761717203\n",
      "Train Loss at iteration 19545: 0.042756414599128226\n",
      "Train Loss at iteration 19546: 0.042756391962837426\n",
      "Train Loss at iteration 19547: 0.042756368958560824\n",
      "Train Loss at iteration 19548: 0.04275634519992156\n",
      "Train Loss at iteration 19549: 0.04275632304176426\n",
      "Train Loss at iteration 19550: 0.042756300430536305\n",
      "Train Loss at iteration 19551: 0.04275627663677338\n",
      "Train Loss at iteration 19552: 0.04275625325275354\n",
      "Train Loss at iteration 19553: 0.042756231551827666\n",
      "Train Loss at iteration 19554: 0.042756208127663446\n",
      "Train Loss at iteration 19555: 0.04275618466944431\n",
      "Train Loss at iteration 19556: 0.04275616189912148\n",
      "Train Loss at iteration 19557: 0.04275613921300047\n",
      "Train Loss at iteration 19558: 0.04275611620361984\n",
      "Train Loss at iteration 19559: 0.042756093008393704\n",
      "Train Loss at iteration 19560: 0.042756070606890476\n",
      "Train Loss at iteration 19561: 0.042756046838536094\n",
      "Train Loss at iteration 19562: 0.04275602405287453\n",
      "Train Loss at iteration 19563: 0.04275600168265408\n",
      "Train Loss at iteration 19564: 0.042755978734535044\n",
      "Train Loss at iteration 19565: 0.04275595498431617\n",
      "Train Loss at iteration 19566: 0.042755932161752615\n",
      "Train Loss at iteration 19567: 0.04275591031971807\n",
      "Train Loss at iteration 19568: 0.042755886572722356\n",
      "Train Loss at iteration 19569: 0.042755863153926085\n",
      "Train Loss at iteration 19570: 0.04275584060450385\n",
      "Train Loss at iteration 19571: 0.04275581837934883\n",
      "Train Loss at iteration 19572: 0.04275579478532567\n",
      "Train Loss at iteration 19573: 0.042755771748537216\n",
      "Train Loss at iteration 19574: 0.04275574924592371\n",
      "Train Loss at iteration 19575: 0.04275572589040517\n",
      "Train Loss at iteration 19576: 0.042755703021521936\n",
      "Train Loss at iteration 19577: 0.04275568047605544\n",
      "Train Loss at iteration 19578: 0.0427556575045284\n",
      "Train Loss at iteration 19579: 0.04275563379093123\n",
      "Train Loss at iteration 19580: 0.04275561163460727\n",
      "Train Loss at iteration 19581: 0.04275558918305343\n",
      "Train Loss at iteration 19582: 0.042755565472647486\n",
      "Train Loss at iteration 19583: 0.04275554209002297\n",
      "Train Loss at iteration 19584: 0.04275552000783539\n",
      "Train Loss at iteration 19585: 0.042755497505335166\n",
      "Train Loss at iteration 19586: 0.04275547381090832\n",
      "Train Loss at iteration 19587: 0.04275545069117878\n",
      "Train Loss at iteration 19588: 0.04275542844824021\n",
      "Train Loss at iteration 19589: 0.04275540553737887\n",
      "Train Loss at iteration 19590: 0.042755382169668024\n",
      "Train Loss at iteration 19591: 0.042755359485514614\n",
      "Train Loss at iteration 19592: 0.04275533670718636\n",
      "Train Loss at iteration 19593: 0.04275531367570301\n",
      "Train Loss at iteration 19594: 0.04275529070401911\n",
      "Train Loss at iteration 19595: 0.042755268476353955\n",
      "Train Loss at iteration 19596: 0.042755244796011456\n",
      "Train Loss at iteration 19597: 0.042755221718473825\n",
      "Train Loss at iteration 19598: 0.042755199533358726\n",
      "Train Loss at iteration 19599: 0.042755176918198586\n",
      "Train Loss at iteration 19600: 0.04275515325321971\n",
      "Train Loss at iteration 19601: 0.04275512991730251\n",
      "Train Loss at iteration 19602: 0.042755108247416664\n",
      "Train Loss at iteration 19603: 0.042755085403980125\n",
      "Train Loss at iteration 19604: 0.04275506175551782\n",
      "Train Loss at iteration 19605: 0.04275503868356723\n",
      "Train Loss at iteration 19606: 0.042755016766625145\n",
      "Train Loss at iteration 19607: 0.04275499359840794\n",
      "Train Loss at iteration 19608: 0.04275497027540616\n",
      "Train Loss at iteration 19609: 0.04275494755561894\n",
      "Train Loss at iteration 19610: 0.04275492493834264\n",
      "Train Loss at iteration 19611: 0.04275490215495094\n",
      "Train Loss at iteration 19612: 0.0427548788442613\n",
      "Train Loss at iteration 19613: 0.04275485641742954\n",
      "Train Loss at iteration 19614: 0.04275483346612712\n",
      "Train Loss at iteration 19615: 0.04275481043703514\n",
      "Train Loss at iteration 19616: 0.04275478769550247\n",
      "Train Loss at iteration 19617: 0.042754765377793366\n",
      "Train Loss at iteration 19618: 0.04275474175244924\n",
      "Train Loss at iteration 19619: 0.04275471868586644\n",
      "Train Loss at iteration 19620: 0.042754696617080645\n",
      "Train Loss at iteration 19621: 0.042754674014685266\n",
      "Train Loss at iteration 19622: 0.04275465040325845\n",
      "Train Loss at iteration 19623: 0.042754627120328556\n",
      "Train Loss at iteration 19624: 0.04275460539449496\n",
      "Train Loss at iteration 19625: 0.04275458269192609\n",
      "Train Loss at iteration 19626: 0.04275455909574839\n",
      "Train Loss at iteration 19627: 0.042754535930446765\n",
      "Train Loss at iteration 19628: 0.04275451416993033\n",
      "Train Loss at iteration 19629: 0.04275449107504758\n",
      "Train Loss at iteration 19630: 0.04275446780319125\n",
      "Train Loss at iteration 19631: 0.042754444905149847\n",
      "Train Loss at iteration 19632: 0.04275442253999818\n",
      "Train Loss at iteration 19633: 0.04275439981742987\n",
      "Train Loss at iteration 19634: 0.04275437655687749\n",
      "Train Loss at iteration 19635: 0.04275435387239407\n",
      "Train Loss at iteration 19636: 0.042754331259308866\n",
      "Train Loss at iteration 19637: 0.04275430828735001\n",
      "Train Loss at iteration 19638: 0.042754285350680675\n",
      "Train Loss at iteration 19639: 0.04275426284805119\n",
      "Train Loss at iteration 19640: 0.0427542400678121\n",
      "Train Loss at iteration 19641: 0.042754216748543274\n",
      "Train Loss at iteration 19642: 0.04275419420966414\n",
      "Train Loss at iteration 19643: 0.04275417213861587\n",
      "Train Loss at iteration 19644: 0.04275414857260907\n",
      "Train Loss at iteration 19645: 0.0427541253367849\n",
      "Train Loss at iteration 19646: 0.0427541031467166\n",
      "Train Loss at iteration 19647: 0.04275408099070092\n",
      "Train Loss at iteration 19648: 0.04275405744133342\n",
      "Train Loss at iteration 19649: 0.04275403421618147\n",
      "Train Loss at iteration 19650: 0.042754011830301815\n",
      "Train Loss at iteration 19651: 0.04275398988546891\n",
      "Train Loss at iteration 19652: 0.04275396634641724\n",
      "Train Loss at iteration 19653: 0.042753943131408616\n",
      "Train Loss at iteration 19654: 0.04275392052898065\n",
      "Train Loss at iteration 19655: 0.04275389881592713\n",
      "Train Loss at iteration 19656: 0.04275387528682884\n",
      "Train Loss at iteration 19657: 0.04275385208162641\n",
      "Train Loss at iteration 19658: 0.04275382948726353\n",
      "Train Loss at iteration 19659: 0.042753807537401836\n",
      "Train Loss at iteration 19660: 0.04275378426388354\n",
      "Train Loss at iteration 19661: 0.04275376106872345\n",
      "Train Loss at iteration 19662: 0.04275373856057005\n",
      "Train Loss at iteration 19663: 0.04275371619057959\n",
      "Train Loss at iteration 19664: 0.042753693275310665\n",
      "Train Loss at iteration 19665: 0.04275367008935521\n",
      "Train Loss at iteration 19666: 0.04275364764925489\n",
      "Train Loss at iteration 19667: 0.042753624905052155\n",
      "Train Loss at iteration 19668: 0.04275360229106992\n",
      "Train Loss at iteration 19669: 0.042753579145538415\n",
      "Train Loss at iteration 19670: 0.04275355675295702\n",
      "Train Loss at iteration 19671: 0.04275353397506766\n",
      "Train Loss at iteration 19672: 0.042753511023822494\n",
      "Train Loss at iteration 19673: 0.042753488234188376\n",
      "Train Loss at iteration 19674: 0.04275346587330953\n",
      "Train Loss at iteration 19675: 0.042753443077486054\n",
      "Train Loss at iteration 19676: 0.042753419792702016\n",
      "Train Loss at iteration 19677: 0.04275339735481635\n",
      "Train Loss at iteration 19678: 0.04275337501169594\n",
      "Train Loss at iteration 19679: 0.04275335221180929\n",
      "Train Loss at iteration 19680: 0.04275332873750816\n",
      "Train Loss at iteration 19681: 0.04275330636550567\n",
      "Train Loss at iteration 19682: 0.042753284171309996\n",
      "Train Loss at iteration 19683: 0.04275326137970217\n",
      "Train Loss at iteration 19684: 0.042753237914288186\n",
      "Train Loss at iteration 19685: 0.04275321521071185\n",
      "Train Loss at iteration 19686: 0.042753193345529045\n",
      "Train Loss at iteration 19687: 0.04275317057931305\n",
      "Train Loss at iteration 19688: 0.04275314712206177\n",
      "Train Loss at iteration 19689: 0.042753124090976216\n",
      "Train Loss at iteration 19690: 0.0427531025395407\n",
      "Train Loss at iteration 19691: 0.04275307980973919\n",
      "Train Loss at iteration 19692: 0.04275305636054118\n",
      "Train Loss at iteration 19693: 0.04275303323825224\n",
      "Train Loss at iteration 19694: 0.042753011520618736\n",
      "Train Loss at iteration 19695: 0.04275298906937182\n",
      "Train Loss at iteration 19696: 0.042752965631995064\n",
      "Train Loss at iteration 19697: 0.04275294251765564\n",
      "Train Loss at iteration 19698: 0.042752920419400826\n",
      "Train Loss at iteration 19699: 0.0427528983632224\n",
      "Train Loss at iteration 19700: 0.04275287493365827\n",
      "Train Loss at iteration 19701: 0.042752851827094936\n",
      "Train Loss at iteration 19702: 0.04275282934353582\n",
      "Train Loss at iteration 19703: 0.042752807687066956\n",
      "Train Loss at iteration 19704: 0.04275278426523559\n",
      "Train Loss at iteration 19705: 0.04275276116637336\n",
      "Train Loss at iteration 19706: 0.0427527385951602\n",
      "Train Loss at iteration 19707: 0.042752716737979514\n",
      "Train Loss at iteration 19708: 0.04275269362857841\n",
      "Train Loss at iteration 19709: 0.04275267053787931\n",
      "Train Loss at iteration 19710: 0.0427526478886476\n",
      "Train Loss at iteration 19711: 0.042752625797195624\n",
      "Train Loss at iteration 19712: 0.042752603021964077\n",
      "Train Loss at iteration 19713: 0.0427525799387817\n",
      "Train Loss at iteration 19714: 0.042752557203939286\n",
      "Train Loss at iteration 19715: 0.04275253492198524\n",
      "Train Loss at iteration 19716: 0.042752512408052244\n",
      "Train Loss at iteration 19717: 0.04275248937145272\n",
      "Train Loss at iteration 19718: 0.042752466539905554\n",
      "Train Loss at iteration 19719: 0.04275244436775812\n",
      "Train Loss at iteration 19720: 0.04275242153903391\n",
      "Train Loss at iteration 19721: 0.042752398833188966\n",
      "Train Loss at iteration 19722: 0.04275237589757885\n",
      "Train Loss at iteration 19723: 0.04275235384257876\n",
      "Train Loss at iteration 19724: 0.04275233070346046\n",
      "Train Loss at iteration 19725: 0.042752308323791365\n",
      "Train Loss at iteration 19726: 0.04275228527994734\n",
      "Train Loss at iteration 19727: 0.04275226300197735\n",
      "Train Loss at iteration 19728: 0.04275224030829305\n",
      "Train Loss at iteration 19729: 0.042752217790545076\n",
      "Train Loss at iteration 19730: 0.042752194822307264\n",
      "Train Loss at iteration 19731: 0.04275217237217634\n",
      "Train Loss at iteration 19732: 0.04275214986371719\n",
      "Train Loss at iteration 19733: 0.042752127033555666\n",
      "Train Loss at iteration 19734: 0.04275210439166373\n",
      "Train Loss at iteration 19735: 0.04275208177111733\n",
      "Train Loss at iteration 19736: 0.04275205944597736\n",
      "Train Loss at iteration 19737: 0.042752036308578586\n",
      "Train Loss at iteration 19738: 0.04275201398818755\n",
      "Train Loss at iteration 19739: 0.04275199119659778\n",
      "Train Loss at iteration 19740: 0.042751969055377204\n",
      "Train Loss at iteration 19741: 0.04275194570840691\n",
      "Train Loss at iteration 19742: 0.042751923517732875\n",
      "Train Loss at iteration 19743: 0.042751900650738764\n",
      "Train Loss at iteration 19744: 0.042751878693968924\n",
      "Train Loss at iteration 19745: 0.04275185535415174\n",
      "Train Loss at iteration 19746: 0.04275183286041041\n",
      "Train Loss at iteration 19747: 0.04275181026689771\n",
      "Train Loss at iteration 19748: 0.042751787873761854\n",
      "Train Loss at iteration 19749: 0.04275176536523252\n",
      "Train Loss at iteration 19750: 0.04275174225240486\n",
      "Train Loss at iteration 19751: 0.04275171996723432\n",
      "Train Loss at iteration 19752: 0.04275169734376415\n",
      "Train Loss at iteration 19753: 0.04275167507856686\n",
      "Train Loss at iteration 19754: 0.042751651760276455\n",
      "Train Loss at iteration 19755: 0.04275162959866398\n",
      "Train Loss at iteration 19756: 0.0427516068481938\n",
      "Train Loss at iteration 19757: 0.04275158481943715\n",
      "Train Loss at iteration 19758: 0.0427515615077111\n",
      "Train Loss at iteration 19759: 0.042751539047375074\n",
      "Train Loss at iteration 19760: 0.04275151647714177\n",
      "Train Loss at iteration 19761: 0.042751494140528\n",
      "Train Loss at iteration 19762: 0.0427514716191435\n",
      "Train Loss at iteration 19763: 0.0427514485443666\n",
      "Train Loss at iteration 19764: 0.04275142627694045\n",
      "Train Loss at iteration 19765: 0.042751403666505684\n",
      "Train Loss at iteration 19766: 0.04275138143186594\n",
      "Train Loss at iteration 19767: 0.042751358140481845\n",
      "Train Loss at iteration 19768: 0.04275133602298361\n",
      "Train Loss at iteration 19769: 0.04275131322911609\n",
      "Train Loss at iteration 19770: 0.04275129127122383\n",
      "Train Loss at iteration 19771: 0.04275126798600828\n",
      "Train Loss at iteration 19772: 0.04275124557497413\n",
      "Train Loss at iteration 19773: 0.04275122300936307\n",
      "Train Loss at iteration 19774: 0.042751200598649494\n",
      "Train Loss at iteration 19775: 0.04275117819461282\n",
      "Train Loss at iteration 19776: 0.04275115517452876\n",
      "Train Loss at iteration 19777: 0.04275113290571951\n",
      "Train Loss at iteration 19778: 0.0427511101865539\n",
      "Train Loss at iteration 19779: 0.04275108810382617\n",
      "Train Loss at iteration 19780: 0.04275106483813539\n",
      "Train Loss at iteration 19781: 0.0427510427822452\n",
      "Train Loss at iteration 19782: 0.04275101990128839\n",
      "Train Loss at iteration 19783: 0.042750997604006344\n",
      "Train Loss at iteration 19784: 0.04275097511684865\n",
      "Train Loss at iteration 19785: 0.04275095245233088\n",
      "Train Loss at iteration 19786: 0.042750929867070506\n",
      "Train Loss at iteration 19787: 0.042750907226956424\n",
      "Train Loss at iteration 19788: 0.042750885095473846\n",
      "Train Loss at iteration 19789: 0.04275086214332588\n",
      "Train Loss at iteration 19790: 0.042750839856520104\n",
      "Train Loss at iteration 19791: 0.042750816947765884\n",
      "Train Loss at iteration 19792: 0.0427507946941233\n",
      "Train Loss at iteration 19793: 0.042750772201349485\n",
      "Train Loss at iteration 19794: 0.042750749890089625\n",
      "Train Loss at iteration 19795: 0.04275072697895144\n",
      "Train Loss at iteration 19796: 0.0427507043578647\n",
      "Train Loss at iteration 19797: 0.04275068223749787\n",
      "Train Loss at iteration 19798: 0.04275065965712365\n",
      "Train Loss at iteration 19799: 0.042750637035765766\n",
      "Train Loss at iteration 19800: 0.04275061414533462\n",
      "Train Loss at iteration 19801: 0.04275059187728482\n",
      "Train Loss at iteration 19802: 0.042750569783535704\n",
      "Train Loss at iteration 19803: 0.04275054713619237\n",
      "Train Loss at iteration 19804: 0.04275052424315299\n",
      "Train Loss at iteration 19805: 0.04275050158235106\n",
      "Train Loss at iteration 19806: 0.042750479610042184\n",
      "Train Loss at iteration 19807: 0.04275045725227154\n",
      "Train Loss at iteration 19808: 0.04275043436372433\n",
      "Train Loss at iteration 19809: 0.042750411491063664\n",
      "Train Loss at iteration 19810: 0.04275038915834714\n",
      "Train Loss at iteration 19811: 0.0427503675239983\n",
      "Train Loss at iteration 19812: 0.0427503445299105\n",
      "Train Loss at iteration 19813: 0.04275032165453454\n",
      "Train Loss at iteration 19814: 0.04275029890687705\n",
      "Train Loss at iteration 19815: 0.04275027741776718\n",
      "Train Loss at iteration 19816: 0.042750254711470335\n",
      "Train Loss at iteration 19817: 0.04275023184041653\n",
      "Train Loss at iteration 19818: 0.04275020898517016\n",
      "Train Loss at iteration 19819: 0.042750186707849205\n",
      "Train Loss at iteration 19820: 0.042750165249107214\n",
      "Train Loss at iteration 19821: 0.042750142069577474\n",
      "Train Loss at iteration 19822: 0.04275011921093108\n",
      "Train Loss at iteration 19823: 0.04275009636844619\n",
      "Train Loss at iteration 19824: 0.04275007500506948\n",
      "Train Loss at iteration 19825: 0.042750052648443046\n",
      "Train Loss at iteration 19826: 0.04275002948152482\n",
      "Train Loss at iteration 19827: 0.04275000663530175\n",
      "Train Loss at iteration 19828: 0.042749984531088775\n",
      "Train Loss at iteration 19829: 0.04274996292961084\n",
      "Train Loss at iteration 19830: 0.04274993976613221\n",
      "Train Loss at iteration 19831: 0.04274991692381935\n",
      "Train Loss at iteration 19832: 0.04274989436782118\n",
      "Train Loss at iteration 19833: 0.04274987262832361\n",
      "Train Loss at iteration 19834: 0.04274985040929172\n",
      "Train Loss at iteration 19835: 0.042749827254800477\n",
      "Train Loss at iteration 19836: 0.042749804424818336\n",
      "Train Loss at iteration 19837: 0.04274978254074278\n",
      "Train Loss at iteration 19838: 0.042749760350779965\n",
      "Train Loss at iteration 19839: 0.04274973793849757\n",
      "Train Loss at iteration 19840: 0.042749714796104114\n",
      "Train Loss at iteration 19841: 0.042749692632733295\n",
      "Train Loss at iteration 19842: 0.04274967022192472\n",
      "Train Loss at iteration 19843: 0.04274964832003622\n",
      "Train Loss at iteration 19844: 0.04274962518079305\n",
      "Train Loss at iteration 19845: 0.0427496027521234\n",
      "Train Loss at iteration 19846: 0.04274958050093336\n",
      "Train Loss at iteration 19847: 0.042749558013000157\n",
      "Train Loss at iteration 19848: 0.04274953592391183\n",
      "Train Loss at iteration 19849: 0.042749512915583404\n",
      "Train Loss at iteration 19850: 0.042749490928540294\n",
      "Train Loss at iteration 19851: 0.042749468137199716\n",
      "Train Loss at iteration 19852: 0.04274944582369818\n",
      "Train Loss at iteration 19853: 0.04274942357708065\n",
      "Train Loss at iteration 19854: 0.04274940124975562\n",
      "Train Loss at iteration 19855: 0.0427493786041696\n",
      "Train Loss at iteration 19856: 0.04274935582434868\n",
      "Train Loss at iteration 19857: 0.04274933365336092\n",
      "Train Loss at iteration 19858: 0.04274931180472977\n",
      "Train Loss at iteration 19859: 0.042749289111289335\n",
      "Train Loss at iteration 19860: 0.04274926632752568\n",
      "Train Loss at iteration 19861: 0.042749243611013676\n",
      "Train Loss at iteration 19862: 0.04274922205807296\n",
      "Train Loss at iteration 19863: 0.04274919963171175\n",
      "Train Loss at iteration 19864: 0.04274917685120368\n",
      "Train Loss at iteration 19865: 0.04274915408598672\n",
      "Train Loss at iteration 19866: 0.04274913152541305\n",
      "Train Loss at iteration 19867: 0.04274911050581828\n",
      "Train Loss at iteration 19868: 0.04274908741587546\n",
      "Train Loss at iteration 19869: 0.04274906464634663\n",
      "Train Loss at iteration 19870: 0.04274904189253444\n",
      "Train Loss at iteration 19871: 0.0427490201333299\n",
      "Train Loss at iteration 19872: 0.04274899832817961\n",
      "Train Loss at iteration 19873: 0.04274897524955286\n",
      "Train Loss at iteration 19874: 0.042748952491214506\n",
      "Train Loss at iteration 19875: 0.04274893017529101\n",
      "Train Loss at iteration 19876: 0.042748908332018186\n",
      "Train Loss at iteration 19877: 0.04274888620145291\n",
      "Train Loss at iteration 19878: 0.04274886312994034\n",
      "Train Loss at iteration 19879: 0.042748840553451815\n",
      "Train Loss at iteration 19880: 0.04274881858675781\n",
      "Train Loss at iteration 19881: 0.042748796302430545\n",
      "Train Loss at iteration 19882: 0.042748774117483984\n",
      "Train Loss at iteration 19883: 0.04274875105616876\n",
      "Train Loss at iteration 19884: 0.042748729173441066\n",
      "Train Loss at iteration 19885: 0.04274870653353169\n",
      "Train Loss at iteration 19886: 0.04274868430163614\n",
      "Train Loss at iteration 19887: 0.04274866208088241\n",
      "Train Loss at iteration 19888: 0.04274863962797256\n",
      "Train Loss at iteration 19889: 0.04274861724089199\n",
      "Train Loss at iteration 19890: 0.0427485945251699\n",
      "Train Loss at iteration 19891: 0.0427485723279691\n",
      "Train Loss at iteration 19892: 0.04274855043701181\n",
      "Train Loss at iteration 19893: 0.042748527987006645\n",
      "Train Loss at iteration 19894: 0.0427485052667346\n",
      "Train Loss at iteration 19895: 0.04274848256123906\n",
      "Train Loss at iteration 19896: 0.042748460483706834\n",
      "Train Loss at iteration 19897: 0.042748439077898145\n",
      "Train Loss at iteration 19898: 0.04274841604731613\n",
      "Train Loss at iteration 19899: 0.04274839333668971\n",
      "Train Loss at iteration 19900: 0.04274837064134886\n",
      "Train Loss at iteration 19901: 0.042748349260371785\n",
      "Train Loss at iteration 19902: 0.04274832717388417\n",
      "Train Loss at iteration 19903: 0.04274830415342237\n",
      "Train Loss at iteration 19904: 0.04274828145285803\n",
      "Train Loss at iteration 19905: 0.042748259310214136\n",
      "Train Loss at iteration 19906: 0.042748237520216156\n",
      "Train Loss at iteration 19907: 0.04274821531746473\n",
      "Train Loss at iteration 19908: 0.042748192303092465\n",
      "Train Loss at iteration 19909: 0.0427481699091458\n",
      "Train Loss at iteration 19910: 0.042748147875744304\n",
      "Train Loss at iteration 19911: 0.042748125662333686\n",
      "Train Loss at iteration 19912: 0.042748103500939605\n",
      "Train Loss at iteration 19913: 0.04274808054963512\n",
      "Train Loss at iteration 19914: 0.04274805875549884\n",
      "Train Loss at iteration 19915: 0.04274803608509724\n",
      "Train Loss at iteration 19916: 0.042748013840285606\n",
      "Train Loss at iteration 19917: 0.042747991726929085\n",
      "Train Loss at iteration 19918: 0.0427479694853767\n",
      "Train Loss at iteration 19919: 0.042747947000343255\n",
      "Train Loss at iteration 19920: 0.04274792433968884\n",
      "Train Loss at iteration 19921: 0.04274790204895517\n",
      "Train Loss at iteration 19922: 0.04274788051253284\n",
      "Train Loss at iteration 19923: 0.04274785795342364\n",
      "Train Loss at iteration 19924: 0.042747835287809295\n",
      "Train Loss at iteration 19925: 0.042747812636757636\n",
      "Train Loss at iteration 19926: 0.04274779056815373\n",
      "Train Loss at iteration 19927: 0.042747769250425366\n",
      "Train Loss at iteration 19928: 0.04274774627412067\n",
      "Train Loss at iteration 19929: 0.042747723617573875\n",
      "Train Loss at iteration 19930: 0.04274770101951618\n",
      "Train Loss at iteration 19931: 0.04274767950581456\n",
      "Train Loss at iteration 19932: 0.04274765760716066\n",
      "Train Loss at iteration 19933: 0.04274763463656437\n",
      "Train Loss at iteration 19934: 0.04274761198952009\n",
      "Train Loss at iteration 19935: 0.04274759011050223\n",
      "Train Loss at iteration 19936: 0.04274756781341966\n",
      "Train Loss at iteration 19937: 0.04274754600448728\n",
      "Train Loss at iteration 19938: 0.04274752304341848\n",
      "Train Loss at iteration 19939: 0.04274750092444582\n",
      "Train Loss at iteration 19940: 0.04274747872515997\n",
      "Train Loss at iteration 19941: 0.04274745615022554\n",
      "Train Loss at iteration 19942: 0.04274743444427947\n",
      "Train Loss at iteration 19943: 0.04274741177948915\n",
      "Train Loss at iteration 19944: 0.042747389807421814\n",
      "Train Loss at iteration 19945: 0.042747367189629765\n",
      "Train Loss at iteration 19946: 0.04274734457936009\n",
      "Train Loss at iteration 19947: 0.04274732258778016\n",
      "Train Loss at iteration 19948: 0.04274730125896312\n",
      "Train Loss at iteration 19949: 0.042747278323119045\n",
      "Train Loss at iteration 19950: 0.04274725570666323\n",
      "Train Loss at iteration 19951: 0.0427472331049996\n",
      "Train Loss at iteration 19952: 0.04274721168334856\n",
      "Train Loss at iteration 19953: 0.04274718980071822\n",
      "Train Loss at iteration 19954: 0.04274716687360316\n",
      "Train Loss at iteration 19955: 0.04274714426593478\n",
      "Train Loss at iteration 19956: 0.04274712222199564\n",
      "Train Loss at iteration 19957: 0.04274710026615595\n",
      "Train Loss at iteration 19958: 0.042747078386350765\n",
      "Train Loss at iteration 19959: 0.04274705546415211\n",
      "Train Loss at iteration 19960: 0.042747033188299866\n",
      "Train Loss at iteration 19961: 0.04274701122510625\n",
      "Train Loss at iteration 19962: 0.04274698872073803\n",
      "Train Loss at iteration 19963: 0.04274696700862529\n",
      "Train Loss at iteration 19964: 0.04274694419525897\n",
      "Train Loss at iteration 19965: 0.04274692245033175\n",
      "Train Loss at iteration 19966: 0.04274689987045029\n",
      "Train Loss at iteration 19967: 0.042746877297911066\n",
      "Train Loss at iteration 19968: 0.0427468552440858\n",
      "Train Loss at iteration 19969: 0.042746833927736844\n",
      "Train Loss at iteration 19970: 0.04274681114766239\n",
      "Train Loss at iteration 19971: 0.04274678856920261\n",
      "Train Loss at iteration 19972: 0.042746766004805994\n",
      "Train Loss at iteration 19973: 0.04274674435274851\n",
      "Train Loss at iteration 19974: 0.04274672276716739\n",
      "Train Loss at iteration 19975: 0.04274669987720761\n",
      "Train Loss at iteration 19976: 0.04274667730654973\n",
      "Train Loss at iteration 19977: 0.04274665513537828\n",
      "Train Loss at iteration 19978: 0.04274663324060655\n",
      "Train Loss at iteration 19979: 0.042746611530548574\n",
      "Train Loss at iteration 19980: 0.042746588645132644\n",
      "Train Loss at iteration 19981: 0.04274656625128762\n",
      "Train Loss at iteration 19982: 0.04274654448167323\n",
      "Train Loss at iteration 19983: 0.04274652193663963\n",
      "Train Loss at iteration 19984: 0.04274649987306704\n",
      "Train Loss at iteration 19985: 0.04274647778759499\n",
      "Train Loss at iteration 19986: 0.04274645581507096\n",
      "Train Loss at iteration 19987: 0.04274643331583923\n",
      "Train Loss at iteration 19988: 0.042746410778720385\n",
      "Train Loss at iteration 19989: 0.04274638847387776\n",
      "Train Loss at iteration 19990: 0.04274636732903739\n",
      "Train Loss at iteration 19991: 0.04274634473096941\n",
      "Train Loss at iteration 19992: 0.04274632218790886\n",
      "Train Loss at iteration 19993: 0.04274629965886159\n",
      "Train Loss at iteration 19994: 0.04274627760731947\n",
      "Train Loss at iteration 19995: 0.04274625612494812\n",
      "Train Loss at iteration 19996: 0.042746233968364876\n",
      "Train Loss at iteration 19997: 0.04274621111672403\n",
      "Train Loss at iteration 19998: 0.042746188844586905\n",
      "Train Loss at iteration 19999: 0.04274616702051962\n",
      "Final Train Loss: 0.04274616702051962\n",
      "Time elapsed: 36.08\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "regressor = LassoRegression(lr = 1e-1, lambdaConstant=1e-4, n_iters= 20000)\n",
    "\n",
    "start = time.time()\n",
    "regressor.fit(trainPredictor, trainResponse)\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {end - start:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor.plot_loss_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance: 0.041683883699919454\n"
     ]
    }
   ],
   "source": [
    "def meanSquaredError(testResponse, predictions):\n",
    "    error = testResponse - predictions\n",
    "    squaredError = np.dot(error.T, error)\n",
    "    meanSquaredError =  1/(testResponse.size) * squaredError\n",
    "    return meanSquaredError\n",
    "\n",
    "predictions = regressor.inference(valPredictor)\n",
    "MSE = meanSquaredError(valResponse, predictions)\n",
    "print(f'Validation Performance: {MSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance: 0.04213254889918555\n"
     ]
    }
   ],
   "source": [
    "predictions = regressor.inference(testPredictor)\n",
    "MSE = meanSquaredError(testResponse, predictions)\n",
    "print(f'Test Performance: {MSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 1)\n",
      "Prediction: [0.49823065] Response: 0.599\n"
     ]
    }
   ],
   "source": [
    "demoInstanceLoc = 11021\n",
    "demoPredictor = predictorData[demoInstanceLoc]\n",
    "demoResponse = responseData[demoInstanceLoc]\n",
    "demoPredictor = np.expand_dims(demoPredictor, axis=0)\n",
    "print(demoPredictor.T.shape)\n",
    "demoPrediction = regressor.inference(demoPredictor)\n",
    "print('Prediction:', demoPrediction, 'Response:', demoResponse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
