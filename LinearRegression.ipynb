{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"./data/dataset.csv\"\n",
    "df = pd.read_csv(dataPath, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['popularity',\n",
       " 'duration_ms',\n",
       " 'explicit',\n",
       " 'danceability',\n",
       " 'energy',\n",
       " 'key',\n",
       " 'loudness',\n",
       " 'mode',\n",
       " 'speechiness',\n",
       " 'acousticness',\n",
       " 'instrumentalness',\n",
       " 'liveness',\n",
       " 'valence',\n",
       " 'tempo',\n",
       " 'time_signature']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = list(df.columns)\n",
    "columnsToKeep = columns[4: -1]\n",
    "columnsToKeep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>key</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>liveness</th>\n",
       "      <th>valence</th>\n",
       "      <th>tempo</th>\n",
       "      <th>time_signature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>False</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>1</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.7150</td>\n",
       "      <td>87.917</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>False</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.2670</td>\n",
       "      <td>77.489</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0.1200</td>\n",
       "      <td>76.332</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>False</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.1320</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>181.740</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>False</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>2</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0829</td>\n",
       "      <td>0.1670</td>\n",
       "      <td>119.949</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113995</th>\n",
       "      <td>21</td>\n",
       "      <td>384999</td>\n",
       "      <td>False</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>5</td>\n",
       "      <td>-16.393</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>0.0863</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>125.995</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113996</th>\n",
       "      <td>22</td>\n",
       "      <td>385000</td>\n",
       "      <td>False</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>0</td>\n",
       "      <td>-18.318</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>85.239</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113997</th>\n",
       "      <td>22</td>\n",
       "      <td>271466</td>\n",
       "      <td>False</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>0</td>\n",
       "      <td>-10.895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0839</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>132.378</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113998</th>\n",
       "      <td>41</td>\n",
       "      <td>283893</td>\n",
       "      <td>False</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>7</td>\n",
       "      <td>-10.889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.3810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.2700</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>135.960</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113999</th>\n",
       "      <td>22</td>\n",
       "      <td>241826</td>\n",
       "      <td>False</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>1</td>\n",
       "      <td>-10.204</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0893</td>\n",
       "      <td>0.7080</td>\n",
       "      <td>79.198</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114000 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        popularity  duration_ms  explicit  danceability  energy  key  \\\n",
       "0               73       230666     False         0.676  0.4610    1   \n",
       "1               55       149610     False         0.420  0.1660    1   \n",
       "2               57       210826     False         0.438  0.3590    0   \n",
       "3               71       201933     False         0.266  0.0596    0   \n",
       "4               82       198853     False         0.618  0.4430    2   \n",
       "...            ...          ...       ...           ...     ...  ...   \n",
       "113995          21       384999     False         0.172  0.2350    5   \n",
       "113996          22       385000     False         0.174  0.1170    0   \n",
       "113997          22       271466     False         0.629  0.3290    0   \n",
       "113998          41       283893     False         0.587  0.5060    7   \n",
       "113999          22       241826     False         0.526  0.4870    1   \n",
       "\n",
       "        loudness  mode  speechiness  acousticness  instrumentalness  liveness  \\\n",
       "0         -6.746     0       0.1430        0.0322          0.000001    0.3580   \n",
       "1        -17.235     1       0.0763        0.9240          0.000006    0.1010   \n",
       "2         -9.734     1       0.0557        0.2100          0.000000    0.1170   \n",
       "3        -18.515     1       0.0363        0.9050          0.000071    0.1320   \n",
       "4         -9.681     1       0.0526        0.4690          0.000000    0.0829   \n",
       "...          ...   ...          ...           ...               ...       ...   \n",
       "113995   -16.393     1       0.0422        0.6400          0.928000    0.0863   \n",
       "113996   -18.318     0       0.0401        0.9940          0.976000    0.1050   \n",
       "113997   -10.895     0       0.0420        0.8670          0.000000    0.0839   \n",
       "113998   -10.889     1       0.0297        0.3810          0.000000    0.2700   \n",
       "113999   -10.204     0       0.0725        0.6810          0.000000    0.0893   \n",
       "\n",
       "        valence    tempo  time_signature  \n",
       "0        0.7150   87.917               4  \n",
       "1        0.2670   77.489               4  \n",
       "2        0.1200   76.332               4  \n",
       "3        0.1430  181.740               3  \n",
       "4        0.1670  119.949               4  \n",
       "...         ...      ...             ...  \n",
       "113995   0.0339  125.995               5  \n",
       "113996   0.0350   85.239               4  \n",
       "113997   0.7430  132.378               4  \n",
       "113998   0.4130  135.960               4  \n",
       "113999   0.7080   79.198               4  \n",
       "\n",
       "[114000 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[columnsToKeep]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bool to numerical data for explicit row\n",
    "df.loc[:, 'explicit'] = df['explicit'].astype(int)\n",
    "\n",
    "# One hot encoding for nominal categroies\n",
    "df = pd.get_dummies(df, columns=['key', 'time_signature'], dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>popularity</th>\n",
       "      <th>duration_ms</th>\n",
       "      <th>explicit</th>\n",
       "      <th>danceability</th>\n",
       "      <th>energy</th>\n",
       "      <th>loudness</th>\n",
       "      <th>mode</th>\n",
       "      <th>speechiness</th>\n",
       "      <th>acousticness</th>\n",
       "      <th>instrumentalness</th>\n",
       "      <th>...</th>\n",
       "      <th>key_7</th>\n",
       "      <th>key_8</th>\n",
       "      <th>key_9</th>\n",
       "      <th>key_10</th>\n",
       "      <th>key_11</th>\n",
       "      <th>time_signature_0</th>\n",
       "      <th>time_signature_1</th>\n",
       "      <th>time_signature_3</th>\n",
       "      <th>time_signature_4</th>\n",
       "      <th>time_signature_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>230666</td>\n",
       "      <td>0</td>\n",
       "      <td>0.676</td>\n",
       "      <td>0.4610</td>\n",
       "      <td>-6.746</td>\n",
       "      <td>0</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.0322</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>149610</td>\n",
       "      <td>0</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.1660</td>\n",
       "      <td>-17.235</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0763</td>\n",
       "      <td>0.9240</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>210826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.3590</td>\n",
       "      <td>-9.734</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0557</td>\n",
       "      <td>0.2100</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>71</td>\n",
       "      <td>201933</td>\n",
       "      <td>0</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.0596</td>\n",
       "      <td>-18.515</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>82</td>\n",
       "      <td>198853</td>\n",
       "      <td>0</td>\n",
       "      <td>0.618</td>\n",
       "      <td>0.4430</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0526</td>\n",
       "      <td>0.4690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113995</th>\n",
       "      <td>21</td>\n",
       "      <td>384999</td>\n",
       "      <td>0</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>-16.393</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0422</td>\n",
       "      <td>0.6400</td>\n",
       "      <td>0.928000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113996</th>\n",
       "      <td>22</td>\n",
       "      <td>385000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.1170</td>\n",
       "      <td>-18.318</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0401</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.976000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113997</th>\n",
       "      <td>22</td>\n",
       "      <td>271466</td>\n",
       "      <td>0</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.3290</td>\n",
       "      <td>-10.895</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0420</td>\n",
       "      <td>0.8670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113998</th>\n",
       "      <td>41</td>\n",
       "      <td>283893</td>\n",
       "      <td>0</td>\n",
       "      <td>0.587</td>\n",
       "      <td>0.5060</td>\n",
       "      <td>-10.889</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0297</td>\n",
       "      <td>0.3810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113999</th>\n",
       "      <td>22</td>\n",
       "      <td>241826</td>\n",
       "      <td>0</td>\n",
       "      <td>0.526</td>\n",
       "      <td>0.4870</td>\n",
       "      <td>-10.204</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0725</td>\n",
       "      <td>0.6810</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>114000 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        popularity  duration_ms  explicit  danceability  energy  loudness  \\\n",
       "0               73       230666         0         0.676  0.4610    -6.746   \n",
       "1               55       149610         0         0.420  0.1660   -17.235   \n",
       "2               57       210826         0         0.438  0.3590    -9.734   \n",
       "3               71       201933         0         0.266  0.0596   -18.515   \n",
       "4               82       198853         0         0.618  0.4430    -9.681   \n",
       "...            ...          ...       ...           ...     ...       ...   \n",
       "113995          21       384999         0         0.172  0.2350   -16.393   \n",
       "113996          22       385000         0         0.174  0.1170   -18.318   \n",
       "113997          22       271466         0         0.629  0.3290   -10.895   \n",
       "113998          41       283893         0         0.587  0.5060   -10.889   \n",
       "113999          22       241826         0         0.526  0.4870   -10.204   \n",
       "\n",
       "        mode  speechiness  acousticness  instrumentalness  ...  key_7  key_8  \\\n",
       "0          0       0.1430        0.0322          0.000001  ...      0      0   \n",
       "1          1       0.0763        0.9240          0.000006  ...      0      0   \n",
       "2          1       0.0557        0.2100          0.000000  ...      0      0   \n",
       "3          1       0.0363        0.9050          0.000071  ...      0      0   \n",
       "4          1       0.0526        0.4690          0.000000  ...      0      0   \n",
       "...      ...          ...           ...               ...  ...    ...    ...   \n",
       "113995     1       0.0422        0.6400          0.928000  ...      0      0   \n",
       "113996     0       0.0401        0.9940          0.976000  ...      0      0   \n",
       "113997     0       0.0420        0.8670          0.000000  ...      0      0   \n",
       "113998     1       0.0297        0.3810          0.000000  ...      1      0   \n",
       "113999     0       0.0725        0.6810          0.000000  ...      0      0   \n",
       "\n",
       "        key_9  key_10  key_11  time_signature_0  time_signature_1  \\\n",
       "0           0       0       0                 0                 0   \n",
       "1           0       0       0                 0                 0   \n",
       "2           0       0       0                 0                 0   \n",
       "3           0       0       0                 0                 0   \n",
       "4           0       0       0                 0                 0   \n",
       "...       ...     ...     ...               ...               ...   \n",
       "113995      0       0       0                 0                 0   \n",
       "113996      0       0       0                 0                 0   \n",
       "113997      0       0       0                 0                 0   \n",
       "113998      0       0       0                 0                 0   \n",
       "113999      0       0       0                 0                 0   \n",
       "\n",
       "        time_signature_3  time_signature_4  time_signature_5  \n",
       "0                      0                 1                 0  \n",
       "1                      0                 1                 0  \n",
       "2                      0                 1                 0  \n",
       "3                      1                 0                 0  \n",
       "4                      0                 1                 0  \n",
       "...                  ...               ...               ...  \n",
       "113995                 0                 0                 1  \n",
       "113996                 0                 1                 0  \n",
       "113997                 0                 1                 0  \n",
       "113998                 0                 1                 0  \n",
       "113999                 0                 1                 0  \n",
       "\n",
       "[114000 rows x 30 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['popularity',\n",
       " 'duration_ms',\n",
       " 'explicit',\n",
       " 'danceability',\n",
       " 'energy',\n",
       " 'loudness',\n",
       " 'mode',\n",
       " 'speechiness',\n",
       " 'acousticness',\n",
       " 'instrumentalness',\n",
       " 'liveness',\n",
       " 'valence',\n",
       " 'tempo',\n",
       " 'key_0',\n",
       " 'key_1',\n",
       " 'key_2',\n",
       " 'key_3',\n",
       " 'key_4',\n",
       " 'key_5',\n",
       " 'key_6',\n",
       " 'key_7',\n",
       " 'key_8',\n",
       " 'key_9',\n",
       " 'key_10',\n",
       " 'key_11',\n",
       " 'time_signature_0',\n",
       " 'time_signature_1',\n",
       " 'time_signature_3',\n",
       " 'time_signature_4',\n",
       " 'time_signature_5']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaling(df):\n",
    "    min_vals = df.min()\n",
    "    max_vals = df.max()\n",
    "\n",
    "    feature_range = max_vals - min_vals\n",
    "\n",
    "    # Check if any feature has zero range\n",
    "    zero_range_features = feature_range[feature_range == 0].index\n",
    "\n",
    "    # Remove features with zero range from normalization\n",
    "    valid_features = feature_range[feature_range != 0].index\n",
    "    df_normalized = (df[valid_features] - min_vals[valid_features]) / feature_range[valid_features]\n",
    "\n",
    "    # Concatenate back the zero range features\n",
    "    if not zero_range_features.empty:\n",
    "        df_normalized = pd.concat([df_normalized, df[zero_range_features]], axis=1)\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "def standard_scaling(df):\n",
    "    mean = df.mean()\n",
    "    std = df.std()\n",
    "    return (df - mean) / std\n",
    "\n",
    "responseFrame = df.pop('valence')\n",
    "predictorFrame = df\n",
    "\n",
    "# Min-Max scaling for predictor variables\n",
    "df_normalized = min_max_scaling(predictorFrame)\n",
    "\n",
    "# Standard scaling for predictor variables\n",
    "df_standardized = standard_scaling(predictorFrame)\n",
    "\n",
    "predictorFrame_scaled = df_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, lr = 0.01, n_iters: int = 1000):\n",
    "        self.lr = lr\n",
    "        self.n_iters = n_iters\n",
    "        self.weightVector = None\n",
    "        self.loss_history = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        biasColumn = np.ones((num_samples, 1))\n",
    "        designMatrix = np.hstack((biasColumn, X))\n",
    "        self.weightVector = np.random.rand(num_features + 1)\n",
    "\n",
    "        # self.normalEquationMethod(designMatrix, y)\n",
    "        # print(f'Train Loss for Normal Equation Method:', self.lossMSE(designMatrix, y))\n",
    "\n",
    "        self.gradientDescent(designMatrix, y)\n",
    "        print('Final Train Loss:', self.lossMSE(designMatrix, y))\n",
    "    \n",
    "    def mse_gradient(self, designMatrix, y):\n",
    "        predictions = self.predict(designMatrix)\n",
    "        return  -(2/y.size) * np.dot(designMatrix.T, (y - predictions))\n",
    "\n",
    "    def gradientDescent(self, designMatrix, y):\n",
    "        for i in range(self.n_iters):\n",
    "            # Calculate predictions\n",
    "            gradientVector = self.mse_gradient(designMatrix, y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weightVector = self.weightVector - self.lr * gradientVector\n",
    "\n",
    "            # Print gradients for debugging\n",
    "            loss = self.lossMSE(designMatrix, y)\n",
    "            print(f'Train Loss at iteration {i}:', loss)\n",
    "            self.loss_history.append((i, loss))\n",
    "\n",
    "        return self\n",
    "\n",
    "    def normalEquationMethod(self, designMatrix, y):\n",
    "        # self.weightVector = np.linalg.inv(np.matmul(designMatrix.T, designMatrix)).dot(designMatrix.T).dot(y)\n",
    "        # return self\n",
    "    \n",
    "         # Compute the SVD of the design matrix\n",
    "        U, S, Vt = np.linalg.svd(designMatrix, full_matrices=False)\n",
    "        \n",
    "        # Compute the pseudo-inverse\n",
    "        S_inv = np.diag(1 / S)\n",
    "        pseudo_inverse = np.dot(np.dot(Vt.T, S_inv), U.T)\n",
    "        \n",
    "        # Calculate the weight vector\n",
    "        self.weightVector = np.dot(pseudo_inverse, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def svdMethod(self, designMatrix, y):\n",
    "        self.weightVector, residuals, _, _ = np.linalg.lstsq(designMatrix, y, rcond=None)\n",
    "        return self\n",
    "\n",
    "    def predict(self, designMatrix):\n",
    "        return np.dot(designMatrix, self.weightVector)\n",
    "    \n",
    "    def inference(self, testData):\n",
    "        num_samples = testData.shape[0]\n",
    "        biasColumn = np.ones((num_samples, 1))\n",
    "        designMatrix = np.hstack((biasColumn, testData))\n",
    "        return np.dot(designMatrix, self.weightVector)\n",
    "    \n",
    "    def lossMSE(self, designMatrix, y):\n",
    "        predictions = self.predict(designMatrix)\n",
    "        error = y - predictions\n",
    "        squaredError = np.dot(error.T, error)\n",
    "        meanSquaredError = 1/(y.size) * squaredError\n",
    "        return meanSquaredError\n",
    "    \n",
    "    def plot_loss_history(self):\n",
    "        iterations, losses = zip(*self.loss_history)\n",
    "        plt.plot(iterations, losses)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Training Loss')\n",
    "        plt.title(f'Training Loss over Iterations - Linear (lr = {self.lr}, iter = {self.n_iters})')\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseData = responseFrame.to_numpy()\n",
    "predictorData = predictorFrame_scaled.to_numpy()\n",
    "\n",
    "trainSplit = 0.8\n",
    "valSplit = 0.1\n",
    "testSplit = 0.1\n",
    "\n",
    "np.random.seed(42)\n",
    "indices = np.arange(len(predictorData))\n",
    "np.random.shuffle(indices)\n",
    "trainIndices = indices[:int(trainSplit * len(indices))]\n",
    "valIndices = indices[int(trainSplit* len(indices)):int((trainSplit + valSplit) * len(indices))]\n",
    "testIndices = indices[int((trainSplit + valSplit) * len(indices)):]\n",
    "\n",
    "trainPredictor, testPredictor, valPredictor = predictorData[trainIndices], predictorData[testIndices], predictorData[valIndices]\n",
    "trainResponse, testResponse, valResponse = responseData[trainIndices], responseData[testIndices], responseData[valIndices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss at iteration 0: 0.5349572464351106\n",
      "Train Loss at iteration 1: 0.2227543793982668\n",
      "Train Loss at iteration 2: 0.2103590895400098\n",
      "Train Loss at iteration 3: 0.2052171438474098\n",
      "Train Loss at iteration 4: 0.2004284088093011\n",
      "Train Loss at iteration 5: 0.19582195500563196\n",
      "Train Loss at iteration 6: 0.1913858392582244\n",
      "Train Loss at iteration 7: 0.18711230764545672\n",
      "Train Loss at iteration 8: 0.18299411883298317\n",
      "Train Loss at iteration 9: 0.17902442950920386\n",
      "Train Loss at iteration 10: 0.17519676592644406\n",
      "Train Loss at iteration 11: 0.17150499947919348\n",
      "Train Loss at iteration 12: 0.16794332421340086\n",
      "Train Loss at iteration 13: 0.16450623606006856\n",
      "Train Loss at iteration 14: 0.1611885136453635\n",
      "Train Loss at iteration 15: 0.1579852005437569\n",
      "Train Loss at iteration 16: 0.1548915888527464\n",
      "Train Loss at iteration 17: 0.15190320397861423\n",
      "Train Loss at iteration 18: 0.1490157905325629\n",
      "Train Loss at iteration 19: 0.146225299245541\n",
      "Train Loss at iteration 20: 0.14352787481821505\n",
      "Train Loss at iteration 21: 0.1409198446299319\n",
      "Train Loss at iteration 22: 0.1383977082372258\n",
      "Train Loss at iteration 23: 0.1359581275985195\n",
      "Train Loss at iteration 24: 0.13359791796719891\n",
      "Train Loss at iteration 25: 0.13131403940027164\n",
      "Train Loss at iteration 26: 0.12910358883438638\n",
      "Train Loss at iteration 27: 0.12696379268514413\n",
      "Train Loss at iteration 28: 0.12489199992940646\n",
      "Train Loss at iteration 29: 0.12288567563374178\n",
      "Train Loss at iteration 30: 0.12094239489527202\n",
      "Train Loss at iteration 31: 0.11905983716402635\n",
      "Train Loss at iteration 32: 0.11723578091849572\n",
      "Train Loss at iteration 33: 0.11546809866843802\n",
      "Train Loss at iteration 34: 0.1137547522611297\n",
      "Train Loss at iteration 35: 0.11209378846921612\n",
      "Train Loss at iteration 36: 0.11048333484009537\n",
      "Train Loss at iteration 37: 0.10892159578839482\n",
      "Train Loss at iteration 38: 0.10740684891458512\n",
      "Train Loss at iteration 39: 0.10593744153412783\n",
      "Train Loss at iteration 40: 0.10451178740278931\n",
      "Train Loss at iteration 41: 0.10312836362488287\n",
      "Train Loss at iteration 42: 0.10178570773223274\n",
      "Train Loss at iteration 43: 0.10048241492259534\n",
      "Train Loss at iteration 44: 0.0992171354471387\n",
      "Train Loss at iteration 45: 0.09798857213736774\n",
      "Train Loss at iteration 46: 0.09679547806260858\n",
      "Train Loss at iteration 47: 0.09563665430982607\n",
      "Train Loss at iteration 48: 0.0945109478781566\n",
      "Train Loss at iteration 49: 0.0934172496810957\n",
      "Train Loss at iteration 50: 0.09235449264978994\n",
      "Train Loss at iteration 51: 0.09132164993135392\n",
      "Train Loss at iteration 52: 0.0903177331765625\n",
      "Train Loss at iteration 53: 0.08934179091166707\n",
      "Train Loss at iteration 54: 0.08839290698944782\n",
      "Train Loss at iteration 55: 0.0874701991149511\n",
      "Train Loss at iteration 56: 0.08657281744167038\n",
      "Train Loss at iteration 57: 0.08569994323421366\n",
      "Train Loss at iteration 58: 0.08485078759376512\n",
      "Train Loss at iteration 59: 0.08402459024288993\n",
      "Train Loss at iteration 60: 0.08322061836645743\n",
      "Train Loss at iteration 61: 0.08243816550566423\n",
      "Train Loss at iteration 62: 0.08167655050233137\n",
      "Train Loss at iteration 63: 0.08093511649082723\n",
      "Train Loss at iteration 64: 0.08021322993513301\n",
      "Train Loss at iteration 65: 0.07951027970872042\n",
      "Train Loss at iteration 66: 0.07882567621505211\n",
      "Train Loss at iteration 67: 0.07815885054664826\n",
      "Train Loss at iteration 68: 0.07750925368078453\n",
      "Train Loss at iteration 69: 0.07687635571000048\n",
      "Train Loss at iteration 70: 0.07625964510570328\n",
      "Train Loss at iteration 71: 0.0756586280132508\n",
      "Train Loss at iteration 72: 0.07507282757699012\n",
      "Train Loss at iteration 73: 0.07450178329381253\n",
      "Train Loss at iteration 74: 0.07394505039386755\n",
      "Train Loss at iteration 75: 0.0734021992471526\n",
      "Train Loss at iteration 76: 0.07287281479476516\n",
      "Train Loss at iteration 77: 0.0723564960036698\n",
      "Train Loss at iteration 78: 0.07185285534389446\n",
      "Train Loss at iteration 79: 0.07136151828712663\n",
      "Train Loss at iteration 80: 0.07088212282573586\n",
      "Train Loss at iteration 81: 0.07041431901129769\n",
      "Train Loss at iteration 82: 0.06995776851174357\n",
      "Train Loss at iteration 83: 0.06951214418630461\n",
      "Train Loss at iteration 84: 0.06907712967746\n",
      "Train Loss at iteration 85: 0.06865241901914022\n",
      "Train Loss at iteration 86: 0.06823771626047198\n",
      "Train Loss at iteration 87: 0.06783273510438821\n",
      "Train Loss at iteration 88: 0.06743719856045816\n",
      "Train Loss at iteration 89: 0.06705083861132478\n",
      "Train Loss at iteration 90: 0.06667339589216599\n",
      "Train Loss at iteration 91: 0.06630461938262398\n",
      "Train Loss at iteration 92: 0.06594426611067351\n",
      "Train Loss at iteration 93: 0.06559210086792486\n",
      "Train Loss at iteration 94: 0.06524789593588043\n",
      "Train Loss at iteration 95: 0.06491143082268705\n",
      "Train Loss at iteration 96: 0.06458249200994651\n",
      "Train Loss at iteration 97: 0.06426087270916697\n",
      "Train Loss at iteration 98: 0.06394637262745778\n",
      "Train Loss at iteration 99: 0.06363879774208661\n",
      "Train Loss at iteration 100: 0.06333796008353688\n",
      "Train Loss at iteration 101: 0.06304367752671805\n",
      "Train Loss at iteration 102: 0.0627557735899979\n",
      "Train Loss at iteration 103: 0.062474077241740034\n",
      "Train Loss at iteration 104: 0.062198422714043834\n",
      "Train Loss at iteration 105: 0.06192864932339774\n",
      "Train Loss at iteration 106: 0.061664601297968934\n",
      "Train Loss at iteration 107: 0.0614061276112646\n",
      "Train Loss at iteration 108: 0.06115308182191157\n",
      "Train Loss at iteration 109: 0.0609053219193119\n",
      "Train Loss at iteration 110: 0.06066271017494234\n",
      "Train Loss at iteration 111: 0.06042511299907549\n",
      "Train Loss at iteration 112: 0.06019240080270999\n",
      "Train Loss at iteration 113: 0.0599644478645059\n",
      "Train Loss at iteration 114: 0.05974113220253043\n",
      "Train Loss at iteration 115: 0.05952233545062647\n",
      "Train Loss at iteration 116: 0.05930794273922542\n",
      "Train Loss at iteration 117: 0.05909784258043205\n",
      "Train Loss at iteration 118: 0.058891926757217335\n",
      "Train Loss at iteration 119: 0.05869009021656089\n",
      "Train Loss at iteration 120: 0.05849223096639224\n",
      "Train Loss at iteration 121: 0.05829824997618561\n",
      "Train Loss at iteration 122: 0.05810805108106897\n",
      "Train Loss at iteration 123: 0.05792154088931413\n",
      "Train Loss at iteration 124: 0.05773862869307953\n",
      "Train Loss at iteration 125: 0.05755922638228326\n",
      "Train Loss at iteration 126: 0.0573832483614878\n",
      "Train Loss at iteration 127: 0.05721061146968403\n",
      "Train Loss at iteration 128: 0.05704123490286509\n",
      "Train Loss at iteration 129: 0.05687504013928667\n",
      "Train Loss at iteration 130: 0.056711950867312816\n",
      "Train Loss at iteration 131: 0.0565518929157517\n",
      "Train Loss at iteration 132: 0.056394794186588604\n",
      "Train Loss at iteration 133: 0.05624058459002777\n",
      "Train Loss at iteration 134: 0.05608919598175775\n",
      "Train Loss at iteration 135: 0.055940562102358454\n",
      "Train Loss at iteration 136: 0.05579461851877136\n",
      "Train Loss at iteration 137: 0.05565130256775724\n",
      "Train Loss at iteration 138: 0.05551055330126882\n",
      "Train Loss at iteration 139: 0.055372311433668796\n",
      "Train Loss at iteration 140: 0.05523651929072582\n",
      "Train Loss at iteration 141: 0.055103120760324295\n",
      "Train Loss at iteration 142: 0.05497206124482573\n",
      "Train Loss at iteration 143: 0.05484328761502242\n",
      "Train Loss at iteration 144: 0.05471674816562582\n",
      "Train Loss at iteration 145: 0.054592392572234666\n",
      "Train Loss at iteration 146: 0.054470171849729895\n",
      "Train Loss at iteration 147: 0.05435003831204528\n",
      "Train Loss at iteration 148: 0.05423194553326486\n",
      "Train Loss at iteration 149: 0.05411584830999989\n",
      "Train Loss at iteration 150: 0.05400170262500015\n",
      "Train Loss at iteration 151: 0.053889465611955525\n",
      "Train Loss at iteration 152: 0.053779095521446385\n",
      "Train Loss at iteration 153: 0.05367055168800188\n",
      "Train Loss at iteration 154: 0.05356379449822746\n",
      "Train Loss at iteration 155: 0.05345878535996403\n",
      "Train Loss at iteration 156: 0.05335548667244291\n",
      "Train Loss at iteration 157: 0.05325386179740154\n",
      "Train Loss at iteration 158: 0.05315387503112677\n",
      "Train Loss at iteration 159: 0.05305549157739345\n",
      "Train Loss at iteration 160: 0.05295867752126734\n",
      "Train Loss at iteration 161: 0.05286339980374243\n",
      "Train Loss at iteration 162: 0.05276962619718414\n",
      "Train Loss at iteration 163: 0.05267732528155027\n",
      "Train Loss at iteration 164: 0.052586466421363595\n",
      "Train Loss at iteration 165: 0.05249701974340989\n",
      "Train Loss at iteration 166: 0.052408956115136913\n",
      "Train Loss at iteration 167: 0.05232224712373049\n",
      "Train Loss at iteration 168: 0.05223686505584444\n",
      "Train Loss at iteration 169: 0.052152782877962656\n",
      "Train Loss at iteration 170: 0.052069974217371376\n",
      "Train Loss at iteration 171: 0.051988413343721736\n",
      "Train Loss at iteration 172: 0.051908075151162136\n",
      "Train Loss at iteration 173: 0.05182893514102175\n",
      "Train Loss at iteration 174: 0.051750969405026465\n",
      "Train Loss at iteration 175: 0.05167415460902962\n",
      "Train Loss at iteration 176: 0.05159846797724038\n",
      "Train Loss at iteration 177: 0.051523887276933\n",
      "Train Loss at iteration 178: 0.05145039080362146\n",
      "Train Loss at iteration 179: 0.05137795736668355\n",
      "Train Loss at iteration 180: 0.05130656627542017\n",
      "Train Loss at iteration 181: 0.05123619732553491\n",
      "Train Loss at iteration 182: 0.05116683078602059\n",
      "Train Loss at iteration 183: 0.05109844738643925\n",
      "Train Loss at iteration 184: 0.05103102830458262\n",
      "Train Loss at iteration 185: 0.05096455515450094\n",
      "Train Loss at iteration 186: 0.05089900997488792\n",
      "Train Loss at iteration 187: 0.050834375217810326\n",
      "Train Loss at iteration 188: 0.05077063373777124\n",
      "Train Loss at iteration 189: 0.05070776878109588\n",
      "Train Loss at iteration 190: 0.05064576397562999\n",
      "Train Loss at iteration 191: 0.05058460332074034\n",
      "Train Loss at iteration 192: 0.05052427117760813\n",
      "Train Loss at iteration 193: 0.05046475225980538\n",
      "Train Loss at iteration 194: 0.050406031624145926\n",
      "Train Loss at iteration 195: 0.05034809466180158\n",
      "Train Loss at iteration 196: 0.05029092708967579\n",
      "Train Loss at iteration 197: 0.05023451494202593\n",
      "Train Loss at iteration 198: 0.05017884456232692\n",
      "Train Loss at iteration 199: 0.05012390259536829\n",
      "Train Loss at iteration 200: 0.05006967597957744\n",
      "Train Loss at iteration 201: 0.050016151939562055\n",
      "Train Loss at iteration 202: 0.04996331797886462\n",
      "Train Loss at iteration 203: 0.04991116187292272\n",
      "Train Loss at iteration 204: 0.0498596716622285\n",
      "Train Loss at iteration 205: 0.049808835645681185\n",
      "Train Loss at iteration 206: 0.0497586423741267\n",
      "Train Loss at iteration 207: 0.04970908064407853\n",
      "Train Loss at iteration 208: 0.04966013949161437\n",
      "Train Loss at iteration 209: 0.04961180818644304\n",
      "Train Loss at iteration 210: 0.04956407622613656\n",
      "Train Loss at iteration 211: 0.049516933330522137\n",
      "Train Loss at iteration 212: 0.04947036943622956\n",
      "Train Loss at iteration 213: 0.049424374691388766\n",
      "Train Loss at iteration 214: 0.04937893945047355\n",
      "Train Loss at iteration 215: 0.04933405426928651\n",
      "Train Loss at iteration 216: 0.04928970990008147\n",
      "Train Loss at iteration 217: 0.04924589728681867\n",
      "Train Loss at iteration 218: 0.049202607560549214\n",
      "Train Loss at iteration 219: 0.04915983203492472\n",
      "Train Loss at iteration 220: 0.04911756220182836\n",
      "Train Loss at iteration 221: 0.049075789727123786\n",
      "Train Loss at iteration 222: 0.04903450644651835\n",
      "Train Loss at iteration 223: 0.04899370436153733\n",
      "Train Loss at iteration 224: 0.048953375635605825\n",
      "Train Loss at iteration 225: 0.04891351259023502\n",
      "Train Loss at iteration 226: 0.04887410770131004\n",
      "Train Loss at iteration 227: 0.048835153595476015\n",
      "Train Loss at iteration 228: 0.04879664304661995\n",
      "Train Loss at iteration 229: 0.048758568972445175\n",
      "Train Loss at iteration 230: 0.04872092443113584\n",
      "Train Loss at iteration 231: 0.04868370261810901\n",
      "Train Loss at iteration 232: 0.04864689686285146\n",
      "Train Loss at iteration 233: 0.04861050062583911\n",
      "Train Loss at iteration 234: 0.04857450749553639\n",
      "Train Loss at iteration 235: 0.048538911185473405\n",
      "Train Loss at iteration 236: 0.04850370553139866\n",
      "Train Loss at iteration 237: 0.04846888448850519\n",
      "Train Loss at iteration 238: 0.04843444212872785\n",
      "Train Loss at iteration 239: 0.04840037263810999\n",
      "Train Loss at iteration 240: 0.048366670314237296\n",
      "Train Loss at iteration 241: 0.048333329563737114\n",
      "Train Loss at iteration 242: 0.048300344899841204\n",
      "Train Loss at iteration 243: 0.048267710940010276\n",
      "Train Loss at iteration 244: 0.04823542240361855\n",
      "Train Loss at iteration 245: 0.04820347410969657\n",
      "Train Loss at iteration 246: 0.04817186097473078\n",
      "Train Loss at iteration 247: 0.04814057801051819\n",
      "Train Loss at iteration 248: 0.04810962032207452\n",
      "Train Loss at iteration 249: 0.048078983105594594\n",
      "Train Loss at iteration 250: 0.04804866164646322\n",
      "Train Loss at iteration 251: 0.04801865131731549\n",
      "Train Loss at iteration 252: 0.047988947576144726\n",
      "Train Loss at iteration 253: 0.047959545964457305\n",
      "Train Loss at iteration 254: 0.047930442105472496\n",
      "Train Loss at iteration 255: 0.04790163170236654\n",
      "Train Loss at iteration 256: 0.0478731105365596\n",
      "Train Loss at iteration 257: 0.0478448744660442\n",
      "Train Loss at iteration 258: 0.0478169194237545\n",
      "Train Loss at iteration 259: 0.047789241415974924\n",
      "Train Loss at iteration 260: 0.04776183652078705\n",
      "Train Loss at iteration 261: 0.04773470088655412\n",
      "Train Loss at iteration 262: 0.04770783073044179\n",
      "Train Loss at iteration 263: 0.04768122233697428\n",
      "Train Loss at iteration 264: 0.04765487205662494\n",
      "Train Loss at iteration 265: 0.04762877630444047\n",
      "Train Loss at iteration 266: 0.04760293155869767\n",
      "Train Loss at iteration 267: 0.04757733435959198\n",
      "Train Loss at iteration 268: 0.047551981307957056\n",
      "Train Loss at iteration 269: 0.04752686906401433\n",
      "Train Loss at iteration 270: 0.047501994346152036\n",
      "Train Loss at iteration 271: 0.04747735392973273\n",
      "Train Loss at iteration 272: 0.04745294464592857\n",
      "Train Loss at iteration 273: 0.047428763380583897\n",
      "Train Loss at iteration 274: 0.04740480707310392\n",
      "Train Loss at iteration 275: 0.047381072715369364\n",
      "Train Loss at iteration 276: 0.047357557350676\n",
      "Train Loss at iteration 277: 0.04733425807269861\n",
      "Train Loss at iteration 278: 0.04731117202447873\n",
      "Train Loss at iteration 279: 0.04728829639743561\n",
      "Train Loss at iteration 280: 0.04726562843039958\n",
      "Train Loss at iteration 281: 0.04724316540866752\n",
      "Train Loss at iteration 282: 0.047220904663079794\n",
      "Train Loss at iteration 283: 0.04719884356911798\n",
      "Train Loss at iteration 284: 0.04717697954602296\n",
      "Train Loss at iteration 285: 0.047155310055932896\n",
      "Train Loss at iteration 286: 0.04713383260304062\n",
      "Train Loss at iteration 287: 0.04711254473276959\n",
      "Train Loss at iteration 288: 0.04709144403096867\n",
      "Train Loss at iteration 289: 0.04707052812312444\n",
      "Train Loss at iteration 290: 0.04704979467359116\n",
      "Train Loss at iteration 291: 0.04702924138483784\n",
      "Train Loss at iteration 292: 0.047008865996711745\n",
      "Train Loss at iteration 293: 0.04698866628571827\n",
      "Train Loss at iteration 294: 0.04696864006431658\n",
      "Train Loss at iteration 295: 0.0469487851802306\n",
      "Train Loss at iteration 296: 0.04692909951577518\n",
      "Train Loss at iteration 297: 0.04690958098719676\n",
      "Train Loss at iteration 298: 0.04689022754402862\n",
      "Train Loss at iteration 299: 0.04687103716845971\n",
      "Train Loss at iteration 300: 0.0468520078747175\n",
      "Train Loss at iteration 301: 0.0468331377084639\n",
      "Train Loss at iteration 302: 0.0468144247462042\n",
      "Train Loss at iteration 303: 0.04679586709470875\n",
      "Train Loss at iteration 304: 0.0467774628904468\n",
      "Train Loss at iteration 305: 0.046759210299032766\n",
      "Train Loss at iteration 306: 0.04674110751468381\n",
      "Train Loss at iteration 307: 0.046723152759689364\n",
      "Train Loss at iteration 308: 0.04670534428389152\n",
      "Train Loss at iteration 309: 0.04668768036417664\n",
      "Train Loss at iteration 310: 0.04667015930397746\n",
      "Train Loss at iteration 311: 0.046652779432785764\n",
      "Train Loss at iteration 312: 0.04663553910567523\n",
      "Train Loss at iteration 313: 0.04661843670283422\n",
      "Train Loss at iteration 314: 0.0466014706291083\n",
      "Train Loss at iteration 315: 0.04658463931355224\n",
      "Train Loss at iteration 316: 0.046567941208991354\n",
      "Train Loss at iteration 317: 0.04655137479159185\n",
      "Train Loss at iteration 318: 0.04653493856044001\n",
      "Train Loss at iteration 319: 0.04651863103713014\n",
      "Train Loss at iteration 320: 0.04650245076536077\n",
      "Train Loss at iteration 321: 0.04648639631053932\n",
      "Train Loss at iteration 322: 0.046470466259394695\n",
      "Train Loss at iteration 323: 0.04645465921959777\n",
      "Train Loss at iteration 324: 0.04643897381938969\n",
      "Train Loss at iteration 325: 0.04642340870721748\n",
      "Train Loss at iteration 326: 0.0464079625513773\n",
      "Train Loss at iteration 327: 0.04639263403966463\n",
      "Train Loss at iteration 328: 0.046377421879031586\n",
      "Train Loss at iteration 329: 0.04636232479525113\n",
      "Train Loss at iteration 330: 0.046347341532587974\n",
      "Train Loss at iteration 331: 0.04633247085347604\n",
      "Train Loss at iteration 332: 0.046317711538202244\n",
      "Train Loss at iteration 333: 0.046303062384596805\n",
      "Train Loss at iteration 334: 0.046288522207729416\n",
      "Train Loss at iteration 335: 0.046274089839611524\n",
      "Train Loss at iteration 336: 0.04625976412890455\n",
      "Train Loss at iteration 337: 0.04624554394063369\n",
      "Train Loss at iteration 338: 0.046231428155907514\n",
      "Train Loss at iteration 339: 0.04621741567164282\n",
      "Train Loss at iteration 340: 0.04620350540029507\n",
      "Train Loss at iteration 341: 0.046189696269593974\n",
      "Train Loss at iteration 342: 0.046175987222284165\n",
      "Train Loss at iteration 343: 0.046162377215871006\n",
      "Train Loss at iteration 344: 0.04614886522237128\n",
      "Train Loss at iteration 345: 0.046135450228068586\n",
      "Train Loss at iteration 346: 0.046122131233273564\n",
      "Train Loss at iteration 347: 0.046108907252088695\n",
      "Train Loss at iteration 348: 0.046095777312177566\n",
      "Train Loss at iteration 349: 0.04608274045453853\n",
      "Train Loss at iteration 350: 0.04606979573328279\n",
      "Train Loss at iteration 351: 0.046056942215416535\n",
      "Train Loss at iteration 352: 0.04604417898062741\n",
      "Train Loss at iteration 353: 0.04603150512107478\n",
      "Train Loss at iteration 354: 0.04601891974118421\n",
      "Train Loss at iteration 355: 0.046006421957445635\n",
      "Train Loss at iteration 356: 0.04599401089821544\n",
      "Train Loss at iteration 357: 0.04598168570352213\n",
      "Train Loss at iteration 358: 0.04596944552487574\n",
      "Train Loss at iteration 359: 0.04595728952508084\n",
      "Train Loss at iteration 360: 0.045945216878052815\n",
      "Train Loss at iteration 361: 0.04593322676863782\n",
      "Train Loss at iteration 362: 0.04592131839243591\n",
      "Train Loss at iteration 363: 0.04590949095562746\n",
      "Train Loss at iteration 364: 0.04589774367480287\n",
      "Train Loss at iteration 365: 0.04588607577679533\n",
      "Train Loss at iteration 366: 0.04587448649851663\n",
      "Train Loss at iteration 367: 0.045862975086796175\n",
      "Train Loss at iteration 368: 0.045851540798222644\n",
      "Train Loss at iteration 369: 0.045840182898988835\n",
      "Train Loss at iteration 370: 0.045828900664739124\n",
      "Train Loss at iteration 371: 0.04581769338041984\n",
      "Train Loss at iteration 372: 0.04580656034013222\n",
      "Train Loss at iteration 373: 0.045795500846988184\n",
      "Train Loss at iteration 374: 0.0457845142129686\n",
      "Train Loss at iteration 375: 0.04577359975878416\n",
      "Train Loss at iteration 376: 0.04576275681373863\n",
      "Train Loss at iteration 377: 0.045751984715594864\n",
      "Train Loss at iteration 378: 0.04574128281044282\n",
      "Train Loss at iteration 379: 0.04573065045257029\n",
      "Train Loss at iteration 380: 0.04572008700433563\n",
      "Train Loss at iteration 381: 0.04570959183604303\n",
      "Train Loss at iteration 382: 0.04569916432581982\n",
      "Train Loss at iteration 383: 0.045688803859495995\n",
      "Train Loss at iteration 384: 0.04567850983048595\n",
      "Train Loss at iteration 385: 0.045668281639672126\n",
      "Train Loss at iteration 386: 0.045658118695290906\n",
      "Train Loss at iteration 387: 0.045648020412820386\n",
      "Train Loss at iteration 388: 0.04563798621487022\n",
      "Train Loss at iteration 389: 0.045628015531073214\n",
      "Train Loss at iteration 390: 0.04561810779797901\n",
      "Train Loss at iteration 391: 0.04560826245894956\n",
      "Train Loss at iteration 392: 0.045598478964056395\n",
      "Train Loss at iteration 393: 0.045588756769979574\n",
      "Train Loss at iteration 394: 0.04557909533990869\n",
      "Train Loss at iteration 395: 0.04556949414344527\n",
      "Train Loss at iteration 396: 0.045559952656506995\n",
      "Train Loss at iteration 397: 0.04555047036123365\n",
      "Train Loss at iteration 398: 0.045541046745894466\n",
      "Train Loss at iteration 399: 0.04553168130479732\n",
      "Train Loss at iteration 400: 0.04552237353819927\n",
      "Train Loss at iteration 401: 0.045513122952218786\n",
      "Train Loss at iteration 402: 0.04550392905874934\n",
      "Train Loss at iteration 403: 0.04549479137537454\n",
      "Train Loss at iteration 404: 0.04548570942528473\n",
      "Train Loss at iteration 405: 0.04547668273719497\n",
      "Train Loss at iteration 406: 0.04546771084526436\n",
      "Train Loss at iteration 407: 0.0454587932890168\n",
      "Train Loss at iteration 408: 0.04544992961326305\n",
      "Train Loss at iteration 409: 0.04544111936802416\n",
      "Train Loss at iteration 410: 0.04543236210845606\n",
      "Train Loss at iteration 411: 0.045423657394775545\n",
      "Train Loss at iteration 412: 0.04541500479218739\n",
      "Train Loss at iteration 413: 0.04540640387081281\n",
      "Train Loss at iteration 414: 0.045397854205618995\n",
      "Train Loss at iteration 415: 0.045389355376349874\n",
      "Train Loss at iteration 416: 0.045380906967457973\n",
      "Train Loss at iteration 417: 0.045372508568037546\n",
      "Train Loss at iteration 418: 0.04536415977175858\n",
      "Train Loss at iteration 419: 0.04535586017680216\n",
      "Train Loss at iteration 420: 0.045347609385796614\n",
      "Train Loss at iteration 421: 0.04533940700575499\n",
      "Train Loss at iteration 422: 0.04533125264801335\n",
      "Train Loss at iteration 423: 0.04532314592817017\n",
      "Train Loss at iteration 424: 0.04531508646602668\n",
      "Train Loss at iteration 425: 0.04530707388552827\n",
      "Train Loss at iteration 426: 0.04529910781470674\n",
      "Train Loss at iteration 427: 0.045291187885623516\n",
      "Train Loss at iteration 428: 0.04528331373431394\n",
      "Train Loss at iteration 429: 0.04527548500073212\n",
      "Train Loss at iteration 430: 0.04526770132869711\n",
      "Train Loss at iteration 431: 0.04525996236583957\n",
      "Train Loss at iteration 432: 0.04525226776354952\n",
      "Train Loss at iteration 433: 0.045244617176924865\n",
      "Train Loss at iteration 434: 0.04523701026472071\n",
      "Train Loss at iteration 435: 0.045229446689299575\n",
      "Train Loss at iteration 436: 0.045221926116582295\n",
      "Train Loss at iteration 437: 0.0452144482159998\n",
      "Train Loss at iteration 438: 0.04520701266044561\n",
      "Train Loss at iteration 439: 0.04519961912622913\n",
      "Train Loss at iteration 440: 0.045192267293029595\n",
      "Train Loss at iteration 441: 0.04518495684385088\n",
      "Train Loss at iteration 442: 0.045177687464976926\n",
      "Train Loss at iteration 443: 0.04517045884592783\n",
      "Train Loss at iteration 444: 0.04516327067941683\n",
      "Train Loss at iteration 445: 0.045156122661307724\n",
      "Train Loss at iteration 446: 0.04514901449057306\n",
      "Train Loss at iteration 447: 0.04514194586925306\n",
      "Train Loss at iteration 448: 0.04513491650241505\n",
      "Train Loss at iteration 449: 0.04512792609811362\n",
      "Train Loss at iteration 450: 0.04512097436735132\n",
      "Train Loss at iteration 451: 0.04511406102404007\n",
      "Train Loss at iteration 452: 0.04510718578496312\n",
      "Train Loss at iteration 453: 0.04510034836973753\n",
      "Train Loss at iteration 454: 0.04509354850077738\n",
      "Train Loss at iteration 455: 0.045086785903257404\n",
      "Train Loss at iteration 456: 0.0450800603050773\n",
      "Train Loss at iteration 457: 0.04507337143682644\n",
      "Train Loss at iteration 458: 0.04506671903174934\n",
      "Train Loss at iteration 459: 0.04506010282571144\n",
      "Train Loss at iteration 460: 0.04505352255716559\n",
      "Train Loss at iteration 461: 0.04504697796711886\n",
      "Train Loss at iteration 462: 0.04504046879910002\n",
      "Train Loss at iteration 463: 0.04503399479912751\n",
      "Train Loss at iteration 464: 0.045027555715677674\n",
      "Train Loss at iteration 465: 0.04502115129965384\n",
      "Train Loss at iteration 466: 0.04501478130435548\n",
      "Train Loss at iteration 467: 0.04500844548544822\n",
      "Train Loss at iteration 468: 0.04500214360093392\n",
      "Train Loss at iteration 469: 0.04499587541112152\n",
      "Train Loss at iteration 470: 0.044989640678598154\n",
      "Train Loss at iteration 471: 0.04498343916820074\n",
      "Train Loss at iteration 472: 0.044977270646988\n",
      "Train Loss at iteration 473: 0.044971134884212866\n",
      "Train Loss at iteration 474: 0.044965031651295374\n",
      "Train Loss at iteration 475: 0.04495896072179589\n",
      "Train Loss at iteration 476: 0.04495292187138873\n",
      "Train Loss at iteration 477: 0.044946914877836246\n",
      "Train Loss at iteration 478: 0.044940939520963195\n",
      "Train Loss at iteration 479: 0.044934995582631594\n",
      "Train Loss at iteration 480: 0.04492908284671584\n",
      "Train Loss at iteration 481: 0.04492320109907827\n",
      "Train Loss at iteration 482: 0.04491735012754507\n",
      "Train Loss at iteration 483: 0.04491152972188253\n",
      "Train Loss at iteration 484: 0.04490573967377358\n",
      "Train Loss at iteration 485: 0.044899979776794795\n",
      "Train Loss at iteration 486: 0.04489424982639367\n",
      "Train Loss at iteration 487: 0.04488854961986615\n",
      "Train Loss at iteration 488: 0.04488287895633469\n",
      "Train Loss at iteration 489: 0.0448772376367264\n",
      "Train Loss at iteration 490: 0.04487162546375164\n",
      "Train Loss at iteration 491: 0.04486604224188293\n",
      "Train Loss at iteration 492: 0.044860487777334083\n",
      "Train Loss at iteration 493: 0.044854961878039744\n",
      "Train Loss at iteration 494: 0.044849464353635106\n",
      "Train Loss at iteration 495: 0.044843995015436004\n",
      "Train Loss at iteration 496: 0.0448385536764193\n",
      "Train Loss at iteration 497: 0.04483314015120345\n",
      "Train Loss at iteration 498: 0.04482775425602946\n",
      "Train Loss at iteration 499: 0.04482239580874204\n",
      "Train Loss at iteration 500: 0.044817064628771094\n",
      "Train Loss at iteration 501: 0.04481176053711335\n",
      "Train Loss at iteration 502: 0.04480648335631444\n",
      "Train Loss at iteration 503: 0.04480123291045102\n",
      "Train Loss at iteration 504: 0.04479600902511332\n",
      "Train Loss at iteration 505: 0.0447908115273878\n",
      "Train Loss at iteration 506: 0.0447856402458402\n",
      "Train Loss at iteration 507: 0.04478049501049865\n",
      "Train Loss at iteration 508: 0.044775375652837165\n",
      "Train Loss at iteration 509: 0.04477028200575931\n",
      "Train Loss at iteration 510: 0.04476521390358209\n",
      "Train Loss at iteration 511: 0.044760171182020046\n",
      "Train Loss at iteration 512: 0.04475515367816963\n",
      "Train Loss at iteration 513: 0.04475016123049377\n",
      "Train Loss at iteration 514: 0.04474519367880662\n",
      "Train Loss at iteration 515: 0.04474025086425851\n",
      "Train Loss at iteration 516: 0.044735332629321216\n",
      "Train Loss at iteration 517: 0.04473043881777328\n",
      "Train Loss at iteration 518: 0.04472556927468564\n",
      "Train Loss at iteration 519: 0.0447207238464074\n",
      "Train Loss at iteration 520: 0.04471590238055185\n",
      "Train Loss at iteration 521: 0.04471110472598259\n",
      "Train Loss at iteration 522: 0.04470633073279995\n",
      "Train Loss at iteration 523: 0.044701580252327455\n",
      "Train Loss at iteration 524: 0.04469685313709867\n",
      "Train Loss at iteration 525: 0.044692149240844\n",
      "Train Loss at iteration 526: 0.04468746841847786\n",
      "Train Loss at iteration 527: 0.04468281052608589\n",
      "Train Loss at iteration 528: 0.04467817542091241\n",
      "Train Loss at iteration 529: 0.044673562961348\n",
      "Train Loss at iteration 530: 0.044668973006917294\n",
      "Train Loss at iteration 531: 0.04466440541826687\n",
      "Train Loss at iteration 532: 0.044659860057153396\n",
      "Train Loss at iteration 533: 0.04465533678643186\n",
      "Train Loss at iteration 534: 0.04465083547004394\n",
      "Train Loss at iteration 535: 0.0446463559730066\n",
      "Train Loss at iteration 536: 0.044641898161400816\n",
      "Train Loss at iteration 537: 0.04463746190236039\n",
      "Train Loss at iteration 538: 0.044633047064061\n",
      "Train Loss at iteration 539: 0.04462865351570933\n",
      "Train Loss at iteration 540: 0.044624281127532354\n",
      "Train Loss at iteration 541: 0.044619929770766814\n",
      "Train Loss at iteration 542: 0.04461559931764873\n",
      "Train Loss at iteration 543: 0.044611289641403165\n",
      "Train Loss at iteration 544: 0.04460700061623404\n",
      "Train Loss at iteration 545: 0.044602732117314074\n",
      "Train Loss at iteration 546: 0.04459848402077501\n",
      "Train Loss at iteration 547: 0.044594256203697655\n",
      "Train Loss at iteration 548: 0.04459004854410244\n",
      "Train Loss at iteration 549: 0.04458586092093973\n",
      "Train Loss at iteration 550: 0.04458169321408058\n",
      "Train Loss at iteration 551: 0.04457754530430733\n",
      "Train Loss at iteration 552: 0.044573417073304505\n",
      "Train Loss at iteration 553: 0.04456930840364981\n",
      "Train Loss at iteration 554: 0.044565219178805154\n",
      "Train Loss at iteration 555: 0.0445611492831078\n",
      "Train Loss at iteration 556: 0.04455709860176182\n",
      "Train Loss at iteration 557: 0.04455306702082932\n",
      "Train Loss at iteration 558: 0.04454905442722211\n",
      "Train Loss at iteration 559: 0.04454506070869317\n",
      "Train Loss at iteration 560: 0.044541085753828605\n",
      "Train Loss at iteration 561: 0.04453712945203921\n",
      "Train Loss at iteration 562: 0.044533191693552694\n",
      "Train Loss at iteration 563: 0.04452927236940543\n",
      "Train Loss at iteration 564: 0.044525371371434835\n",
      "Train Loss at iteration 565: 0.04452148859227146\n",
      "Train Loss at iteration 566: 0.04451762392533136\n",
      "Train Loss at iteration 567: 0.044513777264808534\n",
      "Train Loss at iteration 568: 0.04450994850566745\n",
      "Train Loss at iteration 569: 0.04450613754363561\n",
      "Train Loss at iteration 570: 0.04450234427519633\n",
      "Train Loss at iteration 571: 0.044498568597581445\n",
      "Train Loss at iteration 572: 0.044494810408764233\n",
      "Train Loss at iteration 573: 0.0444910696074524\n",
      "Train Loss at iteration 574: 0.04448734609308112\n",
      "Train Loss at iteration 575: 0.04448363976580615\n",
      "Train Loss at iteration 576: 0.044479950526497065\n",
      "Train Loss at iteration 577: 0.044476278276730584\n",
      "Train Loss at iteration 578: 0.044472622918783886\n",
      "Train Loss at iteration 579: 0.04446898435562818\n",
      "Train Loss at iteration 580: 0.04446536249092216\n",
      "Train Loss at iteration 581: 0.04446175722900564\n",
      "Train Loss at iteration 582: 0.04445816847489328\n",
      "Train Loss at iteration 583: 0.044454596134268325\n",
      "Train Loss at iteration 584: 0.04445104011347649\n",
      "Train Loss at iteration 585: 0.04444750031951983\n",
      "Train Loss at iteration 586: 0.044443976660050746\n",
      "Train Loss at iteration 587: 0.044440469043366085\n",
      "Train Loss at iteration 588: 0.04443697737840121\n",
      "Train Loss at iteration 589: 0.044433501574724234\n",
      "Train Loss at iteration 590: 0.04443004154253032\n",
      "Train Loss at iteration 591: 0.04442659719263594\n",
      "Train Loss at iteration 592: 0.04442316843647336\n",
      "Train Loss at iteration 593: 0.04441975518608504\n",
      "Train Loss at iteration 594: 0.044416357354118224\n",
      "Train Loss at iteration 595: 0.04441297485381943\n",
      "Train Loss at iteration 596: 0.0444096075990293\n",
      "Train Loss at iteration 597: 0.0444062555041771\n",
      "Train Loss at iteration 598: 0.04440291848427569\n",
      "Train Loss at iteration 599: 0.04439959645491621\n",
      "Train Loss at iteration 600: 0.044396289332263056\n",
      "Train Loss at iteration 601: 0.0443929970330489\n",
      "Train Loss at iteration 602: 0.04438971947456955\n",
      "Train Loss at iteration 603: 0.04438645657467917\n",
      "Train Loss at iteration 604: 0.0443832082517853\n",
      "Train Loss at iteration 605: 0.044379974424844135\n",
      "Train Loss at iteration 606: 0.04437675501335568\n",
      "Train Loss at iteration 607: 0.0443735499373591\n",
      "Train Loss at iteration 608: 0.04437035911742801\n",
      "Train Loss at iteration 609: 0.04436718247466597\n",
      "Train Loss at iteration 610: 0.044364019930701784\n",
      "Train Loss at iteration 611: 0.044360871407685146\n",
      "Train Loss at iteration 612: 0.04435773682828211\n",
      "Train Loss at iteration 613: 0.04435461611567068\n",
      "Train Loss at iteration 614: 0.04435150919353655\n",
      "Train Loss at iteration 615: 0.04434841598606867\n",
      "Train Loss at iteration 616: 0.04434533641795507\n",
      "Train Loss at iteration 617: 0.04434227041437865\n",
      "Train Loss at iteration 618: 0.04433921790101303\n",
      "Train Loss at iteration 619: 0.0443361788040183\n",
      "Train Loss at iteration 620: 0.044333153050037176\n",
      "Train Loss at iteration 621: 0.044330140566190765\n",
      "Train Loss at iteration 622: 0.0443271412800747\n",
      "Train Loss at iteration 623: 0.04432415511975516\n",
      "Train Loss at iteration 624: 0.044321182013764934\n",
      "Train Loss at iteration 625: 0.04431822189109966\n",
      "Train Loss at iteration 626: 0.044315274681213906\n",
      "Train Loss at iteration 627: 0.04431234031401748\n",
      "Train Loss at iteration 628: 0.04430941871987162\n",
      "Train Loss at iteration 629: 0.04430650982958538\n",
      "Train Loss at iteration 630: 0.04430361357441191\n",
      "Train Loss at iteration 631: 0.04430072988604489\n",
      "Train Loss at iteration 632: 0.04429785869661487\n",
      "Train Loss at iteration 633: 0.04429499993868587\n",
      "Train Loss at iteration 634: 0.04429215354525174\n",
      "Train Loss at iteration 635: 0.04428931944973277\n",
      "Train Loss at iteration 636: 0.04428649758597227\n",
      "Train Loss at iteration 637: 0.04428368788823311\n",
      "Train Loss at iteration 638: 0.04428089029119442\n",
      "Train Loss at iteration 639: 0.04427810472994826\n",
      "Train Loss at iteration 640: 0.044275331139996314\n",
      "Train Loss at iteration 641: 0.04427256945724665\n",
      "Train Loss at iteration 642: 0.04426981961801047\n",
      "Train Loss at iteration 643: 0.044267081558999034\n",
      "Train Loss at iteration 644: 0.044264355217320306\n",
      "Train Loss at iteration 645: 0.04426164053047604\n",
      "Train Loss at iteration 646: 0.04425893743635854\n",
      "Train Loss at iteration 647: 0.044256245873247696\n",
      "Train Loss at iteration 648: 0.044253565779807914\n",
      "Train Loss at iteration 649: 0.04425089709508511\n",
      "Train Loss at iteration 650: 0.044248239758503775\n",
      "Train Loss at iteration 651: 0.04424559370986403\n",
      "Train Loss at iteration 652: 0.0442429588893387\n",
      "Train Loss at iteration 653: 0.044240335237470466\n",
      "Train Loss at iteration 654: 0.04423772269516901\n",
      "Train Loss at iteration 655: 0.044235121203708164\n",
      "Train Loss at iteration 656: 0.04423253070472317\n",
      "Train Loss at iteration 657: 0.04422995114020786\n",
      "Train Loss at iteration 658: 0.04422738245251199\n",
      "Train Loss at iteration 659: 0.04422482458433844\n",
      "Train Loss at iteration 660: 0.044222277478740635\n",
      "Train Loss at iteration 661: 0.04421974107911981\n",
      "Train Loss at iteration 662: 0.04421721532922239\n",
      "Train Loss at iteration 663: 0.044214700173137444\n",
      "Train Loss at iteration 664: 0.04421219555529402\n",
      "Train Loss at iteration 665: 0.04420970142045869\n",
      "Train Loss at iteration 666: 0.04420721771373289\n",
      "Train Loss at iteration 667: 0.044204744380550595\n",
      "Train Loss at iteration 668: 0.04420228136667567\n",
      "Train Loss at iteration 669: 0.04419982861819949\n",
      "Train Loss at iteration 670: 0.044197386081538524\n",
      "Train Loss at iteration 671: 0.044194953703431884\n",
      "Train Loss at iteration 672: 0.04419253143093898\n",
      "Train Loss at iteration 673: 0.0441901192114371\n",
      "Train Loss at iteration 674: 0.04418771699261916\n",
      "Train Loss at iteration 675: 0.044185324722491294\n",
      "Train Loss at iteration 676: 0.044182942349370584\n",
      "Train Loss at iteration 677: 0.04418056982188282\n",
      "Train Loss at iteration 678: 0.04417820708896027\n",
      "Train Loss at iteration 679: 0.04417585409983931\n",
      "Train Loss at iteration 680: 0.04417351080405837\n",
      "Train Loss at iteration 681: 0.044171177151455666\n",
      "Train Loss at iteration 682: 0.04416885309216705\n",
      "Train Loss at iteration 683: 0.04416653857662386\n",
      "Train Loss at iteration 684: 0.044164233555550755\n",
      "Train Loss at iteration 685: 0.044161937979963685\n",
      "Train Loss at iteration 686: 0.04415965180116774\n",
      "Train Loss at iteration 687: 0.04415737497075506\n",
      "Train Loss at iteration 688: 0.04415510744060286\n",
      "Train Loss at iteration 689: 0.044152849162871335\n",
      "Train Loss at iteration 690: 0.04415060009000168\n",
      "Train Loss at iteration 691: 0.044148360174714046\n",
      "Train Loss at iteration 692: 0.044146129370005636\n",
      "Train Loss at iteration 693: 0.04414390762914867\n",
      "Train Loss at iteration 694: 0.044141694905688506\n",
      "Train Loss at iteration 695: 0.04413949115344168\n",
      "Train Loss at iteration 696: 0.04413729632649399\n",
      "Train Loss at iteration 697: 0.044135110379198664\n",
      "Train Loss at iteration 698: 0.04413293326617441\n",
      "Train Loss at iteration 699: 0.044130764942303585\n",
      "Train Loss at iteration 700: 0.04412860536273043\n",
      "Train Loss at iteration 701: 0.044126454482859116\n",
      "Train Loss at iteration 702: 0.04412431225835207\n",
      "Train Loss at iteration 703: 0.04412217864512806\n",
      "Train Loss at iteration 704: 0.04412005359936051\n",
      "Train Loss at iteration 705: 0.044117937077475716\n",
      "Train Loss at iteration 706: 0.0441158290361511\n",
      "Train Loss at iteration 707: 0.04411372943231344\n",
      "Train Loss at iteration 708: 0.044111638223137246\n",
      "Train Loss at iteration 709: 0.044109555366042975\n",
      "Train Loss at iteration 710: 0.04410748081869542\n",
      "Train Loss at iteration 711: 0.044105414539001964\n",
      "Train Loss at iteration 712: 0.04410335648511098\n",
      "Train Loss at iteration 713: 0.044101306615410195\n",
      "Train Loss at iteration 714: 0.044099264888525024\n",
      "Train Loss at iteration 715: 0.044097231263316994\n",
      "Train Loss at iteration 716: 0.04409520569888213\n",
      "Train Loss at iteration 717: 0.04409318815454936\n",
      "Train Loss at iteration 718: 0.04409117858987896\n",
      "Train Loss at iteration 719: 0.044089176964661005\n",
      "Train Loss at iteration 720: 0.04408718323891381\n",
      "Train Loss at iteration 721: 0.044085197372882405\n",
      "Train Loss at iteration 722: 0.044083219327036995\n",
      "Train Loss at iteration 723: 0.04408124906207148\n",
      "Train Loss at iteration 724: 0.044079286538901974\n",
      "Train Loss at iteration 725: 0.04407733171866527\n",
      "Train Loss at iteration 726: 0.04407538456271746\n",
      "Train Loss at iteration 727: 0.044073445032632365\n",
      "Train Loss at iteration 728: 0.04407151309020021\n",
      "Train Loss at iteration 729: 0.044069588697426124\n",
      "Train Loss at iteration 730: 0.04406767181652868\n",
      "Train Loss at iteration 731: 0.04406576240993865\n",
      "Train Loss at iteration 732: 0.04406386044029738\n",
      "Train Loss at iteration 733: 0.04406196587045559\n",
      "Train Loss at iteration 734: 0.04406007866347196\n",
      "Train Loss at iteration 735: 0.044058198782611684\n",
      "Train Loss at iteration 736: 0.04405632619134522\n",
      "Train Loss at iteration 737: 0.04405446085334688\n",
      "Train Loss at iteration 738: 0.04405260273249357\n",
      "Train Loss at iteration 739: 0.044050751792863406\n",
      "Train Loss at iteration 740: 0.04404890799873445\n",
      "Train Loss at iteration 741: 0.044047071314583366\n",
      "Train Loss at iteration 742: 0.044045241705084205\n",
      "Train Loss at iteration 743: 0.04404341913510707\n",
      "Train Loss at iteration 744: 0.044041603569716876\n",
      "Train Loss at iteration 745: 0.044039794974172077\n",
      "Train Loss at iteration 746: 0.04403799331392343\n",
      "Train Loss at iteration 747: 0.04403619855461279\n",
      "Train Loss at iteration 748: 0.04403441066207182\n",
      "Train Loss at iteration 749: 0.044032629602320836\n",
      "Train Loss at iteration 750: 0.04403085534156756\n",
      "Train Loss at iteration 751: 0.04402908784620595\n",
      "Train Loss at iteration 752: 0.044027327082815\n",
      "Train Loss at iteration 753: 0.04402557301815756\n",
      "Train Loss at iteration 754: 0.04402382561917917\n",
      "Train Loss at iteration 755: 0.044022084853006904\n",
      "Train Loss at iteration 756: 0.044020350686948206\n",
      "Train Loss at iteration 757: 0.0440186230884898\n",
      "Train Loss at iteration 758: 0.04401690202529645\n",
      "Train Loss at iteration 759: 0.044015187465209965\n",
      "Train Loss at iteration 760: 0.044013479376248006\n",
      "Train Loss at iteration 761: 0.044011777726602974\n",
      "Train Loss at iteration 762: 0.04401008248464094\n",
      "Train Loss at iteration 763: 0.04400839361890059\n",
      "Train Loss at iteration 764: 0.04400671109809205\n",
      "Train Loss at iteration 765: 0.04400503489109589\n",
      "Train Loss at iteration 766: 0.04400336496696205\n",
      "Train Loss at iteration 767: 0.04400170129490873\n",
      "Train Loss at iteration 768: 0.044000043844321435\n",
      "Train Loss at iteration 769: 0.04399839258475183\n",
      "Train Loss at iteration 770: 0.043996747485916816\n",
      "Train Loss at iteration 771: 0.0439951085176974\n",
      "Train Loss at iteration 772: 0.04399347565013772\n",
      "Train Loss at iteration 773: 0.0439918488534441\n",
      "Train Loss at iteration 774: 0.04399022809798393\n",
      "Train Loss at iteration 775: 0.04398861335428478\n",
      "Train Loss at iteration 776: 0.04398700459303331\n",
      "Train Loss at iteration 777: 0.043985401785074416\n",
      "Train Loss at iteration 778: 0.04398380490141013\n",
      "Train Loss at iteration 779: 0.04398221391319873\n",
      "Train Loss at iteration 780: 0.04398062879175379\n",
      "Train Loss at iteration 781: 0.04397904950854318\n",
      "Train Loss at iteration 782: 0.04397747603518816\n",
      "Train Loss at iteration 783: 0.04397590834346242\n",
      "Train Loss at iteration 784: 0.043974346405291166\n",
      "Train Loss at iteration 785: 0.04397279019275021\n",
      "Train Loss at iteration 786: 0.04397123967806502\n",
      "Train Loss at iteration 787: 0.04396969483360984\n",
      "Train Loss at iteration 788: 0.04396815563190675\n",
      "Train Loss at iteration 789: 0.04396662204562483\n",
      "Train Loss at iteration 790: 0.04396509404757924\n",
      "Train Loss at iteration 791: 0.043963571610730265\n",
      "Train Loss at iteration 792: 0.043962054708182596\n",
      "Train Loss at iteration 793: 0.04396054331318433\n",
      "Train Loss at iteration 794: 0.043959037399126134\n",
      "Train Loss at iteration 795: 0.04395753693954042\n",
      "Train Loss at iteration 796: 0.04395604190810048\n",
      "Train Loss at iteration 797: 0.043954552278619594\n",
      "Train Loss at iteration 798: 0.04395306802505027\n",
      "Train Loss at iteration 799: 0.04395158912148338\n",
      "Train Loss at iteration 800: 0.04395011554214727\n",
      "Train Loss at iteration 801: 0.043948647261407056\n",
      "Train Loss at iteration 802: 0.04394718425376368\n",
      "Train Loss at iteration 803: 0.043945726493853234\n",
      "Train Loss at iteration 804: 0.043944273956446026\n",
      "Train Loss at iteration 805: 0.04394282661644592\n",
      "Train Loss at iteration 806: 0.04394138444888935\n",
      "Train Loss at iteration 807: 0.04393994742894474\n",
      "Train Loss at iteration 808: 0.0439385155319116\n",
      "Train Loss at iteration 809: 0.0439370887332198\n",
      "Train Loss at iteration 810: 0.04393566700842876\n",
      "Train Loss at iteration 811: 0.043934250333226704\n",
      "Train Loss at iteration 812: 0.04393283868342991\n",
      "Train Loss at iteration 813: 0.043931432034981954\n",
      "Train Loss at iteration 814: 0.04393003036395292\n",
      "Train Loss at iteration 815: 0.043928633646538734\n",
      "Train Loss at iteration 816: 0.04392724185906034\n",
      "Train Loss at iteration 817: 0.04392585497796304\n",
      "Train Loss at iteration 818: 0.04392447297981569\n",
      "Train Loss at iteration 819: 0.04392309584131007\n",
      "Train Loss at iteration 820: 0.04392172353926008\n",
      "Train Loss at iteration 821: 0.043920356050601024\n",
      "Train Loss at iteration 822: 0.043918993352389024\n",
      "Train Loss at iteration 823: 0.04391763542180015\n",
      "Train Loss at iteration 824: 0.04391628223612982\n",
      "Train Loss at iteration 825: 0.04391493377279211\n",
      "Train Loss at iteration 826: 0.04391359000931902\n",
      "Train Loss at iteration 827: 0.043912250923359784\n",
      "Train Loss at iteration 828: 0.04391091649268026\n",
      "Train Loss at iteration 829: 0.04390958669516214\n",
      "Train Loss at iteration 830: 0.043908261508802435\n",
      "Train Loss at iteration 831: 0.043906940911712634\n",
      "Train Loss at iteration 832: 0.04390562488211817\n",
      "Train Loss at iteration 833: 0.04390431339835773\n",
      "Train Loss at iteration 834: 0.04390300643888256\n",
      "Train Loss at iteration 835: 0.043901703982255846\n",
      "Train Loss at iteration 836: 0.04390040600715211\n",
      "Train Loss at iteration 837: 0.0438991124923565\n",
      "Train Loss at iteration 838: 0.043897823416764174\n",
      "Train Loss at iteration 839: 0.04389653875937972\n",
      "Train Loss at iteration 840: 0.04389525849931642\n",
      "Train Loss at iteration 841: 0.04389398261579578\n",
      "Train Loss at iteration 842: 0.043892711088146755\n",
      "Train Loss at iteration 843: 0.04389144389580523\n",
      "Train Loss at iteration 844: 0.0438901810183134\n",
      "Train Loss at iteration 845: 0.04388892243531908\n",
      "Train Loss at iteration 846: 0.043887668126575276\n",
      "Train Loss at iteration 847: 0.04388641807193939\n",
      "Train Loss at iteration 848: 0.04388517225137272\n",
      "Train Loss at iteration 849: 0.04388393064493991\n",
      "Train Loss at iteration 850: 0.04388269323280827\n",
      "Train Loss at iteration 851: 0.043881459995247264\n",
      "Train Loss at iteration 852: 0.0438802309126279\n",
      "Train Loss at iteration 853: 0.04387900596542215\n",
      "Train Loss at iteration 854: 0.04387778513420238\n",
      "Train Loss at iteration 855: 0.04387656839964079\n",
      "Train Loss at iteration 856: 0.043875355742508865\n",
      "Train Loss at iteration 857: 0.043874147143676774\n",
      "Train Loss at iteration 858: 0.04387294258411287\n",
      "Train Loss at iteration 859: 0.04387174204488305\n",
      "Train Loss at iteration 860: 0.04387054550715029\n",
      "Train Loss at iteration 861: 0.04386935295217408\n",
      "Train Loss at iteration 862: 0.043868164361309823\n",
      "Train Loss at iteration 863: 0.043866979716008364\n",
      "Train Loss at iteration 864: 0.04386579899781542\n",
      "Train Loss at iteration 865: 0.04386462218837107\n",
      "Train Loss at iteration 866: 0.043863449269409195\n",
      "Train Loss at iteration 867: 0.043862280222756976\n",
      "Train Loss at iteration 868: 0.04386111503033436\n",
      "Train Loss at iteration 869: 0.04385995367415355\n",
      "Train Loss at iteration 870: 0.043858796136318504\n",
      "Train Loss at iteration 871: 0.04385764239902438\n",
      "Train Loss at iteration 872: 0.043856492444557006\n",
      "Train Loss at iteration 873: 0.04385534625529253\n",
      "Train Loss at iteration 874: 0.04385420381369668\n",
      "Train Loss at iteration 875: 0.043853065102324486\n",
      "Train Loss at iteration 876: 0.043851930103819665\n",
      "Train Loss at iteration 877: 0.043850798800914095\n",
      "Train Loss at iteration 878: 0.043849671176427456\n",
      "Train Loss at iteration 879: 0.04384854721326661\n",
      "Train Loss at iteration 880: 0.04384742689442519\n",
      "Train Loss at iteration 881: 0.04384631020298313\n",
      "Train Loss at iteration 882: 0.04384519712210612\n",
      "Train Loss at iteration 883: 0.04384408763504519\n",
      "Train Loss at iteration 884: 0.04384298172513622\n",
      "Train Loss at iteration 885: 0.04384187937579946\n",
      "Train Loss at iteration 886: 0.04384078057053909\n",
      "Train Loss at iteration 887: 0.04383968529294274\n",
      "Train Loss at iteration 888: 0.04383859352668099\n",
      "Train Loss at iteration 889: 0.04383750525550701\n",
      "Train Loss at iteration 890: 0.04383642046325601\n",
      "Train Loss at iteration 891: 0.043835339133844846\n",
      "Train Loss at iteration 892: 0.04383426125127152\n",
      "Train Loss at iteration 893: 0.04383318679961482\n",
      "Train Loss at iteration 894: 0.043832115763033766\n",
      "Train Loss at iteration 895: 0.04383104812576727\n",
      "Train Loss at iteration 896: 0.04382998387213361\n",
      "Train Loss at iteration 897: 0.043828922986530086\n",
      "Train Loss at iteration 898: 0.04382786545343249\n",
      "Train Loss at iteration 899: 0.04382681125739476\n",
      "Train Loss at iteration 900: 0.0438257603830485\n",
      "Train Loss at iteration 901: 0.0438247128151026\n",
      "Train Loss at iteration 902: 0.043823668538342755\n",
      "Train Loss at iteration 903: 0.04382262753763108\n",
      "Train Loss at iteration 904: 0.04382158979790573\n",
      "Train Loss at iteration 905: 0.043820555304180386\n",
      "Train Loss at iteration 906: 0.04381952404154395\n",
      "Train Loss at iteration 907: 0.043818495995160085\n",
      "Train Loss at iteration 908: 0.0438174711502668\n",
      "Train Loss at iteration 909: 0.04381644949217605\n",
      "Train Loss at iteration 910: 0.04381543100627336\n",
      "Train Loss at iteration 911: 0.04381441567801741\n",
      "Train Loss at iteration 912: 0.043813403492939614\n",
      "Train Loss at iteration 913: 0.04381239443664377\n",
      "Train Loss at iteration 914: 0.04381138849480564\n",
      "Train Loss at iteration 915: 0.04381038565317256\n",
      "Train Loss at iteration 916: 0.043809385897563044\n",
      "Train Loss at iteration 917: 0.043808389213866455\n",
      "Train Loss at iteration 918: 0.043807395588042516\n",
      "Train Loss at iteration 919: 0.043806405006121055\n",
      "Train Loss at iteration 920: 0.04380541745420153\n",
      "Train Loss at iteration 921: 0.0438044329184527\n",
      "Train Loss at iteration 922: 0.04380345138511224\n",
      "Train Loss at iteration 923: 0.04380247284048635\n",
      "Train Loss at iteration 924: 0.04380149727094943\n",
      "Train Loss at iteration 925: 0.04380052466294365\n",
      "Train Loss at iteration 926: 0.043799555002978684\n",
      "Train Loss at iteration 927: 0.04379858827763122\n",
      "Train Loss at iteration 928: 0.04379762447354471\n",
      "Train Loss at iteration 929: 0.04379666357742894\n",
      "Train Loss at iteration 930: 0.04379570557605971\n",
      "Train Loss at iteration 931: 0.043794750456278474\n",
      "Train Loss at iteration 932: 0.04379379820499198\n",
      "Train Loss at iteration 933: 0.043792848809171926\n",
      "Train Loss at iteration 934: 0.04379190225585459\n",
      "Train Loss at iteration 935: 0.04379095853214055\n",
      "Train Loss at iteration 936: 0.04379001762519426\n",
      "Train Loss at iteration 937: 0.043789079522243736\n",
      "Train Loss at iteration 938: 0.043788144210580256\n",
      "Train Loss at iteration 939: 0.04378721167755794\n",
      "Train Loss at iteration 940: 0.043786281910593534\n",
      "Train Loss at iteration 941: 0.043785354897165944\n",
      "Train Loss at iteration 942: 0.043784430624816\n",
      "Train Loss at iteration 943: 0.04378350908114605\n",
      "Train Loss at iteration 944: 0.04378259025381975\n",
      "Train Loss at iteration 945: 0.04378167413056158\n",
      "Train Loss at iteration 946: 0.04378076069915666\n",
      "Train Loss at iteration 947: 0.04377984994745036\n",
      "Train Loss at iteration 948: 0.04377894186334794\n",
      "Train Loss at iteration 949: 0.043778036434814364\n",
      "Train Loss at iteration 950: 0.043777133649873824\n",
      "Train Loss at iteration 951: 0.043776233496609564\n",
      "Train Loss at iteration 952: 0.04377533596316345\n",
      "Train Loss at iteration 953: 0.04377444103773576\n",
      "Train Loss at iteration 954: 0.043773548708584804\n",
      "Train Loss at iteration 955: 0.04377265896402665\n",
      "Train Loss at iteration 956: 0.04377177179243482\n",
      "Train Loss at iteration 957: 0.04377088718223994\n",
      "Train Loss at iteration 958: 0.04377000512192951\n",
      "Train Loss at iteration 959: 0.04376912560004755\n",
      "Train Loss at iteration 960: 0.04376824860519434\n",
      "Train Loss at iteration 961: 0.04376737412602606\n",
      "Train Loss at iteration 962: 0.04376650215125459\n",
      "Train Loss at iteration 963: 0.04376563266964712\n",
      "Train Loss at iteration 964: 0.04376476567002592\n",
      "Train Loss at iteration 965: 0.043763901141268026\n",
      "Train Loss at iteration 966: 0.043763039072304964\n",
      "Train Loss at iteration 967: 0.043762179452122456\n",
      "Train Loss at iteration 968: 0.04376132226976013\n",
      "Train Loss at iteration 969: 0.04376046751431123\n",
      "Train Loss at iteration 970: 0.04375961517492237\n",
      "Train Loss at iteration 971: 0.043758765240793224\n",
      "Train Loss at iteration 972: 0.04375791770117624\n",
      "Train Loss at iteration 973: 0.04375707254537637\n",
      "Train Loss at iteration 974: 0.04375622976275083\n",
      "Train Loss at iteration 975: 0.04375538934270877\n",
      "Train Loss at iteration 976: 0.043754551274711055\n",
      "Train Loss at iteration 977: 0.04375371554826993\n",
      "Train Loss at iteration 978: 0.04375288215294882\n",
      "Train Loss at iteration 979: 0.04375205107836201\n",
      "Train Loss at iteration 980: 0.043751222314174415\n",
      "Train Loss at iteration 981: 0.043750395850101285\n",
      "Train Loss at iteration 982: 0.04374957167590797\n",
      "Train Loss at iteration 983: 0.04374874978140965\n",
      "Train Loss at iteration 984: 0.043747930156471056\n",
      "Train Loss at iteration 985: 0.043747112791006224\n",
      "Train Loss at iteration 986: 0.04374629767497824\n",
      "Train Loss at iteration 987: 0.043745484798398994\n",
      "Train Loss at iteration 988: 0.043744674151328905\n",
      "Train Loss at iteration 989: 0.04374386572387668\n",
      "Train Loss at iteration 990: 0.043743059506199077\n",
      "Train Loss at iteration 991: 0.043742255488500616\n",
      "Train Loss at iteration 992: 0.04374145366103335\n",
      "Train Loss at iteration 993: 0.043740654014096644\n",
      "Train Loss at iteration 994: 0.043739856538036914\n",
      "Train Loss at iteration 995: 0.04373906122324732\n",
      "Train Loss at iteration 996: 0.043738268060167625\n",
      "Train Loss at iteration 997: 0.04373747703928391\n",
      "Train Loss at iteration 998: 0.04373668815112829\n",
      "Train Loss at iteration 999: 0.043735901386278773\n",
      "Train Loss at iteration 1000: 0.04373511673535891\n",
      "Train Loss at iteration 1001: 0.043734334189037626\n",
      "Train Loss at iteration 1002: 0.043733553738029\n",
      "Train Loss at iteration 1003: 0.043732775373091975\n",
      "Train Loss at iteration 1004: 0.04373199908503017\n",
      "Train Loss at iteration 1005: 0.0437312248646916\n",
      "Train Loss at iteration 1006: 0.04373045270296855\n",
      "Train Loss at iteration 1007: 0.04372968259079718\n",
      "Train Loss at iteration 1008: 0.043728914519157495\n",
      "Train Loss at iteration 1009: 0.043728148479072956\n",
      "Train Loss at iteration 1010: 0.0437273844616103\n",
      "Train Loss at iteration 1011: 0.043726622457879394\n",
      "Train Loss at iteration 1012: 0.04372586245903293\n",
      "Train Loss at iteration 1013: 0.043725104456266206\n",
      "Train Loss at iteration 1014: 0.043724348440816935\n",
      "Train Loss at iteration 1015: 0.04372359440396504\n",
      "Train Loss at iteration 1016: 0.043722842337032386\n",
      "Train Loss at iteration 1017: 0.04372209223138263\n",
      "Train Loss at iteration 1018: 0.043721344078420946\n",
      "Train Loss at iteration 1019: 0.04372059786959383\n",
      "Train Loss at iteration 1020: 0.0437198535963889\n",
      "Train Loss at iteration 1021: 0.043719111250334675\n",
      "Train Loss at iteration 1022: 0.043718370823000396\n",
      "Train Loss at iteration 1023: 0.04371763230599574\n",
      "Train Loss at iteration 1024: 0.04371689569097069\n",
      "Train Loss at iteration 1025: 0.043716160969615295\n",
      "Train Loss at iteration 1026: 0.04371542813365947\n",
      "Train Loss at iteration 1027: 0.04371469717487281\n",
      "Train Loss at iteration 1028: 0.0437139680850643\n",
      "Train Loss at iteration 1029: 0.043713240856082254\n",
      "Train Loss at iteration 1030: 0.04371251547981401\n",
      "Train Loss at iteration 1031: 0.04371179194818577\n",
      "Train Loss at iteration 1032: 0.04371107025316237\n",
      "Train Loss at iteration 1033: 0.043710350386747115\n",
      "Train Loss at iteration 1034: 0.04370963234098159\n",
      "Train Loss at iteration 1035: 0.04370891610794541\n",
      "Train Loss at iteration 1036: 0.04370820167975609\n",
      "Train Loss at iteration 1037: 0.04370748904856882\n",
      "Train Loss at iteration 1038: 0.04370677820657626\n",
      "Train Loss at iteration 1039: 0.04370606914600837\n",
      "Train Loss at iteration 1040: 0.043705361859132236\n",
      "Train Loss at iteration 1041: 0.04370465633825183\n",
      "Train Loss at iteration 1042: 0.043703952575707854\n",
      "Train Loss at iteration 1043: 0.043703250563877576\n",
      "Train Loss at iteration 1044: 0.043702550295174565\n",
      "Train Loss at iteration 1045: 0.04370185176204864\n",
      "Train Loss at iteration 1046: 0.043701154956985525\n",
      "Train Loss at iteration 1047: 0.0437004598725068\n",
      "Train Loss at iteration 1048: 0.04369976650116962\n",
      "Train Loss at iteration 1049: 0.04369907483556661\n",
      "Train Loss at iteration 1050: 0.04369838486832566\n",
      "Train Loss at iteration 1051: 0.04369769659210972\n",
      "Train Loss at iteration 1052: 0.04369700999961662\n",
      "Train Loss at iteration 1053: 0.043696325083578985\n",
      "Train Loss at iteration 1054: 0.0436956418367639\n",
      "Train Loss at iteration 1055: 0.04369496025197289\n",
      "Train Loss at iteration 1056: 0.04369428032204165\n",
      "Train Loss at iteration 1057: 0.043693602039839914\n",
      "Train Loss at iteration 1058: 0.04369292539827124\n",
      "Train Loss at iteration 1059: 0.04369225039027291\n",
      "Train Loss at iteration 1060: 0.04369157700881569\n",
      "Train Loss at iteration 1061: 0.04369090524690368\n",
      "Train Loss at iteration 1062: 0.043690235097574186\n",
      "Train Loss at iteration 1063: 0.043689566553897476\n",
      "Train Loss at iteration 1064: 0.0436888996089767\n",
      "Train Loss at iteration 1065: 0.04368823425594765\n",
      "Train Loss at iteration 1066: 0.043687570487978625\n",
      "Train Loss at iteration 1067: 0.0436869082982703\n",
      "Train Loss at iteration 1068: 0.043686247680055496\n",
      "Train Loss at iteration 1069: 0.04368558862659905\n",
      "Train Loss at iteration 1070: 0.04368493113119768\n",
      "Train Loss at iteration 1071: 0.043684275187179794\n",
      "Train Loss at iteration 1072: 0.04368362078790532\n",
      "Train Loss at iteration 1073: 0.04368296792676556\n",
      "Train Loss at iteration 1074: 0.04368231659718307\n",
      "Train Loss at iteration 1075: 0.04368166679261142\n",
      "Train Loss at iteration 1076: 0.04368101850653514\n",
      "Train Loss at iteration 1077: 0.04368037173246946\n",
      "Train Loss at iteration 1078: 0.04367972646396021\n",
      "Train Loss at iteration 1079: 0.04367908269458371\n",
      "Train Loss at iteration 1080: 0.04367844041794652\n",
      "Train Loss at iteration 1081: 0.043677799627685344\n",
      "Train Loss at iteration 1082: 0.04367716031746687\n",
      "Train Loss at iteration 1083: 0.043676522480987634\n",
      "Train Loss at iteration 1084: 0.04367588611197388\n",
      "Train Loss at iteration 1085: 0.0436752512041813\n",
      "Train Loss at iteration 1086: 0.043674617751395084\n",
      "Train Loss at iteration 1087: 0.043673985747429575\n",
      "Train Loss at iteration 1088: 0.04367335518612825\n",
      "Train Loss at iteration 1089: 0.04367272606136355\n",
      "Train Loss at iteration 1090: 0.04367209836703666\n",
      "Train Loss at iteration 1091: 0.04367147209707748\n",
      "Train Loss at iteration 1092: 0.04367084724544439\n",
      "Train Loss at iteration 1093: 0.04367022380612418\n",
      "Train Loss at iteration 1094: 0.04366960177313184\n",
      "Train Loss at iteration 1095: 0.04366898114051045\n",
      "Train Loss at iteration 1096: 0.043668361902331025\n",
      "Train Loss at iteration 1097: 0.04366774405269244\n",
      "Train Loss at iteration 1098: 0.04366712758572121\n",
      "Train Loss at iteration 1099: 0.04366651249557136\n",
      "Train Loss at iteration 1100: 0.04366589877642434\n",
      "Train Loss at iteration 1101: 0.043665286422488854\n",
      "Train Loss at iteration 1102: 0.04366467542800073\n",
      "Train Loss at iteration 1103: 0.04366406578722275\n",
      "Train Loss at iteration 1104: 0.043663457494444606\n",
      "Train Loss at iteration 1105: 0.04366285054398266\n",
      "Train Loss at iteration 1106: 0.04366224493017989\n",
      "Train Loss at iteration 1107: 0.043661640647405665\n",
      "Train Loss at iteration 1108: 0.04366103769005578\n",
      "Train Loss at iteration 1109: 0.04366043605255215\n",
      "Train Loss at iteration 1110: 0.04365983572934274\n",
      "Train Loss at iteration 1111: 0.043659236714901475\n",
      "Train Loss at iteration 1112: 0.0436586390037281\n",
      "Train Loss at iteration 1113: 0.043658042590347966\n",
      "Train Loss at iteration 1114: 0.04365744746931205\n",
      "Train Loss at iteration 1115: 0.04365685363519667\n",
      "Train Loss at iteration 1116: 0.0436562610826035\n",
      "Train Loss at iteration 1117: 0.043655669806159356\n",
      "Train Loss at iteration 1118: 0.04365507980051609\n",
      "Train Loss at iteration 1119: 0.043654491060350464\n",
      "Train Loss at iteration 1120: 0.043653903580364056\n",
      "Train Loss at iteration 1121: 0.043653317355283106\n",
      "Train Loss at iteration 1122: 0.0436527323798584\n",
      "Train Loss at iteration 1123: 0.04365214864886516\n",
      "Train Loss at iteration 1124: 0.0436515661571029\n",
      "Train Loss at iteration 1125: 0.04365098489939532\n",
      "Train Loss at iteration 1126: 0.04365040487059021\n",
      "Train Loss at iteration 1127: 0.04364982606555929\n",
      "Train Loss at iteration 1128: 0.0436492484791981\n",
      "Train Loss at iteration 1129: 0.0436486721064259\n",
      "Train Loss at iteration 1130: 0.043648096942185535\n",
      "Train Loss at iteration 1131: 0.043647522981443365\n",
      "Train Loss at iteration 1132: 0.04364695021918905\n",
      "Train Loss at iteration 1133: 0.04364637865043554\n",
      "Train Loss at iteration 1134: 0.0436458082702189\n",
      "Train Loss at iteration 1135: 0.043645239073598206\n",
      "Train Loss at iteration 1136: 0.04364467105565547\n",
      "Train Loss at iteration 1137: 0.043644104211495435\n",
      "Train Loss at iteration 1138: 0.04364353853624558\n",
      "Train Loss at iteration 1139: 0.04364297402505591\n",
      "Train Loss at iteration 1140: 0.04364241067309891\n",
      "Train Loss at iteration 1141: 0.04364184847556939\n",
      "Train Loss at iteration 1142: 0.04364128742768442\n",
      "Train Loss at iteration 1143: 0.04364072752468317\n",
      "Train Loss at iteration 1144: 0.04364016876182682\n",
      "Train Loss at iteration 1145: 0.04363961113439847\n",
      "Train Loss at iteration 1146: 0.04363905463770303\n",
      "Train Loss at iteration 1147: 0.043638499267067095\n",
      "Train Loss at iteration 1148: 0.04363794501783883\n",
      "Train Loss at iteration 1149: 0.043637391885387906\n",
      "Train Loss at iteration 1150: 0.043636839865105345\n",
      "Train Loss at iteration 1151: 0.043636288952403436\n",
      "Train Loss at iteration 1152: 0.04363573914271567\n",
      "Train Loss at iteration 1153: 0.043635190431496544\n",
      "Train Loss at iteration 1154: 0.04363464281422157\n",
      "Train Loss at iteration 1155: 0.043634096286387064\n",
      "Train Loss at iteration 1156: 0.04363355084351012\n",
      "Train Loss at iteration 1157: 0.043633006481128465\n",
      "Train Loss at iteration 1158: 0.043632463194800415\n",
      "Train Loss at iteration 1159: 0.04363192098010469\n",
      "Train Loss at iteration 1160: 0.04363137983264038\n",
      "Train Loss at iteration 1161: 0.04363083974802682\n",
      "Train Loss at iteration 1162: 0.04363030072190349\n",
      "Train Loss at iteration 1163: 0.04362976274992995\n",
      "Train Loss at iteration 1164: 0.04362922582778565\n",
      "Train Loss at iteration 1165: 0.04362868995116997\n",
      "Train Loss at iteration 1166: 0.04362815511580198\n",
      "Train Loss at iteration 1167: 0.04362762131742048\n",
      "Train Loss at iteration 1168: 0.04362708855178378\n",
      "Train Loss at iteration 1169: 0.043626556814669676\n",
      "Train Loss at iteration 1170: 0.04362602610187535\n",
      "Train Loss at iteration 1171: 0.04362549640921724\n",
      "Train Loss at iteration 1172: 0.04362496773253103\n",
      "Train Loss at iteration 1173: 0.04362444006767143\n",
      "Train Loss at iteration 1174: 0.043623913410512166\n",
      "Train Loss at iteration 1175: 0.0436233877569459\n",
      "Train Loss at iteration 1176: 0.043622863102884074\n",
      "Train Loss at iteration 1177: 0.04362233944425686\n",
      "Train Loss at iteration 1178: 0.043621816777013094\n",
      "Train Loss at iteration 1179: 0.043621295097120115\n",
      "Train Loss at iteration 1180: 0.04362077440056372\n",
      "Train Loss at iteration 1181: 0.04362025468334809\n",
      "Train Loss at iteration 1182: 0.043619735941495645\n",
      "Train Loss at iteration 1183: 0.043619218171047004\n",
      "Train Loss at iteration 1184: 0.043618701368060904\n",
      "Train Loss at iteration 1185: 0.04361818552861403\n",
      "Train Loss at iteration 1186: 0.04361767064880106\n",
      "Train Loss at iteration 1187: 0.04361715672473442\n",
      "Train Loss at iteration 1188: 0.04361664375254438\n",
      "Train Loss at iteration 1189: 0.04361613172837877\n",
      "Train Loss at iteration 1190: 0.043615620648403054\n",
      "Train Loss at iteration 1191: 0.043615110508800174\n",
      "Train Loss at iteration 1192: 0.04361460130577047\n",
      "Train Loss at iteration 1193: 0.04361409303553158\n",
      "Train Loss at iteration 1194: 0.043613585694318426\n",
      "Train Loss at iteration 1195: 0.04361307927838303\n",
      "Train Loss at iteration 1196: 0.04361257378399453\n",
      "Train Loss at iteration 1197: 0.04361206920743902\n",
      "Train Loss at iteration 1198: 0.04361156554501946\n",
      "Train Loss at iteration 1199: 0.04361106279305573\n",
      "Train Loss at iteration 1200: 0.043610560947884354\n",
      "Train Loss at iteration 1201: 0.043610060005858584\n",
      "Train Loss at iteration 1202: 0.04360955996334819\n",
      "Train Loss at iteration 1203: 0.043609060816739494\n",
      "Train Loss at iteration 1204: 0.043608562562435206\n",
      "Train Loss at iteration 1205: 0.04360806519685438\n",
      "Train Loss at iteration 1206: 0.04360756871643232\n",
      "Train Loss at iteration 1207: 0.043607073117620555\n",
      "Train Loss at iteration 1208: 0.043606578396886655\n",
      "Train Loss at iteration 1209: 0.04360608455071427\n",
      "Train Loss at iteration 1210: 0.04360559157560296\n",
      "Train Loss at iteration 1211: 0.04360509946806818\n",
      "Train Loss at iteration 1212: 0.04360460822464118\n",
      "Train Loss at iteration 1213: 0.0436041178418689\n",
      "Train Loss at iteration 1214: 0.04360362831631395\n",
      "Train Loss at iteration 1215: 0.043603139644554506\n",
      "Train Loss at iteration 1216: 0.04360265182318422\n",
      "Train Loss at iteration 1217: 0.04360216484881217\n",
      "Train Loss at iteration 1218: 0.04360167871806281\n",
      "Train Loss at iteration 1219: 0.043601193427575805\n",
      "Train Loss at iteration 1220: 0.043600708974006065\n",
      "Train Loss at iteration 1221: 0.043600225354023574\n",
      "Train Loss at iteration 1222: 0.04359974256431344\n",
      "Train Loss at iteration 1223: 0.043599260601575665\n",
      "Train Loss at iteration 1224: 0.043598779462525224\n",
      "Train Loss at iteration 1225: 0.04359829914389188\n",
      "Train Loss at iteration 1226: 0.04359781964242018\n",
      "Train Loss at iteration 1227: 0.04359734095486938\n",
      "Train Loss at iteration 1228: 0.04359686307801333\n",
      "Train Loss at iteration 1229: 0.04359638600864043\n",
      "Train Loss at iteration 1230: 0.043595909743553575\n",
      "Train Loss at iteration 1231: 0.0435954342795701\n",
      "Train Loss at iteration 1232: 0.043594959613521604\n",
      "Train Loss at iteration 1233: 0.04359448574225405\n",
      "Train Loss at iteration 1234: 0.043594012662627545\n",
      "Train Loss at iteration 1235: 0.04359354037151636\n",
      "Train Loss at iteration 1236: 0.043593068865808826\n",
      "Train Loss at iteration 1237: 0.043592598142407284\n",
      "Train Loss at iteration 1238: 0.04359212819822801\n",
      "Train Loss at iteration 1239: 0.043591659030201145\n",
      "Train Loss at iteration 1240: 0.04359119063527065\n",
      "Train Loss at iteration 1241: 0.043590723010394176\n",
      "Train Loss at iteration 1242: 0.0435902561525431\n",
      "Train Loss at iteration 1243: 0.04358979005870237\n",
      "Train Loss at iteration 1244: 0.04358932472587048\n",
      "Train Loss at iteration 1245: 0.04358886015105943\n",
      "Train Loss at iteration 1246: 0.04358839633129457\n",
      "Train Loss at iteration 1247: 0.04358793326361466\n",
      "Train Loss at iteration 1248: 0.043587470945071684\n",
      "Train Loss at iteration 1249: 0.04358700937273091\n",
      "Train Loss at iteration 1250: 0.04358654854367073\n",
      "Train Loss at iteration 1251: 0.043586088454982616\n",
      "Train Loss at iteration 1252: 0.04358562910377109\n",
      "Train Loss at iteration 1253: 0.04358517048715365\n",
      "Train Loss at iteration 1254: 0.043584712602260677\n",
      "Train Loss at iteration 1255: 0.04358425544623544\n",
      "Train Loss at iteration 1256: 0.04358379901623394\n",
      "Train Loss at iteration 1257: 0.04358334330942495\n",
      "Train Loss at iteration 1258: 0.04358288832298987\n",
      "Train Loss at iteration 1259: 0.04358243405412272\n",
      "Train Loss at iteration 1260: 0.04358198050003009\n",
      "Train Loss at iteration 1261: 0.04358152765793101\n",
      "Train Loss at iteration 1262: 0.043581075525056925\n",
      "Train Loss at iteration 1263: 0.04358062409865171\n",
      "Train Loss at iteration 1264: 0.04358017337597148\n",
      "Train Loss at iteration 1265: 0.04357972335428465\n",
      "Train Loss at iteration 1266: 0.043579274030871785\n",
      "Train Loss at iteration 1267: 0.043578825403025594\n",
      "Train Loss at iteration 1268: 0.04357837746805088\n",
      "Train Loss at iteration 1269: 0.043577930223264436\n",
      "Train Loss at iteration 1270: 0.04357748366599502\n",
      "Train Loss at iteration 1271: 0.04357703779358333\n",
      "Train Loss at iteration 1272: 0.043576592603381835\n",
      "Train Loss at iteration 1273: 0.04357614809275487\n",
      "Train Loss at iteration 1274: 0.043575704259078446\n",
      "Train Loss at iteration 1275: 0.04357526109974032\n",
      "Train Loss at iteration 1276: 0.04357481861213982\n",
      "Train Loss at iteration 1277: 0.043574376793687836\n",
      "Train Loss at iteration 1278: 0.04357393564180681\n",
      "Train Loss at iteration 1279: 0.04357349515393063\n",
      "Train Loss at iteration 1280: 0.04357305532750457\n",
      "Train Loss at iteration 1281: 0.04357261615998529\n",
      "Train Loss at iteration 1282: 0.043572177648840714\n",
      "Train Loss at iteration 1283: 0.043571739791550025\n",
      "Train Loss at iteration 1284: 0.0435713025856036\n",
      "Train Loss at iteration 1285: 0.043570866028502936\n",
      "Train Loss at iteration 1286: 0.04357043011776066\n",
      "Train Loss at iteration 1287: 0.04356999485090037\n",
      "Train Loss at iteration 1288: 0.043569560225456695\n",
      "Train Loss at iteration 1289: 0.043569126238975166\n",
      "Train Loss at iteration 1290: 0.0435686928890122\n",
      "Train Loss at iteration 1291: 0.04356826017313505\n",
      "Train Loss at iteration 1292: 0.04356782808892173\n",
      "Train Loss at iteration 1293: 0.04356739663396097\n",
      "Train Loss at iteration 1294: 0.043566965805852204\n",
      "Train Loss at iteration 1295: 0.04356653560220546\n",
      "Train Loss at iteration 1296: 0.04356610602064137\n",
      "Train Loss at iteration 1297: 0.04356567705879105\n",
      "Train Loss at iteration 1298: 0.04356524871429611\n",
      "Train Loss at iteration 1299: 0.0435648209848086\n",
      "Train Loss at iteration 1300: 0.043564393867990936\n",
      "Train Loss at iteration 1301: 0.04356396736151583\n",
      "Train Loss at iteration 1302: 0.043563541463066326\n",
      "Train Loss at iteration 1303: 0.04356311617033567\n",
      "Train Loss at iteration 1304: 0.04356269148102727\n",
      "Train Loss at iteration 1305: 0.043562267392854695\n",
      "Train Loss at iteration 1306: 0.0435618439035416\n",
      "Train Loss at iteration 1307: 0.04356142101082167\n",
      "Train Loss at iteration 1308: 0.043560998712438584\n",
      "Train Loss at iteration 1309: 0.043560577006145955\n",
      "Train Loss at iteration 1310: 0.043560155889707346\n",
      "Train Loss at iteration 1311: 0.04355973536089608\n",
      "Train Loss at iteration 1312: 0.04355931541749537\n",
      "Train Loss at iteration 1313: 0.04355889605729814\n",
      "Train Loss at iteration 1314: 0.043558477278107056\n",
      "Train Loss at iteration 1315: 0.04355805907773441\n",
      "Train Loss at iteration 1316: 0.043557641454002184\n",
      "Train Loss at iteration 1317: 0.04355722440474187\n",
      "Train Loss at iteration 1318: 0.04355680792779455\n",
      "Train Loss at iteration 1319: 0.043556392021010755\n",
      "Train Loss at iteration 1320: 0.04355597668225047\n",
      "Train Loss at iteration 1321: 0.04355556190938308\n",
      "Train Loss at iteration 1322: 0.04355514770028734\n",
      "Train Loss at iteration 1323: 0.043554734052851284\n",
      "Train Loss at iteration 1324: 0.04355432096497224\n",
      "Train Loss at iteration 1325: 0.04355390843455679\n",
      "Train Loss at iteration 1326: 0.04355349645952062\n",
      "Train Loss at iteration 1327: 0.043553085037788625\n",
      "Train Loss at iteration 1328: 0.04355267416729477\n",
      "Train Loss at iteration 1329: 0.04355226384598206\n",
      "Train Loss at iteration 1330: 0.04355185407180255\n",
      "Train Loss at iteration 1331: 0.04355144484271721\n",
      "Train Loss at iteration 1332: 0.04355103615669599\n",
      "Train Loss at iteration 1333: 0.0435506280117177\n",
      "Train Loss at iteration 1334: 0.043550220405769995\n",
      "Train Loss at iteration 1335: 0.04354981333684936\n",
      "Train Loss at iteration 1336: 0.04354940680296099\n",
      "Train Loss at iteration 1337: 0.04354900080211885\n",
      "Train Loss at iteration 1338: 0.04354859533234555\n",
      "Train Loss at iteration 1339: 0.043548190391672396\n",
      "Train Loss at iteration 1340: 0.0435477859781392\n",
      "Train Loss at iteration 1341: 0.043547382089794445\n",
      "Train Loss at iteration 1342: 0.04354697872469504\n",
      "Train Loss at iteration 1343: 0.04354657588090644\n",
      "Train Loss at iteration 1344: 0.043546173556502504\n",
      "Train Loss at iteration 1345: 0.04354577174956552\n",
      "Train Loss at iteration 1346: 0.043545370458186086\n",
      "Train Loss at iteration 1347: 0.04354496968046321\n",
      "Train Loss at iteration 1348: 0.0435445694145041\n",
      "Train Loss at iteration 1349: 0.04354416965842427\n",
      "Train Loss at iteration 1350: 0.043543770410347404\n",
      "Train Loss at iteration 1351: 0.043543371668405384\n",
      "Train Loss at iteration 1352: 0.04354297343073822\n",
      "Train Loss at iteration 1353: 0.04354257569549401\n",
      "Train Loss at iteration 1354: 0.0435421784608289\n",
      "Train Loss at iteration 1355: 0.04354178172490705\n",
      "Train Loss at iteration 1356: 0.04354138548590065\n",
      "Train Loss at iteration 1357: 0.0435409897419898\n",
      "Train Loss at iteration 1358: 0.04354059449136249\n",
      "Train Loss at iteration 1359: 0.04354019973221464\n",
      "Train Loss at iteration 1360: 0.04353980546274993\n",
      "Train Loss at iteration 1361: 0.04353941168117993\n",
      "Train Loss at iteration 1362: 0.04353901838572388\n",
      "Train Loss at iteration 1363: 0.04353862557460882\n",
      "Train Loss at iteration 1364: 0.04353823324606944\n",
      "Train Loss at iteration 1365: 0.0435378413983481\n",
      "Train Loss at iteration 1366: 0.04353745002969479\n",
      "Train Loss at iteration 1367: 0.043537059138367064\n",
      "Train Loss at iteration 1368: 0.043536668722630054\n",
      "Train Loss at iteration 1369: 0.04353627878075636\n",
      "Train Loss at iteration 1370: 0.04353588931102614\n",
      "Train Loss at iteration 1371: 0.04353550031172691\n",
      "Train Loss at iteration 1372: 0.043535111781153685\n",
      "Train Loss at iteration 1373: 0.043534723717608775\n",
      "Train Loss at iteration 1374: 0.04353433611940189\n",
      "Train Loss at iteration 1375: 0.043533948984850045\n",
      "Train Loss at iteration 1376: 0.04353356231227749\n",
      "Train Loss at iteration 1377: 0.04353317610001576\n",
      "Train Loss at iteration 1378: 0.04353279034640359\n",
      "Train Loss at iteration 1379: 0.04353240504978689\n",
      "Train Loss at iteration 1380: 0.04353202020851869\n",
      "Train Loss at iteration 1381: 0.04353163582095917\n",
      "Train Loss at iteration 1382: 0.04353125188547555\n",
      "Train Loss at iteration 1383: 0.0435308684004421\n",
      "Train Loss at iteration 1384: 0.043530485364240146\n",
      "Train Loss at iteration 1385: 0.043530102775257956\n",
      "Train Loss at iteration 1386: 0.04352972063189072\n",
      "Train Loss at iteration 1387: 0.04352933893254061\n",
      "Train Loss at iteration 1388: 0.04352895767561661\n",
      "Train Loss at iteration 1389: 0.04352857685953464\n",
      "Train Loss at iteration 1390: 0.04352819648271736\n",
      "Train Loss at iteration 1391: 0.043527816543594264\n",
      "Train Loss at iteration 1392: 0.043527437040601594\n",
      "Train Loss at iteration 1393: 0.043527057972182315\n",
      "Train Loss at iteration 1394: 0.043526679336786105\n",
      "Train Loss at iteration 1395: 0.04352630113286929\n",
      "Train Loss at iteration 1396: 0.043525923358894825\n",
      "Train Loss at iteration 1397: 0.043525546013332306\n",
      "Train Loss at iteration 1398: 0.04352516909465786\n",
      "Train Loss at iteration 1399: 0.04352479260135419\n",
      "Train Loss at iteration 1400: 0.04352441653191049\n",
      "Train Loss at iteration 1401: 0.04352404088482247\n",
      "Train Loss at iteration 1402: 0.04352366565859225\n",
      "Train Loss at iteration 1403: 0.04352329085172842\n",
      "Train Loss at iteration 1404: 0.04352291646274594\n",
      "Train Loss at iteration 1405: 0.043522542490166155\n",
      "Train Loss at iteration 1406: 0.043522168932516696\n",
      "Train Loss at iteration 1407: 0.04352179578833158\n",
      "Train Loss at iteration 1408: 0.04352142305615106\n",
      "Train Loss at iteration 1409: 0.04352105073452162\n",
      "Train Loss at iteration 1410: 0.04352067882199603\n",
      "Train Loss at iteration 1411: 0.043520307317133175\n",
      "Train Loss at iteration 1412: 0.043519936218498166\n",
      "Train Loss at iteration 1413: 0.04351956552466225\n",
      "Train Loss at iteration 1414: 0.043519195234202716\n",
      "Train Loss at iteration 1415: 0.04351882534570303\n",
      "Train Loss at iteration 1416: 0.04351845585775265\n",
      "Train Loss at iteration 1417: 0.04351808676894707\n",
      "Train Loss at iteration 1418: 0.04351771807788778\n",
      "Train Loss at iteration 1419: 0.04351734978318226\n",
      "Train Loss at iteration 1420: 0.04351698188344393\n",
      "Train Loss at iteration 1421: 0.04351661437729211\n",
      "Train Loss at iteration 1422: 0.043516247263352015\n",
      "Train Loss at iteration 1423: 0.043515880540254744\n",
      "Train Loss at iteration 1424: 0.043515514206637206\n",
      "Train Loss at iteration 1425: 0.04351514826114213\n",
      "Train Loss at iteration 1426: 0.04351478270241804\n",
      "Train Loss at iteration 1427: 0.043514417529119186\n",
      "Train Loss at iteration 1428: 0.043514052739905586\n",
      "Train Loss at iteration 1429: 0.04351368833344294\n",
      "Train Loss at iteration 1430: 0.043513324308402636\n",
      "Train Loss at iteration 1431: 0.043512960663461706\n",
      "Train Loss at iteration 1432: 0.04351259739730283\n",
      "Train Loss at iteration 1433: 0.043512234508614245\n",
      "Train Loss at iteration 1434: 0.04351187199608982\n",
      "Train Loss at iteration 1435: 0.04351150985842895\n",
      "Train Loss at iteration 1436: 0.04351114809433655\n",
      "Train Loss at iteration 1437: 0.04351078670252304\n",
      "Train Loss at iteration 1438: 0.04351042568170432\n",
      "Train Loss at iteration 1439: 0.04351006503060173\n",
      "Train Loss at iteration 1440: 0.04350970474794205\n",
      "Train Loss at iteration 1441: 0.04350934483245746\n",
      "Train Loss at iteration 1442: 0.04350898528288551\n",
      "Train Loss at iteration 1443: 0.04350862609796911\n",
      "Train Loss at iteration 1444: 0.04350826727645649\n",
      "Train Loss at iteration 1445: 0.04350790881710119\n",
      "Train Loss at iteration 1446: 0.04350755071866202\n",
      "Train Loss at iteration 1447: 0.043507192979903075\n",
      "Train Loss at iteration 1448: 0.04350683559959366\n",
      "Train Loss at iteration 1449: 0.04350647857650827\n",
      "Train Loss at iteration 1450: 0.04350612190942665\n",
      "Train Loss at iteration 1451: 0.04350576559713362\n",
      "Train Loss at iteration 1452: 0.0435054096384192\n",
      "Train Loss at iteration 1453: 0.043505054032078515\n",
      "Train Loss at iteration 1454: 0.04350469877691179\n",
      "Train Loss at iteration 1455: 0.043504343871724284\n",
      "Train Loss at iteration 1456: 0.04350398931532632\n",
      "Train Loss at iteration 1457: 0.04350363510653327\n",
      "Train Loss at iteration 1458: 0.04350328124416547\n",
      "Train Loss at iteration 1459: 0.04350292772704826\n",
      "Train Loss at iteration 1460: 0.04350257455401191\n",
      "Train Loss at iteration 1461: 0.04350222172389166\n",
      "Train Loss at iteration 1462: 0.04350186923552764\n",
      "Train Loss at iteration 1463: 0.04350151708776484\n",
      "Train Loss at iteration 1464: 0.04350116527945317\n",
      "Train Loss at iteration 1465: 0.04350081380944738\n",
      "Train Loss at iteration 1466: 0.04350046267660699\n",
      "Train Loss at iteration 1467: 0.04350011187979638\n",
      "Train Loss at iteration 1468: 0.04349976141788468\n",
      "Train Loss at iteration 1469: 0.04349941128974579\n",
      "Train Loss at iteration 1470: 0.043499061494258334\n",
      "Train Loss at iteration 1471: 0.04349871203030567\n",
      "Train Loss at iteration 1472: 0.04349836289677583\n",
      "Train Loss at iteration 1473: 0.04349801409256156\n",
      "Train Loss at iteration 1474: 0.043497665616560176\n",
      "Train Loss at iteration 1475: 0.043497317467673734\n",
      "Train Loss at iteration 1476: 0.0434969696448088\n",
      "Train Loss at iteration 1477: 0.043496622146876614\n",
      "Train Loss at iteration 1478: 0.04349627497279293\n",
      "Train Loss at iteration 1479: 0.043495928121478046\n",
      "Train Loss at iteration 1480: 0.04349558159185685\n",
      "Train Loss at iteration 1481: 0.043495235382858655\n",
      "Train Loss at iteration 1482: 0.043494889493417344\n",
      "Train Loss at iteration 1483: 0.043494543922471214\n",
      "Train Loss at iteration 1484: 0.043494198668963015\n",
      "Train Loss at iteration 1485: 0.043493853731839935\n",
      "Train Loss at iteration 1486: 0.043493509110053584\n",
      "Train Loss at iteration 1487: 0.043493164802559936\n",
      "Train Loss at iteration 1488: 0.043492820808319343\n",
      "Train Loss at iteration 1489: 0.0434924771262965\n",
      "Train Loss at iteration 1490: 0.04349213375546045\n",
      "Train Loss at iteration 1491: 0.043491790694784524\n",
      "Train Loss at iteration 1492: 0.043491447943246365\n",
      "Train Loss at iteration 1493: 0.043491105499827834\n",
      "Train Loss at iteration 1494: 0.04349076336351514\n",
      "Train Loss at iteration 1495: 0.04349042153329865\n",
      "Train Loss at iteration 1496: 0.043490080008172956\n",
      "Train Loss at iteration 1497: 0.04348973878713687\n",
      "Train Loss at iteration 1498: 0.04348939786919337\n",
      "Train Loss at iteration 1499: 0.043489057253349575\n",
      "Train Loss at iteration 1500: 0.04348871693861677\n",
      "Train Loss at iteration 1501: 0.04348837692401036\n",
      "Train Loss at iteration 1502: 0.04348803720854981\n",
      "Train Loss at iteration 1503: 0.04348769779125874\n",
      "Train Loss at iteration 1504: 0.04348735867116477\n",
      "Train Loss at iteration 1505: 0.043487019847299614\n",
      "Train Loss at iteration 1506: 0.043486681318699004\n",
      "Train Loss at iteration 1507: 0.04348634308440266\n",
      "Train Loss at iteration 1508: 0.04348600514345432\n",
      "Train Loss at iteration 1509: 0.04348566749490172\n",
      "Train Loss at iteration 1510: 0.04348533013779649\n",
      "Train Loss at iteration 1511: 0.04348499307119426\n",
      "Train Loss at iteration 1512: 0.04348465629415456\n",
      "Train Loss at iteration 1513: 0.04348431980574083\n",
      "Train Loss at iteration 1514: 0.043483983605020385\n",
      "Train Loss at iteration 1515: 0.043483647691064456\n",
      "Train Loss at iteration 1516: 0.04348331206294808\n",
      "Train Loss at iteration 1517: 0.04348297671975015\n",
      "Train Loss at iteration 1518: 0.043482641660553364\n",
      "Train Loss at iteration 1519: 0.04348230688444425\n",
      "Train Loss at iteration 1520: 0.04348197239051311\n",
      "Train Loss at iteration 1521: 0.043481638177854005\n",
      "Train Loss at iteration 1522: 0.043481304245564754\n",
      "Train Loss at iteration 1523: 0.04348097059274694\n",
      "Train Loss at iteration 1524: 0.0434806372185058\n",
      "Train Loss at iteration 1525: 0.043480304121950336\n",
      "Train Loss at iteration 1526: 0.04347997130219323\n",
      "Train Loss at iteration 1527: 0.043479638758350786\n",
      "Train Loss at iteration 1528: 0.04347930648954302\n",
      "Train Loss at iteration 1529: 0.04347897449489354\n",
      "Train Loss at iteration 1530: 0.04347864277352962\n",
      "Train Loss at iteration 1531: 0.04347831132458211\n",
      "Train Loss at iteration 1532: 0.043477980147185416\n",
      "Train Loss at iteration 1533: 0.0434776492404776\n",
      "Train Loss at iteration 1534: 0.04347731860360021\n",
      "Train Loss at iteration 1535: 0.043476988235698386\n",
      "Train Loss at iteration 1536: 0.043476658135920755\n",
      "Train Loss at iteration 1537: 0.043476328303419486\n",
      "Train Loss at iteration 1538: 0.043475998737350204\n",
      "Train Loss at iteration 1539: 0.04347566943687207\n",
      "Train Loss at iteration 1540: 0.043475340401147644\n",
      "Train Loss at iteration 1541: 0.04347501162934299\n",
      "Train Loss at iteration 1542: 0.043474683120627564\n",
      "Train Loss at iteration 1543: 0.043474354874174295\n",
      "Train Loss at iteration 1544: 0.04347402688915946\n",
      "Train Loss at iteration 1545: 0.043473699164762736\n",
      "Train Loss at iteration 1546: 0.0434733717001672\n",
      "Train Loss at iteration 1547: 0.04347304449455924\n",
      "Train Loss at iteration 1548: 0.04347271754712867\n",
      "Train Loss at iteration 1549: 0.04347239085706855\n",
      "Train Loss at iteration 1550: 0.04347206442357528\n",
      "Train Loss at iteration 1551: 0.043471738245848573\n",
      "Train Loss at iteration 1552: 0.043471412323091436\n",
      "Train Loss at iteration 1553: 0.0434710866545101\n",
      "Train Loss at iteration 1554: 0.0434707612393141\n",
      "Train Loss at iteration 1555: 0.04347043607671619\n",
      "Train Loss at iteration 1556: 0.04347011116593238\n",
      "Train Loss at iteration 1557: 0.04346978650618182\n",
      "Train Loss at iteration 1558: 0.04346946209668696\n",
      "Train Loss at iteration 1559: 0.043469137936673376\n",
      "Train Loss at iteration 1560: 0.043468814025369784\n",
      "Train Loss at iteration 1561: 0.04346849036200817\n",
      "Train Loss at iteration 1562: 0.04346816694582351\n",
      "Train Loss at iteration 1563: 0.04346784377605405\n",
      "Train Loss at iteration 1564: 0.043467520851941076\n",
      "Train Loss at iteration 1565: 0.04346719817272898\n",
      "Train Loss at iteration 1566: 0.04346687573766527\n",
      "Train Loss at iteration 1567: 0.04346655354600051\n",
      "Train Loss at iteration 1568: 0.043466231596988335\n",
      "Train Loss at iteration 1569: 0.043465909889885405\n",
      "Train Loss at iteration 1570: 0.04346558842395145\n",
      "Train Loss at iteration 1571: 0.04346526719844918\n",
      "Train Loss at iteration 1572: 0.04346494621264437\n",
      "Train Loss at iteration 1573: 0.043464625465805755\n",
      "Train Loss at iteration 1574: 0.04346430495720502\n",
      "Train Loss at iteration 1575: 0.04346398468611687\n",
      "Train Loss at iteration 1576: 0.04346366465181896\n",
      "Train Loss at iteration 1577: 0.043463344853591844\n",
      "Train Loss at iteration 1578: 0.04346302529071906\n",
      "Train Loss at iteration 1579: 0.043462705962487046\n",
      "Train Loss at iteration 1580: 0.04346238686818513\n",
      "Train Loss at iteration 1581: 0.04346206800710553\n",
      "Train Loss at iteration 1582: 0.04346174937854338\n",
      "Train Loss at iteration 1583: 0.043461430981796605\n",
      "Train Loss at iteration 1584: 0.04346111281616608\n",
      "Train Loss at iteration 1585: 0.04346079488095546\n",
      "Train Loss at iteration 1586: 0.04346047717547123\n",
      "Train Loss at iteration 1587: 0.04346015969902271\n",
      "Train Loss at iteration 1588: 0.04345984245092202\n",
      "Train Loss at iteration 1589: 0.04345952543048407\n",
      "Train Loss at iteration 1590: 0.04345920863702655\n",
      "Train Loss at iteration 1591: 0.04345889206986991\n",
      "Train Loss at iteration 1592: 0.04345857572833736\n",
      "Train Loss at iteration 1593: 0.04345825961175487\n",
      "Train Loss at iteration 1594: 0.04345794371945112\n",
      "Train Loss at iteration 1595: 0.04345762805075753\n",
      "Train Loss at iteration 1596: 0.04345731260500818\n",
      "Train Loss at iteration 1597: 0.04345699738153991\n",
      "Train Loss at iteration 1598: 0.04345668237969221\n",
      "Train Loss at iteration 1599: 0.043456367598807256\n",
      "Train Loss at iteration 1600: 0.043456053038229865\n",
      "Train Loss at iteration 1601: 0.04345573869730752\n",
      "Train Loss at iteration 1602: 0.04345542457539031\n",
      "Train Loss at iteration 1603: 0.043455110671831026\n",
      "Train Loss at iteration 1604: 0.043454796985984985\n",
      "Train Loss at iteration 1605: 0.04345448351721017\n",
      "Train Loss at iteration 1606: 0.04345417026486713\n",
      "Train Loss at iteration 1607: 0.04345385722831898\n",
      "Train Loss at iteration 1608: 0.04345354440693145\n",
      "Train Loss at iteration 1609: 0.043453231800072774\n",
      "Train Loss at iteration 1610: 0.043452919407113794\n",
      "Train Loss at iteration 1611: 0.04345260722742783\n",
      "Train Loss at iteration 1612: 0.04345229526039075\n",
      "Train Loss at iteration 1613: 0.04345198350538095\n",
      "Train Loss at iteration 1614: 0.04345167196177932\n",
      "Train Loss at iteration 1615: 0.04345136062896924\n",
      "Train Loss at iteration 1616: 0.04345104950633656\n",
      "Train Loss at iteration 1617: 0.04345073859326963\n",
      "Train Loss at iteration 1618: 0.04345042788915924\n",
      "Train Loss at iteration 1619: 0.04345011739339863\n",
      "Train Loss at iteration 1620: 0.04344980710538347\n",
      "Train Loss at iteration 1621: 0.043449497024511896\n",
      "Train Loss at iteration 1622: 0.04344918715018442\n",
      "Train Loss at iteration 1623: 0.04344887748180398\n",
      "Train Loss at iteration 1624: 0.04344856801877591\n",
      "Train Loss at iteration 1625: 0.043448258760507945\n",
      "Train Loss at iteration 1626: 0.04344794970641015\n",
      "Train Loss at iteration 1627: 0.043447640855895024\n",
      "Train Loss at iteration 1628: 0.043447332208377355\n",
      "Train Loss at iteration 1629: 0.04344702376327432\n",
      "Train Loss at iteration 1630: 0.0434467155200054\n",
      "Train Loss at iteration 1631: 0.04344640747799245\n",
      "Train Loss at iteration 1632: 0.04344609963665958\n",
      "Train Loss at iteration 1633: 0.04344579199543326\n",
      "Train Loss at iteration 1634: 0.043445484553742204\n",
      "Train Loss at iteration 1635: 0.04344517731101746\n",
      "Train Loss at iteration 1636: 0.043444870266692324\n",
      "Train Loss at iteration 1637: 0.04344456342020235\n",
      "Train Loss at iteration 1638: 0.04344425677098538\n",
      "Train Loss at iteration 1639: 0.04344395031848147\n",
      "Train Loss at iteration 1640: 0.04344364406213292\n",
      "Train Loss at iteration 1641: 0.04344333800138427\n",
      "Train Loss at iteration 1642: 0.043443032135682284\n",
      "Train Loss at iteration 1643: 0.043442726464475896\n",
      "Train Loss at iteration 1644: 0.04344242098721627\n",
      "Train Loss at iteration 1645: 0.043442115703356765\n",
      "Train Loss at iteration 1646: 0.043441810612352874\n",
      "Train Loss at iteration 1647: 0.04344150571366231\n",
      "Train Loss at iteration 1648: 0.043441201006744944\n",
      "Train Loss at iteration 1649: 0.04344089649106275\n",
      "Train Loss at iteration 1650: 0.04344059216607989\n",
      "Train Loss at iteration 1651: 0.04344028803126264\n",
      "Train Loss at iteration 1652: 0.04343998408607941\n",
      "Train Loss at iteration 1653: 0.04343968033000071\n",
      "Train Loss at iteration 1654: 0.04343937676249917\n",
      "Train Loss at iteration 1655: 0.04343907338304951\n",
      "Train Loss at iteration 1656: 0.043438770191128545\n",
      "Train Loss at iteration 1657: 0.04343846718621515\n",
      "Train Loss at iteration 1658: 0.04343816436779028\n",
      "Train Loss at iteration 1659: 0.04343786173533695\n",
      "Train Loss at iteration 1660: 0.04343755928834024\n",
      "Train Loss at iteration 1661: 0.04343725702628725\n",
      "Train Loss at iteration 1662: 0.04343695494866714\n",
      "Train Loss at iteration 1663: 0.04343665305497106\n",
      "Train Loss at iteration 1664: 0.04343635134469221\n",
      "Train Loss at iteration 1665: 0.04343604981732578\n",
      "Train Loss at iteration 1666: 0.04343574847236897\n",
      "Train Loss at iteration 1667: 0.04343544730932097\n",
      "Train Loss at iteration 1668: 0.04343514632768293\n",
      "Train Loss at iteration 1669: 0.043434845526958005\n",
      "Train Loss at iteration 1670: 0.0434345449066513\n",
      "Train Loss at iteration 1671: 0.043434244466269895\n",
      "Train Loss at iteration 1672: 0.043433944205322775\n",
      "Train Loss at iteration 1673: 0.04343364412332093\n",
      "Train Loss at iteration 1674: 0.0434333442197772\n",
      "Train Loss at iteration 1675: 0.04343304449420644\n",
      "Train Loss at iteration 1676: 0.04343274494612534\n",
      "Train Loss at iteration 1677: 0.043432445575052554\n",
      "Train Loss at iteration 1678: 0.04343214638050861\n",
      "Train Loss at iteration 1679: 0.04343184736201591\n",
      "Train Loss at iteration 1680: 0.043431548519098806\n",
      "Train Loss at iteration 1681: 0.04343124985128344\n",
      "Train Loss at iteration 1682: 0.04343095135809786\n",
      "Train Loss at iteration 1683: 0.04343065303907199\n",
      "Train Loss at iteration 1684: 0.043430354893737595\n",
      "Train Loss at iteration 1685: 0.043430056921628234\n",
      "Train Loss at iteration 1686: 0.04342975912227939\n",
      "Train Loss at iteration 1687: 0.04342946149522832\n",
      "Train Loss at iteration 1688: 0.04342916404001409\n",
      "Train Loss at iteration 1689: 0.0434288667561776\n",
      "Train Loss at iteration 1690: 0.04342856964326154\n",
      "Train Loss at iteration 1691: 0.04342827270081044\n",
      "Train Loss at iteration 1692: 0.04342797592837055\n",
      "Train Loss at iteration 1693: 0.04342767932548996\n",
      "Train Loss at iteration 1694: 0.04342738289171849\n",
      "Train Loss at iteration 1695: 0.043427086626607776\n",
      "Train Loss at iteration 1696: 0.04342679052971115\n",
      "Train Loss at iteration 1697: 0.043426494600583745\n",
      "Train Loss at iteration 1698: 0.04342619883878242\n",
      "Train Loss at iteration 1699: 0.04342590324386577\n",
      "Train Loss at iteration 1700: 0.04342560781539411\n",
      "Train Loss at iteration 1701: 0.043425312552929494\n",
      "Train Loss at iteration 1702: 0.04342501745603568\n",
      "Train Loss at iteration 1703: 0.04342472252427816\n",
      "Train Loss at iteration 1704: 0.04342442775722406\n",
      "Train Loss at iteration 1705: 0.04342413315444227\n",
      "Train Loss at iteration 1706: 0.04342383871550333\n",
      "Train Loss at iteration 1707: 0.04342354443997945\n",
      "Train Loss at iteration 1708: 0.04342325032744453\n",
      "Train Loss at iteration 1709: 0.043422956377474134\n",
      "Train Loss at iteration 1710: 0.04342266258964547\n",
      "Train Loss at iteration 1711: 0.04342236896353742\n",
      "Train Loss at iteration 1712: 0.04342207549873047\n",
      "Train Loss at iteration 1713: 0.04342178219480677\n",
      "Train Loss at iteration 1714: 0.04342148905135009\n",
      "Train Loss at iteration 1715: 0.04342119606794584\n",
      "Train Loss at iteration 1716: 0.043420903244181\n",
      "Train Loss at iteration 1717: 0.04342061057964423\n",
      "Train Loss at iteration 1718: 0.04342031807392571\n",
      "Train Loss at iteration 1719: 0.04342002572661727\n",
      "Train Loss at iteration 1720: 0.04341973353731231\n",
      "Train Loss at iteration 1721: 0.043419441505605824\n",
      "Train Loss at iteration 1722: 0.043419149631094354\n",
      "Train Loss at iteration 1723: 0.04341885791337604\n",
      "Train Loss at iteration 1724: 0.04341856635205056\n",
      "Train Loss at iteration 1725: 0.04341827494671917\n",
      "Train Loss at iteration 1726: 0.04341798369698464\n",
      "Train Loss at iteration 1727: 0.04341769260245132\n",
      "Train Loss at iteration 1728: 0.043417401662725064\n",
      "Train Loss at iteration 1729: 0.04341711087741328\n",
      "Train Loss at iteration 1730: 0.04341682024612487\n",
      "Train Loss at iteration 1731: 0.043416529768470276\n",
      "Train Loss at iteration 1732: 0.04341623944406145\n",
      "Train Loss at iteration 1733: 0.04341594927251183\n",
      "Train Loss at iteration 1734: 0.04341565925343635\n",
      "Train Loss at iteration 1735: 0.043415369386451456\n",
      "Train Loss at iteration 1736: 0.04341507967117506\n",
      "Train Loss at iteration 1737: 0.04341479010722654\n",
      "Train Loss at iteration 1738: 0.0434145006942268\n",
      "Train Loss at iteration 1739: 0.04341421143179815\n",
      "Train Loss at iteration 1740: 0.043413922319564376\n",
      "Train Loss at iteration 1741: 0.04341363335715073\n",
      "Train Loss at iteration 1742: 0.043413344544183906\n",
      "Train Loss at iteration 1743: 0.043413055880292026\n",
      "Train Loss at iteration 1744: 0.043412767365104646\n",
      "Train Loss at iteration 1745: 0.04341247899825278\n",
      "Train Loss at iteration 1746: 0.04341219077936885\n",
      "Train Loss at iteration 1747: 0.04341190270808669\n",
      "Train Loss at iteration 1748: 0.04341161478404153\n",
      "Train Loss at iteration 1749: 0.04341132700687002\n",
      "Train Loss at iteration 1750: 0.04341103937621023\n",
      "Train Loss at iteration 1751: 0.04341075189170159\n",
      "Train Loss at iteration 1752: 0.043410464552984925\n",
      "Train Loss at iteration 1753: 0.043410177359702457\n",
      "Train Loss at iteration 1754: 0.04340989031149776\n",
      "Train Loss at iteration 1755: 0.04340960340801581\n",
      "Train Loss at iteration 1756: 0.0434093166489029\n",
      "Train Loss at iteration 1757: 0.04340903003380673\n",
      "Train Loss at iteration 1758: 0.04340874356237634\n",
      "Train Loss at iteration 1759: 0.0434084572342621\n",
      "Train Loss at iteration 1760: 0.0434081710491157\n",
      "Train Loss at iteration 1761: 0.04340788500659023\n",
      "Train Loss at iteration 1762: 0.04340759910634009\n",
      "Train Loss at iteration 1763: 0.04340731334802095\n",
      "Train Loss at iteration 1764: 0.04340702773128985\n",
      "Train Loss at iteration 1765: 0.04340674225580515\n",
      "Train Loss at iteration 1766: 0.0434064569212265\n",
      "Train Loss at iteration 1767: 0.04340617172721485\n",
      "Train Loss at iteration 1768: 0.043405886673432446\n",
      "Train Loss at iteration 1769: 0.043405601759542826\n",
      "Train Loss at iteration 1770: 0.04340531698521083\n",
      "Train Loss at iteration 1771: 0.04340503235010256\n",
      "Train Loss at iteration 1772: 0.043404747853885414\n",
      "Train Loss at iteration 1773: 0.04340446349622803\n",
      "Train Loss at iteration 1774: 0.043404179276800334\n",
      "Train Loss at iteration 1775: 0.04340389519527351\n",
      "Train Loss at iteration 1776: 0.043403611251319994\n",
      "Train Loss at iteration 1777: 0.04340332744461347\n",
      "Train Loss at iteration 1778: 0.043403043774828856\n",
      "Train Loss at iteration 1779: 0.04340276024164234\n",
      "Train Loss at iteration 1780: 0.04340247684473128\n",
      "Train Loss at iteration 1781: 0.04340219358377435\n",
      "Train Loss at iteration 1782: 0.04340191045845137\n",
      "Train Loss at iteration 1783: 0.043401627468443414\n",
      "Train Loss at iteration 1784: 0.04340134461343279\n",
      "Train Loss at iteration 1785: 0.04340106189310296\n",
      "Train Loss at iteration 1786: 0.04340077930713862\n",
      "Train Loss at iteration 1787: 0.04340049685522568\n",
      "Train Loss at iteration 1788: 0.043400214537051215\n",
      "Train Loss at iteration 1789: 0.04339993235230349\n",
      "Train Loss at iteration 1790: 0.04339965030067197\n",
      "Train Loss at iteration 1791: 0.04339936838184727\n",
      "Train Loss at iteration 1792: 0.04339908659552124\n",
      "Train Loss at iteration 1793: 0.04339880494138683\n",
      "Train Loss at iteration 1794: 0.043398523419138156\n",
      "Train Loss at iteration 1795: 0.04339824202847055\n",
      "Train Loss at iteration 1796: 0.04339796076908045\n",
      "Train Loss at iteration 1797: 0.04339767964066546\n",
      "Train Loss at iteration 1798: 0.04339739864292433\n",
      "Train Loss at iteration 1799: 0.043397117775556916\n",
      "Train Loss at iteration 1800: 0.043396837038264274\n",
      "Train Loss at iteration 1801: 0.043396556430748544\n",
      "Train Loss at iteration 1802: 0.04339627595271299\n",
      "Train Loss at iteration 1803: 0.04339599560386202\n",
      "Train Loss at iteration 1804: 0.04339571538390114\n",
      "Train Loss at iteration 1805: 0.04339543529253697\n",
      "Train Loss at iteration 1806: 0.04339515532947724\n",
      "Train Loss at iteration 1807: 0.0433948754944308\n",
      "Train Loss at iteration 1808: 0.04339459578710757\n",
      "Train Loss at iteration 1809: 0.04339431620721855\n",
      "Train Loss at iteration 1810: 0.04339403675447588\n",
      "Train Loss at iteration 1811: 0.04339375742859275\n",
      "Train Loss at iteration 1812: 0.04339347822928344\n",
      "Train Loss at iteration 1813: 0.04339319915626328\n",
      "Train Loss at iteration 1814: 0.04339292020924872\n",
      "Train Loss at iteration 1815: 0.043392641387957225\n",
      "Train Loss at iteration 1816: 0.04339236269210737\n",
      "Train Loss at iteration 1817: 0.04339208412141874\n",
      "Train Loss at iteration 1818: 0.04339180567561199\n",
      "Train Loss at iteration 1819: 0.04339152735440885\n",
      "Train Loss at iteration 1820: 0.043391249157532064\n",
      "Train Loss at iteration 1821: 0.043390971084705424\n",
      "Train Loss at iteration 1822: 0.04339069313565376\n",
      "Train Loss at iteration 1823: 0.04339041531010294\n",
      "Train Loss at iteration 1824: 0.04339013760777985\n",
      "Train Loss at iteration 1825: 0.043389860028412386\n",
      "Train Loss at iteration 1826: 0.0433895825717295\n",
      "Train Loss at iteration 1827: 0.04338930523746112\n",
      "Train Loss at iteration 1828: 0.04338902802533821\n",
      "Train Loss at iteration 1829: 0.04338875093509273\n",
      "Train Loss at iteration 1830: 0.04338847396645763\n",
      "Train Loss at iteration 1831: 0.04338819711916691\n",
      "Train Loss at iteration 1832: 0.04338792039295547\n",
      "Train Loss at iteration 1833: 0.04338764378755929\n",
      "Train Loss at iteration 1834: 0.04338736730271531\n",
      "Train Loss at iteration 1835: 0.0433870909381614\n",
      "Train Loss at iteration 1836: 0.0433868146936365\n",
      "Train Loss at iteration 1837: 0.043386538568880424\n",
      "Train Loss at iteration 1838: 0.043386262563634044\n",
      "Train Loss at iteration 1839: 0.04338598667763914\n",
      "Train Loss at iteration 1840: 0.04338571091063848\n",
      "Train Loss at iteration 1841: 0.043385435262375774\n",
      "Train Loss at iteration 1842: 0.04338515973259568\n",
      "Train Loss at iteration 1843: 0.04338488432104385\n",
      "Train Loss at iteration 1844: 0.04338460902746684\n",
      "Train Loss at iteration 1845: 0.04338433385161213\n",
      "Train Loss at iteration 1846: 0.04338405879322821\n",
      "Train Loss at iteration 1847: 0.043383783852064435\n",
      "Train Loss at iteration 1848: 0.04338350902787113\n",
      "Train Loss at iteration 1849: 0.04338323432039952\n",
      "Train Loss at iteration 1850: 0.04338295972940178\n",
      "Train Loss at iteration 1851: 0.04338268525463096\n",
      "Train Loss at iteration 1852: 0.04338241089584108\n",
      "Train Loss at iteration 1853: 0.04338213665278705\n",
      "Train Loss at iteration 1854: 0.04338186252522468\n",
      "Train Loss at iteration 1855: 0.04338158851291067\n",
      "Train Loss at iteration 1856: 0.043381314615602656\n",
      "Train Loss at iteration 1857: 0.04338104083305915\n",
      "Train Loss at iteration 1858: 0.04338076716503957\n",
      "Train Loss at iteration 1859: 0.043380493611304194\n",
      "Train Loss at iteration 1860: 0.043380220171614206\n",
      "Train Loss at iteration 1861: 0.04337994684573169\n",
      "Train Loss at iteration 1862: 0.043379673633419574\n",
      "Train Loss at iteration 1863: 0.04337940053444168\n",
      "Train Loss at iteration 1864: 0.043379127548562685\n",
      "Train Loss at iteration 1865: 0.043378854675548154\n",
      "Train Loss at iteration 1866: 0.04337858191516453\n",
      "Train Loss at iteration 1867: 0.04337830926717906\n",
      "Train Loss at iteration 1868: 0.043378036731359905\n",
      "Train Loss at iteration 1869: 0.043377764307476044\n",
      "Train Loss at iteration 1870: 0.04337749199529733\n",
      "Train Loss at iteration 1871: 0.043377219794594433\n",
      "Train Loss at iteration 1872: 0.04337694770513891\n",
      "Train Loss at iteration 1873: 0.04337667572670311\n",
      "Train Loss at iteration 1874: 0.043376403859060254\n",
      "Train Loss at iteration 1875: 0.04337613210198439\n",
      "Train Loss at iteration 1876: 0.043375860455250374\n",
      "Train Loss at iteration 1877: 0.0433755889186339\n",
      "Train Loss at iteration 1878: 0.04337531749191149\n",
      "Train Loss at iteration 1879: 0.0433750461748605\n",
      "Train Loss at iteration 1880: 0.04337477496725905\n",
      "Train Loss at iteration 1881: 0.043374503868886125\n",
      "Train Loss at iteration 1882: 0.0433742328795215\n",
      "Train Loss at iteration 1883: 0.04337396199894577\n",
      "Train Loss at iteration 1884: 0.0433736912269403\n",
      "Train Loss at iteration 1885: 0.04337342056328727\n",
      "Train Loss at iteration 1886: 0.04337315000776968\n",
      "Train Loss at iteration 1887: 0.04337287956017131\n",
      "Train Loss at iteration 1888: 0.043372609220276706\n",
      "Train Loss at iteration 1889: 0.04337233898787121\n",
      "Train Loss at iteration 1890: 0.043372068862740976\n",
      "Train Loss at iteration 1891: 0.04337179884467289\n",
      "Train Loss at iteration 1892: 0.04337152893345467\n",
      "Train Loss at iteration 1893: 0.043371259128874774\n",
      "Train Loss at iteration 1894: 0.04337098943072243\n",
      "Train Loss at iteration 1895: 0.04337071983878765\n",
      "Train Loss at iteration 1896: 0.04337045035286118\n",
      "Train Loss at iteration 1897: 0.043370180972734575\n",
      "Train Loss at iteration 1898: 0.04336991169820011\n",
      "Train Loss at iteration 1899: 0.04336964252905082\n",
      "Train Loss at iteration 1900: 0.04336937346508051\n",
      "Train Loss at iteration 1901: 0.043369104506083715\n",
      "Train Loss at iteration 1902: 0.04336883565185573\n",
      "Train Loss at iteration 1903: 0.04336856690219257\n",
      "Train Loss at iteration 1904: 0.04336829825689103\n",
      "Train Loss at iteration 1905: 0.04336802971574861\n",
      "Train Loss at iteration 1906: 0.04336776127856354\n",
      "Train Loss at iteration 1907: 0.04336749294513482\n",
      "Train Loss at iteration 1908: 0.04336722471526213\n",
      "Train Loss at iteration 1909: 0.0433669565887459\n",
      "Train Loss at iteration 1910: 0.0433666885653873\n",
      "Train Loss at iteration 1911: 0.043366420644988156\n",
      "Train Loss at iteration 1912: 0.0433661528273511\n",
      "Train Loss at iteration 1913: 0.04336588511227939\n",
      "Train Loss at iteration 1914: 0.04336561749957706\n",
      "Train Loss at iteration 1915: 0.04336534998904882\n",
      "Train Loss at iteration 1916: 0.043365082580500085\n",
      "Train Loss at iteration 1917: 0.04336481527373699\n",
      "Train Loss at iteration 1918: 0.04336454806856634\n",
      "Train Loss at iteration 1919: 0.043364280964795665\n",
      "Train Loss at iteration 1920: 0.04336401396223319\n",
      "Train Loss at iteration 1921: 0.04336374706068778\n",
      "Train Loss at iteration 1922: 0.04336348025996906\n",
      "Train Loss at iteration 1923: 0.043363213559887294\n",
      "Train Loss at iteration 1924: 0.04336294696025343\n",
      "Train Loss at iteration 1925: 0.04336268046087911\n",
      "Train Loss at iteration 1926: 0.04336241406157667\n",
      "Train Loss at iteration 1927: 0.043362147762159076\n",
      "Train Loss at iteration 1928: 0.04336188156243998\n",
      "Train Loss at iteration 1929: 0.04336161546223373\n",
      "Train Loss at iteration 1930: 0.04336134946135532\n",
      "Train Loss at iteration 1931: 0.0433610835596204\n",
      "Train Loss at iteration 1932: 0.04336081775684528\n",
      "Train Loss at iteration 1933: 0.04336055205284695\n",
      "Train Loss at iteration 1934: 0.04336028644744304\n",
      "Train Loss at iteration 1935: 0.043360020940451834\n",
      "Train Loss at iteration 1936: 0.04335975553169225\n",
      "Train Loss at iteration 1937: 0.043359490220983905\n",
      "Train Loss at iteration 1938: 0.043359225008146984\n",
      "Train Loss at iteration 1939: 0.04335895989300238\n",
      "Train Loss at iteration 1940: 0.04335869487537161\n",
      "Train Loss at iteration 1941: 0.04335842995507679\n",
      "Train Loss at iteration 1942: 0.04335816513194073\n",
      "Train Loss at iteration 1943: 0.043357900405786826\n",
      "Train Loss at iteration 1944: 0.04335763577643913\n",
      "Train Loss at iteration 1945: 0.043357371243722284\n",
      "Train Loss at iteration 1946: 0.043357106807461604\n",
      "Train Loss at iteration 1947: 0.04335684246748301\n",
      "Train Loss at iteration 1948: 0.043356578223613024\n",
      "Train Loss at iteration 1949: 0.043356314075678797\n",
      "Train Loss at iteration 1950: 0.04335605002350809\n",
      "Train Loss at iteration 1951: 0.043355786066929296\n",
      "Train Loss at iteration 1952: 0.04335552220577139\n",
      "Train Loss at iteration 1953: 0.043355258439863965\n",
      "Train Loss at iteration 1954: 0.043354994769037224\n",
      "Train Loss at iteration 1955: 0.04335473119312196\n",
      "Train Loss at iteration 1956: 0.043354467711949576\n",
      "Train Loss at iteration 1957: 0.04335420432535207\n",
      "Train Loss at iteration 1958: 0.043353941033162056\n",
      "Train Loss at iteration 1959: 0.04335367783521268\n",
      "Train Loss at iteration 1960: 0.04335341473133775\n",
      "Train Loss at iteration 1961: 0.0433531517213716\n",
      "Train Loss at iteration 1962: 0.0433528888051492\n",
      "Train Loss at iteration 1963: 0.04335262598250609\n",
      "Train Loss at iteration 1964: 0.04335236325327836\n",
      "Train Loss at iteration 1965: 0.0433521006173027\n",
      "Train Loss at iteration 1966: 0.04335183807441641\n",
      "Train Loss at iteration 1967: 0.04335157562445729\n",
      "Train Loss at iteration 1968: 0.043351313267263776\n",
      "Train Loss at iteration 1969: 0.04335105100267486\n",
      "Train Loss at iteration 1970: 0.043350788830530056\n",
      "Train Loss at iteration 1971: 0.04335052675066951\n",
      "Train Loss at iteration 1972: 0.04335026476293389\n",
      "Train Loss at iteration 1973: 0.043350002867164415\n",
      "Train Loss at iteration 1974: 0.043349741063202905\n",
      "Train Loss at iteration 1975: 0.0433494793508917\n",
      "Train Loss at iteration 1976: 0.0433492177300737\n",
      "Train Loss at iteration 1977: 0.04334895620059236\n",
      "Train Loss at iteration 1978: 0.04334869476229169\n",
      "Train Loss at iteration 1979: 0.04334843341501625\n",
      "Train Loss at iteration 1980: 0.04334817215861112\n",
      "Train Loss at iteration 1981: 0.043347910992921936\n",
      "Train Loss at iteration 1982: 0.043347649917794905\n",
      "Train Loss at iteration 1983: 0.043347388933076714\n",
      "Train Loss at iteration 1984: 0.04334712803861462\n",
      "Train Loss at iteration 1985: 0.04334686723425644\n",
      "Train Loss at iteration 1986: 0.04334660651985045\n",
      "Train Loss at iteration 1987: 0.043346345895245546\n",
      "Train Loss at iteration 1988: 0.04334608536029107\n",
      "Train Loss at iteration 1989: 0.04334582491483693\n",
      "Train Loss at iteration 1990: 0.04334556455873356\n",
      "Train Loss at iteration 1991: 0.04334530429183193\n",
      "Train Loss at iteration 1992: 0.04334504411398347\n",
      "Train Loss at iteration 1993: 0.04334478402504016\n",
      "Train Loss at iteration 1994: 0.04334452402485453\n",
      "Train Loss at iteration 1995: 0.04334426411327958\n",
      "Train Loss at iteration 1996: 0.043344004290168846\n",
      "Train Loss at iteration 1997: 0.04334374455537633\n",
      "Train Loss at iteration 1998: 0.04334348490875659\n",
      "Train Loss at iteration 1999: 0.04334322535016468\n",
      "Train Loss at iteration 2000: 0.04334296587945614\n",
      "Train Loss at iteration 2001: 0.04334270649648702\n",
      "Train Loss at iteration 2002: 0.04334244720111385\n",
      "Train Loss at iteration 2003: 0.043342187993193705\n",
      "Train Loss at iteration 2004: 0.043341928872584104\n",
      "Train Loss at iteration 2005: 0.04334166983914309\n",
      "Train Loss at iteration 2006: 0.043341410892729186\n",
      "Train Loss at iteration 2007: 0.0433411520332014\n",
      "Train Loss at iteration 2008: 0.04334089326041924\n",
      "Train Loss at iteration 2009: 0.043340634574242684\n",
      "Train Loss at iteration 2010: 0.043340375974532226\n",
      "Train Loss at iteration 2011: 0.04334011746114878\n",
      "Train Loss at iteration 2012: 0.04333985903395382\n",
      "Train Loss at iteration 2013: 0.04333960069280922\n",
      "Train Loss at iteration 2014: 0.04333934243757738\n",
      "Train Loss at iteration 2015: 0.04333908426812114\n",
      "Train Loss at iteration 2016: 0.04333882618430385\n",
      "Train Loss at iteration 2017: 0.04333856818598933\n",
      "Train Loss at iteration 2018: 0.0433383102730418\n",
      "Train Loss at iteration 2019: 0.04333805244532604\n",
      "Train Loss at iteration 2020: 0.04333779470270722\n",
      "Train Loss at iteration 2021: 0.04333753704505102\n",
      "Train Loss at iteration 2022: 0.04333727947222356\n",
      "Train Loss at iteration 2023: 0.04333702198409142\n",
      "Train Loss at iteration 2024: 0.04333676458052166\n",
      "Train Loss at iteration 2025: 0.043336507261381746\n",
      "Train Loss at iteration 2026: 0.043336250026539645\n",
      "Train Loss at iteration 2027: 0.04333599287586375\n",
      "Train Loss at iteration 2028: 0.043335735809222925\n",
      "Train Loss at iteration 2029: 0.04333547882648648\n",
      "Train Loss at iteration 2030: 0.04333522192752414\n",
      "Train Loss at iteration 2031: 0.043334965112206116\n",
      "Train Loss at iteration 2032: 0.04333470838040302\n",
      "Train Loss at iteration 2033: 0.04333445173198594\n",
      "Train Loss at iteration 2034: 0.04333419516682641\n",
      "Train Loss at iteration 2035: 0.04333393868479636\n",
      "Train Loss at iteration 2036: 0.04333368228576819\n",
      "Train Loss at iteration 2037: 0.04333342596961473\n",
      "Train Loss at iteration 2038: 0.04333316973620924\n",
      "Train Loss at iteration 2039: 0.04333291358542538\n",
      "Train Loss at iteration 2040: 0.04333265751713731\n",
      "Train Loss at iteration 2041: 0.04333240153121955\n",
      "Train Loss at iteration 2042: 0.04333214562754706\n",
      "Train Loss at iteration 2043: 0.04333188980599527\n",
      "Train Loss at iteration 2044: 0.043331634066439986\n",
      "Train Loss at iteration 2045: 0.043331378408757434\n",
      "Train Loss at iteration 2046: 0.04333112283282429\n",
      "Train Loss at iteration 2047: 0.043330867338517626\n",
      "Train Loss at iteration 2048: 0.04333061192571494\n",
      "Train Loss at iteration 2049: 0.043330356594294114\n",
      "Train Loss at iteration 2050: 0.04333010134413349\n",
      "Train Loss at iteration 2051: 0.043329846175111805\n",
      "Train Loss at iteration 2052: 0.043329591087108196\n",
      "Train Loss at iteration 2053: 0.04332933608000219\n",
      "Train Loss at iteration 2054: 0.04332908115367378\n",
      "Train Loss at iteration 2055: 0.04332882630800329\n",
      "Train Loss at iteration 2056: 0.0433285715428715\n",
      "Train Loss at iteration 2057: 0.04332831685815957\n",
      "Train Loss at iteration 2058: 0.04332806225374908\n",
      "Train Loss at iteration 2059: 0.04332780772952198\n",
      "Train Loss at iteration 2060: 0.04332755328536061\n",
      "Train Loss at iteration 2061: 0.043327298921147755\n",
      "Train Loss at iteration 2062: 0.04332704463676655\n",
      "Train Loss at iteration 2063: 0.043326790432100554\n",
      "Train Loss at iteration 2064: 0.04332653630703369\n",
      "Train Loss at iteration 2065: 0.04332628226145025\n",
      "Train Loss at iteration 2066: 0.04332602829523498\n",
      "Train Loss at iteration 2067: 0.043325774408272955\n",
      "Train Loss at iteration 2068: 0.043325520600449675\n",
      "Train Loss at iteration 2069: 0.04332526687165098\n",
      "Train Loss at iteration 2070: 0.04332501322176314\n",
      "Train Loss at iteration 2071: 0.04332475965067276\n",
      "Train Loss at iteration 2072: 0.043324506158266836\n",
      "Train Loss at iteration 2073: 0.043324252744432766\n",
      "Train Loss at iteration 2074: 0.0433239994090583\n",
      "Train Loss at iteration 2075: 0.04332374615203156\n",
      "Train Loss at iteration 2076: 0.04332349297324107\n",
      "Train Loss at iteration 2077: 0.043323239872575685\n",
      "Train Loss at iteration 2078: 0.043322986849924645\n",
      "Train Loss at iteration 2079: 0.04332273390517758\n",
      "Train Loss at iteration 2080: 0.04332248103822447\n",
      "Train Loss at iteration 2081: 0.043322228248955645\n",
      "Train Loss at iteration 2082: 0.04332197553726182\n",
      "Train Loss at iteration 2083: 0.043321722903034064\n",
      "Train Loss at iteration 2084: 0.043321470346163814\n",
      "Train Loss at iteration 2085: 0.04332121786654287\n",
      "Train Loss at iteration 2086: 0.04332096546406338\n",
      "Train Loss at iteration 2087: 0.04332071313861785\n",
      "Train Loss at iteration 2088: 0.043320460890099145\n",
      "Train Loss at iteration 2089: 0.043320208718400484\n",
      "Train Loss at iteration 2090: 0.04331995662341544\n",
      "Train Loss at iteration 2091: 0.04331970460503794\n",
      "Train Loss at iteration 2092: 0.04331945266316224\n",
      "Train Loss at iteration 2093: 0.04331920079768297\n",
      "Train Loss at iteration 2094: 0.04331894900849512\n",
      "Train Loss at iteration 2095: 0.04331869729549398\n",
      "Train Loss at iteration 2096: 0.04331844565857522\n",
      "Train Loss at iteration 2097: 0.04331819409763486\n",
      "Train Loss at iteration 2098: 0.04331794261256922\n",
      "Train Loss at iteration 2099: 0.043317691203275006\n",
      "Train Loss at iteration 2100: 0.04331743986964923\n",
      "Train Loss at iteration 2101: 0.04331718861158927\n",
      "Train Loss at iteration 2102: 0.04331693742899282\n",
      "Train Loss at iteration 2103: 0.04331668632175791\n",
      "Train Loss at iteration 2104: 0.043316435289782934\n",
      "Train Loss at iteration 2105: 0.043316184332966585\n",
      "Train Loss at iteration 2106: 0.04331593345120789\n",
      "Train Loss at iteration 2107: 0.04331568264440622\n",
      "Train Loss at iteration 2108: 0.04331543191246127\n",
      "Train Loss at iteration 2109: 0.04331518125527307\n",
      "Train Loss at iteration 2110: 0.04331493067274196\n",
      "Train Loss at iteration 2111: 0.04331468016476862\n",
      "Train Loss at iteration 2112: 0.043314429731254045\n",
      "Train Loss at iteration 2113: 0.043314179372099575\n",
      "Train Loss at iteration 2114: 0.043313929087206826\n",
      "Train Loss at iteration 2115: 0.04331367887647778\n",
      "Train Loss at iteration 2116: 0.043313428739814704\n",
      "Train Loss at iteration 2117: 0.043313178677120215\n",
      "Train Loss at iteration 2118: 0.043312928688297225\n",
      "Train Loss at iteration 2119: 0.043312678773248965\n",
      "Train Loss at iteration 2120: 0.04331242893187897\n",
      "Train Loss at iteration 2121: 0.04331217916409111\n",
      "Train Loss at iteration 2122: 0.043311929469789553\n",
      "Train Loss at iteration 2123: 0.043311679848878785\n",
      "Train Loss at iteration 2124: 0.043311430301263595\n",
      "Train Loss at iteration 2125: 0.04331118082684908\n",
      "Train Loss at iteration 2126: 0.043310931425540655\n",
      "Train Loss at iteration 2127: 0.04331068209724401\n",
      "Train Loss at iteration 2128: 0.043310432841865164\n",
      "Train Loss at iteration 2129: 0.04331018365931046\n",
      "Train Loss at iteration 2130: 0.0433099345494865\n",
      "Train Loss at iteration 2131: 0.04330968551230022\n",
      "Train Loss at iteration 2132: 0.04330943654765882\n",
      "Train Loss at iteration 2133: 0.043309187655469844\n",
      "Train Loss at iteration 2134: 0.04330893883564109\n",
      "Train Loss at iteration 2135: 0.0433086900880807\n",
      "Train Loss at iteration 2136: 0.04330844141269706\n",
      "Train Loss at iteration 2137: 0.04330819280939887\n",
      "Train Loss at iteration 2138: 0.04330794427809515\n",
      "Train Loss at iteration 2139: 0.04330769581869519\n",
      "Train Loss at iteration 2140: 0.043307447431108544\n",
      "Train Loss at iteration 2141: 0.04330719911524511\n",
      "Train Loss at iteration 2142: 0.04330695087101503\n",
      "Train Loss at iteration 2143: 0.04330670269832876\n",
      "Train Loss at iteration 2144: 0.04330645459709701\n",
      "Train Loss at iteration 2145: 0.043306206567230825\n",
      "Train Loss at iteration 2146: 0.0433059586086415\n",
      "Train Loss at iteration 2147: 0.0433057107212406\n",
      "Train Loss at iteration 2148: 0.043305462904940015\n",
      "Train Loss at iteration 2149: 0.04330521515965187\n",
      "Train Loss at iteration 2150: 0.04330496748528862\n",
      "Train Loss at iteration 2151: 0.04330471988176294\n",
      "Train Loss at iteration 2152: 0.04330447234898783\n",
      "Train Loss at iteration 2153: 0.04330422488687655\n",
      "Train Loss at iteration 2154: 0.04330397749534264\n",
      "Train Loss at iteration 2155: 0.0433037301742999\n",
      "Train Loss at iteration 2156: 0.04330348292366241\n",
      "Train Loss at iteration 2157: 0.04330323574334454\n",
      "Train Loss at iteration 2158: 0.0433029886332609\n",
      "Train Loss at iteration 2159: 0.04330274159332641\n",
      "Train Loss at iteration 2160: 0.04330249462345621\n",
      "Train Loss at iteration 2161: 0.043302247723565744\n",
      "Train Loss at iteration 2162: 0.04330200089357073\n",
      "Train Loss at iteration 2163: 0.04330175413338711\n",
      "Train Loss at iteration 2164: 0.043301507442931146\n",
      "Train Loss at iteration 2165: 0.043301260822119304\n",
      "Train Loss at iteration 2166: 0.04330101427086837\n",
      "Train Loss at iteration 2167: 0.04330076778909535\n",
      "Train Loss at iteration 2168: 0.043300521376717534\n",
      "Train Loss at iteration 2169: 0.043300275033652455\n",
      "Train Loss at iteration 2170: 0.04330002875981795\n",
      "Train Loss at iteration 2171: 0.04329978255513204\n",
      "Train Loss at iteration 2172: 0.043299536419513056\n",
      "Train Loss at iteration 2173: 0.043299290352879584\n",
      "Train Loss at iteration 2174: 0.04329904435515043\n",
      "Train Loss at iteration 2175: 0.04329879842624469\n",
      "Train Loss at iteration 2176: 0.04329855256608172\n",
      "Train Loss at iteration 2177: 0.04329830677458108\n",
      "Train Loss at iteration 2178: 0.04329806105166263\n",
      "Train Loss at iteration 2179: 0.043297815397246454\n",
      "Train Loss at iteration 2180: 0.043297569811252885\n",
      "Train Loss at iteration 2181: 0.043297324293602515\n",
      "Train Loss at iteration 2182: 0.043297078844216186\n",
      "Train Loss at iteration 2183: 0.04329683346301498\n",
      "Train Loss at iteration 2184: 0.04329658814992022\n",
      "Train Loss at iteration 2185: 0.04329634290485348\n",
      "Train Loss at iteration 2186: 0.04329609772773658\n",
      "Train Loss at iteration 2187: 0.04329585261849157\n",
      "Train Loss at iteration 2188: 0.04329560757704077\n",
      "Train Loss at iteration 2189: 0.04329536260330669\n",
      "Train Loss at iteration 2190: 0.04329511769721216\n",
      "Train Loss at iteration 2191: 0.04329487285868016\n",
      "Train Loss at iteration 2192: 0.043294628087633964\n",
      "Train Loss at iteration 2193: 0.04329438338399707\n",
      "Train Loss at iteration 2194: 0.04329413874769322\n",
      "Train Loss at iteration 2195: 0.04329389417864637\n",
      "Train Loss at iteration 2196: 0.04329364967678073\n",
      "Train Loss at iteration 2197: 0.04329340524202073\n",
      "Train Loss at iteration 2198: 0.043293160874291046\n",
      "Train Loss at iteration 2199: 0.04329291657351659\n",
      "Train Loss at iteration 2200: 0.04329267233962247\n",
      "Train Loss at iteration 2201: 0.04329242817253408\n",
      "Train Loss at iteration 2202: 0.04329218407217699\n",
      "Train Loss at iteration 2203: 0.04329194003847704\n",
      "Train Loss at iteration 2204: 0.04329169607136025\n",
      "Train Loss at iteration 2205: 0.04329145217075293\n",
      "Train Loss at iteration 2206: 0.04329120833658156\n",
      "Train Loss at iteration 2207: 0.04329096456877288\n",
      "Train Loss at iteration 2208: 0.04329072086725383\n",
      "Train Loss at iteration 2209: 0.04329047723195158\n",
      "Train Loss at iteration 2210: 0.04329023366279355\n",
      "Train Loss at iteration 2211: 0.043289990159707324\n",
      "Train Loss at iteration 2212: 0.043289746722620766\n",
      "Train Loss at iteration 2213: 0.04328950335146194\n",
      "Train Loss at iteration 2214: 0.04328926004615911\n",
      "Train Loss at iteration 2215: 0.043289016806640775\n",
      "Train Loss at iteration 2216: 0.04328877363283566\n",
      "Train Loss at iteration 2217: 0.04328853052467267\n",
      "Train Loss at iteration 2218: 0.04328828748208099\n",
      "Train Loss at iteration 2219: 0.04328804450498992\n",
      "Train Loss at iteration 2220: 0.043287801593329116\n",
      "Train Loss at iteration 2221: 0.043287558747028304\n",
      "Train Loss at iteration 2222: 0.04328731596601752\n",
      "Train Loss at iteration 2223: 0.04328707325022696\n",
      "Train Loss at iteration 2224: 0.04328683059958705\n",
      "Train Loss at iteration 2225: 0.043286588014028446\n",
      "Train Loss at iteration 2226: 0.04328634549348196\n",
      "Train Loss at iteration 2227: 0.043286103037878665\n",
      "Train Loss at iteration 2228: 0.04328586064714982\n",
      "Train Loss at iteration 2229: 0.04328561832122689\n",
      "Train Loss at iteration 2230: 0.043285376060041555\n",
      "Train Loss at iteration 2231: 0.04328513386352568\n",
      "Train Loss at iteration 2232: 0.04328489173161137\n",
      "Train Loss at iteration 2233: 0.04328464966423089\n",
      "Train Loss at iteration 2234: 0.04328440766131674\n",
      "Train Loss at iteration 2235: 0.043284165722801624\n",
      "Train Loss at iteration 2236: 0.04328392384861843\n",
      "Train Loss at iteration 2237: 0.043283682038700255\n",
      "Train Loss at iteration 2238: 0.043283440292980396\n",
      "Train Loss at iteration 2239: 0.04328319861139235\n",
      "Train Loss at iteration 2240: 0.043282956993869796\n",
      "Train Loss at iteration 2241: 0.043282715440346654\n",
      "Train Loss at iteration 2242: 0.043282473950756996\n",
      "Train Loss at iteration 2243: 0.04328223252503511\n",
      "Train Loss at iteration 2244: 0.04328199116311548\n",
      "Train Loss at iteration 2245: 0.043281749864932786\n",
      "Train Loss at iteration 2246: 0.04328150863042189\n",
      "Train Loss at iteration 2247: 0.043281267459517866\n",
      "Train Loss at iteration 2248: 0.043281026352155964\n",
      "Train Loss at iteration 2249: 0.04328078530827164\n",
      "Train Loss at iteration 2250: 0.04328054432780053\n",
      "Train Loss at iteration 2251: 0.04328030341067845\n",
      "Train Loss at iteration 2252: 0.04328006255684145\n",
      "Train Loss at iteration 2253: 0.043279821766225725\n",
      "Train Loss at iteration 2254: 0.04327958103876768\n",
      "Train Loss at iteration 2255: 0.04327934037440388\n",
      "Train Loss at iteration 2256: 0.04327909977307113\n",
      "Train Loss at iteration 2257: 0.04327885923470636\n",
      "Train Loss at iteration 2258: 0.04327861875924675\n",
      "Train Loss at iteration 2259: 0.043278378346629585\n",
      "Train Loss at iteration 2260: 0.04327813799679243\n",
      "Train Loss at iteration 2261: 0.04327789770967292\n",
      "Train Loss at iteration 2262: 0.043277657485209\n",
      "Train Loss at iteration 2263: 0.043277417323338715\n",
      "Train Loss at iteration 2264: 0.043277177224000284\n",
      "Train Loss at iteration 2265: 0.04327693718713216\n",
      "Train Loss at iteration 2266: 0.043276697212672924\n",
      "Train Loss at iteration 2267: 0.04327645730056139\n",
      "Train Loss at iteration 2268: 0.0432762174507365\n",
      "Train Loss at iteration 2269: 0.04327597766313742\n",
      "Train Loss at iteration 2270: 0.04327573793770344\n",
      "Train Loss at iteration 2271: 0.043275498274374075\n",
      "Train Loss at iteration 2272: 0.04327525867308899\n",
      "Train Loss at iteration 2273: 0.04327501913378804\n",
      "Train Loss at iteration 2274: 0.04327477965641125\n",
      "Train Loss at iteration 2275: 0.0432745402408988\n",
      "Train Loss at iteration 2276: 0.043274300887191086\n",
      "Train Loss at iteration 2277: 0.04327406159522865\n",
      "Train Loss at iteration 2278: 0.043273822364952176\n",
      "Train Loss at iteration 2279: 0.043273583196302576\n",
      "Train Loss at iteration 2280: 0.043273344089220914\n",
      "Train Loss at iteration 2281: 0.04327310504364841\n",
      "Train Loss at iteration 2282: 0.043272866059526474\n",
      "Train Loss at iteration 2283: 0.043272627136796665\n",
      "Train Loss at iteration 2284: 0.043272388275400704\n",
      "Train Loss at iteration 2285: 0.04327214947528051\n",
      "Train Loss at iteration 2286: 0.04327191073637817\n",
      "Train Loss at iteration 2287: 0.0432716720586359\n",
      "Train Loss at iteration 2288: 0.04327143344199611\n",
      "Train Loss at iteration 2289: 0.04327119488640136\n",
      "Train Loss at iteration 2290: 0.04327095639179441\n",
      "Train Loss at iteration 2291: 0.04327071795811814\n",
      "Train Loss at iteration 2292: 0.0432704795853156\n",
      "Train Loss at iteration 2293: 0.04327024127333006\n",
      "Train Loss at iteration 2294: 0.04327000302210485\n",
      "Train Loss at iteration 2295: 0.04326976483158356\n",
      "Train Loss at iteration 2296: 0.04326952670170988\n",
      "Train Loss at iteration 2297: 0.04326928863242769\n",
      "Train Loss at iteration 2298: 0.043269050623681006\n",
      "Train Loss at iteration 2299: 0.043268812675414034\n",
      "Train Loss at iteration 2300: 0.04326857478757112\n",
      "Train Loss at iteration 2301: 0.04326833696009676\n",
      "Train Loss at iteration 2302: 0.04326809919293563\n",
      "Train Loss at iteration 2303: 0.04326786148603253\n",
      "Train Loss at iteration 2304: 0.04326762383933248\n",
      "Train Loss at iteration 2305: 0.04326738625278057\n",
      "Train Loss at iteration 2306: 0.04326714872632211\n",
      "Train Loss at iteration 2307: 0.043266911259902535\n",
      "Train Loss at iteration 2308: 0.043266673853467455\n",
      "Train Loss at iteration 2309: 0.04326643650696261\n",
      "Train Loss at iteration 2310: 0.04326619922033389\n",
      "Train Loss at iteration 2311: 0.04326596199352738\n",
      "Train Loss at iteration 2312: 0.04326572482648927\n",
      "Train Loss at iteration 2313: 0.04326548771916592\n",
      "Train Loss at iteration 2314: 0.04326525067150386\n",
      "Train Loss at iteration 2315: 0.04326501368344972\n",
      "Train Loss at iteration 2316: 0.04326477675495033\n",
      "Train Loss at iteration 2317: 0.04326453988595265\n",
      "Train Loss at iteration 2318: 0.04326430307640376\n",
      "Train Loss at iteration 2319: 0.04326406632625096\n",
      "Train Loss at iteration 2320: 0.04326382963544161\n",
      "Train Loss at iteration 2321: 0.04326359300392329\n",
      "Train Loss at iteration 2322: 0.04326335643164368\n",
      "Train Loss at iteration 2323: 0.04326311991855063\n",
      "Train Loss at iteration 2324: 0.043262883464592124\n",
      "Train Loss at iteration 2325: 0.04326264706971628\n",
      "Train Loss at iteration 2326: 0.0432624107338714\n",
      "Train Loss at iteration 2327: 0.043262174457005885\n",
      "Train Loss at iteration 2328: 0.0432619382390683\n",
      "Train Loss at iteration 2329: 0.04326170208000736\n",
      "Train Loss at iteration 2330: 0.043261465979771915\n",
      "Train Loss at iteration 2331: 0.04326122993831092\n",
      "Train Loss at iteration 2332: 0.04326099395557355\n",
      "Train Loss at iteration 2333: 0.04326075803150905\n",
      "Train Loss at iteration 2334: 0.04326052216606684\n",
      "Train Loss at iteration 2335: 0.04326028635919645\n",
      "Train Loss at iteration 2336: 0.0432600506108476\n",
      "Train Loss at iteration 2337: 0.0432598149209701\n",
      "Train Loss at iteration 2338: 0.04325957928951392\n",
      "Train Loss at iteration 2339: 0.04325934371642915\n",
      "Train Loss at iteration 2340: 0.043259108201666056\n",
      "Train Loss at iteration 2341: 0.04325887274517499\n",
      "Train Loss at iteration 2342: 0.043258637346906464\n",
      "Train Loss at iteration 2343: 0.04325840200681114\n",
      "Train Loss at iteration 2344: 0.043258166724839814\n",
      "Train Loss at iteration 2345: 0.04325793150094336\n",
      "Train Loss at iteration 2346: 0.04325769633507287\n",
      "Train Loss at iteration 2347: 0.0432574612271795\n",
      "Train Loss at iteration 2348: 0.04325722617721459\n",
      "Train Loss at iteration 2349: 0.04325699118512958\n",
      "Train Loss at iteration 2350: 0.04325675625087605\n",
      "Train Loss at iteration 2351: 0.043256521374405714\n",
      "Train Loss at iteration 2352: 0.04325628655567041\n",
      "Train Loss at iteration 2353: 0.04325605179462213\n",
      "Train Loss at iteration 2354: 0.04325581709121297\n",
      "Train Loss at iteration 2355: 0.043255582445395166\n",
      "Train Loss at iteration 2356: 0.04325534785712108\n",
      "Train Loss at iteration 2357: 0.043255113326343214\n",
      "Train Loss at iteration 2358: 0.043254878853014184\n",
      "Train Loss at iteration 2359: 0.04325464443708673\n",
      "Train Loss at iteration 2360: 0.04325441007851373\n",
      "Train Loss at iteration 2361: 0.0432541757772482\n",
      "Train Loss at iteration 2362: 0.04325394153324325\n",
      "Train Loss at iteration 2363: 0.043253707346452154\n",
      "Train Loss at iteration 2364: 0.04325347321682828\n",
      "Train Loss at iteration 2365: 0.04325323914432515\n",
      "Train Loss at iteration 2366: 0.04325300512889638\n",
      "Train Loss at iteration 2367: 0.04325277117049572\n",
      "Train Loss at iteration 2368: 0.04325253726907703\n",
      "Train Loss at iteration 2369: 0.04325230342459437\n",
      "Train Loss at iteration 2370: 0.043252069637001817\n",
      "Train Loss at iteration 2371: 0.04325183590625363\n",
      "Train Loss at iteration 2372: 0.043251602232304176\n",
      "Train Loss at iteration 2373: 0.04325136861510794\n",
      "Train Loss at iteration 2374: 0.04325113505461954\n",
      "Train Loss at iteration 2375: 0.043250901550793704\n",
      "Train Loss at iteration 2376: 0.04325066810358528\n",
      "Train Loss at iteration 2377: 0.04325043471294923\n",
      "Train Loss at iteration 2378: 0.04325020137884066\n",
      "Train Loss at iteration 2379: 0.04324996810121477\n",
      "Train Loss at iteration 2380: 0.04324973488002689\n",
      "Train Loss at iteration 2381: 0.04324950171523246\n",
      "Train Loss at iteration 2382: 0.043249268606787045\n",
      "Train Loss at iteration 2383: 0.04324903555464633\n",
      "Train Loss at iteration 2384: 0.0432488025587661\n",
      "Train Loss at iteration 2385: 0.04324856961910227\n",
      "Train Loss at iteration 2386: 0.04324833673561087\n",
      "Train Loss at iteration 2387: 0.04324810390824804\n",
      "Train Loss at iteration 2388: 0.04324787113697004\n",
      "Train Loss at iteration 2389: 0.043247638421733244\n",
      "Train Loss at iteration 2390: 0.043247405762494126\n",
      "Train Loss at iteration 2391: 0.04324717315920932\n",
      "Train Loss at iteration 2392: 0.043246940611835503\n",
      "Train Loss at iteration 2393: 0.04324670812032952\n",
      "Train Loss at iteration 2394: 0.04324647568464831\n",
      "Train Loss at iteration 2395: 0.04324624330474891\n",
      "Train Loss at iteration 2396: 0.04324601098058851\n",
      "Train Loss at iteration 2397: 0.04324577871212436\n",
      "Train Loss at iteration 2398: 0.04324554649931384\n",
      "Train Loss at iteration 2399: 0.04324531434211447\n",
      "Train Loss at iteration 2400: 0.043245082240483845\n",
      "Train Loss at iteration 2401: 0.04324485019437967\n",
      "Train Loss at iteration 2402: 0.043244618203759785\n",
      "Train Loss at iteration 2403: 0.04324438626858212\n",
      "Train Loss at iteration 2404: 0.043244154388804704\n",
      "Train Loss at iteration 2405: 0.04324392256438571\n",
      "Train Loss at iteration 2406: 0.04324369079528337\n",
      "Train Loss at iteration 2407: 0.04324345908145606\n",
      "Train Loss at iteration 2408: 0.04324322742286226\n",
      "Train Loss at iteration 2409: 0.04324299581946055\n",
      "Train Loss at iteration 2410: 0.0432427642712096\n",
      "Train Loss at iteration 2411: 0.0432425327780682\n",
      "Train Loss at iteration 2412: 0.04324230133999525\n",
      "Train Loss at iteration 2413: 0.043242069956949764\n",
      "Train Loss at iteration 2414: 0.04324183862889083\n",
      "Train Loss at iteration 2415: 0.043241607355777666\n",
      "Train Loss at iteration 2416: 0.043241376137569586\n",
      "Train Loss at iteration 2417: 0.043241144974226015\n",
      "Train Loss at iteration 2418: 0.04324091386570646\n",
      "Train Loss at iteration 2419: 0.04324068281197055\n",
      "Train Loss at iteration 2420: 0.043240451812978\n",
      "Train Loss at iteration 2421: 0.04324022086868865\n",
      "Train Loss at iteration 2422: 0.04323998997906243\n",
      "Train Loss at iteration 2423: 0.04323975914405938\n",
      "Train Loss at iteration 2424: 0.043239528363639604\n",
      "Train Loss at iteration 2425: 0.043239297637763345\n",
      "Train Loss at iteration 2426: 0.04323906696639095\n",
      "Train Loss at iteration 2427: 0.04323883634948285\n",
      "Train Loss at iteration 2428: 0.043238605786999555\n",
      "Train Loss at iteration 2429: 0.04323837527890171\n",
      "Train Loss at iteration 2430: 0.043238144825150045\n",
      "Train Loss at iteration 2431: 0.04323791442570539\n",
      "Train Loss at iteration 2432: 0.04323768408052867\n",
      "Train Loss at iteration 2433: 0.04323745378958091\n",
      "Train Loss at iteration 2434: 0.04323722355282322\n",
      "Train Loss at iteration 2435: 0.043236993370216845\n",
      "Train Loss at iteration 2436: 0.04323676324172307\n",
      "Train Loss at iteration 2437: 0.04323653316730334\n",
      "Train Loss at iteration 2438: 0.043236303146919126\n",
      "Train Loss at iteration 2439: 0.043236073180532054\n",
      "Train Loss at iteration 2440: 0.04323584326810382\n",
      "Train Loss at iteration 2441: 0.04323561340959622\n",
      "Train Loss at iteration 2442: 0.04323538360497114\n",
      "Train Loss at iteration 2443: 0.04323515385419056\n",
      "Train Loss at iteration 2444: 0.043234924157216546\n",
      "Train Loss at iteration 2445: 0.043234694514011296\n",
      "Train Loss at iteration 2446: 0.04323446492453705\n",
      "Train Loss at iteration 2447: 0.04323423538875618\n",
      "Train Loss at iteration 2448: 0.04323400590663113\n",
      "Train Loss at iteration 2449: 0.04323377647812445\n",
      "Train Loss at iteration 2450: 0.04323354710319875\n",
      "Train Loss at iteration 2451: 0.04323331778181679\n",
      "Train Loss at iteration 2452: 0.04323308851394135\n",
      "Train Loss at iteration 2453: 0.04323285929953536\n",
      "Train Loss at iteration 2454: 0.043232630138561816\n",
      "Train Loss at iteration 2455: 0.04323240103098381\n",
      "Train Loss at iteration 2456: 0.043232171976764516\n",
      "Train Loss at iteration 2457: 0.04323194297586719\n",
      "Train Loss at iteration 2458: 0.043231714028255215\n",
      "Train Loss at iteration 2459: 0.04323148513389202\n",
      "Train Loss at iteration 2460: 0.04323125629274116\n",
      "Train Loss at iteration 2461: 0.04323102750476624\n",
      "Train Loss at iteration 2462: 0.04323079876993099\n",
      "Train Loss at iteration 2463: 0.04323057008819918\n",
      "Train Loss at iteration 2464: 0.043230341459534735\n",
      "Train Loss at iteration 2465: 0.04323011288390161\n",
      "Train Loss at iteration 2466: 0.04322988436126389\n",
      "Train Loss at iteration 2467: 0.04322965589158569\n",
      "Train Loss at iteration 2468: 0.043229427474831265\n",
      "Train Loss at iteration 2469: 0.04322919911096493\n",
      "Train Loss at iteration 2470: 0.0432289707999511\n",
      "Train Loss at iteration 2471: 0.04322874254175426\n",
      "Train Loss at iteration 2472: 0.04322851433633899\n",
      "Train Loss at iteration 2473: 0.04322828618366996\n",
      "Train Loss at iteration 2474: 0.04322805808371189\n",
      "Train Loss at iteration 2475: 0.04322783003642965\n",
      "Train Loss at iteration 2476: 0.043227602041788125\n",
      "Train Loss at iteration 2477: 0.04322737409975232\n",
      "Train Loss at iteration 2478: 0.04322714621028733\n",
      "Train Loss at iteration 2479: 0.04322691837335831\n",
      "Train Loss at iteration 2480: 0.04322669058893051\n",
      "Train Loss at iteration 2481: 0.043226462856969246\n",
      "Train Loss at iteration 2482: 0.04322623517743996\n",
      "Train Loss at iteration 2483: 0.043226007550308115\n",
      "Train Loss at iteration 2484: 0.04322577997553931\n",
      "Train Loss at iteration 2485: 0.04322555245309918\n",
      "Train Loss at iteration 2486: 0.04322532498295348\n",
      "Train Loss at iteration 2487: 0.043225097565068024\n",
      "Train Loss at iteration 2488: 0.04322487019940871\n",
      "Train Loss at iteration 2489: 0.04322464288594152\n",
      "Train Loss at iteration 2490: 0.04322441562463252\n",
      "Train Loss at iteration 2491: 0.04322418841544783\n",
      "Train Loss at iteration 2492: 0.04322396125835368\n",
      "Train Loss at iteration 2493: 0.043223734153316366\n",
      "Train Loss at iteration 2494: 0.04322350710030225\n",
      "Train Loss at iteration 2495: 0.04322328009927781\n",
      "Train Loss at iteration 2496: 0.04322305315020958\n",
      "Train Loss at iteration 2497: 0.04322282625306414\n",
      "Train Loss at iteration 2498: 0.043222599407808204\n",
      "Train Loss at iteration 2499: 0.04322237261440852\n",
      "Train Loss at iteration 2500: 0.04322214587283195\n",
      "Train Loss at iteration 2501: 0.0432219191830454\n",
      "Train Loss at iteration 2502: 0.043221692545015865\n",
      "Train Loss at iteration 2503: 0.043221465958710424\n",
      "Train Loss at iteration 2504: 0.04322123942409623\n",
      "Train Loss at iteration 2505: 0.04322101294114048\n",
      "Train Loss at iteration 2506: 0.04322078650981052\n",
      "Train Loss at iteration 2507: 0.043220560130073686\n",
      "Train Loss at iteration 2508: 0.04322033380189744\n",
      "Train Loss at iteration 2509: 0.0432201075252493\n",
      "Train Loss at iteration 2510: 0.043219881300096887\n",
      "Train Loss at iteration 2511: 0.04321965512640786\n",
      "Train Loss at iteration 2512: 0.04321942900414997\n",
      "Train Loss at iteration 2513: 0.04321920293329101\n",
      "Train Loss at iteration 2514: 0.04321897691379892\n",
      "Train Loss at iteration 2515: 0.04321875094564163\n",
      "Train Loss at iteration 2516: 0.0432185250287872\n",
      "Train Loss at iteration 2517: 0.04321829916320375\n",
      "Train Loss at iteration 2518: 0.043218073348859444\n",
      "Train Loss at iteration 2519: 0.04321784758572255\n",
      "Train Loss at iteration 2520: 0.0432176218737614\n",
      "Train Loss at iteration 2521: 0.04321739621294439\n",
      "Train Loss at iteration 2522: 0.04321717060324001\n",
      "Train Loss at iteration 2523: 0.043216945044616775\n",
      "Train Loss at iteration 2524: 0.043216719537043334\n",
      "Train Loss at iteration 2525: 0.043216494080488325\n",
      "Train Loss at iteration 2526: 0.04321626867492055\n",
      "Train Loss at iteration 2527: 0.04321604332030882\n",
      "Train Loss at iteration 2528: 0.04321581801662202\n",
      "Train Loss at iteration 2529: 0.04321559276382912\n",
      "Train Loss at iteration 2530: 0.04321536756189917\n",
      "Train Loss at iteration 2531: 0.04321514241080126\n",
      "Train Loss at iteration 2532: 0.043214917310504565\n",
      "Train Loss at iteration 2533: 0.04321469226097833\n",
      "Train Loss at iteration 2534: 0.04321446726219186\n",
      "Train Loss at iteration 2535: 0.04321424231411455\n",
      "Train Loss at iteration 2536: 0.04321401741671582\n",
      "Train Loss at iteration 2537: 0.043213792569965215\n",
      "Train Loss at iteration 2538: 0.04321356777383229\n",
      "Train Loss at iteration 2539: 0.04321334302828671\n",
      "Train Loss at iteration 2540: 0.0432131183332982\n",
      "Train Loss at iteration 2541: 0.04321289368883652\n",
      "Train Loss at iteration 2542: 0.04321266909487154\n",
      "Train Loss at iteration 2543: 0.04321244455137317\n",
      "Train Loss at iteration 2544: 0.04321222005831139\n",
      "Train Loss at iteration 2545: 0.04321199561565626\n",
      "Train Loss at iteration 2546: 0.043211771223377875\n",
      "Train Loss at iteration 2547: 0.043211546881446426\n",
      "Train Loss at iteration 2548: 0.04321132258983216\n",
      "Train Loss at iteration 2549: 0.04321109834850538\n",
      "Train Loss at iteration 2550: 0.043210874157436484\n",
      "Train Loss at iteration 2551: 0.043210650016595874\n",
      "Train Loss at iteration 2552: 0.043210425925954074\n",
      "Train Loss at iteration 2553: 0.04321020188548166\n",
      "Train Loss at iteration 2554: 0.043209977895149264\n",
      "Train Loss at iteration 2555: 0.043209753954927564\n",
      "Train Loss at iteration 2556: 0.04320953006478733\n",
      "Train Loss at iteration 2557: 0.04320930622469939\n",
      "Train Loss at iteration 2558: 0.043209082434634616\n",
      "Train Loss at iteration 2559: 0.043208858694563965\n",
      "Train Loss at iteration 2560: 0.043208635004458464\n",
      "Train Loss at iteration 2561: 0.043208411364289157\n",
      "Train Loss at iteration 2562: 0.0432081877740272\n",
      "Train Loss at iteration 2563: 0.04320796423364378\n",
      "Train Loss at iteration 2564: 0.04320774074311015\n",
      "Train Loss at iteration 2565: 0.04320751730239766\n",
      "Train Loss at iteration 2566: 0.04320729391147767\n",
      "Train Loss at iteration 2567: 0.04320707057032163\n",
      "Train Loss at iteration 2568: 0.04320684727890104\n",
      "Train Loss at iteration 2569: 0.04320662403718747\n",
      "Train Loss at iteration 2570: 0.043206400845152544\n",
      "Train Loss at iteration 2571: 0.043206177702767946\n",
      "Train Loss at iteration 2572: 0.043205954610005426\n",
      "Train Loss at iteration 2573: 0.04320573156683678\n",
      "Train Loss at iteration 2574: 0.04320550857323391\n",
      "Train Loss at iteration 2575: 0.0432052856291687\n",
      "Train Loss at iteration 2576: 0.04320506273461314\n",
      "Train Loss at iteration 2577: 0.043204839889539284\n",
      "Train Loss at iteration 2578: 0.04320461709391925\n",
      "Train Loss at iteration 2579: 0.04320439434772517\n",
      "Train Loss at iteration 2580: 0.043204171650929275\n",
      "Train Loss at iteration 2581: 0.04320394900350385\n",
      "Train Loss at iteration 2582: 0.04320372640542123\n",
      "Train Loss at iteration 2583: 0.0432035038566538\n",
      "Train Loss at iteration 2584: 0.04320328135717402\n",
      "Train Loss at iteration 2585: 0.043203058906954396\n",
      "Train Loss at iteration 2586: 0.0432028365059675\n",
      "Train Loss at iteration 2587: 0.04320261415418595\n",
      "Train Loss at iteration 2588: 0.04320239185158242\n",
      "Train Loss at iteration 2589: 0.04320216959812967\n",
      "Train Loss at iteration 2590: 0.04320194739380047\n",
      "Train Loss at iteration 2591: 0.043201725238567676\n",
      "Train Loss at iteration 2592: 0.04320150313240421\n",
      "Train Loss at iteration 2593: 0.04320128107528302\n",
      "Train Loss at iteration 2594: 0.043201059067177125\n",
      "Train Loss at iteration 2595: 0.0432008371080596\n",
      "Train Loss at iteration 2596: 0.04320061519790359\n",
      "Train Loss at iteration 2597: 0.04320039333668225\n",
      "Train Loss at iteration 2598: 0.043200171524368836\n",
      "Train Loss at iteration 2599: 0.04319994976093664\n",
      "Train Loss at iteration 2600: 0.043199728046359014\n",
      "Train Loss at iteration 2601: 0.04319950638060936\n",
      "Train Loss at iteration 2602: 0.04319928476366113\n",
      "Train Loss at iteration 2603: 0.04319906319548785\n",
      "Train Loss at iteration 2604: 0.043198841676063075\n",
      "Train Loss at iteration 2605: 0.04319862020536043\n",
      "Train Loss at iteration 2606: 0.04319839878335357\n",
      "Train Loss at iteration 2607: 0.043198177410016254\n",
      "Train Loss at iteration 2608: 0.04319795608532223\n",
      "Train Loss at iteration 2609: 0.043197734809245346\n",
      "Train Loss at iteration 2610: 0.043197513581759495\n",
      "Train Loss at iteration 2611: 0.0431972924028386\n",
      "Train Loss at iteration 2612: 0.04319707127245666\n",
      "Train Loss at iteration 2613: 0.04319685019058771\n",
      "Train Loss at iteration 2614: 0.043196629157205846\n",
      "Train Loss at iteration 2615: 0.04319640817228524\n",
      "Train Loss at iteration 2616: 0.04319618723580005\n",
      "Train Loss at iteration 2617: 0.043195966347724574\n",
      "Train Loss at iteration 2618: 0.04319574550803306\n",
      "Train Loss at iteration 2619: 0.04319552471669989\n",
      "Train Loss at iteration 2620: 0.04319530397369948\n",
      "Train Loss at iteration 2621: 0.04319508327900628\n",
      "Train Loss at iteration 2622: 0.04319486263259477\n",
      "Train Loss at iteration 2623: 0.043194642034439526\n",
      "Train Loss at iteration 2624: 0.04319442148451516\n",
      "Train Loss at iteration 2625: 0.043194200982796326\n",
      "Train Loss at iteration 2626: 0.04319398052925772\n",
      "Train Loss at iteration 2627: 0.043193760123874124\n",
      "Train Loss at iteration 2628: 0.04319353976662032\n",
      "Train Loss at iteration 2629: 0.043193319457471166\n",
      "Train Loss at iteration 2630: 0.04319309919640158\n",
      "Train Loss at iteration 2631: 0.043192878983386515\n",
      "Train Loss at iteration 2632: 0.043192658818400974\n",
      "Train Loss at iteration 2633: 0.04319243870141999\n",
      "Train Loss at iteration 2634: 0.043192218632418705\n",
      "Train Loss at iteration 2635: 0.04319199861137225\n",
      "Train Loss at iteration 2636: 0.043191778638255816\n",
      "Train Loss at iteration 2637: 0.043191558713044656\n",
      "Train Loss at iteration 2638: 0.04319133883571406\n",
      "Train Loss at iteration 2639: 0.0431911190062394\n",
      "Train Loss at iteration 2640: 0.043190899224596034\n",
      "Train Loss at iteration 2641: 0.04319067949075941\n",
      "Train Loss at iteration 2642: 0.043190459804705005\n",
      "Train Loss at iteration 2643: 0.04319024016640838\n",
      "Train Loss at iteration 2644: 0.043190020575845096\n",
      "Train Loss at iteration 2645: 0.043189801032990784\n",
      "Train Loss at iteration 2646: 0.04318958153782113\n",
      "Train Loss at iteration 2647: 0.04318936209031183\n",
      "Train Loss at iteration 2648: 0.043189142690438685\n",
      "Train Loss at iteration 2649: 0.04318892333817747\n",
      "Train Loss at iteration 2650: 0.043188704033504076\n",
      "Train Loss at iteration 2651: 0.043188484776394385\n",
      "Train Loss at iteration 2652: 0.04318826556682439\n",
      "Train Loss at iteration 2653: 0.04318804640477005\n",
      "Train Loss at iteration 2654: 0.04318782729020744\n",
      "Train Loss at iteration 2655: 0.04318760822311262\n",
      "Train Loss at iteration 2656: 0.04318738920346173\n",
      "Train Loss at iteration 2657: 0.04318717023123097\n",
      "Train Loss at iteration 2658: 0.04318695130639655\n",
      "Train Loss at iteration 2659: 0.04318673242893474\n",
      "Train Loss at iteration 2660: 0.043186513598821856\n",
      "Train Loss at iteration 2661: 0.04318629481603427\n",
      "Train Loss at iteration 2662: 0.04318607608054836\n",
      "Train Loss at iteration 2663: 0.043185857392340606\n",
      "Train Loss at iteration 2664: 0.043185638751387465\n",
      "Train Loss at iteration 2665: 0.04318542015766549\n",
      "Train Loss at iteration 2666: 0.04318520161115126\n",
      "Train Loss at iteration 2667: 0.0431849831118214\n",
      "Train Loss at iteration 2668: 0.04318476465965259\n",
      "Train Loss at iteration 2669: 0.043184546254621516\n",
      "Train Loss at iteration 2670: 0.04318432789670495\n",
      "Train Loss at iteration 2671: 0.04318410958587968\n",
      "Train Loss at iteration 2672: 0.04318389132212255\n",
      "Train Loss at iteration 2673: 0.04318367310541044\n",
      "Train Loss at iteration 2674: 0.04318345493572028\n",
      "Train Loss at iteration 2675: 0.04318323681302905\n",
      "Train Loss at iteration 2676: 0.04318301873731372\n",
      "Train Loss at iteration 2677: 0.04318280070855138\n",
      "Train Loss at iteration 2678: 0.04318258272671912\n",
      "Train Loss at iteration 2679: 0.04318236479179406\n",
      "Train Loss at iteration 2680: 0.04318214690375341\n",
      "Train Loss at iteration 2681: 0.043181929062574356\n",
      "Train Loss at iteration 2682: 0.04318171126823418\n",
      "Train Loss at iteration 2683: 0.043181493520710185\n",
      "Train Loss at iteration 2684: 0.043181275819979714\n",
      "Train Loss at iteration 2685: 0.04318105816602016\n",
      "Train Loss at iteration 2686: 0.04318084055880894\n",
      "Train Loss at iteration 2687: 0.04318062299832353\n",
      "Train Loss at iteration 2688: 0.043180405484541426\n",
      "Train Loss at iteration 2689: 0.04318018801744019\n",
      "Train Loss at iteration 2690: 0.043179970596997415\n",
      "Train Loss at iteration 2691: 0.04317975322319072\n",
      "Train Loss at iteration 2692: 0.043179535895997795\n",
      "Train Loss at iteration 2693: 0.04317931861539633\n",
      "Train Loss at iteration 2694: 0.0431791013813641\n",
      "Train Loss at iteration 2695: 0.04317888419387887\n",
      "Train Loss at iteration 2696: 0.04317866705291849\n",
      "Train Loss at iteration 2697: 0.04317844995846082\n",
      "Train Loss at iteration 2698: 0.04317823291048379\n",
      "Train Loss at iteration 2699: 0.043178015908965334\n",
      "Train Loss at iteration 2700: 0.04317779895388343\n",
      "Train Loss at iteration 2701: 0.04317758204521613\n",
      "Train Loss at iteration 2702: 0.04317736518294148\n",
      "Train Loss at iteration 2703: 0.0431771483670376\n",
      "Train Loss at iteration 2704: 0.04317693159748262\n",
      "Train Loss at iteration 2705: 0.04317671487425475\n",
      "Train Loss at iteration 2706: 0.04317649819733219\n",
      "Train Loss at iteration 2707: 0.0431762815666932\n",
      "Train Loss at iteration 2708: 0.04317606498231609\n",
      "Train Loss at iteration 2709: 0.04317584844417919\n",
      "Train Loss at iteration 2710: 0.04317563195226087\n",
      "Train Loss at iteration 2711: 0.043175415506539555\n",
      "Train Loss at iteration 2712: 0.043175199106993696\n",
      "Train Loss at iteration 2713: 0.04317498275360176\n",
      "Train Loss at iteration 2714: 0.04317476644634227\n",
      "Train Loss at iteration 2715: 0.043174550185193825\n",
      "Train Loss at iteration 2716: 0.043174333970134995\n",
      "Train Loss at iteration 2717: 0.043174117801144433\n",
      "Train Loss at iteration 2718: 0.04317390167820081\n",
      "Train Loss at iteration 2719: 0.04317368560128282\n",
      "Train Loss at iteration 2720: 0.04317346957036923\n",
      "Train Loss at iteration 2721: 0.043173253585438824\n",
      "Train Loss at iteration 2722: 0.043173037646470416\n",
      "Train Loss at iteration 2723: 0.04317282175344287\n",
      "Train Loss at iteration 2724: 0.04317260590633506\n",
      "Train Loss at iteration 2725: 0.04317239010512595\n",
      "Train Loss at iteration 2726: 0.04317217434979448\n",
      "Train Loss at iteration 2727: 0.04317195864031966\n",
      "Train Loss at iteration 2728: 0.043171742976680534\n",
      "Train Loss at iteration 2729: 0.04317152735885617\n",
      "Train Loss at iteration 2730: 0.04317131178682568\n",
      "Train Loss at iteration 2731: 0.04317109626056821\n",
      "Train Loss at iteration 2732: 0.04317088078006293\n",
      "Train Loss at iteration 2733: 0.04317066534528908\n",
      "Train Loss at iteration 2734: 0.04317044995622589\n",
      "Train Loss at iteration 2735: 0.043170234612852645\n",
      "Train Loss at iteration 2736: 0.043170019315148675\n",
      "Train Loss at iteration 2737: 0.043169804063093335\n",
      "Train Loss at iteration 2738: 0.04316958885666602\n",
      "Train Loss at iteration 2739: 0.04316937369584614\n",
      "Train Loss at iteration 2740: 0.043169158580613175\n",
      "Train Loss at iteration 2741: 0.043168943510946606\n",
      "Train Loss at iteration 2742: 0.04316872848682596\n",
      "Train Loss at iteration 2743: 0.0431685135082308\n",
      "Train Loss at iteration 2744: 0.043168298575140734\n",
      "Train Loss at iteration 2745: 0.04316808368753541\n",
      "Train Loss at iteration 2746: 0.04316786884539443\n",
      "Train Loss at iteration 2747: 0.04316765404869755\n",
      "Train Loss at iteration 2748: 0.0431674392974245\n",
      "Train Loss at iteration 2749: 0.043167224591554997\n",
      "Train Loss at iteration 2750: 0.043167009931068895\n",
      "Train Loss at iteration 2751: 0.043166795315946\n",
      "Train Loss at iteration 2752: 0.04316658074616619\n",
      "Train Loss at iteration 2753: 0.04316636622170934\n",
      "Train Loss at iteration 2754: 0.04316615174255541\n",
      "Train Loss at iteration 2755: 0.04316593730868435\n",
      "Train Loss at iteration 2756: 0.04316572292007616\n",
      "Train Loss at iteration 2757: 0.04316550857671088\n",
      "Train Loss at iteration 2758: 0.04316529427856855\n",
      "Train Loss at iteration 2759: 0.0431650800256293\n",
      "Train Loss at iteration 2760: 0.04316486581787321\n",
      "Train Loss at iteration 2761: 0.04316465165528049\n",
      "Train Loss at iteration 2762: 0.043164437537831314\n",
      "Train Loss at iteration 2763: 0.04316422346550589\n",
      "Train Loss at iteration 2764: 0.0431640094382845\n",
      "Train Loss at iteration 2765: 0.043163795456147416\n",
      "Train Loss at iteration 2766: 0.043163581519074956\n",
      "Train Loss at iteration 2767: 0.04316336762704749\n",
      "Train Loss at iteration 2768: 0.043163153780045385\n",
      "Train Loss at iteration 2769: 0.04316293997804908\n",
      "Train Loss at iteration 2770: 0.04316272622103897\n",
      "Train Loss at iteration 2771: 0.04316251250899559\n",
      "Train Loss at iteration 2772: 0.04316229884189942\n",
      "Train Loss at iteration 2773: 0.043162085219731014\n",
      "Train Loss at iteration 2774: 0.04316187164247093\n",
      "Train Loss at iteration 2775: 0.043161658110099775\n",
      "Train Loss at iteration 2776: 0.04316144462259819\n",
      "Train Loss at iteration 2777: 0.04316123117994682\n",
      "Train Loss at iteration 2778: 0.04316101778212638\n",
      "Train Loss at iteration 2779: 0.04316080442911758\n",
      "Train Loss at iteration 2780: 0.04316059112090118\n",
      "Train Loss at iteration 2781: 0.04316037785745798\n",
      "Train Loss at iteration 2782: 0.043160164638768775\n",
      "Train Loss at iteration 2783: 0.04315995146481442\n",
      "Train Loss at iteration 2784: 0.04315973833557579\n",
      "Train Loss at iteration 2785: 0.04315952525103381\n",
      "Train Loss at iteration 2786: 0.043159312211169365\n",
      "Train Loss at iteration 2787: 0.043159099215963476\n",
      "Train Loss at iteration 2788: 0.043158886265397114\n",
      "Train Loss at iteration 2789: 0.043158673359451304\n",
      "Train Loss at iteration 2790: 0.04315846049810711\n",
      "Train Loss at iteration 2791: 0.0431582476813456\n",
      "Train Loss at iteration 2792: 0.043158034909147915\n",
      "Train Loss at iteration 2793: 0.04315782218149517\n",
      "Train Loss at iteration 2794: 0.043157609498368533\n",
      "Train Loss at iteration 2795: 0.04315739685974925\n",
      "Train Loss at iteration 2796: 0.04315718426561851\n",
      "Train Loss at iteration 2797: 0.04315697171595757\n",
      "Train Loss at iteration 2798: 0.04315675921074775\n",
      "Train Loss at iteration 2799: 0.04315654674997032\n",
      "Train Loss at iteration 2800: 0.04315633433360667\n",
      "Train Loss at iteration 2801: 0.04315612196163815\n",
      "Train Loss at iteration 2802: 0.04315590963404617\n",
      "Train Loss at iteration 2803: 0.04315569735081216\n",
      "Train Loss at iteration 2804: 0.043155485111917566\n",
      "Train Loss at iteration 2805: 0.04315527291734389\n",
      "Train Loss at iteration 2806: 0.04315506076707265\n",
      "Train Loss at iteration 2807: 0.043154848661085383\n",
      "Train Loss at iteration 2808: 0.043154636599363645\n",
      "Train Loss at iteration 2809: 0.04315442458188906\n",
      "Train Loss at iteration 2810: 0.04315421260864324\n",
      "Train Loss at iteration 2811: 0.043154000679607826\n",
      "Train Loss at iteration 2812: 0.043153788794764524\n",
      "Train Loss at iteration 2813: 0.04315357695409505\n",
      "Train Loss at iteration 2814: 0.043153365157581114\n",
      "Train Loss at iteration 2815: 0.04315315340520448\n",
      "Train Loss at iteration 2816: 0.04315294169694697\n",
      "Train Loss at iteration 2817: 0.04315273003279037\n",
      "Train Loss at iteration 2818: 0.04315251841271654\n",
      "Train Loss at iteration 2819: 0.04315230683670736\n",
      "Train Loss at iteration 2820: 0.04315209530474471\n",
      "Train Loss at iteration 2821: 0.04315188381681054\n",
      "Train Loss at iteration 2822: 0.04315167237288677\n",
      "Train Loss at iteration 2823: 0.04315146097295541\n",
      "Train Loss at iteration 2824: 0.043151249616998455\n",
      "Train Loss at iteration 2825: 0.043151038304997946\n",
      "Train Loss at iteration 2826: 0.04315082703693592\n",
      "Train Loss at iteration 2827: 0.0431506158127945\n",
      "Train Loss at iteration 2828: 0.04315040463255576\n",
      "Train Loss at iteration 2829: 0.04315019349620187\n",
      "Train Loss at iteration 2830: 0.043149982403714954\n",
      "Train Loss at iteration 2831: 0.04314977135507725\n",
      "Train Loss at iteration 2832: 0.04314956035027095\n",
      "Train Loss at iteration 2833: 0.04314934938927827\n",
      "Train Loss at iteration 2834: 0.04314913847208153\n",
      "Train Loss at iteration 2835: 0.043148927598663\n",
      "Train Loss at iteration 2836: 0.04314871676900499\n",
      "Train Loss at iteration 2837: 0.04314850598308984\n",
      "Train Loss at iteration 2838: 0.043148295240899956\n",
      "Train Loss at iteration 2839: 0.04314808454241771\n",
      "Train Loss at iteration 2840: 0.04314787388762552\n",
      "Train Loss at iteration 2841: 0.04314766327650584\n",
      "Train Loss at iteration 2842: 0.04314745270904114\n",
      "Train Loss at iteration 2843: 0.04314724218521391\n",
      "Train Loss at iteration 2844: 0.04314703170500669\n",
      "Train Loss at iteration 2845: 0.04314682126840201\n",
      "Train Loss at iteration 2846: 0.04314661087538244\n",
      "Train Loss at iteration 2847: 0.043146400525930585\n",
      "Train Loss at iteration 2848: 0.04314619022002906\n",
      "Train Loss at iteration 2849: 0.04314597995766054\n",
      "Train Loss at iteration 2850: 0.04314576973880765\n",
      "Train Loss at iteration 2851: 0.043145559563453104\n",
      "Train Loss at iteration 2852: 0.04314534943157963\n",
      "Train Loss at iteration 2853: 0.04314513934316998\n",
      "Train Loss at iteration 2854: 0.04314492929820689\n",
      "Train Loss at iteration 2855: 0.043144719296673156\n",
      "Train Loss at iteration 2856: 0.04314450933855163\n",
      "Train Loss at iteration 2857: 0.043144299423825146\n",
      "Train Loss at iteration 2858: 0.043144089552476526\n",
      "Train Loss at iteration 2859: 0.043143879724488694\n",
      "Train Loss at iteration 2860: 0.043143669939844566\n",
      "Train Loss at iteration 2861: 0.043143460198527064\n",
      "Train Loss at iteration 2862: 0.04314325050051915\n",
      "Train Loss at iteration 2863: 0.043143040845803816\n",
      "Train Loss at iteration 2864: 0.04314283123436405\n",
      "Train Loss at iteration 2865: 0.04314262166618291\n",
      "Train Loss at iteration 2866: 0.04314241214124344\n",
      "Train Loss at iteration 2867: 0.043142202659528714\n",
      "Train Loss at iteration 2868: 0.04314199322102183\n",
      "Train Loss at iteration 2869: 0.043141783825705916\n",
      "Train Loss at iteration 2870: 0.04314157447356412\n",
      "Train Loss at iteration 2871: 0.0431413651645796\n",
      "Train Loss at iteration 2872: 0.04314115589873557\n",
      "Train Loss at iteration 2873: 0.04314094667601525\n",
      "Train Loss at iteration 2874: 0.04314073749640186\n",
      "Train Loss at iteration 2875: 0.043140528359878666\n",
      "Train Loss at iteration 2876: 0.04314031926642897\n",
      "Train Loss at iteration 2877: 0.04314011021603606\n",
      "Train Loss at iteration 2878: 0.043139901208683266\n",
      "Train Loss at iteration 2879: 0.043139692244353975\n",
      "Train Loss at iteration 2880: 0.04313948332303151\n",
      "Train Loss at iteration 2881: 0.04313927444469932\n",
      "Train Loss at iteration 2882: 0.043139065609340776\n",
      "Train Loss at iteration 2883: 0.043138856816939374\n",
      "Train Loss at iteration 2884: 0.04313864806747854\n",
      "Train Loss at iteration 2885: 0.04313843936094177\n",
      "Train Loss at iteration 2886: 0.04313823069731257\n",
      "Train Loss at iteration 2887: 0.04313802207657448\n",
      "Train Loss at iteration 2888: 0.043137813498711056\n",
      "Train Loss at iteration 2889: 0.043137604963705875\n",
      "Train Loss at iteration 2890: 0.04313739647154252\n",
      "Train Loss at iteration 2891: 0.04313718802220461\n",
      "Train Loss at iteration 2892: 0.04313697961567581\n",
      "Train Loss at iteration 2893: 0.04313677125193976\n",
      "Train Loss at iteration 2894: 0.043136562930980143\n",
      "Train Loss at iteration 2895: 0.04313635465278067\n",
      "Train Loss at iteration 2896: 0.04313614641732508\n",
      "Train Loss at iteration 2897: 0.04313593822459711\n",
      "Train Loss at iteration 2898: 0.04313573007458051\n",
      "Train Loss at iteration 2899: 0.04313552196725912\n",
      "Train Loss at iteration 2900: 0.04313531390261672\n",
      "Train Loss at iteration 2901: 0.04313510588063713\n",
      "Train Loss at iteration 2902: 0.04313489790130424\n",
      "Train Loss at iteration 2903: 0.043134689964601895\n",
      "Train Loss at iteration 2904: 0.043134482070514006\n",
      "Train Loss at iteration 2905: 0.043134274219024486\n",
      "Train Loss at iteration 2906: 0.04313406641011729\n",
      "Train Loss at iteration 2907: 0.04313385864377636\n",
      "Train Loss at iteration 2908: 0.043133650919985674\n",
      "Train Loss at iteration 2909: 0.04313344323872924\n",
      "Train Loss at iteration 2910: 0.04313323559999109\n",
      "Train Loss at iteration 2911: 0.04313302800375524\n",
      "Train Loss at iteration 2912: 0.04313282045000578\n",
      "Train Loss at iteration 2913: 0.04313261293872678\n",
      "Train Loss at iteration 2914: 0.04313240546990233\n",
      "Train Loss at iteration 2915: 0.043132198043516576\n",
      "Train Loss at iteration 2916: 0.04313199065955366\n",
      "Train Loss at iteration 2917: 0.04313178331799773\n",
      "Train Loss at iteration 2918: 0.04313157601883298\n",
      "Train Loss at iteration 2919: 0.04313136876204362\n",
      "Train Loss at iteration 2920: 0.043131161547613865\n",
      "Train Loss at iteration 2921: 0.04313095437552796\n",
      "Train Loss at iteration 2922: 0.04313074724577017\n",
      "Train Loss at iteration 2923: 0.0431305401583248\n",
      "Train Loss at iteration 2924: 0.04313033311317612\n",
      "Train Loss at iteration 2925: 0.04313012611030848\n",
      "Train Loss at iteration 2926: 0.043129919149706215\n",
      "Train Loss at iteration 2927: 0.043129712231353694\n",
      "Train Loss at iteration 2928: 0.04312950535523528\n",
      "Train Loss at iteration 2929: 0.043129298521335395\n",
      "Train Loss at iteration 2930: 0.043129091729638455\n",
      "Train Loss at iteration 2931: 0.04312888498012891\n",
      "Train Loss at iteration 2932: 0.0431286782727912\n",
      "Train Loss at iteration 2933: 0.043128471607609825\n",
      "Train Loss at iteration 2934: 0.04312826498456928\n",
      "Train Loss at iteration 2935: 0.04312805840365408\n",
      "Train Loss at iteration 2936: 0.04312785186484875\n",
      "Train Loss at iteration 2937: 0.04312764536813788\n",
      "Train Loss at iteration 2938: 0.04312743891350602\n",
      "Train Loss at iteration 2939: 0.04312723250093777\n",
      "Train Loss at iteration 2940: 0.043127026130417745\n",
      "Train Loss at iteration 2941: 0.043126819801930595\n",
      "Train Loss at iteration 2942: 0.04312661351546093\n",
      "Train Loss at iteration 2943: 0.04312640727099346\n",
      "Train Loss at iteration 2944: 0.04312620106851286\n",
      "Train Loss at iteration 2945: 0.04312599490800384\n",
      "Train Loss at iteration 2946: 0.043125788789451114\n",
      "Train Loss at iteration 2947: 0.04312558271283945\n",
      "Train Loss at iteration 2948: 0.043125376678153596\n",
      "Train Loss at iteration 2949: 0.04312517068537834\n",
      "Train Loss at iteration 2950: 0.04312496473449848\n",
      "Train Loss at iteration 2951: 0.04312475882549883\n",
      "Train Loss at iteration 2952: 0.04312455295836423\n",
      "Train Loss at iteration 2953: 0.043124347133079555\n",
      "Train Loss at iteration 2954: 0.043124141349629656\n",
      "Train Loss at iteration 2955: 0.043123935607999424\n",
      "Train Loss at iteration 2956: 0.04312372990817378\n",
      "Train Loss at iteration 2957: 0.04312352425013765\n",
      "Train Loss at iteration 2958: 0.04312331863387597\n",
      "Train Loss at iteration 2959: 0.04312311305937372\n",
      "Train Loss at iteration 2960: 0.04312290752661587\n",
      "Train Loss at iteration 2961: 0.043122702035587435\n",
      "Train Loss at iteration 2962: 0.043122496586273415\n",
      "Train Loss at iteration 2963: 0.04312229117865886\n",
      "Train Loss at iteration 2964: 0.043122085812728805\n",
      "Train Loss at iteration 2965: 0.04312188048846834\n",
      "Train Loss at iteration 2966: 0.04312167520586255\n",
      "Train Loss at iteration 2967: 0.04312146996489653\n",
      "Train Loss at iteration 2968: 0.0431212647655554\n",
      "Train Loss at iteration 2969: 0.04312105960782433\n",
      "Train Loss at iteration 2970: 0.04312085449168846\n",
      "Train Loss at iteration 2971: 0.04312064941713296\n",
      "Train Loss at iteration 2972: 0.04312044438414304\n",
      "Train Loss at iteration 2973: 0.04312023939270389\n",
      "Train Loss at iteration 2974: 0.04312003444280075\n",
      "Train Loss at iteration 2975: 0.043119829534418855\n",
      "Train Loss at iteration 2976: 0.0431196246675435\n",
      "Train Loss at iteration 2977: 0.043119419842159924\n",
      "Train Loss at iteration 2978: 0.04311921505825344\n",
      "Train Loss at iteration 2979: 0.043119010315809364\n",
      "Train Loss at iteration 2980: 0.04311880561481302\n",
      "Train Loss at iteration 2981: 0.04311860095524977\n",
      "Train Loss at iteration 2982: 0.043118396337104965\n",
      "Train Loss at iteration 2983: 0.04311819176036398\n",
      "Train Loss at iteration 2984: 0.04311798722501223\n",
      "Train Loss at iteration 2985: 0.043117782731035115\n",
      "Train Loss at iteration 2986: 0.04311757827841807\n",
      "Train Loss at iteration 2987: 0.04311737386714654\n",
      "Train Loss at iteration 2988: 0.04311716949720601\n",
      "Train Loss at iteration 2989: 0.04311696516858193\n",
      "Train Loss at iteration 2990: 0.04311676088125982\n",
      "Train Loss at iteration 2991: 0.04311655663522518\n",
      "Train Loss at iteration 2992: 0.04311635243046356\n",
      "Train Loss at iteration 2993: 0.04311614826696049\n",
      "Train Loss at iteration 2994: 0.04311594414470153\n",
      "Train Loss at iteration 2995: 0.043115740063672275\n",
      "Train Loss at iteration 2996: 0.04311553602385831\n",
      "Train Loss at iteration 2997: 0.043115332025245236\n",
      "Train Loss at iteration 2998: 0.04311512806781872\n",
      "Train Loss at iteration 2999: 0.04311492415156435\n",
      "Train Loss at iteration 3000: 0.04311472027646783\n",
      "Train Loss at iteration 3001: 0.043114516442514836\n",
      "Train Loss at iteration 3002: 0.04311431264969104\n",
      "Train Loss at iteration 3003: 0.04311410889798215\n",
      "Train Loss at iteration 3004: 0.0431139051873739\n",
      "Train Loss at iteration 3005: 0.04311370151785203\n",
      "Train Loss at iteration 3006: 0.04311349788940229\n",
      "Train Loss at iteration 3007: 0.04311329430201047\n",
      "Train Loss at iteration 3008: 0.04311309075566232\n",
      "Train Loss at iteration 3009: 0.04311288725034368\n",
      "Train Loss at iteration 3010: 0.043112683786040336\n",
      "Train Loss at iteration 3011: 0.04311248036273816\n",
      "Train Loss at iteration 3012: 0.04311227698042296\n",
      "Train Loss at iteration 3013: 0.04311207363908066\n",
      "Train Loss at iteration 3014: 0.043111870338697074\n",
      "Train Loss at iteration 3015: 0.04311166707925812\n",
      "Train Loss at iteration 3016: 0.04311146386074974\n",
      "Train Loss at iteration 3017: 0.04311126068315782\n",
      "Train Loss at iteration 3018: 0.04311105754646833\n",
      "Train Loss at iteration 3019: 0.04311085445066723\n",
      "Train Loss at iteration 3020: 0.04311065139574047\n",
      "Train Loss at iteration 3021: 0.04311044838167404\n",
      "Train Loss at iteration 3022: 0.043110245408453964\n",
      "Train Loss at iteration 3023: 0.04311004247606626\n",
      "Train Loss at iteration 3024: 0.04310983958449693\n",
      "Train Loss at iteration 3025: 0.04310963673373206\n",
      "Train Loss at iteration 3026: 0.04310943392375768\n",
      "Train Loss at iteration 3027: 0.0431092311545599\n",
      "Train Loss at iteration 3028: 0.04310902842612478\n",
      "Train Loss at iteration 3029: 0.043108825738438473\n",
      "Train Loss at iteration 3030: 0.04310862309148705\n",
      "Train Loss at iteration 3031: 0.04310842048525669\n",
      "Train Loss at iteration 3032: 0.043108217919733516\n",
      "Train Loss at iteration 3033: 0.04310801539490373\n",
      "Train Loss at iteration 3034: 0.043107812910753475\n",
      "Train Loss at iteration 3035: 0.04310761046726897\n",
      "Train Loss at iteration 3036: 0.04310740806443642\n",
      "Train Loss at iteration 3037: 0.04310720570224207\n",
      "Train Loss at iteration 3038: 0.04310700338067213\n",
      "Train Loss at iteration 3039: 0.04310680109971287\n",
      "Train Loss at iteration 3040: 0.04310659885935056\n",
      "Train Loss at iteration 3041: 0.04310639665957148\n",
      "Train Loss at iteration 3042: 0.043106194500361926\n",
      "Train Loss at iteration 3043: 0.04310599238170821\n",
      "Train Loss at iteration 3044: 0.04310579030359668\n",
      "Train Loss at iteration 3045: 0.043105588266013656\n",
      "Train Loss at iteration 3046: 0.043105386268945486\n",
      "Train Loss at iteration 3047: 0.043105184312378565\n",
      "Train Loss at iteration 3048: 0.043104982396299264\n",
      "Train Loss at iteration 3049: 0.04310478052069397\n",
      "Train Loss at iteration 3050: 0.043104578685549115\n",
      "Train Loss at iteration 3051: 0.043104376890851115\n",
      "Train Loss at iteration 3052: 0.04310417513658642\n",
      "Train Loss at iteration 3053: 0.04310397342274146\n",
      "Train Loss at iteration 3054: 0.043103771749302716\n",
      "Train Loss at iteration 3055: 0.04310357011625667\n",
      "Train Loss at iteration 3056: 0.04310336852358983\n",
      "Train Loss at iteration 3057: 0.043103166971288694\n",
      "Train Loss at iteration 3058: 0.04310296545933978\n",
      "Train Loss at iteration 3059: 0.043102763987729636\n",
      "Train Loss at iteration 3060: 0.04310256255644481\n",
      "Train Loss at iteration 3061: 0.043102361165471864\n",
      "Train Loss at iteration 3062: 0.04310215981479737\n",
      "Train Loss at iteration 3063: 0.04310195850440794\n",
      "Train Loss at iteration 3064: 0.04310175723429017\n",
      "Train Loss at iteration 3065: 0.043101556004430656\n",
      "Train Loss at iteration 3066: 0.04310135481481607\n",
      "Train Loss at iteration 3067: 0.04310115366543303\n",
      "Train Loss at iteration 3068: 0.04310095255626822\n",
      "Train Loss at iteration 3069: 0.04310075148730828\n",
      "Train Loss at iteration 3070: 0.04310055045853991\n",
      "Train Loss at iteration 3071: 0.04310034946994983\n",
      "Train Loss at iteration 3072: 0.04310014852152474\n",
      "Train Loss at iteration 3073: 0.043099947613251345\n",
      "Train Loss at iteration 3074: 0.04309974674511642\n",
      "Train Loss at iteration 3075: 0.04309954591710669\n",
      "Train Loss at iteration 3076: 0.04309934512920894\n",
      "Train Loss at iteration 3077: 0.04309914438140994\n",
      "Train Loss at iteration 3078: 0.043098943673696474\n",
      "Train Loss at iteration 3079: 0.04309874300605537\n",
      "Train Loss at iteration 3080: 0.04309854237847343\n",
      "Train Loss at iteration 3081: 0.043098341790937494\n",
      "Train Loss at iteration 3082: 0.0430981412434344\n",
      "Train Loss at iteration 3083: 0.04309794073595101\n",
      "Train Loss at iteration 3084: 0.04309774026847419\n",
      "Train Loss at iteration 3085: 0.04309753984099082\n",
      "Train Loss at iteration 3086: 0.04309733945348781\n",
      "Train Loss at iteration 3087: 0.04309713910595205\n",
      "Train Loss at iteration 3088: 0.04309693879837049\n",
      "Train Loss at iteration 3089: 0.04309673853073003\n",
      "Train Loss at iteration 3090: 0.04309653830301765\n",
      "Train Loss at iteration 3091: 0.04309633811522029\n",
      "Train Loss at iteration 3092: 0.04309613796732492\n",
      "Train Loss at iteration 3093: 0.04309593785931854\n",
      "Train Loss at iteration 3094: 0.04309573779118814\n",
      "Train Loss at iteration 3095: 0.04309553776292073\n",
      "Train Loss at iteration 3096: 0.04309533777450334\n",
      "Train Loss at iteration 3097: 0.04309513782592299\n",
      "Train Loss at iteration 3098: 0.043094937917166745\n",
      "Train Loss at iteration 3099: 0.04309473804822167\n",
      "Train Loss at iteration 3100: 0.04309453821907481\n",
      "Train Loss at iteration 3101: 0.04309433842971327\n",
      "Train Loss at iteration 3102: 0.04309413868012415\n",
      "Train Loss at iteration 3103: 0.043093938970294565\n",
      "Train Loss at iteration 3104: 0.04309373930021162\n",
      "Train Loss at iteration 3105: 0.04309353966986245\n",
      "Train Loss at iteration 3106: 0.043093340079234224\n",
      "Train Loss at iteration 3107: 0.043093140528314075\n",
      "Train Loss at iteration 3108: 0.04309294101708919\n",
      "Train Loss at iteration 3109: 0.04309274154554676\n",
      "Train Loss at iteration 3110: 0.043092542113673964\n",
      "Train Loss at iteration 3111: 0.043092342721458\n",
      "Train Loss at iteration 3112: 0.04309214336888612\n",
      "Train Loss at iteration 3113: 0.04309194405594554\n",
      "Train Loss at iteration 3114: 0.0430917447826235\n",
      "Train Loss at iteration 3115: 0.043091545548907255\n",
      "Train Loss at iteration 3116: 0.04309134635478408\n",
      "Train Loss at iteration 3117: 0.04309114720024125\n",
      "Train Loss at iteration 3118: 0.04309094808526607\n",
      "Train Loss at iteration 3119: 0.04309074900984581\n",
      "Train Loss at iteration 3120: 0.04309054997396783\n",
      "Train Loss at iteration 3121: 0.04309035097761942\n",
      "Train Loss at iteration 3122: 0.04309015202078794\n",
      "Train Loss at iteration 3123: 0.043089953103460726\n",
      "Train Loss at iteration 3124: 0.043089754225625165\n",
      "Train Loss at iteration 3125: 0.043089555387268594\n",
      "Train Loss at iteration 3126: 0.04308935658837844\n",
      "Train Loss at iteration 3127: 0.04308915782894207\n",
      "Train Loss at iteration 3128: 0.043088959108946896\n",
      "Train Loss at iteration 3129: 0.043088760428380356\n",
      "Train Loss at iteration 3130: 0.04308856178722987\n",
      "Train Loss at iteration 3131: 0.043088363185482884\n",
      "Train Loss at iteration 3132: 0.04308816462312685\n",
      "Train Loss at iteration 3133: 0.043087966100149235\n",
      "Train Loss at iteration 3134: 0.04308776761653752\n",
      "Train Loss at iteration 3135: 0.04308756917227919\n",
      "Train Loss at iteration 3136: 0.04308737076736177\n",
      "Train Loss at iteration 3137: 0.04308717240177273\n",
      "Train Loss at iteration 3138: 0.043086974075499634\n",
      "Train Loss at iteration 3139: 0.043086775788529985\n",
      "Train Loss at iteration 3140: 0.04308657754085136\n",
      "Train Loss at iteration 3141: 0.04308637933245129\n",
      "Train Loss at iteration 3142: 0.04308618116331735\n",
      "Train Loss at iteration 3143: 0.04308598303343712\n",
      "Train Loss at iteration 3144: 0.04308578494279821\n",
      "Train Loss at iteration 3145: 0.043085586891388195\n",
      "Train Loss at iteration 3146: 0.043085388879194705\n",
      "Train Loss at iteration 3147: 0.04308519090620537\n",
      "Train Loss at iteration 3148: 0.043084992972407796\n",
      "Train Loss at iteration 3149: 0.04308479507778965\n",
      "Train Loss at iteration 3150: 0.0430845972223386\n",
      "Train Loss at iteration 3151: 0.043084399406042306\n",
      "Train Loss at iteration 3152: 0.04308420162888844\n",
      "Train Loss at iteration 3153: 0.043084003890864704\n",
      "Train Loss at iteration 3154: 0.043083806191958784\n",
      "Train Loss at iteration 3155: 0.04308360853215841\n",
      "Train Loss at iteration 3156: 0.043083410911451285\n",
      "Train Loss at iteration 3157: 0.04308321332982517\n",
      "Train Loss at iteration 3158: 0.04308301578726778\n",
      "Train Loss at iteration 3159: 0.0430828182837669\n",
      "Train Loss at iteration 3160: 0.04308262081931029\n",
      "Train Loss at iteration 3161: 0.04308242339388569\n",
      "Train Loss at iteration 3162: 0.043082226007480934\n",
      "Train Loss at iteration 3163: 0.04308202866008381\n",
      "Train Loss at iteration 3164: 0.04308183135168211\n",
      "Train Loss at iteration 3165: 0.043081634082263676\n",
      "Train Loss at iteration 3166: 0.04308143685181633\n",
      "Train Loss at iteration 3167: 0.0430812396603279\n",
      "Train Loss at iteration 3168: 0.04308104250778625\n",
      "Train Loss at iteration 3169: 0.04308084539417925\n",
      "Train Loss at iteration 3170: 0.04308064831949476\n",
      "Train Loss at iteration 3171: 0.04308045128372067\n",
      "Train Loss at iteration 3172: 0.04308025428684488\n",
      "Train Loss at iteration 3173: 0.04308005732885527\n",
      "Train Loss at iteration 3174: 0.04307986040973978\n",
      "Train Loss at iteration 3175: 0.04307966352948633\n",
      "Train Loss at iteration 3176: 0.04307946668808283\n",
      "Train Loss at iteration 3177: 0.043079269885517274\n",
      "Train Loss at iteration 3178: 0.04307907312177758\n",
      "Train Loss at iteration 3179: 0.04307887639685172\n",
      "Train Loss at iteration 3180: 0.04307867971072768\n",
      "Train Loss at iteration 3181: 0.043078483063393444\n",
      "Train Loss at iteration 3182: 0.043078286454837\n",
      "Train Loss at iteration 3183: 0.04307808988504638\n",
      "Train Loss at iteration 3184: 0.043077893354009585\n",
      "Train Loss at iteration 3185: 0.04307769686171462\n",
      "Train Loss at iteration 3186: 0.04307750040814958\n",
      "Train Loss at iteration 3187: 0.04307730399330247\n",
      "Train Loss at iteration 3188: 0.04307710761716135\n",
      "Train Loss at iteration 3189: 0.0430769112797143\n",
      "Train Loss at iteration 3190: 0.043076714980949414\n",
      "Train Loss at iteration 3191: 0.043076518720854724\n",
      "Train Loss at iteration 3192: 0.04307632249941839\n",
      "Train Loss at iteration 3193: 0.043076126316628494\n",
      "Train Loss at iteration 3194: 0.04307593017247316\n",
      "Train Loss at iteration 3195: 0.043075734066940506\n",
      "Train Loss at iteration 3196: 0.043075538000018676\n",
      "Train Loss at iteration 3197: 0.04307534197169583\n",
      "Train Loss at iteration 3198: 0.0430751459819601\n",
      "Train Loss at iteration 3199: 0.04307495003079968\n",
      "Train Loss at iteration 3200: 0.04307475411820273\n",
      "Train Loss at iteration 3201: 0.04307455824415746\n",
      "Train Loss at iteration 3202: 0.04307436240865204\n",
      "Train Loss at iteration 3203: 0.043074166611674694\n",
      "Train Loss at iteration 3204: 0.04307397085321363\n",
      "Train Loss at iteration 3205: 0.043073775133257074\n",
      "Train Loss at iteration 3206: 0.04307357945179328\n",
      "Train Loss at iteration 3207: 0.04307338380881046\n",
      "Train Loss at iteration 3208: 0.043073188204296904\n",
      "Train Loss at iteration 3209: 0.04307299263824087\n",
      "Train Loss at iteration 3210: 0.04307279711063062\n",
      "Train Loss at iteration 3211: 0.043072601621454445\n",
      "Train Loss at iteration 3212: 0.04307240617070064\n",
      "Train Loss at iteration 3213: 0.04307221075835751\n",
      "Train Loss at iteration 3214: 0.043072015384413354\n",
      "Train Loss at iteration 3215: 0.043071820048856506\n",
      "Train Loss at iteration 3216: 0.043071624751675315\n",
      "Train Loss at iteration 3217: 0.043071429492858096\n",
      "Train Loss at iteration 3218: 0.04307123427239321\n",
      "Train Loss at iteration 3219: 0.043071039090269034\n",
      "Train Loss at iteration 3220: 0.043070843946473904\n",
      "Train Loss at iteration 3221: 0.04307064884099622\n",
      "Train Loss at iteration 3222: 0.04307045377382438\n",
      "Train Loss at iteration 3223: 0.04307025874494675\n",
      "Train Loss at iteration 3224: 0.04307006375435177\n",
      "Train Loss at iteration 3225: 0.043069868802027844\n",
      "Train Loss at iteration 3226: 0.04306967388796339\n",
      "Train Loss at iteration 3227: 0.04306947901214686\n",
      "Train Loss at iteration 3228: 0.04306928417456669\n",
      "Train Loss at iteration 3229: 0.043069089375211325\n",
      "Train Loss at iteration 3230: 0.04306889461406924\n",
      "Train Loss at iteration 3231: 0.04306869989112891\n",
      "Train Loss at iteration 3232: 0.0430685052063788\n",
      "Train Loss at iteration 3233: 0.04306831055980743\n",
      "Train Loss at iteration 3234: 0.04306811595140325\n",
      "Train Loss at iteration 3235: 0.04306792138115481\n",
      "Train Loss at iteration 3236: 0.04306772684905063\n",
      "Train Loss at iteration 3237: 0.0430675323550792\n",
      "Train Loss at iteration 3238: 0.0430673378992291\n",
      "Train Loss at iteration 3239: 0.04306714348148883\n",
      "Train Loss at iteration 3240: 0.043066949101847\n",
      "Train Loss at iteration 3241: 0.04306675476029212\n",
      "Train Loss at iteration 3242: 0.04306656045681278\n",
      "Train Loss at iteration 3243: 0.04306636619139758\n",
      "Train Loss at iteration 3244: 0.04306617196403509\n",
      "Train Loss at iteration 3245: 0.043065977774713896\n",
      "Train Loss at iteration 3246: 0.043065783623422636\n",
      "Train Loss at iteration 3247: 0.04306558951014992\n",
      "Train Loss at iteration 3248: 0.04306539543488436\n",
      "Train Loss at iteration 3249: 0.043065201397614584\n",
      "Train Loss at iteration 3250: 0.043065007398329254\n",
      "Train Loss at iteration 3251: 0.04306481343701702\n",
      "Train Loss at iteration 3252: 0.04306461951366653\n",
      "Train Loss at iteration 3253: 0.04306442562826648\n",
      "Train Loss at iteration 3254: 0.043064231780805505\n",
      "Train Loss at iteration 3255: 0.04306403797127232\n",
      "Train Loss at iteration 3256: 0.043063844199655636\n",
      "Train Loss at iteration 3257: 0.043063650465944125\n",
      "Train Loss at iteration 3258: 0.04306345677012651\n",
      "Train Loss at iteration 3259: 0.043063263112191524\n",
      "Train Loss at iteration 3260: 0.04306306949212788\n",
      "Train Loss at iteration 3261: 0.043062875909924345\n",
      "Train Loss at iteration 3262: 0.043062682365569635\n",
      "Train Loss at iteration 3263: 0.04306248885905251\n",
      "Train Loss at iteration 3264: 0.043062295390361764\n",
      "Train Loss at iteration 3265: 0.04306210195948614\n",
      "Train Loss at iteration 3266: 0.043061908566414425\n",
      "Train Loss at iteration 3267: 0.043061715211135415\n",
      "Train Loss at iteration 3268: 0.04306152189363792\n",
      "Train Loss at iteration 3269: 0.04306132861391073\n",
      "Train Loss at iteration 3270: 0.043061135371942665\n",
      "Train Loss at iteration 3271: 0.04306094216772254\n",
      "Train Loss at iteration 3272: 0.04306074900123922\n",
      "Train Loss at iteration 3273: 0.04306055587248151\n",
      "Train Loss at iteration 3274: 0.04306036278143827\n",
      "Train Loss at iteration 3275: 0.043060169728098376\n",
      "Train Loss at iteration 3276: 0.04305997671245067\n",
      "Train Loss at iteration 3277: 0.043059783734484026\n",
      "Train Loss at iteration 3278: 0.043059590794187357\n",
      "Train Loss at iteration 3279: 0.04305939789154953\n",
      "Train Loss at iteration 3280: 0.04305920502655944\n",
      "Train Loss at iteration 3281: 0.043059012199206013\n",
      "Train Loss at iteration 3282: 0.043058819409478144\n",
      "Train Loss at iteration 3283: 0.04305862665736478\n",
      "Train Loss at iteration 3284: 0.04305843394285485\n",
      "Train Loss at iteration 3285: 0.04305824126593727\n",
      "Train Loss at iteration 3286: 0.04305804862660101\n",
      "Train Loss at iteration 3287: 0.04305785602483503\n",
      "Train Loss at iteration 3288: 0.04305766346062828\n",
      "Train Loss at iteration 3289: 0.043057470933969756\n",
      "Train Loss at iteration 3290: 0.04305727844484842\n",
      "Train Loss at iteration 3291: 0.043057085993253266\n",
      "Train Loss at iteration 3292: 0.04305689357917328\n",
      "Train Loss at iteration 3293: 0.04305670120259749\n",
      "Train Loss at iteration 3294: 0.0430565088635149\n",
      "Train Loss at iteration 3295: 0.04305631656191454\n",
      "Train Loss at iteration 3296: 0.04305612429778543\n",
      "Train Loss at iteration 3297: 0.0430559320711166\n",
      "Train Loss at iteration 3298: 0.04305573988189713\n",
      "Train Loss at iteration 3299: 0.04305554773011604\n",
      "Train Loss at iteration 3300: 0.043055355615762396\n",
      "Train Loss at iteration 3301: 0.043055163538825277\n",
      "Train Loss at iteration 3302: 0.04305497149929376\n",
      "Train Loss at iteration 3303: 0.04305477949715692\n",
      "Train Loss at iteration 3304: 0.043054587532403876\n",
      "Train Loss at iteration 3305: 0.0430543956050237\n",
      "Train Loss at iteration 3306: 0.043054203715005526\n",
      "Train Loss at iteration 3307: 0.043054011862338454\n",
      "Train Loss at iteration 3308: 0.043053820047011616\n",
      "Train Loss at iteration 3309: 0.043053628269014134\n",
      "Train Loss at iteration 3310: 0.043053436528335175\n",
      "Train Loss at iteration 3311: 0.04305324482496387\n",
      "Train Loss at iteration 3312: 0.04305305315888936\n",
      "Train Loss at iteration 3313: 0.04305286153010085\n",
      "Train Loss at iteration 3314: 0.043052669938587466\n",
      "Train Loss at iteration 3315: 0.04305247838433843\n",
      "Train Loss at iteration 3316: 0.0430522868673429\n",
      "Train Loss at iteration 3317: 0.04305209538759008\n",
      "Train Loss at iteration 3318: 0.04305190394506916\n",
      "Train Loss at iteration 3319: 0.04305171253976938\n",
      "Train Loss at iteration 3320: 0.04305152117167994\n",
      "Train Loss at iteration 3321: 0.04305132984079006\n",
      "Train Loss at iteration 3322: 0.043051138547088974\n",
      "Train Loss at iteration 3323: 0.04305094729056593\n",
      "Train Loss at iteration 3324: 0.04305075607121019\n",
      "Train Loss at iteration 3325: 0.04305056488901099\n",
      "Train Loss at iteration 3326: 0.04305037374395761\n",
      "Train Loss at iteration 3327: 0.04305018263603929\n",
      "Train Loss at iteration 3328: 0.04304999156524533\n",
      "Train Loss at iteration 3329: 0.04304980053156503\n",
      "Train Loss at iteration 3330: 0.04304960953498766\n",
      "Train Loss at iteration 3331: 0.04304941857550254\n",
      "Train Loss at iteration 3332: 0.043049227653098966\n",
      "Train Loss at iteration 3333: 0.043049036767766266\n",
      "Train Loss at iteration 3334: 0.04304884591949374\n",
      "Train Loss at iteration 3335: 0.043048655108270745\n",
      "Train Loss at iteration 3336: 0.043048464334086614\n",
      "Train Loss at iteration 3337: 0.043048273596930677\n",
      "Train Loss at iteration 3338: 0.04304808289679231\n",
      "Train Loss at iteration 3339: 0.04304789223366085\n",
      "Train Loss at iteration 3340: 0.0430477016075257\n",
      "Train Loss at iteration 3341: 0.04304751101837621\n",
      "Train Loss at iteration 3342: 0.04304732046620175\n",
      "Train Loss at iteration 3343: 0.04304712995099174\n",
      "Train Loss at iteration 3344: 0.043046939472735565\n",
      "Train Loss at iteration 3345: 0.04304674903142263\n",
      "Train Loss at iteration 3346: 0.04304655862704234\n",
      "Train Loss at iteration 3347: 0.04304636825958414\n",
      "Train Loss at iteration 3348: 0.04304617792903742\n",
      "Train Loss at iteration 3349: 0.04304598763539164\n",
      "Train Loss at iteration 3350: 0.04304579737863623\n",
      "Train Loss at iteration 3351: 0.04304560715876065\n",
      "Train Loss at iteration 3352: 0.04304541697575434\n",
      "Train Loss at iteration 3353: 0.04304522682960678\n",
      "Train Loss at iteration 3354: 0.04304503672030743\n",
      "Train Loss at iteration 3355: 0.04304484664784576\n",
      "Train Loss at iteration 3356: 0.04304465661221126\n",
      "Train Loss at iteration 3357: 0.043044466613393444\n",
      "Train Loss at iteration 3358: 0.04304427665138177\n",
      "Train Loss at iteration 3359: 0.043044086726165776\n",
      "Train Loss at iteration 3360: 0.043043896837734956\n",
      "Train Loss at iteration 3361: 0.04304370698607884\n",
      "Train Loss at iteration 3362: 0.04304351717118694\n",
      "Train Loss at iteration 3363: 0.04304332739304882\n",
      "Train Loss at iteration 3364: 0.043043137651654\n",
      "Train Loss at iteration 3365: 0.04304294794699201\n",
      "Train Loss at iteration 3366: 0.04304275827905246\n",
      "Train Loss at iteration 3367: 0.04304256864782486\n",
      "Train Loss at iteration 3368: 0.0430423790532988\n",
      "Train Loss at iteration 3369: 0.043042189495463866\n",
      "Train Loss at iteration 3370: 0.0430419999743096\n",
      "Train Loss at iteration 3371: 0.04304181048982565\n",
      "Train Loss at iteration 3372: 0.04304162104200158\n",
      "Train Loss at iteration 3373: 0.04304143163082699\n",
      "Train Loss at iteration 3374: 0.043041242256291495\n",
      "Train Loss at iteration 3375: 0.04304105291838474\n",
      "Train Loss at iteration 3376: 0.0430408636170963\n",
      "Train Loss at iteration 3377: 0.043040674352415854\n",
      "Train Loss at iteration 3378: 0.043040485124333006\n",
      "Train Loss at iteration 3379: 0.043040295932837425\n",
      "Train Loss at iteration 3380: 0.043040106777918745\n",
      "Train Loss at iteration 3381: 0.04303991765956664\n",
      "Train Loss at iteration 3382: 0.04303972857777077\n",
      "Train Loss at iteration 3383: 0.043039539532520814\n",
      "Train Loss at iteration 3384: 0.04303935052380644\n",
      "Train Loss at iteration 3385: 0.04303916155161733\n",
      "Train Loss at iteration 3386: 0.0430389726159432\n",
      "Train Loss at iteration 3387: 0.04303878371677372\n",
      "Train Loss at iteration 3388: 0.04303859485409862\n",
      "Train Loss at iteration 3389: 0.0430384060279076\n",
      "Train Loss at iteration 3390: 0.043038217238190395\n",
      "Train Loss at iteration 3391: 0.04303802848493672\n",
      "Train Loss at iteration 3392: 0.04303783976813632\n",
      "Train Loss at iteration 3393: 0.0430376510877789\n",
      "Train Loss at iteration 3394: 0.04303746244385426\n",
      "Train Loss at iteration 3395: 0.04303727383635211\n",
      "Train Loss at iteration 3396: 0.04303708526526223\n",
      "Train Loss at iteration 3397: 0.04303689673057439\n",
      "Train Loss at iteration 3398: 0.04303670823227835\n",
      "Train Loss at iteration 3399: 0.0430365197703639\n",
      "Train Loss at iteration 3400: 0.04303633134482082\n",
      "Train Loss at iteration 3401: 0.04303614295563891\n",
      "Train Loss at iteration 3402: 0.043035954602807965\n",
      "Train Loss at iteration 3403: 0.0430357662863178\n",
      "Train Loss at iteration 3404: 0.04303557800615821\n",
      "Train Loss at iteration 3405: 0.043035389762319014\n",
      "Train Loss at iteration 3406: 0.04303520155479007\n",
      "Train Loss at iteration 3407: 0.04303501338356118\n",
      "Train Loss at iteration 3408: 0.0430348252486222\n",
      "Train Loss at iteration 3409: 0.043034637149962966\n",
      "Train Loss at iteration 3410: 0.043034449087573326\n",
      "Train Loss at iteration 3411: 0.04303426106144315\n",
      "Train Loss at iteration 3412: 0.043034073071562304\n",
      "Train Loss at iteration 3413: 0.04303388511792064\n",
      "Train Loss at iteration 3414: 0.04303369720050806\n",
      "Train Loss at iteration 3415: 0.04303350931931444\n",
      "Train Loss at iteration 3416: 0.043033321474329665\n",
      "Train Loss at iteration 3417: 0.043033133665543635\n",
      "Train Loss at iteration 3418: 0.04303294589294625\n",
      "Train Loss at iteration 3419: 0.04303275815652743\n",
      "Train Loss at iteration 3420: 0.04303257045627708\n",
      "Train Loss at iteration 3421: 0.04303238279218514\n",
      "Train Loss at iteration 3422: 0.04303219516424152\n",
      "Train Loss at iteration 3423: 0.04303200757243616\n",
      "Train Loss at iteration 3424: 0.04303182001675901\n",
      "Train Loss at iteration 3425: 0.04303163249720002\n",
      "Train Loss at iteration 3426: 0.04303144501374913\n",
      "Train Loss at iteration 3427: 0.04303125756639631\n",
      "Train Loss at iteration 3428: 0.04303107015513152\n",
      "Train Loss at iteration 3429: 0.043030882779944754\n",
      "Train Loss at iteration 3430: 0.04303069544082597\n",
      "Train Loss at iteration 3431: 0.043030508137765155\n",
      "Train Loss at iteration 3432: 0.04303032087075232\n",
      "Train Loss at iteration 3433: 0.043030133639777444\n",
      "Train Loss at iteration 3434: 0.04302994644483053\n",
      "Train Loss at iteration 3435: 0.043029759285901595\n",
      "Train Loss at iteration 3436: 0.04302957216298066\n",
      "Train Loss at iteration 3437: 0.043029385076057754\n",
      "Train Loss at iteration 3438: 0.0430291980251229\n",
      "Train Loss at iteration 3439: 0.04302901101016613\n",
      "Train Loss at iteration 3440: 0.043028824031177484\n",
      "Train Loss at iteration 3441: 0.04302863708814702\n",
      "Train Loss at iteration 3442: 0.043028450181064784\n",
      "Train Loss at iteration 3443: 0.04302826330992083\n",
      "Train Loss at iteration 3444: 0.043028076474705246\n",
      "Train Loss at iteration 3445: 0.04302788967540808\n",
      "Train Loss at iteration 3446: 0.04302770291201943\n",
      "Train Loss at iteration 3447: 0.04302751618452937\n",
      "Train Loss at iteration 3448: 0.043027329492928\n",
      "Train Loss at iteration 3449: 0.043027142837205413\n",
      "Train Loss at iteration 3450: 0.043026956217351704\n",
      "Train Loss at iteration 3451: 0.04302676963335697\n",
      "Train Loss at iteration 3452: 0.04302658308521136\n",
      "Train Loss at iteration 3453: 0.04302639657290498\n",
      "Train Loss at iteration 3454: 0.04302621009642796\n",
      "Train Loss at iteration 3455: 0.04302602365577042\n",
      "Train Loss at iteration 3456: 0.04302583725092251\n",
      "Train Loss at iteration 3457: 0.04302565088187438\n",
      "Train Loss at iteration 3458: 0.04302546454861616\n",
      "Train Loss at iteration 3459: 0.043025278251138034\n",
      "Train Loss at iteration 3460: 0.04302509198943014\n",
      "Train Loss at iteration 3461: 0.043024905763482675\n",
      "Train Loss at iteration 3462: 0.043024719573285804\n",
      "Train Loss at iteration 3463: 0.04302453341882969\n",
      "Train Loss at iteration 3464: 0.043024347300104536\n",
      "Train Loss at iteration 3465: 0.043024161217100516\n",
      "Train Loss at iteration 3466: 0.04302397516980786\n",
      "Train Loss at iteration 3467: 0.04302378915821676\n",
      "Train Loss at iteration 3468: 0.043023603182317426\n",
      "Train Loss at iteration 3469: 0.043023417242100064\n",
      "Train Loss at iteration 3470: 0.0430232313375549\n",
      "Train Loss at iteration 3471: 0.04302304546867217\n",
      "Train Loss at iteration 3472: 0.04302285963544212\n",
      "Train Loss at iteration 3473: 0.04302267383785494\n",
      "Train Loss at iteration 3474: 0.04302248807590094\n",
      "Train Loss at iteration 3475: 0.043022302349570336\n",
      "Train Loss at iteration 3476: 0.04302211665885339\n",
      "Train Loss at iteration 3477: 0.043021931003740355\n",
      "Train Loss at iteration 3478: 0.043021745384221516\n",
      "Train Loss at iteration 3479: 0.04302155980028716\n",
      "Train Loss at iteration 3480: 0.04302137425192752\n",
      "Train Loss at iteration 3481: 0.04302118873913294\n",
      "Train Loss at iteration 3482: 0.04302100326189367\n",
      "Train Loss at iteration 3483: 0.043020817820200015\n",
      "Train Loss at iteration 3484: 0.043020632414042304\n",
      "Train Loss at iteration 3485: 0.04302044704341081\n",
      "Train Loss at iteration 3486: 0.04302026170829588\n",
      "Train Loss at iteration 3487: 0.04302007640868782\n",
      "Train Loss at iteration 3488: 0.043019891144576945\n",
      "Train Loss at iteration 3489: 0.04301970591595362\n",
      "Train Loss at iteration 3490: 0.04301952072280814\n",
      "Train Loss at iteration 3491: 0.043019335565130885\n",
      "Train Loss at iteration 3492: 0.04301915044291218\n",
      "Train Loss at iteration 3493: 0.04301896535614239\n",
      "Train Loss at iteration 3494: 0.0430187803048119\n",
      "Train Loss at iteration 3495: 0.04301859528891102\n",
      "Train Loss at iteration 3496: 0.04301841030843018\n",
      "Train Loss at iteration 3497: 0.04301822536335971\n",
      "Train Loss at iteration 3498: 0.043018040453690023\n",
      "Train Loss at iteration 3499: 0.0430178555794115\n",
      "Train Loss at iteration 3500: 0.04301767074051452\n",
      "Train Loss at iteration 3501: 0.04301748593698951\n",
      "Train Loss at iteration 3502: 0.04301730116882686\n",
      "Train Loss at iteration 3503: 0.04301711643601699\n",
      "Train Loss at iteration 3504: 0.0430169317385503\n",
      "Train Loss at iteration 3505: 0.043016747076417214\n",
      "Train Loss at iteration 3506: 0.04301656244960817\n",
      "Train Loss at iteration 3507: 0.043016377858113616\n",
      "Train Loss at iteration 3508: 0.043016193301923966\n",
      "Train Loss at iteration 3509: 0.043016008781029666\n",
      "Train Loss at iteration 3510: 0.043015824295421175\n",
      "Train Loss at iteration 3511: 0.04301563984508894\n",
      "Train Loss at iteration 3512: 0.04301545543002344\n",
      "Train Loss at iteration 3513: 0.04301527105021512\n",
      "Train Loss at iteration 3514: 0.043015086705654446\n",
      "Train Loss at iteration 3515: 0.043014902396331926\n",
      "Train Loss at iteration 3516: 0.043014718122238026\n",
      "Train Loss at iteration 3517: 0.04301453388336324\n",
      "Train Loss at iteration 3518: 0.04301434967969803\n",
      "Train Loss at iteration 3519: 0.04301416551123294\n",
      "Train Loss at iteration 3520: 0.04301398137795845\n",
      "Train Loss at iteration 3521: 0.04301379727986508\n",
      "Train Loss at iteration 3522: 0.043013613216943344\n",
      "Train Loss at iteration 3523: 0.04301342918918375\n",
      "Train Loss at iteration 3524: 0.043013245196576844\n",
      "Train Loss at iteration 3525: 0.043013061239113144\n",
      "Train Loss at iteration 3526: 0.04301287731678318\n",
      "Train Loss at iteration 3527: 0.04301269342957752\n",
      "Train Loss at iteration 3528: 0.04301250957748669\n",
      "Train Loss at iteration 3529: 0.04301232576050125\n",
      "Train Loss at iteration 3530: 0.04301214197861175\n",
      "Train Loss at iteration 3531: 0.04301195823180877\n",
      "Train Loss at iteration 3532: 0.043011774520082875\n",
      "Train Loss at iteration 3533: 0.04301159084342463\n",
      "Train Loss at iteration 3534: 0.04301140720182461\n",
      "Train Loss at iteration 3535: 0.04301122359527343\n",
      "Train Loss at iteration 3536: 0.04301104002376164\n",
      "Train Loss at iteration 3537: 0.04301085648727987\n",
      "Train Loss at iteration 3538: 0.04301067298581871\n",
      "Train Loss at iteration 3539: 0.04301048951936876\n",
      "Train Loss at iteration 3540: 0.04301030608792063\n",
      "Train Loss at iteration 3541: 0.043010122691464946\n",
      "Train Loss at iteration 3542: 0.04300993932999233\n",
      "Train Loss at iteration 3543: 0.04300975600349339\n",
      "Train Loss at iteration 3544: 0.04300957271195879\n",
      "Train Loss at iteration 3545: 0.043009389455379146\n",
      "Train Loss at iteration 3546: 0.04300920623374511\n",
      "Train Loss at iteration 3547: 0.04300902304704732\n",
      "Train Loss at iteration 3548: 0.043008839895276436\n",
      "Train Loss at iteration 3549: 0.043008656778423124\n",
      "Train Loss at iteration 3550: 0.04300847369647804\n",
      "Train Loss at iteration 3551: 0.04300829064943185\n",
      "Train Loss at iteration 3552: 0.04300810763727524\n",
      "Train Loss at iteration 3553: 0.04300792465999888\n",
      "Train Loss at iteration 3554: 0.04300774171759345\n",
      "Train Loss at iteration 3555: 0.04300755881004965\n",
      "Train Loss at iteration 3556: 0.04300737593735817\n",
      "Train Loss at iteration 3557: 0.04300719309950971\n",
      "Train Loss at iteration 3558: 0.04300701029649498\n",
      "Train Loss at iteration 3559: 0.04300682752830469\n",
      "Train Loss at iteration 3560: 0.04300664479492954\n",
      "Train Loss at iteration 3561: 0.04300646209636027\n",
      "Train Loss at iteration 3562: 0.043006279432587606\n",
      "Train Loss at iteration 3563: 0.04300609680360226\n",
      "Train Loss at iteration 3564: 0.04300591420939499\n",
      "Train Loss at iteration 3565: 0.043005731649956504\n",
      "Train Loss at iteration 3566: 0.04300554912527759\n",
      "Train Loss at iteration 3567: 0.04300536663534897\n",
      "Train Loss at iteration 3568: 0.043005184180161415\n",
      "Train Loss at iteration 3569: 0.04300500175970569\n",
      "Train Loss at iteration 3570: 0.043004819373972526\n",
      "Train Loss at iteration 3571: 0.04300463702295274\n",
      "Train Loss at iteration 3572: 0.04300445470663708\n",
      "Train Loss at iteration 3573: 0.04300427242501635\n",
      "Train Loss at iteration 3574: 0.0430040901780813\n",
      "Train Loss at iteration 3575: 0.04300390796582276\n",
      "Train Loss at iteration 3576: 0.043003725788231514\n",
      "Train Loss at iteration 3577: 0.04300354364529836\n",
      "Train Loss at iteration 3578: 0.04300336153701409\n",
      "Train Loss at iteration 3579: 0.04300317946336955\n",
      "Train Loss at iteration 3580: 0.04300299742435552\n",
      "Train Loss at iteration 3581: 0.04300281541996284\n",
      "Train Loss at iteration 3582: 0.04300263345018234\n",
      "Train Loss at iteration 3583: 0.04300245151500485\n",
      "Train Loss at iteration 3584: 0.04300226961442118\n",
      "Train Loss at iteration 3585: 0.043002087748422214\n",
      "Train Loss at iteration 3586: 0.043001905916998776\n",
      "Train Loss at iteration 3587: 0.04300172412014171\n",
      "Train Loss at iteration 3588: 0.04300154235784189\n",
      "Train Loss at iteration 3589: 0.043001360630090156\n",
      "Train Loss at iteration 3590: 0.043001178936877396\n",
      "Train Loss at iteration 3591: 0.04300099727819447\n",
      "Train Loss at iteration 3592: 0.04300081565403225\n",
      "Train Loss at iteration 3593: 0.04300063406438162\n",
      "Train Loss at iteration 3594: 0.04300045250923346\n",
      "Train Loss at iteration 3595: 0.04300027098857868\n",
      "Train Loss at iteration 3596: 0.04300008950240815\n",
      "Train Loss at iteration 3597: 0.04299990805071279\n",
      "Train Loss at iteration 3598: 0.0429997266334835\n",
      "Train Loss at iteration 3599: 0.042999545250711174\n",
      "Train Loss at iteration 3600: 0.04299936390238675\n",
      "Train Loss at iteration 3601: 0.04299918258850113\n",
      "Train Loss at iteration 3602: 0.04299900130904523\n",
      "Train Loss at iteration 3603: 0.04299882006401003\n",
      "Train Loss at iteration 3604: 0.0429986388533864\n",
      "Train Loss at iteration 3605: 0.04299845767716532\n",
      "Train Loss at iteration 3606: 0.04299827653533771\n",
      "Train Loss at iteration 3607: 0.042998095427894546\n",
      "Train Loss at iteration 3608: 0.042997914354826756\n",
      "Train Loss at iteration 3609: 0.042997733316125296\n",
      "Train Loss at iteration 3610: 0.042997552311781144\n",
      "Train Loss at iteration 3611: 0.04299737134178527\n",
      "Train Loss at iteration 3612: 0.04299719040612863\n",
      "Train Loss at iteration 3613: 0.042997009504802224\n",
      "Train Loss at iteration 3614: 0.042996828637797004\n",
      "Train Loss at iteration 3615: 0.04299664780510398\n",
      "Train Loss at iteration 3616: 0.04299646700671414\n",
      "Train Loss at iteration 3617: 0.04299628624261846\n",
      "Train Loss at iteration 3618: 0.04299610551280797\n",
      "Train Loss at iteration 3619: 0.04299592481727366\n",
      "Train Loss at iteration 3620: 0.04299574415600654\n",
      "Train Loss at iteration 3621: 0.04299556352899763\n",
      "Train Loss at iteration 3622: 0.04299538293623794\n",
      "Train Loss at iteration 3623: 0.04299520237771852\n",
      "Train Loss at iteration 3624: 0.04299502185343035\n",
      "Train Loss at iteration 3625: 0.042994841363364525\n",
      "Train Loss at iteration 3626: 0.04299466090751203\n",
      "Train Loss at iteration 3627: 0.04299448048586394\n",
      "Train Loss at iteration 3628: 0.04299430009841129\n",
      "Train Loss at iteration 3629: 0.04299411974514514\n",
      "Train Loss at iteration 3630: 0.04299393942605654\n",
      "Train Loss at iteration 3631: 0.04299375914113656\n",
      "Train Loss at iteration 3632: 0.042993578890376256\n",
      "Train Loss at iteration 3633: 0.04299339867376671\n",
      "Train Loss at iteration 3634: 0.04299321849129898\n",
      "Train Loss at iteration 3635: 0.04299303834296417\n",
      "Train Loss at iteration 3636: 0.042992858228753345\n",
      "Train Loss at iteration 3637: 0.04299267814865761\n",
      "Train Loss at iteration 3638: 0.042992498102668045\n",
      "Train Loss at iteration 3639: 0.042992318090775763\n",
      "Train Loss at iteration 3640: 0.04299213811297185\n",
      "Train Loss at iteration 3641: 0.04299195816924743\n",
      "Train Loss at iteration 3642: 0.042991778259593626\n",
      "Train Loss at iteration 3643: 0.04299159838400152\n",
      "Train Loss at iteration 3644: 0.042991418542462245\n",
      "Train Loss at iteration 3645: 0.04299123873496694\n",
      "Train Loss at iteration 3646: 0.04299105896150673\n",
      "Train Loss at iteration 3647: 0.04299087922207275\n",
      "Train Loss at iteration 3648: 0.04299069951665615\n",
      "Train Loss at iteration 3649: 0.04299051984524806\n",
      "Train Loss at iteration 3650: 0.04299034020783962\n",
      "Train Loss at iteration 3651: 0.04299016060442201\n",
      "Train Loss at iteration 3652: 0.042989981034986385\n",
      "Train Loss at iteration 3653: 0.04298980149952389\n",
      "Train Loss at iteration 3654: 0.04298962199802569\n",
      "Train Loss at iteration 3655: 0.04298944253048297\n",
      "Train Loss at iteration 3656: 0.04298926309688691\n",
      "Train Loss at iteration 3657: 0.04298908369722868\n",
      "Train Loss at iteration 3658: 0.04298890433149947\n",
      "Train Loss at iteration 3659: 0.04298872499969046\n",
      "Train Loss at iteration 3660: 0.04298854570179286\n",
      "Train Loss at iteration 3661: 0.04298836643779786\n",
      "Train Loss at iteration 3662: 0.04298818720769667\n",
      "Train Loss at iteration 3663: 0.042988008011480484\n",
      "Train Loss at iteration 3664: 0.04298782884914053\n",
      "Train Loss at iteration 3665: 0.04298764972066799\n",
      "Train Loss at iteration 3666: 0.04298747062605413\n",
      "Train Loss at iteration 3667: 0.042987291565290166\n",
      "Train Loss at iteration 3668: 0.0429871125383673\n",
      "Train Loss at iteration 3669: 0.04298693354527679\n",
      "Train Loss at iteration 3670: 0.04298675458600986\n",
      "Train Loss at iteration 3671: 0.04298657566055776\n",
      "Train Loss at iteration 3672: 0.04298639676891175\n",
      "Train Loss at iteration 3673: 0.042986217911063065\n",
      "Train Loss at iteration 3674: 0.04298603908700297\n",
      "Train Loss at iteration 3675: 0.042985860296722724\n",
      "Train Loss at iteration 3676: 0.04298568154021358\n",
      "Train Loss at iteration 3677: 0.04298550281746683\n",
      "Train Loss at iteration 3678: 0.04298532412847373\n",
      "Train Loss at iteration 3679: 0.04298514547322556\n",
      "Train Loss at iteration 3680: 0.04298496685171361\n",
      "Train Loss at iteration 3681: 0.04298478826392917\n",
      "Train Loss at iteration 3682: 0.04298460970986351\n",
      "Train Loss at iteration 3683: 0.04298443118950795\n",
      "Train Loss at iteration 3684: 0.04298425270285378\n",
      "Train Loss at iteration 3685: 0.04298407424989229\n",
      "Train Loss at iteration 3686: 0.04298389583061482\n",
      "Train Loss at iteration 3687: 0.042983717445012656\n",
      "Train Loss at iteration 3688: 0.04298353909307713\n",
      "Train Loss at iteration 3689: 0.04298336077479956\n",
      "Train Loss at iteration 3690: 0.042983182490171266\n",
      "Train Loss at iteration 3691: 0.042983004239183595\n",
      "Train Loss at iteration 3692: 0.042982826021827854\n",
      "Train Loss at iteration 3693: 0.04298264783809541\n",
      "Train Loss at iteration 3694: 0.04298246968797759\n",
      "Train Loss at iteration 3695: 0.04298229157146574\n",
      "Train Loss at iteration 3696: 0.04298211348855122\n",
      "Train Loss at iteration 3697: 0.04298193543922539\n",
      "Train Loss at iteration 3698: 0.0429817574234796\n",
      "Train Loss at iteration 3699: 0.04298157944130521\n",
      "Train Loss at iteration 3700: 0.042981401492693604\n",
      "Train Loss at iteration 3701: 0.04298122357763615\n",
      "Train Loss at iteration 3702: 0.04298104569612422\n",
      "Train Loss at iteration 3703: 0.0429808678481492\n",
      "Train Loss at iteration 3704: 0.042980690033702494\n",
      "Train Loss at iteration 3705: 0.042980512252775445\n",
      "Train Loss at iteration 3706: 0.042980334505359485\n",
      "Train Loss at iteration 3707: 0.04298015679144601\n",
      "Train Loss at iteration 3708: 0.04297997911102641\n",
      "Train Loss at iteration 3709: 0.0429798014640921\n",
      "Train Loss at iteration 3710: 0.04297962385063449\n",
      "Train Loss at iteration 3711: 0.04297944627064498\n",
      "Train Loss at iteration 3712: 0.042979268724115026\n",
      "Train Loss at iteration 3713: 0.04297909121103603\n",
      "Train Loss at iteration 3714: 0.04297891373139943\n",
      "Train Loss at iteration 3715: 0.04297873628519662\n",
      "Train Loss at iteration 3716: 0.04297855887241908\n",
      "Train Loss at iteration 3717: 0.04297838149305825\n",
      "Train Loss at iteration 3718: 0.04297820414710554\n",
      "Train Loss at iteration 3719: 0.042978026834552416\n",
      "Train Loss at iteration 3720: 0.04297784955539036\n",
      "Train Loss at iteration 3721: 0.04297767230961079\n",
      "Train Loss at iteration 3722: 0.042977495097205186\n",
      "Train Loss at iteration 3723: 0.04297731791816501\n",
      "Train Loss at iteration 3724: 0.042977140772481735\n",
      "Train Loss at iteration 3725: 0.04297696366014682\n",
      "Train Loss at iteration 3726: 0.04297678658115177\n",
      "Train Loss at iteration 3727: 0.04297660953548804\n",
      "Train Loss at iteration 3728: 0.042976432523147125\n",
      "Train Loss at iteration 3729: 0.04297625554412054\n",
      "Train Loss at iteration 3730: 0.04297607859839974\n",
      "Train Loss at iteration 3731: 0.042975901685976245\n",
      "Train Loss at iteration 3732: 0.042975724806841566\n",
      "Train Loss at iteration 3733: 0.042975547960987195\n",
      "Train Loss at iteration 3734: 0.042975371148404634\n",
      "Train Loss at iteration 3735: 0.04297519436908541\n",
      "Train Loss at iteration 3736: 0.04297501762302107\n",
      "Train Loss at iteration 3737: 0.04297484091020309\n",
      "Train Loss at iteration 3738: 0.04297466423062302\n",
      "Train Loss at iteration 3739: 0.0429744875842724\n",
      "Train Loss at iteration 3740: 0.042974310971142744\n",
      "Train Loss at iteration 3741: 0.04297413439122561\n",
      "Train Loss at iteration 3742: 0.04297395784451253\n",
      "Train Loss at iteration 3743: 0.04297378133099506\n",
      "Train Loss at iteration 3744: 0.04297360485066474\n",
      "Train Loss at iteration 3745: 0.04297342840351315\n",
      "Train Loss at iteration 3746: 0.04297325198953182\n",
      "Train Loss at iteration 3747: 0.04297307560871234\n",
      "Train Loss at iteration 3748: 0.042972899261046256\n",
      "Train Loss at iteration 3749: 0.04297272294652515\n",
      "Train Loss at iteration 3750: 0.042972546665140596\n",
      "Train Loss at iteration 3751: 0.04297237041688418\n",
      "Train Loss at iteration 3752: 0.04297219420174748\n",
      "Train Loss at iteration 3753: 0.04297201801972209\n",
      "Train Loss at iteration 3754: 0.0429718418707996\n",
      "Train Loss at iteration 3755: 0.042971665754971605\n",
      "Train Loss at iteration 3756: 0.0429714896722297\n",
      "Train Loss at iteration 3757: 0.0429713136225655\n",
      "Train Loss at iteration 3758: 0.0429711376059706\n",
      "Train Loss at iteration 3759: 0.04297096162243662\n",
      "Train Loss at iteration 3760: 0.042970785671955176\n",
      "Train Loss at iteration 3761: 0.042970609754517886\n",
      "Train Loss at iteration 3762: 0.04297043387011638\n",
      "Train Loss at iteration 3763: 0.04297025801874227\n",
      "Train Loss at iteration 3764: 0.04297008220038721\n",
      "Train Loss at iteration 3765: 0.04296990641504282\n",
      "Train Loss at iteration 3766: 0.042969730662700746\n",
      "Train Loss at iteration 3767: 0.042969554943352625\n",
      "Train Loss at iteration 3768: 0.04296937925699012\n",
      "Train Loss at iteration 3769: 0.04296920360360487\n",
      "Train Loss at iteration 3770: 0.04296902798318854\n",
      "Train Loss at iteration 3771: 0.042968852395732775\n",
      "Train Loss at iteration 3772: 0.04296867684122925\n",
      "Train Loss at iteration 3773: 0.04296850131966963\n",
      "Train Loss at iteration 3774: 0.04296832583104558\n",
      "Train Loss at iteration 3775: 0.04296815037534879\n",
      "Train Loss at iteration 3776: 0.042967974952570924\n",
      "Train Loss at iteration 3777: 0.04296779956270368\n",
      "Train Loss at iteration 3778: 0.04296762420573873\n",
      "Train Loss at iteration 3779: 0.04296744888166777\n",
      "Train Loss at iteration 3780: 0.04296727359048251\n",
      "Train Loss at iteration 3781: 0.042967098332174616\n",
      "Train Loss at iteration 3782: 0.04296692310673581\n",
      "Train Loss at iteration 3783: 0.04296674791415781\n",
      "Train Loss at iteration 3784: 0.04296657275443232\n",
      "Train Loss at iteration 3785: 0.042966397627551035\n",
      "Train Loss at iteration 3786: 0.042966222533505696\n",
      "Train Loss at iteration 3787: 0.04296604747228801\n",
      "Train Loss at iteration 3788: 0.04296587244388971\n",
      "Train Loss at iteration 3789: 0.04296569744830254\n",
      "Train Loss at iteration 3790: 0.042965522485518194\n",
      "Train Loss at iteration 3791: 0.042965347555528444\n",
      "Train Loss at iteration 3792: 0.04296517265832501\n",
      "Train Loss at iteration 3793: 0.042964997793899666\n",
      "Train Loss at iteration 3794: 0.04296482296224414\n",
      "Train Loss at iteration 3795: 0.04296464816335018\n",
      "Train Loss at iteration 3796: 0.04296447339720955\n",
      "Train Loss at iteration 3797: 0.042964298663814016\n",
      "Train Loss at iteration 3798: 0.04296412396315534\n",
      "Train Loss at iteration 3799: 0.04296394929522528\n",
      "Train Loss at iteration 3800: 0.04296377466001562\n",
      "Train Loss at iteration 3801: 0.042963600057518134\n",
      "Train Loss at iteration 3802: 0.042963425487724596\n",
      "Train Loss at iteration 3803: 0.04296325095062679\n",
      "Train Loss at iteration 3804: 0.042963076446216504\n",
      "Train Loss at iteration 3805: 0.04296290197448554\n",
      "Train Loss at iteration 3806: 0.04296272753542567\n",
      "Train Loss at iteration 3807: 0.0429625531290287\n",
      "Train Loss at iteration 3808: 0.04296237875528646\n",
      "Train Loss at iteration 3809: 0.042962204414190706\n",
      "Train Loss at iteration 3810: 0.042962030105733295\n",
      "Train Loss at iteration 3811: 0.04296185582990602\n",
      "Train Loss at iteration 3812: 0.0429616815867007\n",
      "Train Loss at iteration 3813: 0.04296150737610914\n",
      "Train Loss at iteration 3814: 0.04296133319812319\n",
      "Train Loss at iteration 3815: 0.04296115905273467\n",
      "Train Loss at iteration 3816: 0.042960984939935415\n",
      "Train Loss at iteration 3817: 0.04296081085971725\n",
      "Train Loss at iteration 3818: 0.042960636812072024\n",
      "Train Loss at iteration 3819: 0.04296046279699158\n",
      "Train Loss at iteration 3820: 0.04296028881446777\n",
      "Train Loss at iteration 3821: 0.04296011486449243\n",
      "Train Loss at iteration 3822: 0.042959940947057434\n",
      "Train Loss at iteration 3823: 0.042959767062154625\n",
      "Train Loss at iteration 3824: 0.042959593209775875\n",
      "Train Loss at iteration 3825: 0.04295941938991304\n",
      "Train Loss at iteration 3826: 0.04295924560255799\n",
      "Train Loss at iteration 3827: 0.04295907184770262\n",
      "Train Loss at iteration 3828: 0.04295889812533879\n",
      "Train Loss at iteration 3829: 0.042958724435458374\n",
      "Train Loss at iteration 3830: 0.04295855077805327\n",
      "Train Loss at iteration 3831: 0.04295837715311536\n",
      "Train Loss at iteration 3832: 0.04295820356063654\n",
      "Train Loss at iteration 3833: 0.0429580300006087\n",
      "Train Loss at iteration 3834: 0.042957856473023755\n",
      "Train Loss at iteration 3835: 0.04295768297787357\n",
      "Train Loss at iteration 3836: 0.04295750951515009\n",
      "Train Loss at iteration 3837: 0.04295733608484522\n",
      "Train Loss at iteration 3838: 0.04295716268695085\n",
      "Train Loss at iteration 3839: 0.04295698932145891\n",
      "Train Loss at iteration 3840: 0.04295681598836134\n",
      "Train Loss at iteration 3841: 0.04295664268765003\n",
      "Train Loss at iteration 3842: 0.04295646941931696\n",
      "Train Loss at iteration 3843: 0.042956296183354004\n",
      "Train Loss at iteration 3844: 0.04295612297975314\n",
      "Train Loss at iteration 3845: 0.04295594980850628\n",
      "Train Loss at iteration 3846: 0.042955776669605386\n",
      "Train Loss at iteration 3847: 0.042955603563042406\n",
      "Train Loss at iteration 3848: 0.04295543048880926\n",
      "Train Loss at iteration 3849: 0.042955257446897946\n",
      "Train Loss at iteration 3850: 0.04295508443730039\n",
      "Train Loss at iteration 3851: 0.04295491146000857\n",
      "Train Loss at iteration 3852: 0.04295473851501443\n",
      "Train Loss at iteration 3853: 0.042954565602309966\n",
      "Train Loss at iteration 3854: 0.04295439272188714\n",
      "Train Loss at iteration 3855: 0.042954219873737924\n",
      "Train Loss at iteration 3856: 0.04295404705785429\n",
      "Train Loss at iteration 3857: 0.04295387427422824\n",
      "Train Loss at iteration 3858: 0.04295370152285174\n",
      "Train Loss at iteration 3859: 0.042953528803716785\n",
      "Train Loss at iteration 3860: 0.04295335611681539\n",
      "Train Loss at iteration 3861: 0.04295318346213953\n",
      "Train Loss at iteration 3862: 0.04295301083968121\n",
      "Train Loss at iteration 3863: 0.04295283824943244\n",
      "Train Loss at iteration 3864: 0.04295266569138523\n",
      "Train Loss at iteration 3865: 0.042952493165531576\n",
      "Train Loss at iteration 3866: 0.042952320671863504\n",
      "Train Loss at iteration 3867: 0.042952148210373034\n",
      "Train Loss at iteration 3868: 0.04295197578105219\n",
      "Train Loss at iteration 3869: 0.04295180338389299\n",
      "Train Loss at iteration 3870: 0.04295163101888748\n",
      "Train Loss at iteration 3871: 0.04295145868602768\n",
      "Train Loss at iteration 3872: 0.04295128638530561\n",
      "Train Loss at iteration 3873: 0.04295111411671333\n",
      "Train Loss at iteration 3874: 0.04295094188024289\n",
      "Train Loss at iteration 3875: 0.04295076967588633\n",
      "Train Loss at iteration 3876: 0.042950597503635696\n",
      "Train Loss at iteration 3877: 0.04295042536348306\n",
      "Train Loss at iteration 3878: 0.042950253255420436\n",
      "Train Loss at iteration 3879: 0.042950081179439936\n",
      "Train Loss at iteration 3880: 0.0429499091355336\n",
      "Train Loss at iteration 3881: 0.042949737123693495\n",
      "Train Loss at iteration 3882: 0.0429495651439117\n",
      "Train Loss at iteration 3883: 0.04294939319618028\n",
      "Train Loss at iteration 3884: 0.04294922128049133\n",
      "Train Loss at iteration 3885: 0.04294904939683692\n",
      "Train Loss at iteration 3886: 0.04294887754520915\n",
      "Train Loss at iteration 3887: 0.042948705725600084\n",
      "Train Loss at iteration 3888: 0.04294853393800184\n",
      "Train Loss at iteration 3889: 0.0429483621824065\n",
      "Train Loss at iteration 3890: 0.042948190458806164\n",
      "Train Loss at iteration 3891: 0.042948018767192946\n",
      "Train Loss at iteration 3892: 0.042947847107558945\n",
      "Train Loss at iteration 3893: 0.04294767547989628\n",
      "Train Loss at iteration 3894: 0.04294750388419705\n",
      "Train Loss at iteration 3895: 0.04294733232045338\n",
      "Train Loss at iteration 3896: 0.04294716078865739\n",
      "Train Loss at iteration 3897: 0.042946989288801196\n",
      "Train Loss at iteration 3898: 0.04294681782087694\n",
      "Train Loss at iteration 3899: 0.04294664638487674\n",
      "Train Loss at iteration 3900: 0.042946474980792744\n",
      "Train Loss at iteration 3901: 0.042946303608617085\n",
      "Train Loss at iteration 3902: 0.0429461322683419\n",
      "Train Loss at iteration 3903: 0.04294596095995934\n",
      "Train Loss at iteration 3904: 0.042945789683461535\n",
      "Train Loss at iteration 3905: 0.04294561843884066\n",
      "Train Loss at iteration 3906: 0.04294544722608887\n",
      "Train Loss at iteration 3907: 0.04294527604519831\n",
      "Train Loss at iteration 3908: 0.042945104896161136\n",
      "Train Loss at iteration 3909: 0.04294493377896953\n",
      "Train Loss at iteration 3910: 0.042944762693615655\n",
      "Train Loss at iteration 3911: 0.04294459164009169\n",
      "Train Loss at iteration 3912: 0.0429444206183898\n",
      "Train Loss at iteration 3913: 0.042944249628502164\n",
      "Train Loss at iteration 3914: 0.04294407867042098\n",
      "Train Loss at iteration 3915: 0.042943907744138414\n",
      "Train Loss at iteration 3916: 0.04294373684964666\n",
      "Train Loss at iteration 3917: 0.042943565986937916\n",
      "Train Loss at iteration 3918: 0.042943395156004385\n",
      "Train Loss at iteration 3919: 0.04294322435683825\n",
      "Train Loss at iteration 3920: 0.04294305358943172\n",
      "Train Loss at iteration 3921: 0.04294288285377701\n",
      "Train Loss at iteration 3922: 0.042942712149866315\n",
      "Train Loss at iteration 3923: 0.042942541477691866\n",
      "Train Loss at iteration 3924: 0.04294237083724586\n",
      "Train Loss at iteration 3925: 0.04294220022852053\n",
      "Train Loss at iteration 3926: 0.0429420296515081\n",
      "Train Loss at iteration 3927: 0.042941859106200776\n",
      "Train Loss at iteration 3928: 0.04294168859259082\n",
      "Train Loss at iteration 3929: 0.04294151811067044\n",
      "Train Loss at iteration 3930: 0.042941347660431874\n",
      "Train Loss at iteration 3931: 0.04294117724186738\n",
      "Train Loss at iteration 3932: 0.04294100685496919\n",
      "Train Loss at iteration 3933: 0.04294083649972956\n",
      "Train Loss at iteration 3934: 0.042940666176140715\n",
      "Train Loss at iteration 3935: 0.04294049588419492\n",
      "Train Loss at iteration 3936: 0.042940325623884446\n",
      "Train Loss at iteration 3937: 0.04294015539520154\n",
      "Train Loss at iteration 3938: 0.04293998519813847\n",
      "Train Loss at iteration 3939: 0.0429398150326875\n",
      "Train Loss at iteration 3940: 0.04293964489884091\n",
      "Train Loss at iteration 3941: 0.04293947479659095\n",
      "Train Loss at iteration 3942: 0.042939304725929925\n",
      "Train Loss at iteration 3943: 0.0429391346868501\n",
      "Train Loss at iteration 3944: 0.04293896467934375\n",
      "Train Loss at iteration 3945: 0.04293879470340317\n",
      "Train Loss at iteration 3946: 0.04293862475902066\n",
      "Train Loss at iteration 3947: 0.04293845484618849\n",
      "Train Loss at iteration 3948: 0.04293828496489899\n",
      "Train Loss at iteration 3949: 0.04293811511514443\n",
      "Train Loss at iteration 3950: 0.04293794529691713\n",
      "Train Loss at iteration 3951: 0.042937775510209365\n",
      "Train Loss at iteration 3952: 0.04293760575501349\n",
      "Train Loss at iteration 3953: 0.042937436031321795\n",
      "Train Loss at iteration 3954: 0.042937266339126604\n",
      "Train Loss at iteration 3955: 0.042937096678420224\n",
      "Train Loss at iteration 3956: 0.042936927049194995\n",
      "Train Loss at iteration 3957: 0.042936757451443215\n",
      "Train Loss at iteration 3958: 0.042936587885157244\n",
      "Train Loss at iteration 3959: 0.0429364183503294\n",
      "Train Loss at iteration 3960: 0.042936248846952026\n",
      "Train Loss at iteration 3961: 0.04293607937501745\n",
      "Train Loss at iteration 3962: 0.04293590993451802\n",
      "Train Loss at iteration 3963: 0.04293574052544609\n",
      "Train Loss at iteration 3964: 0.04293557114779399\n",
      "Train Loss at iteration 3965: 0.0429354018015541\n",
      "Train Loss at iteration 3966: 0.042935232486718745\n",
      "Train Loss at iteration 3967: 0.04293506320328031\n",
      "Train Loss at iteration 3968: 0.04293489395123113\n",
      "Train Loss at iteration 3969: 0.042934724730563596\n",
      "Train Loss at iteration 3970: 0.04293455554127008\n",
      "Train Loss at iteration 3971: 0.042934386383342904\n",
      "Train Loss at iteration 3972: 0.04293421725677451\n",
      "Train Loss at iteration 3973: 0.04293404816155723\n",
      "Train Loss at iteration 3974: 0.04293387909768346\n",
      "Train Loss at iteration 3975: 0.04293371006514558\n",
      "Train Loss at iteration 3976: 0.042933541063935994\n",
      "Train Loss at iteration 3977: 0.04293337209404708\n",
      "Train Loss at iteration 3978: 0.042933203155471236\n",
      "Train Loss at iteration 3979: 0.04293303424820086\n",
      "Train Loss at iteration 3980: 0.04293286537222833\n",
      "Train Loss at iteration 3981: 0.04293269652754608\n",
      "Train Loss at iteration 3982: 0.04293252771414651\n",
      "Train Loss at iteration 3983: 0.042932358932022026\n",
      "Train Loss at iteration 3984: 0.04293219018116505\n",
      "Train Loss at iteration 3985: 0.042932021461567986\n",
      "Train Loss at iteration 3986: 0.04293185277322326\n",
      "Train Loss at iteration 3987: 0.042931684116123296\n",
      "Train Loss at iteration 3988: 0.0429315154902605\n",
      "Train Loss at iteration 3989: 0.042931346895627345\n",
      "Train Loss at iteration 3990: 0.04293117833221624\n",
      "Train Loss at iteration 3991: 0.04293100980001961\n",
      "Train Loss at iteration 3992: 0.0429308412990299\n",
      "Train Loss at iteration 3993: 0.04293067282923957\n",
      "Train Loss at iteration 3994: 0.042930504390641035\n",
      "Train Loss at iteration 3995: 0.04293033598322677\n",
      "Train Loss at iteration 3996: 0.042930167606989214\n",
      "Train Loss at iteration 3997: 0.04292999926192083\n",
      "Train Loss at iteration 3998: 0.042929830948014065\n",
      "Train Loss at iteration 3999: 0.04292966266526139\n",
      "Train Loss at iteration 4000: 0.04292949441365526\n",
      "Train Loss at iteration 4001: 0.04292932619318816\n",
      "Train Loss at iteration 4002: 0.04292915800385253\n",
      "Train Loss at iteration 4003: 0.042928989845640884\n",
      "Train Loss at iteration 4004: 0.042928821718545666\n",
      "Train Loss at iteration 4005: 0.04292865362255936\n",
      "Train Loss at iteration 4006: 0.04292848555767447\n",
      "Train Loss at iteration 4007: 0.04292831752388347\n",
      "Train Loss at iteration 4008: 0.04292814952117885\n",
      "Train Loss at iteration 4009: 0.04292798154955309\n",
      "Train Loss at iteration 4010: 0.0429278136089987\n",
      "Train Loss at iteration 4011: 0.04292764569950819\n",
      "Train Loss at iteration 4012: 0.04292747782107403\n",
      "Train Loss at iteration 4013: 0.04292730997368875\n",
      "Train Loss at iteration 4014: 0.042927142157344854\n",
      "Train Loss at iteration 4015: 0.04292697437203485\n",
      "Train Loss at iteration 4016: 0.04292680661775126\n",
      "Train Loss at iteration 4017: 0.04292663889448659\n",
      "Train Loss at iteration 4018: 0.04292647120223336\n",
      "Train Loss at iteration 4019: 0.042926303540984104\n",
      "Train Loss at iteration 4020: 0.04292613591073135\n",
      "Train Loss at iteration 4021: 0.04292596831146762\n",
      "Train Loss at iteration 4022: 0.04292580074318545\n",
      "Train Loss at iteration 4023: 0.04292563320587738\n",
      "Train Loss at iteration 4024: 0.04292546569953594\n",
      "Train Loss at iteration 4025: 0.04292529822415368\n",
      "Train Loss at iteration 4026: 0.04292513077972315\n",
      "Train Loss at iteration 4027: 0.04292496336623688\n",
      "Train Loss at iteration 4028: 0.042924795983687446\n",
      "Train Loss at iteration 4029: 0.04292462863206739\n",
      "Train Loss at iteration 4030: 0.04292446131136926\n",
      "Train Loss at iteration 4031: 0.04292429402158564\n",
      "Train Loss at iteration 4032: 0.04292412676270908\n",
      "Train Loss at iteration 4033: 0.042923959534732145\n",
      "Train Loss at iteration 4034: 0.04292379233764741\n",
      "Train Loss at iteration 4035: 0.04292362517144745\n",
      "Train Loss at iteration 4036: 0.04292345803612484\n",
      "Train Loss at iteration 4037: 0.04292329093167214\n",
      "Train Loss at iteration 4038: 0.04292312385808196\n",
      "Train Loss at iteration 4039: 0.04292295681534689\n",
      "Train Loss at iteration 4040: 0.0429227898034595\n",
      "Train Loss at iteration 4041: 0.04292262282241238\n",
      "Train Loss at iteration 4042: 0.04292245587219813\n",
      "Train Loss at iteration 4043: 0.04292228895280935\n",
      "Train Loss at iteration 4044: 0.042922122064238635\n",
      "Train Loss at iteration 4045: 0.0429219552064786\n",
      "Train Loss at iteration 4046: 0.04292178837952184\n",
      "Train Loss at iteration 4047: 0.04292162158336097\n",
      "Train Loss at iteration 4048: 0.042921454817988605\n",
      "Train Loss at iteration 4049: 0.04292128808339736\n",
      "Train Loss at iteration 4050: 0.042921121379579846\n",
      "Train Loss at iteration 4051: 0.0429209547065287\n",
      "Train Loss at iteration 4052: 0.04292078806423652\n",
      "Train Loss at iteration 4053: 0.042920621452695956\n",
      "Train Loss at iteration 4054: 0.04292045487189964\n",
      "Train Loss at iteration 4055: 0.04292028832184021\n",
      "Train Loss at iteration 4056: 0.042920121802510286\n",
      "Train Loss at iteration 4057: 0.04291995531390252\n",
      "Train Loss at iteration 4058: 0.04291978885600954\n",
      "Train Loss at iteration 4059: 0.04291962242882402\n",
      "Train Loss at iteration 4060: 0.04291945603233857\n",
      "Train Loss at iteration 4061: 0.04291928966654589\n",
      "Train Loss at iteration 4062: 0.04291912333143859\n",
      "Train Loss at iteration 4063: 0.04291895702700935\n",
      "Train Loss at iteration 4064: 0.042918790753250835\n",
      "Train Loss at iteration 4065: 0.0429186245101557\n",
      "Train Loss at iteration 4066: 0.04291845829771662\n",
      "Train Loss at iteration 4067: 0.04291829211592626\n",
      "Train Loss at iteration 4068: 0.04291812596477728\n",
      "Train Loss at iteration 4069: 0.04291795984426238\n",
      "Train Loss at iteration 4070: 0.04291779375437422\n",
      "Train Loss at iteration 4071: 0.0429176276951055\n",
      "Train Loss at iteration 4072: 0.04291746166644889\n",
      "Train Loss at iteration 4073: 0.042917295668397075\n",
      "Train Loss at iteration 4074: 0.04291712970094277\n",
      "Train Loss at iteration 4075: 0.04291696376407863\n",
      "Train Loss at iteration 4076: 0.04291679785779739\n",
      "Train Loss at iteration 4077: 0.04291663198209173\n",
      "Train Loss at iteration 4078: 0.04291646613695436\n",
      "Train Loss at iteration 4079: 0.04291630032237797\n",
      "Train Loss at iteration 4080: 0.04291613453835529\n",
      "Train Loss at iteration 4081: 0.042915968784879036\n",
      "Train Loss at iteration 4082: 0.042915803061941885\n",
      "Train Loss at iteration 4083: 0.042915637369536595\n",
      "Train Loss at iteration 4084: 0.04291547170765587\n",
      "Train Loss at iteration 4085: 0.042915306076292424\n",
      "Train Loss at iteration 4086: 0.042915140475439\n",
      "Train Loss at iteration 4087: 0.04291497490508832\n",
      "Train Loss at iteration 4088: 0.04291480936523312\n",
      "Train Loss at iteration 4089: 0.04291464385586611\n",
      "Train Loss at iteration 4090: 0.04291447837698007\n",
      "Train Loss at iteration 4091: 0.042914312928567726\n",
      "Train Loss at iteration 4092: 0.0429141475106218\n",
      "Train Loss at iteration 4093: 0.04291398212313507\n",
      "Train Loss at iteration 4094: 0.04291381676610027\n",
      "Train Loss at iteration 4095: 0.04291365143951014\n",
      "Train Loss at iteration 4096: 0.042913486143357465\n",
      "Train Loss at iteration 4097: 0.042913320877634976\n",
      "Train Loss at iteration 4098: 0.04291315564233546\n",
      "Train Loss at iteration 4099: 0.04291299043745167\n",
      "Train Loss at iteration 4100: 0.042912825262976365\n",
      "Train Loss at iteration 4101: 0.04291266011890232\n",
      "Train Loss at iteration 4102: 0.04291249500522231\n",
      "Train Loss at iteration 4103: 0.042912329921929115\n",
      "Train Loss at iteration 4104: 0.0429121648690155\n",
      "Train Loss at iteration 4105: 0.042911999846474265\n",
      "Train Loss at iteration 4106: 0.04291183485429818\n",
      "Train Loss at iteration 4107: 0.042911669892480055\n",
      "Train Loss at iteration 4108: 0.04291150496101266\n",
      "Train Loss at iteration 4109: 0.04291134005988878\n",
      "Train Loss at iteration 4110: 0.042911175189101244\n",
      "Train Loss at iteration 4111: 0.04291101034864281\n",
      "Train Loss at iteration 4112: 0.04291084553850632\n",
      "Train Loss at iteration 4113: 0.042910680758684545\n",
      "Train Loss at iteration 4114: 0.04291051600917031\n",
      "Train Loss at iteration 4115: 0.04291035128995644\n",
      "Train Loss at iteration 4116: 0.042910186601035724\n",
      "Train Loss at iteration 4117: 0.04291002194240098\n",
      "Train Loss at iteration 4118: 0.04290985731404505\n",
      "Train Loss at iteration 4119: 0.042909692715960734\n",
      "Train Loss at iteration 4120: 0.042909528148140856\n",
      "Train Loss at iteration 4121: 0.04290936361057827\n",
      "Train Loss at iteration 4122: 0.042909199103265766\n",
      "Train Loss at iteration 4123: 0.04290903462619622\n",
      "Train Loss at iteration 4124: 0.04290887017936244\n",
      "Train Loss at iteration 4125: 0.042908705762757265\n",
      "Train Loss at iteration 4126: 0.04290854137637358\n",
      "Train Loss at iteration 4127: 0.042908377020204155\n",
      "Train Loss at iteration 4128: 0.04290821269424191\n",
      "Train Loss at iteration 4129: 0.04290804839847965\n",
      "Train Loss at iteration 4130: 0.042907884132910246\n",
      "Train Loss at iteration 4131: 0.04290771989752655\n",
      "Train Loss at iteration 4132: 0.04290755569232143\n",
      "Train Loss at iteration 4133: 0.04290739151728773\n",
      "Train Loss at iteration 4134: 0.04290722737241834\n",
      "Train Loss at iteration 4135: 0.04290706325770611\n",
      "Train Loss at iteration 4136: 0.04290689917314391\n",
      "Train Loss at iteration 4137: 0.042906735118724615\n",
      "Train Loss at iteration 4138: 0.04290657109444112\n",
      "Train Loss at iteration 4139: 0.04290640710028628\n",
      "Train Loss at iteration 4140: 0.042906243136252985\n",
      "Train Loss at iteration 4141: 0.04290607920233412\n",
      "Train Loss at iteration 4142: 0.04290591529852258\n",
      "Train Loss at iteration 4143: 0.04290575142481124\n",
      "Train Loss at iteration 4144: 0.04290558758119302\n",
      "Train Loss at iteration 4145: 0.04290542376766077\n",
      "Train Loss at iteration 4146: 0.04290525998420743\n",
      "Train Loss at iteration 4147: 0.0429050962308259\n",
      "Train Loss at iteration 4148: 0.042904932507509055\n",
      "Train Loss at iteration 4149: 0.04290476881424983\n",
      "Train Loss at iteration 4150: 0.04290460515104112\n",
      "Train Loss at iteration 4151: 0.04290444151787585\n",
      "Train Loss at iteration 4152: 0.042904277914746915\n",
      "Train Loss at iteration 4153: 0.04290411434164726\n",
      "Train Loss at iteration 4154: 0.04290395079856979\n",
      "Train Loss at iteration 4155: 0.04290378728550742\n",
      "Train Loss at iteration 4156: 0.042903623802453106\n",
      "Train Loss at iteration 4157: 0.04290346034939976\n",
      "Train Loss at iteration 4158: 0.042903296926340304\n",
      "Train Loss at iteration 4159: 0.0429031335332677\n",
      "Train Loss at iteration 4160: 0.04290297017017486\n",
      "Train Loss at iteration 4161: 0.04290280683705473\n",
      "Train Loss at iteration 4162: 0.04290264353390027\n",
      "Train Loss at iteration 4163: 0.0429024802607044\n",
      "Train Loss at iteration 4164: 0.0429023170174601\n",
      "Train Loss at iteration 4165: 0.0429021538041603\n",
      "Train Loss at iteration 4166: 0.04290199062079795\n",
      "Train Loss at iteration 4167: 0.04290182746736603\n",
      "Train Loss at iteration 4168: 0.04290166434385748\n",
      "Train Loss at iteration 4169: 0.04290150125026529\n",
      "Train Loss at iteration 4170: 0.042901338186582386\n",
      "Train Loss at iteration 4171: 0.04290117515280176\n",
      "Train Loss at iteration 4172: 0.04290101214891638\n",
      "Train Loss at iteration 4173: 0.04290084917491923\n",
      "Train Loss at iteration 4174: 0.04290068623080327\n",
      "Train Loss at iteration 4175: 0.04290052331656149\n",
      "Train Loss at iteration 4176: 0.042900360432186854\n",
      "Train Loss at iteration 4177: 0.04290019757767237\n",
      "Train Loss at iteration 4178: 0.04290003475301102\n",
      "Train Loss at iteration 4179: 0.04289987195819579\n",
      "Train Loss at iteration 4180: 0.042899709193219664\n",
      "Train Loss at iteration 4181: 0.042899546458075664\n",
      "Train Loss at iteration 4182: 0.04289938375275677\n",
      "Train Loss at iteration 4183: 0.04289922107725598\n",
      "Train Loss at iteration 4184: 0.04289905843156631\n",
      "Train Loss at iteration 4185: 0.04289889581568076\n",
      "Train Loss at iteration 4186: 0.042898733229592326\n",
      "Train Loss at iteration 4187: 0.04289857067329406\n",
      "Train Loss at iteration 4188: 0.04289840814677895\n",
      "Train Loss at iteration 4189: 0.04289824565004001\n",
      "Train Loss at iteration 4190: 0.04289808318307024\n",
      "Train Loss at iteration 4191: 0.042897920745862725\n",
      "Train Loss at iteration 4192: 0.04289775833841045\n",
      "Train Loss at iteration 4193: 0.042897595960706435\n",
      "Train Loss at iteration 4194: 0.04289743361274373\n",
      "Train Loss at iteration 4195: 0.04289727129451536\n",
      "Train Loss at iteration 4196: 0.04289710900601438\n",
      "Train Loss at iteration 4197: 0.0428969467472338\n",
      "Train Loss at iteration 4198: 0.04289678451816669\n",
      "Train Loss at iteration 4199: 0.04289662231880609\n",
      "Train Loss at iteration 4200: 0.04289646014914503\n",
      "Train Loss at iteration 4201: 0.04289629800917657\n",
      "Train Loss at iteration 4202: 0.04289613589889375\n",
      "Train Loss at iteration 4203: 0.042895973818289655\n",
      "Train Loss at iteration 4204: 0.04289581176735732\n",
      "Train Loss at iteration 4205: 0.042895649746089816\n",
      "Train Loss at iteration 4206: 0.04289548775448021\n",
      "Train Loss at iteration 4207: 0.04289532579252157\n",
      "Train Loss at iteration 4208: 0.04289516386020694\n",
      "Train Loss at iteration 4209: 0.042895001957529405\n",
      "Train Loss at iteration 4210: 0.042894840084482064\n",
      "Train Loss at iteration 4211: 0.042894678241057964\n",
      "Train Loss at iteration 4212: 0.042894516427250194\n",
      "Train Loss at iteration 4213: 0.04289435464305184\n",
      "Train Loss at iteration 4214: 0.04289419288845597\n",
      "Train Loss at iteration 4215: 0.0428940311634557\n",
      "Train Loss at iteration 4216: 0.04289386946804411\n",
      "Train Loss at iteration 4217: 0.04289370780221428\n",
      "Train Loss at iteration 4218: 0.04289354616595933\n",
      "Train Loss at iteration 4219: 0.042893384559272336\n",
      "Train Loss at iteration 4220: 0.0428932229821464\n",
      "Train Loss at iteration 4221: 0.04289306143457464\n",
      "Train Loss at iteration 4222: 0.04289289991655016\n",
      "Train Loss at iteration 4223: 0.042892738428066075\n",
      "Train Loss at iteration 4224: 0.04289257696911546\n",
      "Train Loss at iteration 4225: 0.042892415539691475\n",
      "Train Loss at iteration 4226: 0.042892254139787214\n",
      "Train Loss at iteration 4227: 0.0428920927693958\n",
      "Train Loss at iteration 4228: 0.04289193142851036\n",
      "Train Loss at iteration 4229: 0.042891770117124015\n",
      "Train Loss at iteration 4230: 0.042891608835229904\n",
      "Train Loss at iteration 4231: 0.042891447582821124\n",
      "Train Loss at iteration 4232: 0.04289128635989085\n",
      "Train Loss at iteration 4233: 0.04289112516643218\n",
      "Train Loss at iteration 4234: 0.04289096400243828\n",
      "Train Loss at iteration 4235: 0.04289080286790229\n",
      "Train Loss at iteration 4236: 0.04289064176281734\n",
      "Train Loss at iteration 4237: 0.042890480687176574\n",
      "Train Loss at iteration 4238: 0.04289031964097315\n",
      "Train Loss at iteration 4239: 0.04289015862420023\n",
      "Train Loss at iteration 4240: 0.04288999763685093\n",
      "Train Loss at iteration 4241: 0.042889836678918464\n",
      "Train Loss at iteration 4242: 0.04288967575039595\n",
      "Train Loss at iteration 4243: 0.04288951485127655\n",
      "Train Loss at iteration 4244: 0.042889353981553434\n",
      "Train Loss at iteration 4245: 0.042889193141219785\n",
      "Train Loss at iteration 4246: 0.04288903233026876\n",
      "Train Loss at iteration 4247: 0.04288887154869352\n",
      "Train Loss at iteration 4248: 0.04288871079648727\n",
      "Train Loss at iteration 4249: 0.04288855007364315\n",
      "Train Loss at iteration 4250: 0.04288838938015437\n",
      "Train Loss at iteration 4251: 0.042888228716014117\n",
      "Train Loss at iteration 4252: 0.042888068081215536\n",
      "Train Loss at iteration 4253: 0.042887907475751864\n",
      "Train Loss at iteration 4254: 0.042887746899616254\n",
      "Train Loss at iteration 4255: 0.042887586352801925\n",
      "Train Loss at iteration 4256: 0.04288742583530205\n",
      "Train Loss at iteration 4257: 0.042887265347109856\n",
      "Train Loss at iteration 4258: 0.042887104888218515\n",
      "Train Loss at iteration 4259: 0.042886944458621234\n",
      "Train Loss at iteration 4260: 0.04288678405831125\n",
      "Train Loss at iteration 4261: 0.042886623687281744\n",
      "Train Loss at iteration 4262: 0.04288646334552592\n",
      "Train Loss at iteration 4263: 0.04288630303303702\n",
      "Train Loss at iteration 4264: 0.042886142749808225\n",
      "Train Loss at iteration 4265: 0.0428859824958328\n",
      "Train Loss at iteration 4266: 0.04288582227110392\n",
      "Train Loss at iteration 4267: 0.042885662075614846\n",
      "Train Loss at iteration 4268: 0.042885501909358796\n",
      "Train Loss at iteration 4269: 0.04288534177232897\n",
      "Train Loss at iteration 4270: 0.042885181664518636\n",
      "Train Loss at iteration 4271: 0.04288502158592101\n",
      "Train Loss at iteration 4272: 0.04288486153652934\n",
      "Train Loss at iteration 4273: 0.04288470151633686\n",
      "Train Loss at iteration 4274: 0.04288454152533681\n",
      "Train Loss at iteration 4275: 0.042884381563522436\n",
      "Train Loss at iteration 4276: 0.042884221630886996\n",
      "Train Loss at iteration 4277: 0.042884061727423727\n",
      "Train Loss at iteration 4278: 0.04288390185312588\n",
      "Train Loss at iteration 4279: 0.04288374200798671\n",
      "Train Loss at iteration 4280: 0.04288358219199949\n",
      "Train Loss at iteration 4281: 0.04288342240515747\n",
      "Train Loss at iteration 4282: 0.042883262647453915\n",
      "Train Loss at iteration 4283: 0.04288310291888208\n",
      "Train Loss at iteration 4284: 0.04288294321943524\n",
      "Train Loss at iteration 4285: 0.04288278354910667\n",
      "Train Loss at iteration 4286: 0.04288262390788963\n",
      "Train Loss at iteration 4287: 0.04288246429577741\n",
      "Train Loss at iteration 4288: 0.042882304712763296\n",
      "Train Loss at iteration 4289: 0.042882145158840534\n",
      "Train Loss at iteration 4290: 0.042881985634002445\n",
      "Train Loss at iteration 4291: 0.04288182613824229\n",
      "Train Loss at iteration 4292: 0.042881666671553346\n",
      "Train Loss at iteration 4293: 0.042881507233928945\n",
      "Train Loss at iteration 4294: 0.04288134782536236\n",
      "Train Loss at iteration 4295: 0.042881188445846874\n",
      "Train Loss at iteration 4296: 0.0428810290953758\n",
      "Train Loss at iteration 4297: 0.042880869773942426\n",
      "Train Loss at iteration 4298: 0.042880710481540074\n",
      "Train Loss at iteration 4299: 0.042880551218162026\n",
      "Train Loss at iteration 4300: 0.04288039198380162\n",
      "Train Loss at iteration 4301: 0.04288023277845214\n",
      "Train Loss at iteration 4302: 0.04288007360210691\n",
      "Train Loss at iteration 4303: 0.04287991445475924\n",
      "Train Loss at iteration 4304: 0.04287975533640246\n",
      "Train Loss at iteration 4305: 0.042879596247029896\n",
      "Train Loss at iteration 4306: 0.042879437186634846\n",
      "Train Loss at iteration 4307: 0.04287927815521064\n",
      "Train Loss at iteration 4308: 0.04287911915275063\n",
      "Train Loss at iteration 4309: 0.042878960179248145\n",
      "Train Loss at iteration 4310: 0.042878801234696506\n",
      "Train Loss at iteration 4311: 0.04287864231908903\n",
      "Train Loss at iteration 4312: 0.042878483432419075\n",
      "Train Loss at iteration 4313: 0.042878324574679993\n",
      "Train Loss at iteration 4314: 0.04287816574586511\n",
      "Train Loss at iteration 4315: 0.042878006945967787\n",
      "Train Loss at iteration 4316: 0.042877848174981366\n",
      "Train Loss at iteration 4317: 0.04287768943289918\n",
      "Train Loss at iteration 4318: 0.042877530719714604\n",
      "Train Loss at iteration 4319: 0.04287737203542099\n",
      "Train Loss at iteration 4320: 0.04287721338001169\n",
      "Train Loss at iteration 4321: 0.042877054753480075\n",
      "Train Loss at iteration 4322: 0.04287689615581951\n",
      "Train Loss at iteration 4323: 0.04287673758702334\n",
      "Train Loss at iteration 4324: 0.04287657904708495\n",
      "Train Loss at iteration 4325: 0.04287642053599771\n",
      "Train Loss at iteration 4326: 0.04287626205375499\n",
      "Train Loss at iteration 4327: 0.04287610360035016\n",
      "Train Loss at iteration 4328: 0.042875945175776614\n",
      "Train Loss at iteration 4329: 0.042875786780027715\n",
      "Train Loss at iteration 4330: 0.04287562841309687\n",
      "Train Loss at iteration 4331: 0.04287547007497743\n",
      "Train Loss at iteration 4332: 0.04287531176566281\n",
      "Train Loss at iteration 4333: 0.04287515348514639\n",
      "Train Loss at iteration 4334: 0.042874995233421566\n",
      "Train Loss at iteration 4335: 0.04287483701048173\n",
      "Train Loss at iteration 4336: 0.04287467881632028\n",
      "Train Loss at iteration 4337: 0.042874520650930635\n",
      "Train Loss at iteration 4338: 0.04287436251430617\n",
      "Train Loss at iteration 4339: 0.042874204406440304\n",
      "Train Loss at iteration 4340: 0.04287404632732646\n",
      "Train Loss at iteration 4341: 0.042873888276958004\n",
      "Train Loss at iteration 4342: 0.04287373025532839\n",
      "Train Loss at iteration 4343: 0.04287357226243101\n",
      "Train Loss at iteration 4344: 0.042873414298259305\n",
      "Train Loss at iteration 4345: 0.04287325636280667\n",
      "Train Loss at iteration 4346: 0.04287309845606655\n",
      "Train Loss at iteration 4347: 0.04287294057803234\n",
      "Train Loss at iteration 4348: 0.04287278272869749\n",
      "Train Loss at iteration 4349: 0.04287262490805543\n",
      "Train Loss at iteration 4350: 0.042872467116099586\n",
      "Train Loss at iteration 4351: 0.042872309352823386\n",
      "Train Loss at iteration 4352: 0.04287215161822029\n",
      "Train Loss at iteration 4353: 0.04287199391228371\n",
      "Train Loss at iteration 4354: 0.042871836235007114\n",
      "Train Loss at iteration 4355: 0.042871678586383924\n",
      "Train Loss at iteration 4356: 0.04287152096640758\n",
      "Train Loss at iteration 4357: 0.04287136337507157\n",
      "Train Loss at iteration 4358: 0.042871205812369306\n",
      "Train Loss at iteration 4359: 0.04287104827829427\n",
      "Train Loss at iteration 4360: 0.042870890772839905\n",
      "Train Loss at iteration 4361: 0.04287073329599966\n",
      "Train Loss at iteration 4362: 0.04287057584776701\n",
      "Train Loss at iteration 4363: 0.04287041842813544\n",
      "Train Loss at iteration 4364: 0.042870261037098366\n",
      "Train Loss at iteration 4365: 0.042870103674649294\n",
      "Train Loss at iteration 4366: 0.04286994634078168\n",
      "Train Loss at iteration 4367: 0.04286978903548901\n",
      "Train Loss at iteration 4368: 0.04286963175876475\n",
      "Train Loss at iteration 4369: 0.04286947451060238\n",
      "Train Loss at iteration 4370: 0.04286931729099537\n",
      "Train Loss at iteration 4371: 0.04286916009993722\n",
      "Train Loss at iteration 4372: 0.0428690029374214\n",
      "Train Loss at iteration 4373: 0.04286884580344143\n",
      "Train Loss at iteration 4374: 0.04286868869799075\n",
      "Train Loss at iteration 4375: 0.04286853162106289\n",
      "Train Loss at iteration 4376: 0.04286837457265134\n",
      "Train Loss at iteration 4377: 0.04286821755274959\n",
      "Train Loss at iteration 4378: 0.04286806056135113\n",
      "Train Loss at iteration 4379: 0.042867903598449476\n",
      "Train Loss at iteration 4380: 0.04286774666403814\n",
      "Train Loss at iteration 4381: 0.04286758975811063\n",
      "Train Loss at iteration 4382: 0.04286743288066042\n",
      "Train Loss at iteration 4383: 0.042867276031681056\n",
      "Train Loss at iteration 4384: 0.04286711921116605\n",
      "Train Loss at iteration 4385: 0.042866962419108895\n",
      "Train Loss at iteration 4386: 0.04286680565550315\n",
      "Train Loss at iteration 4387: 0.042866648920342296\n",
      "Train Loss at iteration 4388: 0.042866492213619864\n",
      "Train Loss at iteration 4389: 0.042866335535329404\n",
      "Train Loss at iteration 4390: 0.04286617888546443\n",
      "Train Loss at iteration 4391: 0.04286602226401847\n",
      "Train Loss at iteration 4392: 0.04286586567098505\n",
      "Train Loss at iteration 4393: 0.042865709106357716\n",
      "Train Loss at iteration 4394: 0.042865552570130014\n",
      "Train Loss at iteration 4395: 0.042865396062295474\n",
      "Train Loss at iteration 4396: 0.04286523958284763\n",
      "Train Loss at iteration 4397: 0.04286508313178005\n",
      "Train Loss at iteration 4398: 0.042864926709086267\n",
      "Train Loss at iteration 4399: 0.042864770314759826\n",
      "Train Loss at iteration 4400: 0.042864613948794275\n",
      "Train Loss at iteration 4401: 0.042864457611183196\n",
      "Train Loss at iteration 4402: 0.042864301301920114\n",
      "Train Loss at iteration 4403: 0.04286414502099861\n",
      "Train Loss at iteration 4404: 0.042863988768412235\n",
      "Train Loss at iteration 4405: 0.042863832544154545\n",
      "Train Loss at iteration 4406: 0.04286367634821914\n",
      "Train Loss at iteration 4407: 0.04286352018059955\n",
      "Train Loss at iteration 4408: 0.04286336404128936\n",
      "Train Loss at iteration 4409: 0.04286320793028215\n",
      "Train Loss at iteration 4410: 0.042863051847571484\n",
      "Train Loss at iteration 4411: 0.042862895793150946\n",
      "Train Loss at iteration 4412: 0.04286273976701413\n",
      "Train Loss at iteration 4413: 0.04286258376915458\n",
      "Train Loss at iteration 4414: 0.04286242779956592\n",
      "Train Loss at iteration 4415: 0.042862271858241735\n",
      "Train Loss at iteration 4416: 0.04286211594517559\n",
      "Train Loss at iteration 4417: 0.04286196006036108\n",
      "Train Loss at iteration 4418: 0.04286180420379182\n",
      "Train Loss at iteration 4419: 0.04286164837546139\n",
      "Train Loss at iteration 4420: 0.042861492575363404\n",
      "Train Loss at iteration 4421: 0.04286133680349145\n",
      "Train Loss at iteration 4422: 0.042861181059839125\n",
      "Train Loss at iteration 4423: 0.04286102534440006\n",
      "Train Loss at iteration 4424: 0.042860869657167844\n",
      "Train Loss at iteration 4425: 0.042860713998136096\n",
      "Train Loss at iteration 4426: 0.042860558367298414\n",
      "Train Loss at iteration 4427: 0.042860402764648434\n",
      "Train Loss at iteration 4428: 0.04286024719017976\n",
      "Train Loss at iteration 4429: 0.04286009164388602\n",
      "Train Loss at iteration 4430: 0.04285993612576082\n",
      "Train Loss at iteration 4431: 0.0428597806357978\n",
      "Train Loss at iteration 4432: 0.0428596251739906\n",
      "Train Loss at iteration 4433: 0.04285946974033282\n",
      "Train Loss at iteration 4434: 0.042859314334818106\n",
      "Train Loss at iteration 4435: 0.04285915895744009\n",
      "Train Loss at iteration 4436: 0.042859003608192406\n",
      "Train Loss at iteration 4437: 0.04285884828706869\n",
      "Train Loss at iteration 4438: 0.042858692994062586\n",
      "Train Loss at iteration 4439: 0.04285853772916775\n",
      "Train Loss at iteration 4440: 0.042858382492377806\n",
      "Train Loss at iteration 4441: 0.042858227283686406\n",
      "Train Loss at iteration 4442: 0.042858072103087216\n",
      "Train Loss at iteration 4443: 0.04285791695057386\n",
      "Train Loss at iteration 4444: 0.04285776182614003\n",
      "Train Loss at iteration 4445: 0.042857606729779354\n",
      "Train Loss at iteration 4446: 0.04285745166148549\n",
      "Train Loss at iteration 4447: 0.04285729662125211\n",
      "Train Loss at iteration 4448: 0.04285714160907288\n",
      "Train Loss at iteration 4449: 0.04285698662494147\n",
      "Train Loss at iteration 4450: 0.04285683166885153\n",
      "Train Loss at iteration 4451: 0.04285667674079674\n",
      "Train Loss at iteration 4452: 0.04285652184077077\n",
      "Train Loss at iteration 4453: 0.0428563669687673\n",
      "Train Loss at iteration 4454: 0.04285621212478001\n",
      "Train Loss at iteration 4455: 0.04285605730880258\n",
      "Train Loss at iteration 4456: 0.04285590252082868\n",
      "Train Loss at iteration 4457: 0.042855747760852\n",
      "Train Loss at iteration 4458: 0.042855593028866226\n",
      "Train Loss at iteration 4459: 0.04285543832486506\n",
      "Train Loss at iteration 4460: 0.042855283648842175\n",
      "Train Loss at iteration 4461: 0.04285512900079126\n",
      "Train Loss at iteration 4462: 0.04285497438070604\n",
      "Train Loss at iteration 4463: 0.04285481978858018\n",
      "Train Loss at iteration 4464: 0.042854665224407396\n",
      "Train Loss at iteration 4465: 0.04285451068818139\n",
      "Train Loss at iteration 4466: 0.042854356179895865\n",
      "Train Loss at iteration 4467: 0.042854201699544535\n",
      "Train Loss at iteration 4468: 0.042854047247121094\n",
      "Train Loss at iteration 4469: 0.04285389282261926\n",
      "Train Loss at iteration 4470: 0.042853738426032746\n",
      "Train Loss at iteration 4471: 0.042853584057355266\n",
      "Train Loss at iteration 4472: 0.042853429716580564\n",
      "Train Loss at iteration 4473: 0.04285327540370232\n",
      "Train Loss at iteration 4474: 0.04285312111871428\n",
      "Train Loss at iteration 4475: 0.042852966861610164\n",
      "Train Loss at iteration 4476: 0.042852812632383705\n",
      "Train Loss at iteration 4477: 0.04285265843102863\n",
      "Train Loss at iteration 4478: 0.04285250425753865\n",
      "Train Loss at iteration 4479: 0.04285235011190752\n",
      "Train Loss at iteration 4480: 0.042852195994128976\n",
      "Train Loss at iteration 4481: 0.04285204190419676\n",
      "Train Loss at iteration 4482: 0.0428518878421046\n",
      "Train Loss at iteration 4483: 0.042851733807846244\n",
      "Train Loss at iteration 4484: 0.042851579801415435\n",
      "Train Loss at iteration 4485: 0.04285142582280593\n",
      "Train Loss at iteration 4486: 0.04285127187201146\n",
      "Train Loss at iteration 4487: 0.042851117949025797\n",
      "Train Loss at iteration 4488: 0.042850964053842686\n",
      "Train Loss at iteration 4489: 0.042850810186455875\n",
      "Train Loss at iteration 4490: 0.04285065634685914\n",
      "Train Loss at iteration 4491: 0.042850502535046235\n",
      "Train Loss at iteration 4492: 0.042850348751010923\n",
      "Train Loss at iteration 4493: 0.042850194994746967\n",
      "Train Loss at iteration 4494: 0.042850041266248126\n",
      "Train Loss at iteration 4495: 0.04284988756550817\n",
      "Train Loss at iteration 4496: 0.0428497338925209\n",
      "Train Loss at iteration 4497: 0.04284958024728006\n",
      "Train Loss at iteration 4498: 0.04284942662977943\n",
      "Train Loss at iteration 4499: 0.042849273040012806\n",
      "Train Loss at iteration 4500: 0.04284911947797395\n",
      "Train Loss at iteration 4501: 0.04284896594365665\n",
      "Train Loss at iteration 4502: 0.042848812437054705\n",
      "Train Loss at iteration 4503: 0.04284865895816188\n",
      "Train Loss at iteration 4504: 0.04284850550697197\n",
      "Train Loss at iteration 4505: 0.04284835208347879\n",
      "Train Loss at iteration 4506: 0.04284819868767611\n",
      "Train Loss at iteration 4507: 0.04284804531955772\n",
      "Train Loss at iteration 4508: 0.04284789197911743\n",
      "Train Loss at iteration 4509: 0.04284773866634906\n",
      "Train Loss at iteration 4510: 0.042847585381246396\n",
      "Train Loss at iteration 4511: 0.042847432123803215\n",
      "Train Loss at iteration 4512: 0.04284727889401337\n",
      "Train Loss at iteration 4513: 0.04284712569187066\n",
      "Train Loss at iteration 4514: 0.04284697251736887\n",
      "Train Loss at iteration 4515: 0.04284681937050184\n",
      "Train Loss at iteration 4516: 0.04284666625126338\n",
      "Train Loss at iteration 4517: 0.0428465131596473\n",
      "Train Loss at iteration 4518: 0.042846360095647434\n",
      "Train Loss at iteration 4519: 0.04284620705925758\n",
      "Train Loss at iteration 4520: 0.0428460540504716\n",
      "Train Loss at iteration 4521: 0.04284590106928329\n",
      "Train Loss at iteration 4522: 0.042845748115686505\n",
      "Train Loss at iteration 4523: 0.04284559518967505\n",
      "Train Loss at iteration 4524: 0.04284544229124278\n",
      "Train Loss at iteration 4525: 0.04284528942038351\n",
      "Train Loss at iteration 4526: 0.04284513657709109\n",
      "Train Loss at iteration 4527: 0.04284498376135937\n",
      "Train Loss at iteration 4528: 0.04284483097318218\n",
      "Train Loss at iteration 4529: 0.042844678212553354\n",
      "Train Loss at iteration 4530: 0.04284452547946676\n",
      "Train Loss at iteration 4531: 0.04284437277391625\n",
      "Train Loss at iteration 4532: 0.042844220095895634\n",
      "Train Loss at iteration 4533: 0.042844067445398815\n",
      "Train Loss at iteration 4534: 0.04284391482241962\n",
      "Train Loss at iteration 4535: 0.04284376222695192\n",
      "Train Loss at iteration 4536: 0.042843609658989576\n",
      "Train Loss at iteration 4537: 0.042843457118526426\n",
      "Train Loss at iteration 4538: 0.04284330460555636\n",
      "Train Loss at iteration 4539: 0.042843152120073225\n",
      "Train Loss at iteration 4540: 0.04284299966207091\n",
      "Train Loss at iteration 4541: 0.042842847231543274\n",
      "Train Loss at iteration 4542: 0.04284269482848419\n",
      "Train Loss at iteration 4543: 0.04284254245288753\n",
      "Train Loss at iteration 4544: 0.042842390104747184\n",
      "Train Loss at iteration 4545: 0.04284223778405701\n",
      "Train Loss at iteration 4546: 0.042842085490810915\n",
      "Train Loss at iteration 4547: 0.042841933225002765\n",
      "Train Loss at iteration 4548: 0.04284178098662644\n",
      "Train Loss at iteration 4549: 0.04284162877567585\n",
      "Train Loss at iteration 4550: 0.042841476592144864\n",
      "Train Loss at iteration 4551: 0.04284132443602738\n",
      "Train Loss at iteration 4552: 0.04284117230731731\n",
      "Train Loss at iteration 4553: 0.04284102020600852\n",
      "Train Loss at iteration 4554: 0.04284086813209493\n",
      "Train Loss at iteration 4555: 0.04284071608557044\n",
      "Train Loss at iteration 4556: 0.042840564066428936\n",
      "Train Loss at iteration 4557: 0.04284041207466433\n",
      "Train Loss at iteration 4558: 0.04284026011027055\n",
      "Train Loss at iteration 4559: 0.042840108173241485\n",
      "Train Loss at iteration 4560: 0.04283995626357105\n",
      "Train Loss at iteration 4561: 0.04283980438125315\n",
      "Train Loss at iteration 4562: 0.04283965252628172\n",
      "Train Loss at iteration 4563: 0.04283950069865066\n",
      "Train Loss at iteration 4564: 0.04283934889835391\n",
      "Train Loss at iteration 4565: 0.042839197125385364\n",
      "Train Loss at iteration 4566: 0.04283904537973897\n",
      "Train Loss at iteration 4567: 0.04283889366140864\n",
      "Train Loss at iteration 4568: 0.04283874197038832\n",
      "Train Loss at iteration 4569: 0.042838590306671934\n",
      "Train Loss at iteration 4570: 0.042838438670253394\n",
      "Train Loss at iteration 4571: 0.04283828706112665\n",
      "Train Loss at iteration 4572: 0.04283813547928565\n",
      "Train Loss at iteration 4573: 0.04283798392472433\n",
      "Train Loss at iteration 4574: 0.04283783239743661\n",
      "Train Loss at iteration 4575: 0.04283768089741646\n",
      "Train Loss at iteration 4576: 0.0428375294246578\n",
      "Train Loss at iteration 4577: 0.0428373779791546\n",
      "Train Loss at iteration 4578: 0.04283722656090078\n",
      "Train Loss at iteration 4579: 0.04283707516989034\n",
      "Train Loss at iteration 4580: 0.04283692380611721\n",
      "Train Loss at iteration 4581: 0.04283677246957531\n",
      "Train Loss at iteration 4582: 0.04283662116025866\n",
      "Train Loss at iteration 4583: 0.042836469878161185\n",
      "Train Loss at iteration 4584: 0.04283631862327684\n",
      "Train Loss at iteration 4585: 0.04283616739559962\n",
      "Train Loss at iteration 4586: 0.042836016195123464\n",
      "Train Loss at iteration 4587: 0.04283586502184237\n",
      "Train Loss at iteration 4588: 0.042835713875750255\n",
      "Train Loss at iteration 4589: 0.04283556275684116\n",
      "Train Loss at iteration 4590: 0.04283541166510902\n",
      "Train Loss at iteration 4591: 0.042835260600547806\n",
      "Train Loss at iteration 4592: 0.04283510956315152\n",
      "Train Loss at iteration 4593: 0.04283495855291413\n",
      "Train Loss at iteration 4594: 0.04283480756982963\n",
      "Train Loss at iteration 4595: 0.042834656613892\n",
      "Train Loss at iteration 4596: 0.04283450568509522\n",
      "Train Loss at iteration 4597: 0.04283435478343329\n",
      "Train Loss at iteration 4598: 0.04283420390890019\n",
      "Train Loss at iteration 4599: 0.04283405306148992\n",
      "Train Loss at iteration 4600: 0.0428339022411965\n",
      "Train Loss at iteration 4601: 0.042833751448013886\n",
      "Train Loss at iteration 4602: 0.04283360068193611\n",
      "Train Loss at iteration 4603: 0.042833449942957146\n",
      "Train Loss at iteration 4604: 0.04283329923107104\n",
      "Train Loss at iteration 4605: 0.042833148546271765\n",
      "Train Loss at iteration 4606: 0.042832997888553326\n",
      "Train Loss at iteration 4607: 0.04283284725790974\n",
      "Train Loss at iteration 4608: 0.04283269665433504\n",
      "Train Loss at iteration 4609: 0.042832546077823226\n",
      "Train Loss at iteration 4610: 0.04283239552836831\n",
      "Train Loss at iteration 4611: 0.04283224500596432\n",
      "Train Loss at iteration 4612: 0.04283209451060526\n",
      "Train Loss at iteration 4613: 0.04283194404228518\n",
      "Train Loss at iteration 4614: 0.042831793600998085\n",
      "Train Loss at iteration 4615: 0.04283164318673801\n",
      "Train Loss at iteration 4616: 0.04283149279949898\n",
      "Train Loss at iteration 4617: 0.042831342439275034\n",
      "Train Loss at iteration 4618: 0.04283119210606018\n",
      "Train Loss at iteration 4619: 0.042831041799848486\n",
      "Train Loss at iteration 4620: 0.04283089152063399\n",
      "Train Loss at iteration 4621: 0.04283074126841071\n",
      "Train Loss at iteration 4622: 0.04283059104317269\n",
      "Train Loss at iteration 4623: 0.04283044084491398\n",
      "Train Loss at iteration 4624: 0.042830290673628625\n",
      "Train Loss at iteration 4625: 0.04283014052931068\n",
      "Train Loss at iteration 4626: 0.04282999041195419\n",
      "Train Loss at iteration 4627: 0.042829840321553195\n",
      "Train Loss at iteration 4628: 0.042829690258101766\n",
      "Train Loss at iteration 4629: 0.04282954022159395\n",
      "Train Loss at iteration 4630: 0.0428293902120238\n",
      "Train Loss at iteration 4631: 0.04282924022938541\n",
      "Train Loss at iteration 4632: 0.04282909027367279\n",
      "Train Loss at iteration 4633: 0.042828940344880044\n",
      "Train Loss at iteration 4634: 0.042828790443001225\n",
      "Train Loss at iteration 4635: 0.042828640568030395\n",
      "Train Loss at iteration 4636: 0.042828490719961636\n",
      "Train Loss at iteration 4637: 0.04282834089878901\n",
      "Train Loss at iteration 4638: 0.04282819110450659\n",
      "Train Loss at iteration 4639: 0.04282804133710848\n",
      "Train Loss at iteration 4640: 0.04282789159658871\n",
      "Train Loss at iteration 4641: 0.0428277418829414\n",
      "Train Loss at iteration 4642: 0.04282759219616063\n",
      "Train Loss at iteration 4643: 0.04282744253624046\n",
      "Train Loss at iteration 4644: 0.042827292903175\n",
      "Train Loss at iteration 4645: 0.04282714329695834\n",
      "Train Loss at iteration 4646: 0.04282699371758455\n",
      "Train Loss at iteration 4647: 0.042826844165047735\n",
      "Train Loss at iteration 4648: 0.042826694639341994\n",
      "Train Loss at iteration 4649: 0.04282654514046142\n",
      "Train Loss at iteration 4650: 0.04282639566840011\n",
      "Train Loss at iteration 4651: 0.042826246223152176\n",
      "Train Loss at iteration 4652: 0.0428260968047117\n",
      "Train Loss at iteration 4653: 0.042825947413072794\n",
      "Train Loss at iteration 4654: 0.04282579804822958\n",
      "Train Loss at iteration 4655: 0.04282564871017615\n",
      "Train Loss at iteration 4656: 0.042825499398906626\n",
      "Train Loss at iteration 4657: 0.042825350114415114\n",
      "Train Loss at iteration 4658: 0.042825200856695736\n",
      "Train Loss at iteration 4659: 0.042825051625742606\n",
      "Train Loss at iteration 4660: 0.04282490242154984\n",
      "Train Loss at iteration 4661: 0.04282475324411156\n",
      "Train Loss at iteration 4662: 0.0428246040934219\n",
      "Train Loss at iteration 4663: 0.04282445496947497\n",
      "Train Loss at iteration 4664: 0.0428243058722649\n",
      "Train Loss at iteration 4665: 0.04282415680178583\n",
      "Train Loss at iteration 4666: 0.04282400775803187\n",
      "Train Loss at iteration 4667: 0.04282385874099718\n",
      "Train Loss at iteration 4668: 0.042823709750675884\n",
      "Train Loss at iteration 4669: 0.042823560787062113\n",
      "Train Loss at iteration 4670: 0.04282341185015002\n",
      "Train Loss at iteration 4671: 0.04282326293993372\n",
      "Train Loss at iteration 4672: 0.042823114056407374\n",
      "Train Loss at iteration 4673: 0.04282296519956515\n",
      "Train Loss at iteration 4674: 0.04282281636940116\n",
      "Train Loss at iteration 4675: 0.042822667565909564\n",
      "Train Loss at iteration 4676: 0.04282251878908451\n",
      "Train Loss at iteration 4677: 0.04282237003892017\n",
      "Train Loss at iteration 4678: 0.04282222131541067\n",
      "Train Loss at iteration 4679: 0.04282207261855021\n",
      "Train Loss at iteration 4680: 0.04282192394833289\n",
      "Train Loss at iteration 4681: 0.04282177530475292\n",
      "Train Loss at iteration 4682: 0.042821626687804454\n",
      "Train Loss at iteration 4683: 0.04282147809748164\n",
      "Train Loss at iteration 4684: 0.04282132953377865\n",
      "Train Loss at iteration 4685: 0.042821180996689676\n",
      "Train Loss at iteration 4686: 0.04282103248620887\n",
      "Train Loss at iteration 4687: 0.04282088400233038\n",
      "Train Loss at iteration 4688: 0.042820735545048436\n",
      "Train Loss at iteration 4689: 0.04282058711435718\n",
      "Train Loss at iteration 4690: 0.04282043871025081\n",
      "Train Loss at iteration 4691: 0.042820290332723485\n",
      "Train Loss at iteration 4692: 0.0428201419817694\n",
      "Train Loss at iteration 4693: 0.04281999365738275\n",
      "Train Loss at iteration 4694: 0.0428198453595577\n",
      "Train Loss at iteration 4695: 0.04281969708828845\n",
      "Train Loss at iteration 4696: 0.04281954884356921\n",
      "Train Loss at iteration 4697: 0.04281940062539414\n",
      "Train Loss at iteration 4698: 0.042819252433757456\n",
      "Train Loss at iteration 4699: 0.04281910426865334\n",
      "Train Loss at iteration 4700: 0.042818956130076025\n",
      "Train Loss at iteration 4701: 0.04281880801801965\n",
      "Train Loss at iteration 4702: 0.04281865993247849\n",
      "Train Loss at iteration 4703: 0.042818511873446694\n",
      "Train Loss at iteration 4704: 0.04281836384091849\n",
      "Train Loss at iteration 4705: 0.0428182158348881\n",
      "Train Loss at iteration 4706: 0.042818067855349716\n",
      "Train Loss at iteration 4707: 0.042817919902297566\n",
      "Train Loss at iteration 4708: 0.04281777197572584\n",
      "Train Loss at iteration 4709: 0.04281762407562877\n",
      "Train Loss at iteration 4710: 0.04281747620200058\n",
      "Train Loss at iteration 4711: 0.042817328354835496\n",
      "Train Loss at iteration 4712: 0.04281718053412771\n",
      "Train Loss at iteration 4713: 0.04281703273987149\n",
      "Train Loss at iteration 4714: 0.04281688497206102\n",
      "Train Loss at iteration 4715: 0.04281673723069056\n",
      "Train Loss at iteration 4716: 0.04281658951575432\n",
      "Train Loss at iteration 4717: 0.04281644182724653\n",
      "Train Loss at iteration 4718: 0.04281629416516145\n",
      "Train Loss at iteration 4719: 0.0428161465294933\n",
      "Train Loss at iteration 4720: 0.04281599892023632\n",
      "Train Loss at iteration 4721: 0.04281585133738475\n",
      "Train Loss at iteration 4722: 0.04281570378093283\n",
      "Train Loss at iteration 4723: 0.042815556250874805\n",
      "Train Loss at iteration 4724: 0.0428154087472049\n",
      "Train Loss at iteration 4725: 0.04281526126991742\n",
      "Train Loss at iteration 4726: 0.04281511381900656\n",
      "Train Loss at iteration 4727: 0.042814966394466594\n",
      "Train Loss at iteration 4728: 0.04281481899629178\n",
      "Train Loss at iteration 4729: 0.042814671624476365\n",
      "Train Loss at iteration 4730: 0.0428145242790146\n",
      "Train Loss at iteration 4731: 0.04281437695990076\n",
      "Train Loss at iteration 4732: 0.0428142296671291\n",
      "Train Loss at iteration 4733: 0.04281408240069388\n",
      "Train Loss at iteration 4734: 0.04281393516058937\n",
      "Train Loss at iteration 4735: 0.042813787946809845\n",
      "Train Loss at iteration 4736: 0.042813640759349555\n",
      "Train Loss at iteration 4737: 0.04281349359820279\n",
      "Train Loss at iteration 4738: 0.04281334646336381\n",
      "Train Loss at iteration 4739: 0.04281319935482689\n",
      "Train Loss at iteration 4740: 0.04281305227258632\n",
      "Train Loss at iteration 4741: 0.042812905216636374\n",
      "Train Loss at iteration 4742: 0.04281275818697131\n",
      "Train Loss at iteration 4743: 0.04281261118358545\n",
      "Train Loss at iteration 4744: 0.042812464206473055\n",
      "Train Loss at iteration 4745: 0.04281231725562841\n",
      "Train Loss at iteration 4746: 0.04281217033104582\n",
      "Train Loss at iteration 4747: 0.04281202343271956\n",
      "Train Loss at iteration 4748: 0.04281187656064392\n",
      "Train Loss at iteration 4749: 0.04281172971481322\n",
      "Train Loss at iteration 4750: 0.042811582895221724\n",
      "Train Loss at iteration 4751: 0.042811436101863755\n",
      "Train Loss at iteration 4752: 0.0428112893347336\n",
      "Train Loss at iteration 4753: 0.04281114259382556\n",
      "Train Loss at iteration 4754: 0.042810995879133944\n",
      "Train Loss at iteration 4755: 0.04281084919065305\n",
      "Train Loss at iteration 4756: 0.042810702528377195\n",
      "Train Loss at iteration 4757: 0.0428105558923007\n",
      "Train Loss at iteration 4758: 0.04281040928241785\n",
      "Train Loss at iteration 4759: 0.04281026269872298\n",
      "Train Loss at iteration 4760: 0.04281011614121038\n",
      "Train Loss at iteration 4761: 0.04280996960987439\n",
      "Train Loss at iteration 4762: 0.042809823104709335\n",
      "Train Loss at iteration 4763: 0.04280967662570952\n",
      "Train Loss at iteration 4764: 0.04280953017286926\n",
      "Train Loss at iteration 4765: 0.04280938374618291\n",
      "Train Loss at iteration 4766: 0.042809237345644756\n",
      "Train Loss at iteration 4767: 0.04280909097124916\n",
      "Train Loss at iteration 4768: 0.04280894462299045\n",
      "Train Loss at iteration 4769: 0.04280879830086294\n",
      "Train Loss at iteration 4770: 0.042808652004860975\n",
      "Train Loss at iteration 4771: 0.04280850573497891\n",
      "Train Loss at iteration 4772: 0.04280835949121104\n",
      "Train Loss at iteration 4773: 0.04280821327355174\n",
      "Train Loss at iteration 4774: 0.04280806708199534\n",
      "Train Loss at iteration 4775: 0.04280792091653619\n",
      "Train Loss at iteration 4776: 0.042807774777168635\n",
      "Train Loss at iteration 4777: 0.04280762866388701\n",
      "Train Loss at iteration 4778: 0.04280748257668568\n",
      "Train Loss at iteration 4779: 0.042807336515558986\n",
      "Train Loss at iteration 4780: 0.042807190480501287\n",
      "Train Loss at iteration 4781: 0.04280704447150694\n",
      "Train Loss at iteration 4782: 0.042806898488570305\n",
      "Train Loss at iteration 4783: 0.04280675253168572\n",
      "Train Loss at iteration 4784: 0.042806606600847556\n",
      "Train Loss at iteration 4785: 0.0428064606960502\n",
      "Train Loss at iteration 4786: 0.04280631481728799\n",
      "Train Loss at iteration 4787: 0.0428061689645553\n",
      "Train Loss at iteration 4788: 0.0428060231378465\n",
      "Train Loss at iteration 4789: 0.042805877337155955\n",
      "Train Loss at iteration 4790: 0.042805731562478036\n",
      "Train Loss at iteration 4791: 0.04280558581380714\n",
      "Train Loss at iteration 4792: 0.04280544009113761\n",
      "Train Loss at iteration 4793: 0.04280529439446384\n",
      "Train Loss at iteration 4794: 0.042805148723780204\n",
      "Train Loss at iteration 4795: 0.0428050030790811\n",
      "Train Loss at iteration 4796: 0.04280485746036088\n",
      "Train Loss at iteration 4797: 0.04280471186761397\n",
      "Train Loss at iteration 4798: 0.04280456630083472\n",
      "Train Loss at iteration 4799: 0.04280442076001754\n",
      "Train Loss at iteration 4800: 0.04280427524515682\n",
      "Train Loss at iteration 4801: 0.04280412975624694\n",
      "Train Loss at iteration 4802: 0.04280398429328231\n",
      "Train Loss at iteration 4803: 0.04280383885625731\n",
      "Train Loss at iteration 4804: 0.042803693445166345\n",
      "Train Loss at iteration 4805: 0.042803548060003825\n",
      "Train Loss at iteration 4806: 0.04280340270076415\n",
      "Train Loss at iteration 4807: 0.04280325736744172\n",
      "Train Loss at iteration 4808: 0.04280311206003093\n",
      "Train Loss at iteration 4809: 0.0428029667785262\n",
      "Train Loss at iteration 4810: 0.04280282152292194\n",
      "Train Loss at iteration 4811: 0.04280267629321255\n",
      "Train Loss at iteration 4812: 0.042802531089392463\n",
      "Train Loss at iteration 4813: 0.04280238591145607\n",
      "Train Loss at iteration 4814: 0.04280224075939781\n",
      "Train Loss at iteration 4815: 0.04280209563321208\n",
      "Train Loss at iteration 4816: 0.04280195053289332\n",
      "Train Loss at iteration 4817: 0.04280180545843595\n",
      "Train Loss at iteration 4818: 0.04280166040983439\n",
      "Train Loss at iteration 4819: 0.04280151538708306\n",
      "Train Loss at iteration 4820: 0.04280137039017638\n",
      "Train Loss at iteration 4821: 0.04280122541910881\n",
      "Train Loss at iteration 4822: 0.04280108047387476\n",
      "Train Loss at iteration 4823: 0.042800935554468664\n",
      "Train Loss at iteration 4824: 0.04280079066088496\n",
      "Train Loss at iteration 4825: 0.04280064579311811\n",
      "Train Loss at iteration 4826: 0.04280050095116251\n",
      "Train Loss at iteration 4827: 0.042800356135012624\n",
      "Train Loss at iteration 4828: 0.0428002113446629\n",
      "Train Loss at iteration 4829: 0.04280006658010776\n",
      "Train Loss at iteration 4830: 0.042799921841341675\n",
      "Train Loss at iteration 4831: 0.042799777128359084\n",
      "Train Loss at iteration 4832: 0.04279963244115442\n",
      "Train Loss at iteration 4833: 0.04279948777972216\n",
      "Train Loss at iteration 4834: 0.04279934314405676\n",
      "Train Loss at iteration 4835: 0.042799198534152656\n",
      "Train Loss at iteration 4836: 0.04279905395000432\n",
      "Train Loss at iteration 4837: 0.04279890939160619\n",
      "Train Loss at iteration 4838: 0.04279876485895275\n",
      "Train Loss at iteration 4839: 0.04279862035203846\n",
      "Train Loss at iteration 4840: 0.04279847587085776\n",
      "Train Loss at iteration 4841: 0.04279833141540517\n",
      "Train Loss at iteration 4842: 0.042798186985675106\n",
      "Train Loss at iteration 4843: 0.042798042581662046\n",
      "Train Loss at iteration 4844: 0.04279789820336049\n",
      "Train Loss at iteration 4845: 0.04279775385076489\n",
      "Train Loss at iteration 4846: 0.04279760952386972\n",
      "Train Loss at iteration 4847: 0.04279746522266947\n",
      "Train Loss at iteration 4848: 0.04279732094715861\n",
      "Train Loss at iteration 4849: 0.04279717669733163\n",
      "Train Loss at iteration 4850: 0.042797032473183\n",
      "Train Loss at iteration 4851: 0.042796888274707214\n",
      "Train Loss at iteration 4852: 0.04279674410189875\n",
      "Train Loss at iteration 4853: 0.0427965999547521\n",
      "Train Loss at iteration 4854: 0.042796455833261776\n",
      "Train Loss at iteration 4855: 0.04279631173742225\n",
      "Train Loss at iteration 4856: 0.04279616766722799\n",
      "Train Loss at iteration 4857: 0.04279602362267355\n",
      "Train Loss at iteration 4858: 0.04279587960375338\n",
      "Train Loss at iteration 4859: 0.042795735610461995\n",
      "Train Loss at iteration 4860: 0.042795591642793895\n",
      "Train Loss at iteration 4861: 0.04279544770074358\n",
      "Train Loss at iteration 4862: 0.042795303784305565\n",
      "Train Loss at iteration 4863: 0.04279515989347435\n",
      "Train Loss at iteration 4864: 0.042795016028244454\n",
      "Train Loss at iteration 4865: 0.04279487218861037\n",
      "Train Loss at iteration 4866: 0.042794728374566604\n",
      "Train Loss at iteration 4867: 0.042794584586107694\n",
      "Train Loss at iteration 4868: 0.04279444082322814\n",
      "Train Loss at iteration 4869: 0.04279429708592246\n",
      "Train Loss at iteration 4870: 0.04279415337418518\n",
      "Train Loss at iteration 4871: 0.042794009688010816\n",
      "Train Loss at iteration 4872: 0.042793866027393886\n",
      "Train Loss at iteration 4873: 0.042793722392328926\n",
      "Train Loss at iteration 4874: 0.04279357878281044\n",
      "Train Loss at iteration 4875: 0.04279343519883299\n",
      "Train Loss at iteration 4876: 0.042793291640391085\n",
      "Train Loss at iteration 4877: 0.04279314810747925\n",
      "Train Loss at iteration 4878: 0.042793004600092034\n",
      "Train Loss at iteration 4879: 0.042792861118223974\n",
      "Train Loss at iteration 4880: 0.042792717661869595\n",
      "Train Loss at iteration 4881: 0.04279257423102343\n",
      "Train Loss at iteration 4882: 0.04279243082568004\n",
      "Train Loss at iteration 4883: 0.042792287445833944\n",
      "Train Loss at iteration 4884: 0.04279214409147972\n",
      "Train Loss at iteration 4885: 0.042792000762611866\n",
      "Train Loss at iteration 4886: 0.04279185745922496\n",
      "Train Loss at iteration 4887: 0.04279171418131358\n",
      "Train Loss at iteration 4888: 0.0427915709288722\n",
      "Train Loss at iteration 4889: 0.04279142770189545\n",
      "Train Loss at iteration 4890: 0.042791284500377835\n",
      "Train Loss at iteration 4891: 0.04279114132431394\n",
      "Train Loss at iteration 4892: 0.04279099817369831\n",
      "Train Loss at iteration 4893: 0.042790855048525506\n",
      "Train Loss at iteration 4894: 0.0427907119487901\n",
      "Train Loss at iteration 4895: 0.04279056887448664\n",
      "Train Loss at iteration 4896: 0.0427904258256097\n",
      "Train Loss at iteration 4897: 0.04279028280215384\n",
      "Train Loss at iteration 4898: 0.04279013980411363\n",
      "Train Loss at iteration 4899: 0.042789996831483657\n",
      "Train Loss at iteration 4900: 0.042789853884258496\n",
      "Train Loss at iteration 4901: 0.04278971096243269\n",
      "Train Loss at iteration 4902: 0.04278956806600083\n",
      "Train Loss at iteration 4903: 0.042789425194957514\n",
      "Train Loss at iteration 4904: 0.04278928234929728\n",
      "Train Loss at iteration 4905: 0.042789139529014746\n",
      "Train Loss at iteration 4906: 0.04278899673410449\n",
      "Train Loss at iteration 4907: 0.042788853964561085\n",
      "Train Loss at iteration 4908: 0.042788711220379125\n",
      "Train Loss at iteration 4909: 0.04278856850155318\n",
      "Train Loss at iteration 4910: 0.04278842580807787\n",
      "Train Loss at iteration 4911: 0.04278828313994778\n",
      "Train Loss at iteration 4912: 0.04278814049715747\n",
      "Train Loss at iteration 4913: 0.042787997879701584\n",
      "Train Loss at iteration 4914: 0.042787855287574686\n",
      "Train Loss at iteration 4915: 0.04278771272077139\n",
      "Train Loss at iteration 4916: 0.04278757017928629\n",
      "Train Loss at iteration 4917: 0.04278742766311399\n",
      "Train Loss at iteration 4918: 0.042787285172249105\n",
      "Train Loss at iteration 4919: 0.04278714270668623\n",
      "Train Loss at iteration 4920: 0.04278700026641996\n",
      "Train Loss at iteration 4921: 0.04278685785144494\n",
      "Train Loss at iteration 4922: 0.04278671546175574\n",
      "Train Loss at iteration 4923: 0.042786573097347\n",
      "Train Loss at iteration 4924: 0.042786430758213326\n",
      "Train Loss at iteration 4925: 0.042786288444349334\n",
      "Train Loss at iteration 4926: 0.042786146155749646\n",
      "Train Loss at iteration 4927: 0.04278600389240888\n",
      "Train Loss at iteration 4928: 0.04278586165432166\n",
      "Train Loss at iteration 4929: 0.04278571944148261\n",
      "Train Loss at iteration 4930: 0.04278557725388634\n",
      "Train Loss at iteration 4931: 0.0427854350915275\n",
      "Train Loss at iteration 4932: 0.042785292954400686\n",
      "Train Loss at iteration 4933: 0.042785150842500584\n",
      "Train Loss at iteration 4934: 0.042785008755821766\n",
      "Train Loss at iteration 4935: 0.042784866694358915\n",
      "Train Loss at iteration 4936: 0.04278472465810663\n",
      "Train Loss at iteration 4937: 0.04278458264705956\n",
      "Train Loss at iteration 4938: 0.042784440661212335\n",
      "Train Loss at iteration 4939: 0.04278429870055962\n",
      "Train Loss at iteration 4940: 0.04278415676509604\n",
      "Train Loss at iteration 4941: 0.042784014854816255\n",
      "Train Loss at iteration 4942: 0.04278387296971489\n",
      "Train Loss at iteration 4943: 0.0427837311097866\n",
      "Train Loss at iteration 4944: 0.04278358927502605\n",
      "Train Loss at iteration 4945: 0.04278344746542786\n",
      "Train Loss at iteration 4946: 0.0427833056809867\n",
      "Train Loss at iteration 4947: 0.04278316392169723\n",
      "Train Loss at iteration 4948: 0.04278302218755411\n",
      "Train Loss at iteration 4949: 0.04278288047855198\n",
      "Train Loss at iteration 4950: 0.04278273879468551\n",
      "Train Loss at iteration 4951: 0.04278259713594936\n",
      "Train Loss at iteration 4952: 0.04278245550233819\n",
      "Train Loss at iteration 4953: 0.04278231389384668\n",
      "Train Loss at iteration 4954: 0.042782172310469484\n",
      "Train Loss at iteration 4955: 0.042782030752201265\n",
      "Train Loss at iteration 4956: 0.0427818892190367\n",
      "Train Loss at iteration 4957: 0.042781747710970465\n",
      "Train Loss at iteration 4958: 0.042781606227997225\n",
      "Train Loss at iteration 4959: 0.04278146477011168\n",
      "Train Loss at iteration 4960: 0.04278132333730846\n",
      "Train Loss at iteration 4961: 0.04278118192958228\n",
      "Train Loss at iteration 4962: 0.04278104054692781\n",
      "Train Loss at iteration 4963: 0.04278089918933973\n",
      "Train Loss at iteration 4964: 0.042780757856812734\n",
      "Train Loss at iteration 4965: 0.04278061654934149\n",
      "Train Loss at iteration 4966: 0.042780475266920706\n",
      "Train Loss at iteration 4967: 0.042780334009545055\n",
      "Train Loss at iteration 4968: 0.04278019277720922\n",
      "Train Loss at iteration 4969: 0.04278005156990791\n",
      "Train Loss at iteration 4970: 0.04277991038763583\n",
      "Train Loss at iteration 4971: 0.04277976923038765\n",
      "Train Loss at iteration 4972: 0.04277962809815808\n",
      "Train Loss at iteration 4973: 0.04277948699094183\n",
      "Train Loss at iteration 4974: 0.04277934590873357\n",
      "Train Loss at iteration 4975: 0.04277920485152803\n",
      "Train Loss at iteration 4976: 0.04277906381931991\n",
      "Train Loss at iteration 4977: 0.04277892281210389\n",
      "Train Loss at iteration 4978: 0.04277878182987471\n",
      "Train Loss at iteration 4979: 0.04277864087262708\n",
      "Train Loss at iteration 4980: 0.04277849994035568\n",
      "Train Loss at iteration 4981: 0.042778359033055255\n",
      "Train Loss at iteration 4982: 0.0427782181507205\n",
      "Train Loss at iteration 4983: 0.04277807729334614\n",
      "Train Loss at iteration 4984: 0.042777936460926876\n",
      "Train Loss at iteration 4985: 0.04277779565345745\n",
      "Train Loss at iteration 4986: 0.04277765487093258\n",
      "Train Loss at iteration 4987: 0.04277751411334698\n",
      "Train Loss at iteration 4988: 0.042777373380695385\n",
      "Train Loss at iteration 4989: 0.0427772326729725\n",
      "Train Loss at iteration 4990: 0.04277709199017307\n",
      "Train Loss at iteration 4991: 0.042776951332291833\n",
      "Train Loss at iteration 4992: 0.04277681069932349\n",
      "Train Loss at iteration 4993: 0.042776670091262806\n",
      "Train Loss at iteration 4994: 0.04277652950810451\n",
      "Train Loss at iteration 4995: 0.04277638894984332\n",
      "Train Loss at iteration 4996: 0.04277624841647399\n",
      "Train Loss at iteration 4997: 0.04277610790799125\n",
      "Train Loss at iteration 4998: 0.04277596742438985\n",
      "Train Loss at iteration 4999: 0.04277582696566453\n",
      "Train Loss at iteration 5000: 0.04277568653181004\n",
      "Train Loss at iteration 5001: 0.0427755461228211\n",
      "Train Loss at iteration 5002: 0.04277540573869249\n",
      "Train Loss at iteration 5003: 0.04277526537941895\n",
      "Train Loss at iteration 5004: 0.04277512504499523\n",
      "Train Loss at iteration 5005: 0.04277498473541608\n",
      "Train Loss at iteration 5006: 0.04277484445067625\n",
      "Train Loss at iteration 5007: 0.04277470419077053\n",
      "Train Loss at iteration 5008: 0.04277456395569362\n",
      "Train Loss at iteration 5009: 0.042774423745440326\n",
      "Train Loss at iteration 5010: 0.0427742835600054\n",
      "Train Loss at iteration 5011: 0.042774143399383596\n",
      "Train Loss at iteration 5012: 0.042774003263569685\n",
      "Train Loss at iteration 5013: 0.04277386315255843\n",
      "Train Loss at iteration 5014: 0.0427737230663446\n",
      "Train Loss at iteration 5015: 0.04277358300492297\n",
      "Train Loss at iteration 5016: 0.042773442968288305\n",
      "Train Loss at iteration 5017: 0.04277330295643538\n",
      "Train Loss at iteration 5018: 0.04277316296935897\n",
      "Train Loss at iteration 5019: 0.04277302300705386\n",
      "Train Loss at iteration 5020: 0.042772883069514815\n",
      "Train Loss at iteration 5021: 0.042772743156736605\n",
      "Train Loss at iteration 5022: 0.04277260326871405\n",
      "Train Loss at iteration 5023: 0.0427724634054419\n",
      "Train Loss at iteration 5024: 0.04277232356691494\n",
      "Train Loss at iteration 5025: 0.042772183753127975\n",
      "Train Loss at iteration 5026: 0.04277204396407578\n",
      "Train Loss at iteration 5027: 0.04277190419975315\n",
      "Train Loss at iteration 5028: 0.042771764460154876\n",
      "Train Loss at iteration 5029: 0.04277162474527575\n",
      "Train Loss at iteration 5030: 0.04277148505511056\n",
      "Train Loss at iteration 5031: 0.04277134538965411\n",
      "Train Loss at iteration 5032: 0.04277120574890119\n",
      "Train Loss at iteration 5033: 0.042771066132846626\n",
      "Train Loss at iteration 5034: 0.04277092654148519\n",
      "Train Loss at iteration 5035: 0.04277078697481169\n",
      "Train Loss at iteration 5036: 0.04277064743282094\n",
      "Train Loss at iteration 5037: 0.04277050791550774\n",
      "Train Loss at iteration 5038: 0.04277036842286688\n",
      "Train Loss at iteration 5039: 0.04277022895489322\n",
      "Train Loss at iteration 5040: 0.042770089511581524\n",
      "Train Loss at iteration 5041: 0.042769950092926626\n",
      "Train Loss at iteration 5042: 0.04276981069892332\n",
      "Train Loss at iteration 5043: 0.04276967132956645\n",
      "Train Loss at iteration 5044: 0.04276953198485083\n",
      "Train Loss at iteration 5045: 0.042769392664771255\n",
      "Train Loss at iteration 5046: 0.04276925336932256\n",
      "Train Loss at iteration 5047: 0.04276911409849958\n",
      "Train Loss at iteration 5048: 0.042768974852297116\n",
      "Train Loss at iteration 5049: 0.04276883563071001\n",
      "Train Loss at iteration 5050: 0.042768696433733086\n",
      "Train Loss at iteration 5051: 0.042768557261361174\n",
      "Train Loss at iteration 5052: 0.04276841811358911\n",
      "Train Loss at iteration 5053: 0.042768278990411704\n",
      "Train Loss at iteration 5054: 0.04276813989182381\n",
      "Train Loss at iteration 5055: 0.04276800081782026\n",
      "Train Loss at iteration 5056: 0.0427678617683959\n",
      "Train Loss at iteration 5057: 0.042767722743545566\n",
      "Train Loss at iteration 5058: 0.04276758374326407\n",
      "Train Loss at iteration 5059: 0.04276744476754629\n",
      "Train Loss at iteration 5060: 0.04276730581638706\n",
      "Train Loss at iteration 5061: 0.04276716688978122\n",
      "Train Loss at iteration 5062: 0.04276702798772361\n",
      "Train Loss at iteration 5063: 0.04276688911020909\n",
      "Train Loss at iteration 5064: 0.04276675025723251\n",
      "Train Loss at iteration 5065: 0.042766611428788716\n",
      "Train Loss at iteration 5066: 0.042766472624872584\n",
      "Train Loss at iteration 5067: 0.042766333845478934\n",
      "Train Loss at iteration 5068: 0.04276619509060264\n",
      "Train Loss at iteration 5069: 0.04276605636023856\n",
      "Train Loss at iteration 5070: 0.04276591765438156\n",
      "Train Loss at iteration 5071: 0.042765778973026504\n",
      "Train Loss at iteration 5072: 0.04276564031616822\n",
      "Train Loss at iteration 5073: 0.042765501683801624\n",
      "Train Loss at iteration 5074: 0.04276536307592155\n",
      "Train Loss at iteration 5075: 0.04276522449252288\n",
      "Train Loss at iteration 5076: 0.042765085933600476\n",
      "Train Loss at iteration 5077: 0.042764947399149204\n",
      "Train Loss at iteration 5078: 0.042764808889163945\n",
      "Train Loss at iteration 5079: 0.04276467040363958\n",
      "Train Loss at iteration 5080: 0.04276453194257099\n",
      "Train Loss at iteration 5081: 0.04276439350595302\n",
      "Train Loss at iteration 5082: 0.04276425509378059\n",
      "Train Loss at iteration 5083: 0.04276411670604855\n",
      "Train Loss at iteration 5084: 0.0427639783427518\n",
      "Train Loss at iteration 5085: 0.042763840003885216\n",
      "Train Loss at iteration 5086: 0.042763701689443684\n",
      "Train Loss at iteration 5087: 0.0427635633994221\n",
      "Train Loss at iteration 5088: 0.042763425133815335\n",
      "Train Loss at iteration 5089: 0.042763286892618296\n",
      "Train Loss at iteration 5090: 0.042763148675825875\n",
      "Train Loss at iteration 5091: 0.042763010483432964\n",
      "Train Loss at iteration 5092: 0.04276287231543446\n",
      "Train Loss at iteration 5093: 0.04276273417182524\n",
      "Train Loss at iteration 5094: 0.04276259605260024\n",
      "Train Loss at iteration 5095: 0.04276245795775432\n",
      "Train Loss at iteration 5096: 0.042762319887282414\n",
      "Train Loss at iteration 5097: 0.042762181841179414\n",
      "Train Loss at iteration 5098: 0.04276204381944022\n",
      "Train Loss at iteration 5099: 0.04276190582205973\n",
      "Train Loss at iteration 5100: 0.042761767849032885\n",
      "Train Loss at iteration 5101: 0.04276162990035455\n",
      "Train Loss at iteration 5102: 0.042761491976019673\n",
      "Train Loss at iteration 5103: 0.042761354076023154\n",
      "Train Loss at iteration 5104: 0.042761216200359914\n",
      "Train Loss at iteration 5105: 0.04276107834902485\n",
      "Train Loss at iteration 5106: 0.04276094052201291\n",
      "Train Loss at iteration 5107: 0.04276080271931899\n",
      "Train Loss at iteration 5108: 0.042760664940938\n",
      "Train Loss at iteration 5109: 0.042760527186864905\n",
      "Train Loss at iteration 5110: 0.0427603894570946\n",
      "Train Loss at iteration 5111: 0.042760251751622\n",
      "Train Loss at iteration 5112: 0.042760114070442064\n",
      "Train Loss at iteration 5113: 0.042759976413549707\n",
      "Train Loss at iteration 5114: 0.04275983878093983\n",
      "Train Loss at iteration 5115: 0.04275970117260741\n",
      "Train Loss at iteration 5116: 0.042759563588547364\n",
      "Train Loss at iteration 5117: 0.04275942602875463\n",
      "Train Loss at iteration 5118: 0.04275928849322412\n",
      "Train Loss at iteration 5119: 0.04275915098195081\n",
      "Train Loss at iteration 5120: 0.0427590134949296\n",
      "Train Loss at iteration 5121: 0.04275887603215548\n",
      "Train Loss at iteration 5122: 0.042758738593623356\n",
      "Train Loss at iteration 5123: 0.042758601179328184\n",
      "Train Loss at iteration 5124: 0.04275846378926491\n",
      "Train Loss at iteration 5125: 0.04275832642342847\n",
      "Train Loss at iteration 5126: 0.04275818908181384\n",
      "Train Loss at iteration 5127: 0.04275805176441595\n",
      "Train Loss at iteration 5128: 0.042757914471229765\n",
      "Train Loss at iteration 5129: 0.04275777720225021\n",
      "Train Loss at iteration 5130: 0.042757639957472296\n",
      "Train Loss at iteration 5131: 0.042757502736890926\n",
      "Train Loss at iteration 5132: 0.042757365540501094\n",
      "Train Loss at iteration 5133: 0.042757228368297734\n",
      "Train Loss at iteration 5134: 0.04275709122027583\n",
      "Train Loss at iteration 5135: 0.04275695409643032\n",
      "Train Loss at iteration 5136: 0.0427568169967562\n",
      "Train Loss at iteration 5137: 0.04275667992124841\n",
      "Train Loss at iteration 5138: 0.04275654286990194\n",
      "Train Loss at iteration 5139: 0.04275640584271175\n",
      "Train Loss at iteration 5140: 0.04275626883967281\n",
      "Train Loss at iteration 5141: 0.042756131860780075\n",
      "Train Loss at iteration 5142: 0.04275599490602857\n",
      "Train Loss at iteration 5143: 0.04275585797541322\n",
      "Train Loss at iteration 5144: 0.04275572106892902\n",
      "Train Loss at iteration 5145: 0.042755584186570945\n",
      "Train Loss at iteration 5146: 0.042755447328334\n",
      "Train Loss at iteration 5147: 0.042755310494213154\n",
      "Train Loss at iteration 5148: 0.042755173684203354\n",
      "Train Loss at iteration 5149: 0.042755036898299646\n",
      "Train Loss at iteration 5150: 0.042754900136496965\n",
      "Train Loss at iteration 5151: 0.042754763398790335\n",
      "Train Loss at iteration 5152: 0.042754626685174726\n",
      "Train Loss at iteration 5153: 0.04275448999564514\n",
      "Train Loss at iteration 5154: 0.04275435333019658\n",
      "Train Loss at iteration 5155: 0.042754216688824016\n",
      "Train Loss at iteration 5156: 0.04275408007152245\n",
      "Train Loss at iteration 5157: 0.0427539434782869\n",
      "Train Loss at iteration 5158: 0.042753806909112325\n",
      "Train Loss at iteration 5159: 0.042753670363993766\n",
      "Train Loss at iteration 5160: 0.042753533842926225\n",
      "Train Loss at iteration 5161: 0.04275339734590467\n",
      "Train Loss at iteration 5162: 0.04275326087292414\n",
      "Train Loss at iteration 5163: 0.04275312442397963\n",
      "Train Loss at iteration 5164: 0.04275298799906614\n",
      "Train Loss at iteration 5165: 0.0427528515981787\n",
      "Train Loss at iteration 5166: 0.04275271522131229\n",
      "Train Loss at iteration 5167: 0.04275257886846196\n",
      "Train Loss at iteration 5168: 0.04275244253962271\n",
      "Train Loss at iteration 5169: 0.042752306234789546\n",
      "Train Loss at iteration 5170: 0.042752169953957496\n",
      "Train Loss at iteration 5171: 0.04275203369712159\n",
      "Train Loss at iteration 5172: 0.04275189746427682\n",
      "Train Loss at iteration 5173: 0.04275176125541824\n",
      "Train Loss at iteration 5174: 0.042751625070540845\n",
      "Train Loss at iteration 5175: 0.04275148890963968\n",
      "Train Loss at iteration 5176: 0.042751352772709764\n",
      "Train Loss at iteration 5177: 0.04275121665974613\n",
      "Train Loss at iteration 5178: 0.0427510805707438\n",
      "Train Loss at iteration 5179: 0.04275094450569782\n",
      "Train Loss at iteration 5180: 0.04275080846460322\n",
      "Train Loss at iteration 5181: 0.042750672447455014\n",
      "Train Loss at iteration 5182: 0.04275053645424826\n",
      "Train Loss at iteration 5183: 0.04275040048497798\n",
      "Train Loss at iteration 5184: 0.04275026453963924\n",
      "Train Loss at iteration 5185: 0.042750128618227065\n",
      "Train Loss at iteration 5186: 0.04274999272073648\n",
      "Train Loss at iteration 5187: 0.042749856847162544\n",
      "Train Loss at iteration 5188: 0.042749720997500316\n",
      "Train Loss at iteration 5189: 0.04274958517174482\n",
      "Train Loss at iteration 5190: 0.04274944936989111\n",
      "Train Loss at iteration 5191: 0.04274931359193425\n",
      "Train Loss at iteration 5192: 0.04274917783786928\n",
      "Train Loss at iteration 5193: 0.042749042107691246\n",
      "Train Loss at iteration 5194: 0.04274890640139522\n",
      "Train Loss at iteration 5195: 0.04274877071897624\n",
      "Train Loss at iteration 5196: 0.04274863506042938\n",
      "Train Loss at iteration 5197: 0.04274849942574968\n",
      "Train Loss at iteration 5198: 0.04274836381493222\n",
      "Train Loss at iteration 5199: 0.042748228227972064\n",
      "Train Loss at iteration 5200: 0.04274809266486424\n",
      "Train Loss at iteration 5201: 0.04274795712560387\n",
      "Train Loss at iteration 5202: 0.042747821610185956\n",
      "Train Loss at iteration 5203: 0.042747686118605625\n",
      "Train Loss at iteration 5204: 0.04274755065085791\n",
      "Train Loss at iteration 5205: 0.04274741520693789\n",
      "Train Loss at iteration 5206: 0.04274727978684065\n",
      "Train Loss at iteration 5207: 0.04274714439056125\n",
      "Train Loss at iteration 5208: 0.042747009018094764\n",
      "Train Loss at iteration 5209: 0.04274687366943629\n",
      "Train Loss at iteration 5210: 0.04274673834458088\n",
      "Train Loss at iteration 5211: 0.04274660304352363\n",
      "Train Loss at iteration 5212: 0.04274646776625962\n",
      "Train Loss at iteration 5213: 0.04274633251278393\n",
      "Train Loss at iteration 5214: 0.042746197283091654\n",
      "Train Loss at iteration 5215: 0.042746062077177856\n",
      "Train Loss at iteration 5216: 0.04274592689503765\n",
      "Train Loss at iteration 5217: 0.0427457917366661\n",
      "Train Loss at iteration 5218: 0.04274565660205832\n",
      "Train Loss at iteration 5219: 0.04274552149120939\n",
      "Train Loss at iteration 5220: 0.04274538640411441\n",
      "Train Loss at iteration 5221: 0.04274525134076847\n",
      "Train Loss at iteration 5222: 0.04274511630116666\n",
      "Train Loss at iteration 5223: 0.04274498128530408\n",
      "Train Loss at iteration 5224: 0.04274484629317587\n",
      "Train Loss at iteration 5225: 0.04274471132477706\n",
      "Train Loss at iteration 5226: 0.04274457638010281\n",
      "Train Loss at iteration 5227: 0.0427444414591482\n",
      "Train Loss at iteration 5228: 0.042744306561908335\n",
      "Train Loss at iteration 5229: 0.042744171688378324\n",
      "Train Loss at iteration 5230: 0.04274403683855329\n",
      "Train Loss at iteration 5231: 0.042743902012428336\n",
      "Train Loss at iteration 5232: 0.042743767209998555\n",
      "Train Loss at iteration 5233: 0.04274363243125908\n",
      "Train Loss at iteration 5234: 0.04274349767620502\n",
      "Train Loss at iteration 5235: 0.042743362944831484\n",
      "Train Loss at iteration 5236: 0.042743228237133606\n",
      "Train Loss at iteration 5237: 0.04274309355310649\n",
      "Train Loss at iteration 5238: 0.04274295889274528\n",
      "Train Loss at iteration 5239: 0.04274282425604506\n",
      "Train Loss at iteration 5240: 0.042742689643000986\n",
      "Train Loss at iteration 5241: 0.04274255505360815\n",
      "Train Loss at iteration 5242: 0.04274242048786173\n",
      "Train Loss at iteration 5243: 0.04274228594575681\n",
      "Train Loss at iteration 5244: 0.04274215142728853\n",
      "Train Loss at iteration 5245: 0.042742016932452026\n",
      "Train Loss at iteration 5246: 0.042741882461242435\n",
      "Train Loss at iteration 5247: 0.04274174801365488\n",
      "Train Loss at iteration 5248: 0.04274161358968451\n",
      "Train Loss at iteration 5249: 0.042741479189326435\n",
      "Train Loss at iteration 5250: 0.042741344812575834\n",
      "Train Loss at iteration 5251: 0.0427412104594278\n",
      "Train Loss at iteration 5252: 0.04274107612987752\n",
      "Train Loss at iteration 5253: 0.0427409418239201\n",
      "Train Loss at iteration 5254: 0.04274080754155071\n",
      "Train Loss at iteration 5255: 0.042740673282764485\n",
      "Train Loss at iteration 5256: 0.04274053904755657\n",
      "Train Loss at iteration 5257: 0.04274040483592212\n",
      "Train Loss at iteration 5258: 0.04274027064785628\n",
      "Train Loss at iteration 5259: 0.0427401364833542\n",
      "Train Loss at iteration 5260: 0.04274000234241104\n",
      "Train Loss at iteration 5261: 0.04273986822502196\n",
      "Train Loss at iteration 5262: 0.042739734131182094\n",
      "Train Loss at iteration 5263: 0.04273960006088661\n",
      "Train Loss at iteration 5264: 0.04273946601413068\n",
      "Train Loss at iteration 5265: 0.04273933199090945\n",
      "Train Loss at iteration 5266: 0.04273919799121808\n",
      "Train Loss at iteration 5267: 0.042739064015051745\n",
      "Train Loss at iteration 5268: 0.04273893006240561\n",
      "Train Loss at iteration 5269: 0.04273879613327483\n",
      "Train Loss at iteration 5270: 0.04273866222765457\n",
      "Train Loss at iteration 5271: 0.04273852834554002\n",
      "Train Loss at iteration 5272: 0.04273839448692633\n",
      "Train Loss at iteration 5273: 0.0427382606518087\n",
      "Train Loss at iteration 5274: 0.042738126840182265\n",
      "Train Loss at iteration 5275: 0.04273799305204223\n",
      "Train Loss at iteration 5276: 0.04273785928738375\n",
      "Train Loss at iteration 5277: 0.04273772554620202\n",
      "Train Loss at iteration 5278: 0.04273759182849222\n",
      "Train Loss at iteration 5279: 0.042737458134249506\n",
      "Train Loss at iteration 5280: 0.04273732446346909\n",
      "Train Loss at iteration 5281: 0.04273719081614614\n",
      "Train Loss at iteration 5282: 0.04273705719227586\n",
      "Train Loss at iteration 5283: 0.04273692359185341\n",
      "Train Loss at iteration 5284: 0.042736790014873995\n",
      "Train Loss at iteration 5285: 0.0427366564613328\n",
      "Train Loss at iteration 5286: 0.04273652293122502\n",
      "Train Loss at iteration 5287: 0.04273638942454583\n",
      "Train Loss at iteration 5288: 0.04273625594129044\n",
      "Train Loss at iteration 5289: 0.042736122481454066\n",
      "Train Loss at iteration 5290: 0.04273598904503186\n",
      "Train Loss at iteration 5291: 0.042735855632019044\n",
      "Train Loss at iteration 5292: 0.04273572224241083\n",
      "Train Loss at iteration 5293: 0.0427355888762024\n",
      "Train Loss at iteration 5294: 0.042735455533388964\n",
      "Train Loss at iteration 5295: 0.04273532221396572\n",
      "Train Loss at iteration 5296: 0.042735188917927894\n",
      "Train Loss at iteration 5297: 0.04273505564527066\n",
      "Train Loss at iteration 5298: 0.04273492239598925\n",
      "Train Loss at iteration 5299: 0.04273478917007886\n",
      "Train Loss at iteration 5300: 0.042734655967534715\n",
      "Train Loss at iteration 5301: 0.04273452278835203\n",
      "Train Loss at iteration 5302: 0.042734389632526\n",
      "Train Loss at iteration 5303: 0.04273425650005186\n",
      "Train Loss at iteration 5304: 0.04273412339092481\n",
      "Train Loss at iteration 5305: 0.042733990305140086\n",
      "Train Loss at iteration 5306: 0.042733857242692894\n",
      "Train Loss at iteration 5307: 0.04273372420357846\n",
      "Train Loss at iteration 5308: 0.04273359118779202\n",
      "Train Loss at iteration 5309: 0.04273345819532877\n",
      "Train Loss at iteration 5310: 0.042733325226183956\n",
      "Train Loss at iteration 5311: 0.042733192280352814\n",
      "Train Loss at iteration 5312: 0.042733059357830554\n",
      "Train Loss at iteration 5313: 0.042732926458612396\n",
      "Train Loss at iteration 5314: 0.0427327935826936\n",
      "Train Loss at iteration 5315: 0.04273266073006939\n",
      "Train Loss at iteration 5316: 0.042732527900734986\n",
      "Train Loss at iteration 5317: 0.04273239509468565\n",
      "Train Loss at iteration 5318: 0.0427322623119166\n",
      "Train Loss at iteration 5319: 0.04273212955242306\n",
      "Train Loss at iteration 5320: 0.0427319968162003\n",
      "Train Loss at iteration 5321: 0.04273186410324356\n",
      "Train Loss at iteration 5322: 0.04273173141354806\n",
      "Train Loss at iteration 5323: 0.04273159874710906\n",
      "Train Loss at iteration 5324: 0.04273146610392179\n",
      "Train Loss at iteration 5325: 0.04273133348398153\n",
      "Train Loss at iteration 5326: 0.04273120088728351\n",
      "Train Loss at iteration 5327: 0.04273106831382296\n",
      "Train Loss at iteration 5328: 0.042730935763595164\n",
      "Train Loss at iteration 5329: 0.04273080323659535\n",
      "Train Loss at iteration 5330: 0.04273067073281878\n",
      "Train Loss at iteration 5331: 0.04273053825226073\n",
      "Train Loss at iteration 5332: 0.04273040579491643\n",
      "Train Loss at iteration 5333: 0.042730273360781154\n",
      "Train Loss at iteration 5334: 0.04273014094985015\n",
      "Train Loss at iteration 5335: 0.04273000856211869\n",
      "Train Loss at iteration 5336: 0.042729876197582034\n",
      "Train Loss at iteration 5337: 0.04272974385623544\n",
      "Train Loss at iteration 5338: 0.042729611538074176\n",
      "Train Loss at iteration 5339: 0.04272947924309352\n",
      "Train Loss at iteration 5340: 0.04272934697128873\n",
      "Train Loss at iteration 5341: 0.042729214722655076\n",
      "Train Loss at iteration 5342: 0.042729082497187834\n",
      "Train Loss at iteration 5343: 0.042728950294882266\n",
      "Train Loss at iteration 5344: 0.04272881811573367\n",
      "Train Loss at iteration 5345: 0.04272868595973729\n",
      "Train Loss at iteration 5346: 0.04272855382688843\n",
      "Train Loss at iteration 5347: 0.04272842171718234\n",
      "Train Loss at iteration 5348: 0.04272828963061433\n",
      "Train Loss at iteration 5349: 0.04272815756717966\n",
      "Train Loss at iteration 5350: 0.042728025526873624\n",
      "Train Loss at iteration 5351: 0.042727893509691514\n",
      "Train Loss at iteration 5352: 0.0427277615156286\n",
      "Train Loss at iteration 5353: 0.04272762954468017\n",
      "Train Loss at iteration 5354: 0.0427274975968415\n",
      "Train Loss at iteration 5355: 0.042727365672107914\n",
      "Train Loss at iteration 5356: 0.042727233770474674\n",
      "Train Loss at iteration 5357: 0.04272710189193708\n",
      "Train Loss at iteration 5358: 0.04272697003649043\n",
      "Train Loss at iteration 5359: 0.04272683820413002\n",
      "Train Loss at iteration 5360: 0.04272670639485115\n",
      "Train Loss at iteration 5361: 0.0427265746086491\n",
      "Train Loss at iteration 5362: 0.042726442845519186\n",
      "Train Loss at iteration 5363: 0.0427263111054567\n",
      "Train Loss at iteration 5364: 0.04272617938845695\n",
      "Train Loss at iteration 5365: 0.04272604769451524\n",
      "Train Loss at iteration 5366: 0.04272591602362688\n",
      "Train Loss at iteration 5367: 0.042725784375787154\n",
      "Train Loss at iteration 5368: 0.04272565275099138\n",
      "Train Loss at iteration 5369: 0.04272552114923489\n",
      "Train Loss at iteration 5370: 0.042725389570512966\n",
      "Train Loss at iteration 5371: 0.04272525801482094\n",
      "Train Loss at iteration 5372: 0.0427251264821541\n",
      "Train Loss at iteration 5373: 0.04272499497250778\n",
      "Train Loss at iteration 5374: 0.042724863485877305\n",
      "Train Loss at iteration 5375: 0.04272473202225797\n",
      "Train Loss at iteration 5376: 0.04272460058164511\n",
      "Train Loss at iteration 5377: 0.04272446916403404\n",
      "Train Loss at iteration 5378: 0.04272433776942008\n",
      "Train Loss at iteration 5379: 0.042724206397798545\n",
      "Train Loss at iteration 5380: 0.04272407504916478\n",
      "Train Loss at iteration 5381: 0.04272394372351408\n",
      "Train Loss at iteration 5382: 0.04272381242084182\n",
      "Train Loss at iteration 5383: 0.042723681141143284\n",
      "Train Loss at iteration 5384: 0.042723549884413826\n",
      "Train Loss at iteration 5385: 0.04272341865064876\n",
      "Train Loss at iteration 5386: 0.04272328743984344\n",
      "Train Loss at iteration 5387: 0.04272315625199318\n",
      "Train Loss at iteration 5388: 0.04272302508709334\n",
      "Train Loss at iteration 5389: 0.04272289394513922\n",
      "Train Loss at iteration 5390: 0.042722762826126186\n",
      "Train Loss at iteration 5391: 0.04272263173004958\n",
      "Train Loss at iteration 5392: 0.042722500656904736\n",
      "Train Loss at iteration 5393: 0.04272236960668699\n",
      "Train Loss at iteration 5394: 0.04272223857939168\n",
      "Train Loss at iteration 5395: 0.04272210757501418\n",
      "Train Loss at iteration 5396: 0.04272197659354981\n",
      "Train Loss at iteration 5397: 0.04272184563499391\n",
      "Train Loss at iteration 5398: 0.04272171469934187\n",
      "Train Loss at iteration 5399: 0.042721583786589\n",
      "Train Loss at iteration 5400: 0.04272145289673069\n",
      "Train Loss at iteration 5401: 0.042721322029762254\n",
      "Train Loss at iteration 5402: 0.04272119118567907\n",
      "Train Loss at iteration 5403: 0.042721060364476496\n",
      "Train Loss at iteration 5404: 0.04272092956614987\n",
      "Train Loss at iteration 5405: 0.04272079879069458\n",
      "Train Loss at iteration 5406: 0.04272066803810595\n",
      "Train Loss at iteration 5407: 0.04272053730837938\n",
      "Train Loss at iteration 5408: 0.0427204066015102\n",
      "Train Loss at iteration 5409: 0.04272027591749379\n",
      "Train Loss at iteration 5410: 0.042720145256325524\n",
      "Train Loss at iteration 5411: 0.042720014618000755\n",
      "Train Loss at iteration 5412: 0.042719884002514856\n",
      "Train Loss at iteration 5413: 0.042719753409863205\n",
      "Train Loss at iteration 5414: 0.04271962284004116\n",
      "Train Loss at iteration 5415: 0.042719492293044094\n",
      "Train Loss at iteration 5416: 0.0427193617688674\n",
      "Train Loss at iteration 5417: 0.04271923126750643\n",
      "Train Loss at iteration 5418: 0.04271910078895656\n",
      "Train Loss at iteration 5419: 0.042718970333213195\n",
      "Train Loss at iteration 5420: 0.04271883990027169\n",
      "Train Loss at iteration 5421: 0.042718709490127446\n",
      "Train Loss at iteration 5422: 0.042718579102775815\n",
      "Train Loss at iteration 5423: 0.042718448738212206\n",
      "Train Loss at iteration 5424: 0.04271831839643199\n",
      "Train Loss at iteration 5425: 0.04271818807743057\n",
      "Train Loss at iteration 5426: 0.04271805778120332\n",
      "Train Loss at iteration 5427: 0.042717927507745626\n",
      "Train Loss at iteration 5428: 0.04271779725705289\n",
      "Train Loss at iteration 5429: 0.0427176670291205\n",
      "Train Loss at iteration 5430: 0.04271753682394384\n",
      "Train Loss at iteration 5431: 0.0427174066415183\n",
      "Train Loss at iteration 5432: 0.04271727648183931\n",
      "Train Loss at iteration 5433: 0.042717146344902236\n",
      "Train Loss at iteration 5434: 0.04271701623070248\n",
      "Train Loss at iteration 5435: 0.042716886139235445\n",
      "Train Loss at iteration 5436: 0.042716756070496534\n",
      "Train Loss at iteration 5437: 0.04271662602448115\n",
      "Train Loss at iteration 5438: 0.04271649600118469\n",
      "Train Loss at iteration 5439: 0.042716366000602565\n",
      "Train Loss at iteration 5440: 0.0427162360227302\n",
      "Train Loss at iteration 5441: 0.04271610606756296\n",
      "Train Loss at iteration 5442: 0.04271597613509629\n",
      "Train Loss at iteration 5443: 0.04271584622532558\n",
      "Train Loss at iteration 5444: 0.04271571633824625\n",
      "Train Loss at iteration 5445: 0.04271558647385373\n",
      "Train Loss at iteration 5446: 0.0427154566321434\n",
      "Train Loss at iteration 5447: 0.0427153268131107\n",
      "Train Loss at iteration 5448: 0.04271519701675104\n",
      "Train Loss at iteration 5449: 0.04271506724305985\n",
      "Train Loss at iteration 5450: 0.04271493749203253\n",
      "Train Loss at iteration 5451: 0.04271480776366451\n",
      "Train Loss at iteration 5452: 0.04271467805795121\n",
      "Train Loss at iteration 5453: 0.04271454837488807\n",
      "Train Loss at iteration 5454: 0.042714418714470494\n",
      "Train Loss at iteration 5455: 0.04271428907669392\n",
      "Train Loss at iteration 5456: 0.04271415946155377\n",
      "Train Loss at iteration 5457: 0.04271402986904548\n",
      "Train Loss at iteration 5458: 0.04271390029916449\n",
      "Train Loss at iteration 5459: 0.042713770751906195\n",
      "Train Loss at iteration 5460: 0.04271364122726608\n",
      "Train Loss at iteration 5461: 0.042713511725239524\n",
      "Train Loss at iteration 5462: 0.04271338224582202\n",
      "Train Loss at iteration 5463: 0.042713252789008956\n",
      "Train Loss at iteration 5464: 0.04271312335479579\n",
      "Train Loss at iteration 5465: 0.04271299394317798\n",
      "Train Loss at iteration 5466: 0.04271286455415093\n",
      "Train Loss at iteration 5467: 0.04271273518771011\n",
      "Train Loss at iteration 5468: 0.04271260584385096\n",
      "Train Loss at iteration 5469: 0.04271247652256894\n",
      "Train Loss at iteration 5470: 0.042712347223859456\n",
      "Train Loss at iteration 5471: 0.042712217947717984\n",
      "Train Loss at iteration 5472: 0.04271208869413998\n",
      "Train Loss at iteration 5473: 0.04271195946312086\n",
      "Train Loss at iteration 5474: 0.042711830254656115\n",
      "Train Loss at iteration 5475: 0.042711701068741184\n",
      "Train Loss at iteration 5476: 0.04271157190537151\n",
      "Train Loss at iteration 5477: 0.04271144276454257\n",
      "Train Loss at iteration 5478: 0.0427113136462498\n",
      "Train Loss at iteration 5479: 0.04271118455048868\n",
      "Train Loss at iteration 5480: 0.04271105547725465\n",
      "Train Loss at iteration 5481: 0.042710926426543186\n",
      "Train Loss at iteration 5482: 0.04271079739834974\n",
      "Train Loss at iteration 5483: 0.04271066839266977\n",
      "Train Loss at iteration 5484: 0.04271053940949877\n",
      "Train Loss at iteration 5485: 0.04271041044883217\n",
      "Train Loss at iteration 5486: 0.042710281510665464\n",
      "Train Loss at iteration 5487: 0.04271015259499411\n",
      "Train Loss at iteration 5488: 0.042710023701813586\n",
      "Train Loss at iteration 5489: 0.042709894831119345\n",
      "Train Loss at iteration 5490: 0.04270976598290689\n",
      "Train Loss at iteration 5491: 0.04270963715717167\n",
      "Train Loss at iteration 5492: 0.04270950835390918\n",
      "Train Loss at iteration 5493: 0.042709379573114874\n",
      "Train Loss at iteration 5494: 0.04270925081478425\n",
      "Train Loss at iteration 5495: 0.04270912207891278\n",
      "Train Loss at iteration 5496: 0.04270899336549595\n",
      "Train Loss at iteration 5497: 0.04270886467452923\n",
      "Train Loss at iteration 5498: 0.04270873600600812\n",
      "Train Loss at iteration 5499: 0.04270860735992809\n",
      "Train Loss at iteration 5500: 0.04270847873628463\n",
      "Train Loss at iteration 5501: 0.042708350135073236\n",
      "Train Loss at iteration 5502: 0.042708221556289394\n",
      "Train Loss at iteration 5503: 0.04270809299992859\n",
      "Train Loss at iteration 5504: 0.0427079644659863\n",
      "Train Loss at iteration 5505: 0.04270783595445806\n",
      "Train Loss at iteration 5506: 0.04270770746533932\n",
      "Train Loss at iteration 5507: 0.042707578998625595\n",
      "Train Loss at iteration 5508: 0.04270745055431238\n",
      "Train Loss at iteration 5509: 0.04270732213239517\n",
      "Train Loss at iteration 5510: 0.04270719373286947\n",
      "Train Loss at iteration 5511: 0.04270706535573078\n",
      "Train Loss at iteration 5512: 0.042706937000974596\n",
      "Train Loss at iteration 5513: 0.04270680866859642\n",
      "Train Loss at iteration 5514: 0.04270668035859176\n",
      "Train Loss at iteration 5515: 0.04270655207095613\n",
      "Train Loss at iteration 5516: 0.04270642380568503\n",
      "Train Loss at iteration 5517: 0.04270629556277396\n",
      "Train Loss at iteration 5518: 0.042706167342218444\n",
      "Train Loss at iteration 5519: 0.04270603914401398\n",
      "Train Loss at iteration 5520: 0.04270591096815609\n",
      "Train Loss at iteration 5521: 0.04270578281464029\n",
      "Train Loss at iteration 5522: 0.042705654683462074\n",
      "Train Loss at iteration 5523: 0.042705526574617\n",
      "Train Loss at iteration 5524: 0.04270539848810053\n",
      "Train Loss at iteration 5525: 0.04270527042390822\n",
      "Train Loss at iteration 5526: 0.04270514238203558\n",
      "Train Loss at iteration 5527: 0.04270501436247814\n",
      "Train Loss at iteration 5528: 0.04270488636523141\n",
      "Train Loss at iteration 5529: 0.04270475839029093\n",
      "Train Loss at iteration 5530: 0.04270463043765219\n",
      "Train Loss at iteration 5531: 0.04270450250731076\n",
      "Train Loss at iteration 5532: 0.04270437459926215\n",
      "Train Loss at iteration 5533: 0.042704246713501884\n",
      "Train Loss at iteration 5534: 0.04270411885002549\n",
      "Train Loss at iteration 5535: 0.04270399100882851\n",
      "Train Loss at iteration 5536: 0.04270386318990648\n",
      "Train Loss at iteration 5537: 0.042703735393254914\n",
      "Train Loss at iteration 5538: 0.04270360761886937\n",
      "Train Loss at iteration 5539: 0.04270347986674538\n",
      "Train Loss at iteration 5540: 0.04270335213687847\n",
      "Train Loss at iteration 5541: 0.04270322442926418\n",
      "Train Loss at iteration 5542: 0.04270309674389807\n",
      "Train Loss at iteration 5543: 0.04270296908077566\n",
      "Train Loss at iteration 5544: 0.04270284143989251\n",
      "Train Loss at iteration 5545: 0.04270271382124416\n",
      "Train Loss at iteration 5546: 0.042702586224826145\n",
      "Train Loss at iteration 5547: 0.04270245865063402\n",
      "Train Loss at iteration 5548: 0.04270233109866333\n",
      "Train Loss at iteration 5549: 0.042702203568909636\n",
      "Train Loss at iteration 5550: 0.04270207606136846\n",
      "Train Loss at iteration 5551: 0.04270194857603541\n",
      "Train Loss at iteration 5552: 0.042701821112905984\n",
      "Train Loss at iteration 5553: 0.04270169367197575\n",
      "Train Loss at iteration 5554: 0.042701566253240286\n",
      "Train Loss at iteration 5555: 0.04270143885669513\n",
      "Train Loss at iteration 5556: 0.04270131148233584\n",
      "Train Loss at iteration 5557: 0.042701184130157985\n",
      "Train Loss at iteration 5558: 0.042701056800157115\n",
      "Train Loss at iteration 5559: 0.042700929492328815\n",
      "Train Loss at iteration 5560: 0.04270080220666862\n",
      "Train Loss at iteration 5561: 0.042700674943172125\n",
      "Train Loss at iteration 5562: 0.04270054770183486\n",
      "Train Loss at iteration 5563: 0.042700420482652414\n",
      "Train Loss at iteration 5564: 0.04270029328562036\n",
      "Train Loss at iteration 5565: 0.04270016611073426\n",
      "Train Loss at iteration 5566: 0.04270003895798969\n",
      "Train Loss at iteration 5567: 0.042699911827382205\n",
      "Train Loss at iteration 5568: 0.04269978471890741\n",
      "Train Loss at iteration 5569: 0.042699657632560846\n",
      "Train Loss at iteration 5570: 0.04269953056833812\n",
      "Train Loss at iteration 5571: 0.0426994035262348\n",
      "Train Loss at iteration 5572: 0.042699276506246456\n",
      "Train Loss at iteration 5573: 0.042699149508368665\n",
      "Train Loss at iteration 5574: 0.042699022532597025\n",
      "Train Loss at iteration 5575: 0.042698895578927115\n",
      "Train Loss at iteration 5576: 0.04269876864735451\n",
      "Train Loss at iteration 5577: 0.042698641737874785\n",
      "Train Loss at iteration 5578: 0.042698514850483565\n",
      "Train Loss at iteration 5579: 0.0426983879851764\n",
      "Train Loss at iteration 5580: 0.042698261141948905\n",
      "Train Loss at iteration 5581: 0.04269813432079663\n",
      "Train Loss at iteration 5582: 0.04269800752171521\n",
      "Train Loss at iteration 5583: 0.042697880744700226\n",
      "Train Loss at iteration 5584: 0.04269775398974728\n",
      "Train Loss at iteration 5585: 0.042697627256851944\n",
      "Train Loss at iteration 5586: 0.04269750054600982\n",
      "Train Loss at iteration 5587: 0.04269737385721652\n",
      "Train Loss at iteration 5588: 0.042697247190467635\n",
      "Train Loss at iteration 5589: 0.04269712054575876\n",
      "Train Loss at iteration 5590: 0.04269699392308551\n",
      "Train Loss at iteration 5591: 0.04269686732244348\n",
      "Train Loss at iteration 5592: 0.04269674074382827\n",
      "Train Loss at iteration 5593: 0.04269661418723549\n",
      "Train Loss at iteration 5594: 0.04269648765266075\n",
      "Train Loss at iteration 5595: 0.04269636114009965\n",
      "Train Loss at iteration 5596: 0.0426962346495478\n",
      "Train Loss at iteration 5597: 0.042696108181000815\n",
      "Train Loss at iteration 5598: 0.042695981734454304\n",
      "Train Loss at iteration 5599: 0.04269585530990388\n",
      "Train Loss at iteration 5600: 0.04269572890734517\n",
      "Train Loss at iteration 5601: 0.04269560252677377\n",
      "Train Loss at iteration 5602: 0.042695476168185295\n",
      "Train Loss at iteration 5603: 0.04269534983157538\n",
      "Train Loss at iteration 5604: 0.04269522351693964\n",
      "Train Loss at iteration 5605: 0.04269509722427369\n",
      "Train Loss at iteration 5606: 0.04269497095357314\n",
      "Train Loss at iteration 5607: 0.04269484470483363\n",
      "Train Loss at iteration 5608: 0.04269471847805079\n",
      "Train Loss at iteration 5609: 0.04269459227322024\n",
      "Train Loss at iteration 5610: 0.04269446609033759\n",
      "Train Loss at iteration 5611: 0.04269433992939849\n",
      "Train Loss at iteration 5612: 0.04269421379039855\n",
      "Train Loss at iteration 5613: 0.04269408767333341\n",
      "Train Loss at iteration 5614: 0.04269396157819869\n",
      "Train Loss at iteration 5615: 0.04269383550499004\n",
      "Train Loss at iteration 5616: 0.0426937094537031\n",
      "Train Loss at iteration 5617: 0.042693583424333485\n",
      "Train Loss at iteration 5618: 0.04269345741687684\n",
      "Train Loss at iteration 5619: 0.0426933314313288\n",
      "Train Loss at iteration 5620: 0.04269320546768501\n",
      "Train Loss at iteration 5621: 0.04269307952594109\n",
      "Train Loss at iteration 5622: 0.042692953606092716\n",
      "Train Loss at iteration 5623: 0.042692827708135504\n",
      "Train Loss at iteration 5624: 0.04269270183206512\n",
      "Train Loss at iteration 5625: 0.042692575977877185\n",
      "Train Loss at iteration 5626: 0.04269245014556736\n",
      "Train Loss at iteration 5627: 0.042692324335131274\n",
      "Train Loss at iteration 5628: 0.0426921985465646\n",
      "Train Loss at iteration 5629: 0.042692072779862984\n",
      "Train Loss at iteration 5630: 0.04269194703502207\n",
      "Train Loss at iteration 5631: 0.042691821312037505\n",
      "Train Loss at iteration 5632: 0.042691695610904946\n",
      "Train Loss at iteration 5633: 0.042691569931620064\n",
      "Train Loss at iteration 5634: 0.042691444274178494\n",
      "Train Loss at iteration 5635: 0.04269131863857591\n",
      "Train Loss at iteration 5636: 0.04269119302480798\n",
      "Train Loss at iteration 5637: 0.04269106743287032\n",
      "Train Loss at iteration 5638: 0.04269094186275863\n",
      "Train Loss at iteration 5639: 0.04269081631446857\n",
      "Train Loss at iteration 5640: 0.042690690787995776\n",
      "Train Loss at iteration 5641: 0.04269056528333594\n",
      "Train Loss at iteration 5642: 0.04269043980048473\n",
      "Train Loss at iteration 5643: 0.0426903143394378\n",
      "Train Loss at iteration 5644: 0.04269018890019081\n",
      "Train Loss at iteration 5645: 0.04269006348273945\n",
      "Train Loss at iteration 5646: 0.04268993808707938\n",
      "Train Loss at iteration 5647: 0.04268981271320628\n",
      "Train Loss at iteration 5648: 0.04268968736111581\n",
      "Train Loss at iteration 5649: 0.042689562030803666\n",
      "Train Loss at iteration 5650: 0.0426894367222655\n",
      "Train Loss at iteration 5651: 0.042689311435497014\n",
      "Train Loss at iteration 5652: 0.042689186170493845\n",
      "Train Loss at iteration 5653: 0.04268906092725172\n",
      "Train Loss at iteration 5654: 0.04268893570576629\n",
      "Train Loss at iteration 5655: 0.04268881050603326\n",
      "Train Loss at iteration 5656: 0.04268868532804828\n",
      "Train Loss at iteration 5657: 0.04268856017180707\n",
      "Train Loss at iteration 5658: 0.042688435037305295\n",
      "Train Loss at iteration 5659: 0.04268830992453865\n",
      "Train Loss at iteration 5660: 0.04268818483350281\n",
      "Train Loss at iteration 5661: 0.042688059764193494\n",
      "Train Loss at iteration 5662: 0.042687934716606354\n",
      "Train Loss at iteration 5663: 0.0426878096907371\n",
      "Train Loss at iteration 5664: 0.04268768468658143\n",
      "Train Loss at iteration 5665: 0.04268755970413503\n",
      "Train Loss at iteration 5666: 0.04268743474339361\n",
      "Train Loss at iteration 5667: 0.04268730980435285\n",
      "Train Loss at iteration 5668: 0.04268718488700844\n",
      "Train Loss at iteration 5669: 0.04268705999135611\n",
      "Train Loss at iteration 5670: 0.04268693511739152\n",
      "Train Loss at iteration 5671: 0.04268681026511041\n",
      "Train Loss at iteration 5672: 0.04268668543450847\n",
      "Train Loss at iteration 5673: 0.0426865606255814\n",
      "Train Loss at iteration 5674: 0.04268643583832489\n",
      "Train Loss at iteration 5675: 0.04268631107273467\n",
      "Train Loss at iteration 5676: 0.042686186328806434\n",
      "Train Loss at iteration 5677: 0.0426860616065359\n",
      "Train Loss at iteration 5678: 0.04268593690591877\n",
      "Train Loss at iteration 5679: 0.042685812226950774\n",
      "Train Loss at iteration 5680: 0.04268568756962759\n",
      "Train Loss at iteration 5681: 0.04268556293394496\n",
      "Train Loss at iteration 5682: 0.04268543831989858\n",
      "Train Loss at iteration 5683: 0.04268531372748417\n",
      "Train Loss at iteration 5684: 0.042685189156697466\n",
      "Train Loss at iteration 5685: 0.04268506460753416\n",
      "Train Loss at iteration 5686: 0.04268494007998999\n",
      "Train Loss at iteration 5687: 0.04268481557406066\n",
      "Train Loss at iteration 5688: 0.04268469108974192\n",
      "Train Loss at iteration 5689: 0.04268456662702946\n",
      "Train Loss at iteration 5690: 0.04268444218591901\n",
      "Train Loss at iteration 5691: 0.04268431776640634\n",
      "Train Loss at iteration 5692: 0.04268419336848711\n",
      "Train Loss at iteration 5693: 0.04268406899215709\n",
      "Train Loss at iteration 5694: 0.042683944637412\n",
      "Train Loss at iteration 5695: 0.042683820304247555\n",
      "Train Loss at iteration 5696: 0.04268369599265953\n",
      "Train Loss at iteration 5697: 0.04268357170264359\n",
      "Train Loss at iteration 5698: 0.04268344743419554\n",
      "Train Loss at iteration 5699: 0.042683323187311065\n",
      "Train Loss at iteration 5700: 0.04268319896198592\n",
      "Train Loss at iteration 5701: 0.04268307475821583\n",
      "Train Loss at iteration 5702: 0.04268295057599657\n",
      "Train Loss at iteration 5703: 0.04268282641532383\n",
      "Train Loss at iteration 5704: 0.042682702276193384\n",
      "Train Loss at iteration 5705: 0.04268257815860096\n",
      "Train Loss at iteration 5706: 0.042682454062542306\n",
      "Train Loss at iteration 5707: 0.04268232998801316\n",
      "Train Loss at iteration 5708: 0.04268220593500929\n",
      "Train Loss at iteration 5709: 0.04268208190352643\n",
      "Train Loss at iteration 5710: 0.042681957893560316\n",
      "Train Loss at iteration 5711: 0.04268183390510671\n",
      "Train Loss at iteration 5712: 0.04268170993816136\n",
      "Train Loss at iteration 5713: 0.04268158599272001\n",
      "Train Loss at iteration 5714: 0.042681462068778424\n",
      "Train Loss at iteration 5715: 0.04268133816633236\n",
      "Train Loss at iteration 5716: 0.04268121428537756\n",
      "Train Loss at iteration 5717: 0.042681090425909785\n",
      "Train Loss at iteration 5718: 0.0426809665879248\n",
      "Train Loss at iteration 5719: 0.04268084277141835\n",
      "Train Loss at iteration 5720: 0.0426807189763862\n",
      "Train Loss at iteration 5721: 0.04268059520282412\n",
      "Train Loss at iteration 5722: 0.04268047145072787\n",
      "Train Loss at iteration 5723: 0.0426803477200932\n",
      "Train Loss at iteration 5724: 0.042680224010915896\n",
      "Train Loss at iteration 5725: 0.0426801003231917\n",
      "Train Loss at iteration 5726: 0.04267997665691639\n",
      "Train Loss at iteration 5727: 0.042679853012085746\n",
      "Train Loss at iteration 5728: 0.04267972938869552\n",
      "Train Loss at iteration 5729: 0.042679605786741494\n",
      "Train Loss at iteration 5730: 0.042679482206219435\n",
      "Train Loss at iteration 5731: 0.042679358647125104\n",
      "Train Loss at iteration 5732: 0.0426792351094543\n",
      "Train Loss at iteration 5733: 0.04267911159320279\n",
      "Train Loss at iteration 5734: 0.042678988098366344\n",
      "Train Loss at iteration 5735: 0.042678864624940735\n",
      "Train Loss at iteration 5736: 0.04267874117292175\n",
      "Train Loss at iteration 5737: 0.04267861774230517\n",
      "Train Loss at iteration 5738: 0.04267849433308678\n",
      "Train Loss at iteration 5739: 0.042678370945262355\n",
      "Train Loss at iteration 5740: 0.04267824757882767\n",
      "Train Loss at iteration 5741: 0.04267812423377853\n",
      "Train Loss at iteration 5742: 0.04267800091011071\n",
      "Train Loss at iteration 5743: 0.04267787760781999\n",
      "Train Loss at iteration 5744: 0.04267775432690217\n",
      "Train Loss at iteration 5745: 0.04267763106735303\n",
      "Train Loss at iteration 5746: 0.042677507829168376\n",
      "Train Loss at iteration 5747: 0.042677384612343994\n",
      "Train Loss at iteration 5748: 0.04267726141687565\n",
      "Train Loss at iteration 5749: 0.04267713824275919\n",
      "Train Loss at iteration 5750: 0.04267701508999035\n",
      "Train Loss at iteration 5751: 0.04267689195856498\n",
      "Train Loss at iteration 5752: 0.04267676884847885\n",
      "Train Loss at iteration 5753: 0.04267664575972776\n",
      "Train Loss at iteration 5754: 0.042676522692307504\n",
      "Train Loss at iteration 5755: 0.0426763996462139\n",
      "Train Loss at iteration 5756: 0.042676276621442744\n",
      "Train Loss at iteration 5757: 0.04267615361798983\n",
      "Train Loss at iteration 5758: 0.042676030635850966\n",
      "Train Loss at iteration 5759: 0.042675907675021975\n",
      "Train Loss at iteration 5760: 0.04267578473549864\n",
      "Train Loss at iteration 5761: 0.042675661817276785\n",
      "Train Loss at iteration 5762: 0.04267553892035221\n",
      "Train Loss at iteration 5763: 0.042675416044720735\n",
      "Train Loss at iteration 5764: 0.04267529319037816\n",
      "Train Loss at iteration 5765: 0.04267517035732031\n",
      "Train Loss at iteration 5766: 0.042675047545542986\n",
      "Train Loss at iteration 5767: 0.04267492475504201\n",
      "Train Loss at iteration 5768: 0.04267480198581319\n",
      "Train Loss at iteration 5769: 0.04267467923785236\n",
      "Train Loss at iteration 5770: 0.042674556511155336\n",
      "Train Loss at iteration 5771: 0.042674433805717914\n",
      "Train Loss at iteration 5772: 0.042674311121535934\n",
      "Train Loss at iteration 5773: 0.04267418845860522\n",
      "Train Loss at iteration 5774: 0.0426740658169216\n",
      "Train Loss at iteration 5775: 0.04267394319648087\n",
      "Train Loss at iteration 5776: 0.042673820597278894\n",
      "Train Loss at iteration 5777: 0.042673698019311465\n",
      "Train Loss at iteration 5778: 0.04267357546257443\n",
      "Train Loss at iteration 5779: 0.04267345292706361\n",
      "Train Loss at iteration 5780: 0.042673330412774824\n",
      "Train Loss at iteration 5781: 0.04267320791970393\n",
      "Train Loss at iteration 5782: 0.04267308544784675\n",
      "Train Loss at iteration 5783: 0.04267296299719912\n",
      "Train Loss at iteration 5784: 0.04267284056775685\n",
      "Train Loss at iteration 5785: 0.04267271815951581\n",
      "Train Loss at iteration 5786: 0.042672595772471816\n",
      "Train Loss at iteration 5787: 0.0426724734066207\n",
      "Train Loss at iteration 5788: 0.04267235106195833\n",
      "Train Loss at iteration 5789: 0.04267222873848051\n",
      "Train Loss at iteration 5790: 0.04267210643618312\n",
      "Train Loss at iteration 5791: 0.04267198415506197\n",
      "Train Loss at iteration 5792: 0.04267186189511293\n",
      "Train Loss at iteration 5793: 0.04267173965633181\n",
      "Train Loss at iteration 5794: 0.0426716174387145\n",
      "Train Loss at iteration 5795: 0.042671495242256816\n",
      "Train Loss at iteration 5796: 0.04267137306695461\n",
      "Train Loss at iteration 5797: 0.04267125091280375\n",
      "Train Loss at iteration 5798: 0.04267112877980006\n",
      "Train Loss at iteration 5799: 0.04267100666793941\n",
      "Train Loss at iteration 5800: 0.042670884577217655\n",
      "Train Loss at iteration 5801: 0.04267076250763064\n",
      "Train Loss at iteration 5802: 0.042670640459174215\n",
      "Train Loss at iteration 5803: 0.04267051843184426\n",
      "Train Loss at iteration 5804: 0.042670396425636595\n",
      "Train Loss at iteration 5805: 0.042670274440547114\n",
      "Train Loss at iteration 5806: 0.04267015247657166\n",
      "Train Loss at iteration 5807: 0.04267003053370609\n",
      "Train Loss at iteration 5808: 0.042669908611946285\n",
      "Train Loss at iteration 5809: 0.042669786711288085\n",
      "Train Loss at iteration 5810: 0.04266966483172737\n",
      "Train Loss at iteration 5811: 0.04266954297326\n",
      "Train Loss at iteration 5812: 0.04266942113588184\n",
      "Train Loss at iteration 5813: 0.04266929931958876\n",
      "Train Loss at iteration 5814: 0.04266917752437663\n",
      "Train Loss at iteration 5815: 0.04266905575024132\n",
      "Train Loss at iteration 5816: 0.0426689339971787\n",
      "Train Loss at iteration 5817: 0.04266881226518464\n",
      "Train Loss at iteration 5818: 0.04266869055425502\n",
      "Train Loss at iteration 5819: 0.0426685688643857\n",
      "Train Loss at iteration 5820: 0.04266844719557257\n",
      "Train Loss at iteration 5821: 0.042668325547811504\n",
      "Train Loss at iteration 5822: 0.04266820392109838\n",
      "Train Loss at iteration 5823: 0.042668082315429075\n",
      "Train Loss at iteration 5824: 0.04266796073079947\n",
      "Train Loss at iteration 5825: 0.042667839167205436\n",
      "Train Loss at iteration 5826: 0.04266771762464288\n",
      "Train Loss at iteration 5827: 0.042667596103107656\n",
      "Train Loss at iteration 5828: 0.04266747460259568\n",
      "Train Loss at iteration 5829: 0.042667353123102804\n",
      "Train Loss at iteration 5830: 0.042667231664624945\n",
      "Train Loss at iteration 5831: 0.042667110227157974\n",
      "Train Loss at iteration 5832: 0.042666988810697776\n",
      "Train Loss at iteration 5833: 0.04266686741524025\n",
      "Train Loss at iteration 5834: 0.0426667460407813\n",
      "Train Loss at iteration 5835: 0.0426666246873168\n",
      "Train Loss at iteration 5836: 0.04266650335484265\n",
      "Train Loss at iteration 5837: 0.04266638204335474\n",
      "Train Loss at iteration 5838: 0.04266626075284897\n",
      "Train Loss at iteration 5839: 0.04266613948332124\n",
      "Train Loss at iteration 5840: 0.04266601823476744\n",
      "Train Loss at iteration 5841: 0.04266589700718348\n",
      "Train Loss at iteration 5842: 0.04266577580056525\n",
      "Train Loss at iteration 5843: 0.04266565461490866\n",
      "Train Loss at iteration 5844: 0.0426655334502096\n",
      "Train Loss at iteration 5845: 0.04266541230646399\n",
      "Train Loss at iteration 5846: 0.042665291183667735\n",
      "Train Loss at iteration 5847: 0.042665170081816715\n",
      "Train Loss at iteration 5848: 0.042665049000906875\n",
      "Train Loss at iteration 5849: 0.04266492794093409\n",
      "Train Loss at iteration 5850: 0.04266480690189429\n",
      "Train Loss at iteration 5851: 0.042664685883783374\n",
      "Train Loss at iteration 5852: 0.04266456488659726\n",
      "Train Loss at iteration 5853: 0.04266444391033184\n",
      "Train Loss at iteration 5854: 0.042664322954983076\n",
      "Train Loss at iteration 5855: 0.04266420202054685\n",
      "Train Loss at iteration 5856: 0.04266408110701907\n",
      "Train Loss at iteration 5857: 0.04266396021439567\n",
      "Train Loss at iteration 5858: 0.042663839342672566\n",
      "Train Loss at iteration 5859: 0.04266371849184565\n",
      "Train Loss at iteration 5860: 0.04266359766191089\n",
      "Train Loss at iteration 5861: 0.04266347685286418\n",
      "Train Loss at iteration 5862: 0.042663356064701445\n",
      "Train Loss at iteration 5863: 0.04266323529741861\n",
      "Train Loss at iteration 5864: 0.0426631145510116\n",
      "Train Loss at iteration 5865: 0.04266299382547634\n",
      "Train Loss at iteration 5866: 0.04266287312080877\n",
      "Train Loss at iteration 5867: 0.0426627524370048\n",
      "Train Loss at iteration 5868: 0.042662631774060364\n",
      "Train Loss at iteration 5869: 0.04266251113197139\n",
      "Train Loss at iteration 5870: 0.042662390510733825\n",
      "Train Loss at iteration 5871: 0.042662269910343574\n",
      "Train Loss at iteration 5872: 0.0426621493307966\n",
      "Train Loss at iteration 5873: 0.04266202877208882\n",
      "Train Loss at iteration 5874: 0.042661908234216186\n",
      "Train Loss at iteration 5875: 0.04266178771717461\n",
      "Train Loss at iteration 5876: 0.04266166722096005\n",
      "Train Loss at iteration 5877: 0.04266154674556845\n",
      "Train Loss at iteration 5878: 0.04266142629099572\n",
      "Train Loss at iteration 5879: 0.04266130585723784\n",
      "Train Loss at iteration 5880: 0.042661185444290727\n",
      "Train Loss at iteration 5881: 0.04266106505215033\n",
      "Train Loss at iteration 5882: 0.0426609446808126\n",
      "Train Loss at iteration 5883: 0.042660824330273475\n",
      "Train Loss at iteration 5884: 0.042660704000528904\n",
      "Train Loss at iteration 5885: 0.042660583691574834\n",
      "Train Loss at iteration 5886: 0.042660463403407234\n",
      "Train Loss at iteration 5887: 0.042660343136022016\n",
      "Train Loss at iteration 5888: 0.04266022288941516\n",
      "Train Loss at iteration 5889: 0.04266010266358262\n",
      "Train Loss at iteration 5890: 0.04265998245852033\n",
      "Train Loss at iteration 5891: 0.042659862274224254\n",
      "Train Loss at iteration 5892: 0.04265974211069035\n",
      "Train Loss at iteration 5893: 0.04265962196791458\n",
      "Train Loss at iteration 5894: 0.042659501845892886\n",
      "Train Loss at iteration 5895: 0.04265938174462124\n",
      "Train Loss at iteration 5896: 0.0426592616640956\n",
      "Train Loss at iteration 5897: 0.04265914160431193\n",
      "Train Loss at iteration 5898: 0.04265902156526618\n",
      "Train Loss at iteration 5899: 0.04265890154695433\n",
      "Train Loss at iteration 5900: 0.04265878154937233\n",
      "Train Loss at iteration 5901: 0.04265866157251616\n",
      "Train Loss at iteration 5902: 0.04265854161638177\n",
      "Train Loss at iteration 5903: 0.04265842168096514\n",
      "Train Loss at iteration 5904: 0.04265830176626222\n",
      "Train Loss at iteration 5905: 0.04265818187226902\n",
      "Train Loss at iteration 5906: 0.04265806199898148\n",
      "Train Loss at iteration 5907: 0.04265794214639556\n",
      "Train Loss at iteration 5908: 0.04265782231450727\n",
      "Train Loss at iteration 5909: 0.04265770250331255\n",
      "Train Loss at iteration 5910: 0.04265758271280741\n",
      "Train Loss at iteration 5911: 0.042657462942987785\n",
      "Train Loss at iteration 5912: 0.042657343193849694\n",
      "Train Loss at iteration 5913: 0.042657223465389096\n",
      "Train Loss at iteration 5914: 0.042657103757601955\n",
      "Train Loss at iteration 5915: 0.04265698407048429\n",
      "Train Loss at iteration 5916: 0.04265686440403205\n",
      "Train Loss at iteration 5917: 0.04265674475824122\n",
      "Train Loss at iteration 5918: 0.042656625133107806\n",
      "Train Loss at iteration 5919: 0.04265650552862778\n",
      "Train Loss at iteration 5920: 0.04265638594479712\n",
      "Train Loss at iteration 5921: 0.04265626638161183\n",
      "Train Loss at iteration 5922: 0.04265614683906789\n",
      "Train Loss at iteration 5923: 0.042656027317161276\n",
      "Train Loss at iteration 5924: 0.042655907815888\n",
      "Train Loss at iteration 5925: 0.042655788335244055\n",
      "Train Loss at iteration 5926: 0.042655668875225414\n",
      "Train Loss at iteration 5927: 0.042655549435828076\n",
      "Train Loss at iteration 5928: 0.04265543001704805\n",
      "Train Loss at iteration 5929: 0.04265531061888132\n",
      "Train Loss at iteration 5930: 0.04265519124132391\n",
      "Train Loss at iteration 5931: 0.042655071884371766\n",
      "Train Loss at iteration 5932: 0.042654952548020916\n",
      "Train Loss at iteration 5933: 0.04265483323226736\n",
      "Train Loss at iteration 5934: 0.04265471393710711\n",
      "Train Loss at iteration 5935: 0.042654594662536155\n",
      "Train Loss at iteration 5936: 0.042654475408550496\n",
      "Train Loss at iteration 5937: 0.042654356175146145\n",
      "Train Loss at iteration 5938: 0.0426542369623191\n",
      "Train Loss at iteration 5939: 0.04265411777006538\n",
      "Train Loss at iteration 5940: 0.04265399859838098\n",
      "Train Loss at iteration 5941: 0.042653879447261914\n",
      "Train Loss at iteration 5942: 0.042653760316704196\n",
      "Train Loss at iteration 5943: 0.042653641206703834\n",
      "Train Loss at iteration 5944: 0.042653522117256835\n",
      "Train Loss at iteration 5945: 0.04265340304835922\n",
      "Train Loss at iteration 5946: 0.04265328400000699\n",
      "Train Loss at iteration 5947: 0.042653164972196164\n",
      "Train Loss at iteration 5948: 0.04265304596492278\n",
      "Train Loss at iteration 5949: 0.04265292697818284\n",
      "Train Loss at iteration 5950: 0.042652808011972354\n",
      "Train Loss at iteration 5951: 0.04265268906628736\n",
      "Train Loss at iteration 5952: 0.042652570141123856\n",
      "Train Loss at iteration 5953: 0.04265245123647789\n",
      "Train Loss at iteration 5954: 0.042652332352345446\n",
      "Train Loss at iteration 5955: 0.04265221348872259\n",
      "Train Loss at iteration 5956: 0.04265209464560532\n",
      "Train Loss at iteration 5957: 0.042651975822989685\n",
      "Train Loss at iteration 5958: 0.042651857020871695\n",
      "Train Loss at iteration 5959: 0.04265173823924736\n",
      "Train Loss at iteration 5960: 0.04265161947811275\n",
      "Train Loss at iteration 5961: 0.04265150073746387\n",
      "Train Loss at iteration 5962: 0.04265138201729677\n",
      "Train Loss at iteration 5963: 0.04265126331760744\n",
      "Train Loss at iteration 5964: 0.04265114463839196\n",
      "Train Loss at iteration 5965: 0.04265102597964634\n",
      "Train Loss at iteration 5966: 0.04265090734136663\n",
      "Train Loss at iteration 5967: 0.042650788723548855\n",
      "Train Loss at iteration 5968: 0.042650670126189054\n",
      "Train Loss at iteration 5969: 0.042650551549283267\n",
      "Train Loss at iteration 5970: 0.04265043299282754\n",
      "Train Loss at iteration 5971: 0.042650314456817905\n",
      "Train Loss at iteration 5972: 0.04265019594125041\n",
      "Train Loss at iteration 5973: 0.04265007744612109\n",
      "Train Loss at iteration 5974: 0.042649958971426004\n",
      "Train Loss at iteration 5975: 0.04264984051716117\n",
      "Train Loss at iteration 5976: 0.04264972208332266\n",
      "Train Loss at iteration 5977: 0.04264960366990651\n",
      "Train Loss at iteration 5978: 0.04264948527690878\n",
      "Train Loss at iteration 5979: 0.0426493669043255\n",
      "Train Loss at iteration 5980: 0.04264924855215273\n",
      "Train Loss at iteration 5981: 0.04264913022038652\n",
      "Train Loss at iteration 5982: 0.04264901190902292\n",
      "Train Loss at iteration 5983: 0.042648893618057994\n",
      "Train Loss at iteration 5984: 0.04264877534748779\n",
      "Train Loss at iteration 5985: 0.04264865709730836\n",
      "Train Loss at iteration 5986: 0.042648538867515774\n",
      "Train Loss at iteration 5987: 0.04264842065810606\n",
      "Train Loss at iteration 5988: 0.04264830246907532\n",
      "Train Loss at iteration 5989: 0.04264818430041957\n",
      "Train Loss at iteration 5990: 0.0426480661521349\n",
      "Train Loss at iteration 5991: 0.04264794802421735\n",
      "Train Loss at iteration 5992: 0.042647829916663\n",
      "Train Loss at iteration 5993: 0.042647711829467916\n",
      "Train Loss at iteration 5994: 0.042647593762628146\n",
      "Train Loss at iteration 5995: 0.042647475716139784\n",
      "Train Loss at iteration 5996: 0.04264735768999886\n",
      "Train Loss at iteration 5997: 0.04264723968420148\n",
      "Train Loss at iteration 5998: 0.04264712169874368\n",
      "Train Loss at iteration 5999: 0.04264700373362154\n",
      "Train Loss at iteration 6000: 0.04264688578883115\n",
      "Train Loss at iteration 6001: 0.042646767864368565\n",
      "Train Loss at iteration 6002: 0.042646649960229856\n",
      "Train Loss at iteration 6003: 0.0426465320764111\n",
      "Train Loss at iteration 6004: 0.042646414212908386\n",
      "Train Loss at iteration 6005: 0.04264629636971778\n",
      "Train Loss at iteration 6006: 0.04264617854683534\n",
      "Train Loss at iteration 6007: 0.04264606074425718\n",
      "Train Loss at iteration 6008: 0.04264594296197937\n",
      "Train Loss at iteration 6009: 0.04264582519999797\n",
      "Train Loss at iteration 6010: 0.042645707458309076\n",
      "Train Loss at iteration 6011: 0.04264558973690879\n",
      "Train Loss at iteration 6012: 0.042645472035793164\n",
      "Train Loss at iteration 6013: 0.04264535435495828\n",
      "Train Loss at iteration 6014: 0.04264523669440026\n",
      "Train Loss at iteration 6015: 0.042645119054115166\n",
      "Train Loss at iteration 6016: 0.042645001434099085\n",
      "Train Loss at iteration 6017: 0.04264488383434811\n",
      "Train Loss at iteration 6018: 0.04264476625485834\n",
      "Train Loss at iteration 6019: 0.04264464869562585\n",
      "Train Loss at iteration 6020: 0.04264453115664674\n",
      "Train Loss at iteration 6021: 0.04264441363791711\n",
      "Train Loss at iteration 6022: 0.04264429613943303\n",
      "Train Loss at iteration 6023: 0.04264417866119063\n",
      "Train Loss at iteration 6024: 0.04264406120318598\n",
      "Train Loss at iteration 6025: 0.04264394376541519\n",
      "Train Loss at iteration 6026: 0.042643826347874364\n",
      "Train Loss at iteration 6027: 0.04264370895055956\n",
      "Train Loss at iteration 6028: 0.04264359157346694\n",
      "Train Loss at iteration 6029: 0.04264347421659256\n",
      "Train Loss at iteration 6030: 0.042643356879932544\n",
      "Train Loss at iteration 6031: 0.04264323956348299\n",
      "Train Loss at iteration 6032: 0.04264312226724\n",
      "Train Loss at iteration 6033: 0.04264300499119967\n",
      "Train Loss at iteration 6034: 0.042642887735358126\n",
      "Train Loss at iteration 6035: 0.04264277049971147\n",
      "Train Loss at iteration 6036: 0.042642653284255795\n",
      "Train Loss at iteration 6037: 0.04264253608898723\n",
      "Train Loss at iteration 6038: 0.042642418913901886\n",
      "Train Loss at iteration 6039: 0.04264230175899586\n",
      "Train Loss at iteration 6040: 0.042642184624265264\n",
      "Train Loss at iteration 6041: 0.042642067509706234\n",
      "Train Loss at iteration 6042: 0.042641950415314864\n",
      "Train Loss at iteration 6043: 0.04264183334108727\n",
      "Train Loss at iteration 6044: 0.04264171628701959\n",
      "Train Loss at iteration 6045: 0.04264159925310792\n",
      "Train Loss at iteration 6046: 0.04264148223934838\n",
      "Train Loss at iteration 6047: 0.042641365245737105\n",
      "Train Loss at iteration 6048: 0.0426412482722702\n",
      "Train Loss at iteration 6049: 0.04264113131894379\n",
      "Train Loss at iteration 6050: 0.042641014385753996\n",
      "Train Loss at iteration 6051: 0.04264089747269696\n",
      "Train Loss at iteration 6052: 0.0426407805797688\n",
      "Train Loss at iteration 6053: 0.04264066370696562\n",
      "Train Loss at iteration 6054: 0.04264054685428357\n",
      "Train Loss at iteration 6055: 0.042640430021718766\n",
      "Train Loss at iteration 6056: 0.04264031320926735\n",
      "Train Loss at iteration 6057: 0.042640196416925444\n",
      "Train Loss at iteration 6058: 0.042640079644689174\n",
      "Train Loss at iteration 6059: 0.042639962892554686\n",
      "Train Loss at iteration 6060: 0.042639846160518094\n",
      "Train Loss at iteration 6061: 0.042639729448575546\n",
      "Train Loss at iteration 6062: 0.042639612756723184\n",
      "Train Loss at iteration 6063: 0.04263949608495712\n",
      "Train Loss at iteration 6064: 0.04263937943327351\n",
      "Train Loss at iteration 6065: 0.04263926280166848\n",
      "Train Loss at iteration 6066: 0.04263914619013819\n",
      "Train Loss at iteration 6067: 0.04263902959867876\n",
      "Train Loss at iteration 6068: 0.04263891302728635\n",
      "Train Loss at iteration 6069: 0.04263879647595707\n",
      "Train Loss at iteration 6070: 0.0426386799446871\n",
      "Train Loss at iteration 6071: 0.04263856343347256\n",
      "Train Loss at iteration 6072: 0.0426384469423096\n",
      "Train Loss at iteration 6073: 0.042638330471194384\n",
      "Train Loss at iteration 6074: 0.042638214020123036\n",
      "Train Loss at iteration 6075: 0.04263809758909172\n",
      "Train Loss at iteration 6076: 0.04263798117809657\n",
      "Train Loss at iteration 6077: 0.04263786478713375\n",
      "Train Loss at iteration 6078: 0.042637748416199404\n",
      "Train Loss at iteration 6079: 0.04263763206528968\n",
      "Train Loss at iteration 6080: 0.042637515734400756\n",
      "Train Loss at iteration 6081: 0.04263739942352876\n",
      "Train Loss at iteration 6082: 0.04263728313266986\n",
      "Train Loss at iteration 6083: 0.0426371668618202\n",
      "Train Loss at iteration 6084: 0.042637050610975946\n",
      "Train Loss at iteration 6085: 0.04263693438013326\n",
      "Train Loss at iteration 6086: 0.042636818169288306\n",
      "Train Loss at iteration 6087: 0.04263670197843722\n",
      "Train Loss at iteration 6088: 0.04263658580757619\n",
      "Train Loss at iteration 6089: 0.04263646965670138\n",
      "Train Loss at iteration 6090: 0.042636353525808926\n",
      "Train Loss at iteration 6091: 0.04263623741489501\n",
      "Train Loss at iteration 6092: 0.042636121323955806\n",
      "Train Loss at iteration 6093: 0.04263600525298745\n",
      "Train Loss at iteration 6094: 0.04263588920198615\n",
      "Train Loss at iteration 6095: 0.04263577317094805\n",
      "Train Loss at iteration 6096: 0.04263565715986932\n",
      "Train Loss at iteration 6097: 0.04263554116874613\n",
      "Train Loss at iteration 6098: 0.04263542519757466\n",
      "Train Loss at iteration 6099: 0.042635309246351075\n",
      "Train Loss at iteration 6100: 0.04263519331507156\n",
      "Train Loss at iteration 6101: 0.04263507740373228\n",
      "Train Loss at iteration 6102: 0.04263496151232941\n",
      "Train Loss at iteration 6103: 0.04263484564085913\n",
      "Train Loss at iteration 6104: 0.042634729789317614\n",
      "Train Loss at iteration 6105: 0.042634613957701045\n",
      "Train Loss at iteration 6106: 0.042634498146005596\n",
      "Train Loss at iteration 6107: 0.04263438235422745\n",
      "Train Loss at iteration 6108: 0.042634266582362794\n",
      "Train Loss at iteration 6109: 0.0426341508304078\n",
      "Train Loss at iteration 6110: 0.04263403509835866\n",
      "Train Loss at iteration 6111: 0.04263391938621156\n",
      "Train Loss at iteration 6112: 0.04263380369396268\n",
      "Train Loss at iteration 6113: 0.04263368802160822\n",
      "Train Loss at iteration 6114: 0.04263357236914435\n",
      "Train Loss at iteration 6115: 0.04263345673656725\n",
      "Train Loss at iteration 6116: 0.04263334112387314\n",
      "Train Loss at iteration 6117: 0.04263322553105819\n",
      "Train Loss at iteration 6118: 0.04263310995811859\n",
      "Train Loss at iteration 6119: 0.04263299440505054\n",
      "Train Loss at iteration 6120: 0.04263287887185024\n",
      "Train Loss at iteration 6121: 0.04263276335851386\n",
      "Train Loss at iteration 6122: 0.042632647865037634\n",
      "Train Loss at iteration 6123: 0.04263253239141771\n",
      "Train Loss at iteration 6124: 0.04263241693765033\n",
      "Train Loss at iteration 6125: 0.04263230150373167\n",
      "Train Loss at iteration 6126: 0.04263218608965793\n",
      "Train Loss at iteration 6127: 0.04263207069542531\n",
      "Train Loss at iteration 6128: 0.04263195532103002\n",
      "Train Loss at iteration 6129: 0.04263183996646826\n",
      "Train Loss at iteration 6130: 0.04263172463173623\n",
      "Train Loss at iteration 6131: 0.04263160931683013\n",
      "Train Loss at iteration 6132: 0.04263149402174616\n",
      "Train Loss at iteration 6133: 0.04263137874648056\n",
      "Train Loss at iteration 6134: 0.0426312634910295\n",
      "Train Loss at iteration 6135: 0.042631148255389203\n",
      "Train Loss at iteration 6136: 0.04263103303955587\n",
      "Train Loss at iteration 6137: 0.042630917843525726\n",
      "Train Loss at iteration 6138: 0.04263080266729498\n",
      "Train Loss at iteration 6139: 0.04263068751085984\n",
      "Train Loss at iteration 6140: 0.04263057237421651\n",
      "Train Loss at iteration 6141: 0.04263045725736121\n",
      "Train Loss at iteration 6142: 0.04263034216029017\n",
      "Train Loss at iteration 6143: 0.042630227082999586\n",
      "Train Loss at iteration 6144: 0.04263011202548568\n",
      "Train Loss at iteration 6145: 0.04262999698774468\n",
      "Train Loss at iteration 6146: 0.042629881969772784\n",
      "Train Loss at iteration 6147: 0.042629766971566244\n",
      "Train Loss at iteration 6148: 0.04262965199312125\n",
      "Train Loss at iteration 6149: 0.04262953703443405\n",
      "Train Loss at iteration 6150: 0.042629422095500843\n",
      "Train Loss at iteration 6151: 0.04262930717631787\n",
      "Train Loss at iteration 6152: 0.04262919227688135\n",
      "Train Loss at iteration 6153: 0.04262907739718749\n",
      "Train Loss at iteration 6154: 0.04262896253723257\n",
      "Train Loss at iteration 6155: 0.042628847697012764\n",
      "Train Loss at iteration 6156: 0.042628732876524314\n",
      "Train Loss at iteration 6157: 0.04262861807576347\n",
      "Train Loss at iteration 6158: 0.04262850329472646\n",
      "Train Loss at iteration 6159: 0.042628388533409486\n",
      "Train Loss at iteration 6160: 0.04262827379180881\n",
      "Train Loss at iteration 6161: 0.04262815906992065\n",
      "Train Loss at iteration 6162: 0.042628044367741244\n",
      "Train Loss at iteration 6163: 0.042627929685266845\n",
      "Train Loss at iteration 6164: 0.042627815022493666\n",
      "Train Loss at iteration 6165: 0.04262770037941795\n",
      "Train Loss at iteration 6166: 0.04262758575603594\n",
      "Train Loss at iteration 6167: 0.04262747115234389\n",
      "Train Loss at iteration 6168: 0.04262735656833801\n",
      "Train Loss at iteration 6169: 0.04262724200401456\n",
      "Train Loss at iteration 6170: 0.04262712745936978\n",
      "Train Loss at iteration 6171: 0.04262701293439991\n",
      "Train Loss at iteration 6172: 0.04262689842910121\n",
      "Train Loss at iteration 6173: 0.04262678394346991\n",
      "Train Loss at iteration 6174: 0.04262666947750225\n",
      "Train Loss at iteration 6175: 0.042626555031194495\n",
      "Train Loss at iteration 6176: 0.042626440604542876\n",
      "Train Loss at iteration 6177: 0.04262632619754366\n",
      "Train Loss at iteration 6178: 0.04262621181019309\n",
      "Train Loss at iteration 6179: 0.04262609744248741\n",
      "Train Loss at iteration 6180: 0.04262598309442288\n",
      "Train Loss at iteration 6181: 0.04262586876599576\n",
      "Train Loss at iteration 6182: 0.042625754457202285\n",
      "Train Loss at iteration 6183: 0.04262564016803873\n",
      "Train Loss at iteration 6184: 0.042625525898501326\n",
      "Train Loss at iteration 6185: 0.04262541164858636\n",
      "Train Loss at iteration 6186: 0.042625297418290076\n",
      "Train Loss at iteration 6187: 0.04262518320760875\n",
      "Train Loss at iteration 6188: 0.04262506901653859\n",
      "Train Loss at iteration 6189: 0.04262495484507592\n",
      "Train Loss at iteration 6190: 0.04262484069321697\n",
      "Train Loss at iteration 6191: 0.04262472656095799\n",
      "Train Loss at iteration 6192: 0.04262461244829527\n",
      "Train Loss at iteration 6193: 0.04262449835522507\n",
      "Train Loss at iteration 6194: 0.042624384281743644\n",
      "Train Loss at iteration 6195: 0.042624270227847275\n",
      "Train Loss at iteration 6196: 0.042624156193532214\n",
      "Train Loss at iteration 6197: 0.04262404217879474\n",
      "Train Loss at iteration 6198: 0.04262392818363112\n",
      "Train Loss at iteration 6199: 0.04262381420803762\n",
      "Train Loss at iteration 6200: 0.04262370025201052\n",
      "Train Loss at iteration 6201: 0.04262358631554608\n",
      "Train Loss at iteration 6202: 0.0426234723986406\n",
      "Train Loss at iteration 6203: 0.04262335850129033\n",
      "Train Loss at iteration 6204: 0.04262324462349154\n",
      "Train Loss at iteration 6205: 0.04262313076524052\n",
      "Train Loss at iteration 6206: 0.04262301692653356\n",
      "Train Loss at iteration 6207: 0.04262290310736691\n",
      "Train Loss at iteration 6208: 0.042622789307736865\n",
      "Train Loss at iteration 6209: 0.04262267552763969\n",
      "Train Loss at iteration 6210: 0.042622561767071694\n",
      "Train Loss at iteration 6211: 0.04262244802602914\n",
      "Train Loss at iteration 6212: 0.04262233430450832\n",
      "Train Loss at iteration 6213: 0.0426222206025055\n",
      "Train Loss at iteration 6214: 0.042622106920016986\n",
      "Train Loss at iteration 6215: 0.04262199325703906\n",
      "Train Loss at iteration 6216: 0.04262187961356799\n",
      "Train Loss at iteration 6217: 0.04262176598960009\n",
      "Train Loss at iteration 6218: 0.04262165238513164\n",
      "Train Loss at iteration 6219: 0.04262153880015891\n",
      "Train Loss at iteration 6220: 0.042621425234678226\n",
      "Train Loss at iteration 6221: 0.04262131168868584\n",
      "Train Loss at iteration 6222: 0.042621198162178076\n",
      "Train Loss at iteration 6223: 0.042621084655151215\n",
      "Train Loss at iteration 6224: 0.04262097116760156\n",
      "Train Loss at iteration 6225: 0.042620857699525394\n",
      "Train Loss at iteration 6226: 0.04262074425091901\n",
      "Train Loss at iteration 6227: 0.042620630821778714\n",
      "Train Loss at iteration 6228: 0.04262051741210081\n",
      "Train Loss at iteration 6229: 0.04262040402188159\n",
      "Train Loss at iteration 6230: 0.04262029065111736\n",
      "Train Loss at iteration 6231: 0.042620177299804406\n",
      "Train Loss at iteration 6232: 0.042620063967939055\n",
      "Train Loss at iteration 6233: 0.042619950655517574\n",
      "Train Loss at iteration 6234: 0.042619837362536314\n",
      "Train Loss at iteration 6235: 0.042619724088991534\n",
      "Train Loss at iteration 6236: 0.04261961083487957\n",
      "Train Loss at iteration 6237: 0.04261949760019672\n",
      "Train Loss at iteration 6238: 0.042619384384939285\n",
      "Train Loss at iteration 6239: 0.04261927118910357\n",
      "Train Loss at iteration 6240: 0.04261915801268591\n",
      "Train Loss at iteration 6241: 0.04261904485568259\n",
      "Train Loss at iteration 6242: 0.04261893171808994\n",
      "Train Loss at iteration 6243: 0.04261881859990425\n",
      "Train Loss at iteration 6244: 0.042618705501121845\n",
      "Train Loss at iteration 6245: 0.04261859242173904\n",
      "Train Loss at iteration 6246: 0.04261847936175216\n",
      "Train Loss at iteration 6247: 0.042618366321157515\n",
      "Train Loss at iteration 6248: 0.042618253299951406\n",
      "Train Loss at iteration 6249: 0.04261814029813017\n",
      "Train Loss at iteration 6250: 0.04261802731569012\n",
      "Train Loss at iteration 6251: 0.04261791435262759\n",
      "Train Loss at iteration 6252: 0.04261780140893886\n",
      "Train Loss at iteration 6253: 0.04261768848462029\n",
      "Train Loss at iteration 6254: 0.04261757557966819\n",
      "Train Loss at iteration 6255: 0.04261746269407889\n",
      "Train Loss at iteration 6256: 0.04261734982784871\n",
      "Train Loss at iteration 6257: 0.04261723698097397\n",
      "Train Loss at iteration 6258: 0.042617124153451\n",
      "Train Loss at iteration 6259: 0.04261701134527613\n",
      "Train Loss at iteration 6260: 0.042616898556445695\n",
      "Train Loss at iteration 6261: 0.04261678578695601\n",
      "Train Loss at iteration 6262: 0.042616673036803415\n",
      "Train Loss at iteration 6263: 0.042616560305984236\n",
      "Train Loss at iteration 6264: 0.0426164475944948\n",
      "Train Loss at iteration 6265: 0.04261633490233145\n",
      "Train Loss at iteration 6266: 0.04261622222949052\n",
      "Train Loss at iteration 6267: 0.04261610957596834\n",
      "Train Loss at iteration 6268: 0.04261599694176126\n",
      "Train Loss at iteration 6269: 0.0426158843268656\n",
      "Train Loss at iteration 6270: 0.042615771731277695\n",
      "Train Loss at iteration 6271: 0.0426156591549939\n",
      "Train Loss at iteration 6272: 0.042615546598010534\n",
      "Train Loss at iteration 6273: 0.042615434060323955\n",
      "Train Loss at iteration 6274: 0.04261532154193049\n",
      "Train Loss at iteration 6275: 0.04261520904282649\n",
      "Train Loss at iteration 6276: 0.04261509656300832\n",
      "Train Loss at iteration 6277: 0.04261498410247228\n",
      "Train Loss at iteration 6278: 0.04261487166121475\n",
      "Train Loss at iteration 6279: 0.04261475923923205\n",
      "Train Loss at iteration 6280: 0.042614646836520574\n",
      "Train Loss at iteration 6281: 0.0426145344530766\n",
      "Train Loss at iteration 6282: 0.04261442208889652\n",
      "Train Loss at iteration 6283: 0.04261430974397668\n",
      "Train Loss at iteration 6284: 0.04261419741831344\n",
      "Train Loss at iteration 6285: 0.04261408511190313\n",
      "Train Loss at iteration 6286: 0.04261397282474212\n",
      "Train Loss at iteration 6287: 0.04261386055682673\n",
      "Train Loss at iteration 6288: 0.042613748308153364\n",
      "Train Loss at iteration 6289: 0.042613636078718355\n",
      "Train Loss at iteration 6290: 0.04261352386851804\n",
      "Train Loss at iteration 6291: 0.042613411677548806\n",
      "Train Loss at iteration 6292: 0.042613299505807\n",
      "Train Loss at iteration 6293: 0.04261318735328897\n",
      "Train Loss at iteration 6294: 0.0426130752199911\n",
      "Train Loss at iteration 6295: 0.04261296310590973\n",
      "Train Loss at iteration 6296: 0.04261285101104123\n",
      "Train Loss at iteration 6297: 0.04261273893538196\n",
      "Train Loss at iteration 6298: 0.04261262687892827\n",
      "Train Loss at iteration 6299: 0.04261251484167657\n",
      "Train Loss at iteration 6300: 0.04261240282362318\n",
      "Train Loss at iteration 6301: 0.042612290824764476\n",
      "Train Loss at iteration 6302: 0.04261217884509683\n",
      "Train Loss at iteration 6303: 0.04261206688461662\n",
      "Train Loss at iteration 6304: 0.0426119549433202\n",
      "Train Loss at iteration 6305: 0.04261184302120396\n",
      "Train Loss at iteration 6306: 0.04261173111826423\n",
      "Train Loss at iteration 6307: 0.04261161923449743\n",
      "Train Loss at iteration 6308: 0.0426115073698999\n",
      "Train Loss at iteration 6309: 0.04261139552446804\n",
      "Train Loss at iteration 6310: 0.04261128369819821\n",
      "Train Loss at iteration 6311: 0.04261117189108678\n",
      "Train Loss at iteration 6312: 0.04261106010313012\n",
      "Train Loss at iteration 6313: 0.04261094833432464\n",
      "Train Loss at iteration 6314: 0.042610836584666696\n",
      "Train Loss at iteration 6315: 0.04261072485415267\n",
      "Train Loss at iteration 6316: 0.04261061314277893\n",
      "Train Loss at iteration 6317: 0.04261050145054188\n",
      "Train Loss at iteration 6318: 0.04261038977743788\n",
      "Train Loss at iteration 6319: 0.04261027812346333\n",
      "Train Loss at iteration 6320: 0.04261016648861461\n",
      "Train Loss at iteration 6321: 0.04261005487288811\n",
      "Train Loss at iteration 6322: 0.0426099432762802\n",
      "Train Loss at iteration 6323: 0.04260983169878727\n",
      "Train Loss at iteration 6324: 0.042609720140405725\n",
      "Train Loss at iteration 6325: 0.04260960860113192\n",
      "Train Loss at iteration 6326: 0.04260949708096228\n",
      "Train Loss at iteration 6327: 0.04260938557989318\n",
      "Train Loss at iteration 6328: 0.04260927409792101\n",
      "Train Loss at iteration 6329: 0.042609162635042164\n",
      "Train Loss at iteration 6330: 0.04260905119125303\n",
      "Train Loss at iteration 6331: 0.042608939766550016\n",
      "Train Loss at iteration 6332: 0.042608828360929496\n",
      "Train Loss at iteration 6333: 0.042608716974387874\n",
      "Train Loss at iteration 6334: 0.04260860560692156\n",
      "Train Loss at iteration 6335: 0.04260849425852693\n",
      "Train Loss at iteration 6336: 0.042608382929200404\n",
      "Train Loss at iteration 6337: 0.04260827161893836\n",
      "Train Loss at iteration 6338: 0.04260816032773721\n",
      "Train Loss at iteration 6339: 0.04260804905559336\n",
      "Train Loss at iteration 6340: 0.0426079378025032\n",
      "Train Loss at iteration 6341: 0.04260782656846314\n",
      "Train Loss at iteration 6342: 0.04260771535346959\n",
      "Train Loss at iteration 6343: 0.04260760415751893\n",
      "Train Loss at iteration 6344: 0.0426074929806076\n",
      "Train Loss at iteration 6345: 0.04260738182273199\n",
      "Train Loss at iteration 6346: 0.0426072706838885\n",
      "Train Loss at iteration 6347: 0.042607159564073536\n",
      "Train Loss at iteration 6348: 0.04260704846328353\n",
      "Train Loss at iteration 6349: 0.042606937381514866\n",
      "Train Loss at iteration 6350: 0.04260682631876398\n",
      "Train Loss at iteration 6351: 0.04260671527502727\n",
      "Train Loss at iteration 6352: 0.04260660425030115\n",
      "Train Loss at iteration 6353: 0.042606493244582024\n",
      "Train Loss at iteration 6354: 0.04260638225786633\n",
      "Train Loss at iteration 6355: 0.04260627129015046\n",
      "Train Loss at iteration 6356: 0.04260616034143085\n",
      "Train Loss at iteration 6357: 0.0426060494117039\n",
      "Train Loss at iteration 6358: 0.04260593850096604\n",
      "Train Loss at iteration 6359: 0.0426058276092137\n",
      "Train Loss at iteration 6360: 0.04260571673644326\n",
      "Train Loss at iteration 6361: 0.04260560588265118\n",
      "Train Loss at iteration 6362: 0.04260549504783387\n",
      "Train Loss at iteration 6363: 0.042605384231987745\n",
      "Train Loss at iteration 6364: 0.04260527343510924\n",
      "Train Loss at iteration 6365: 0.04260516265719478\n",
      "Train Loss at iteration 6366: 0.042605051898240764\n",
      "Train Loss at iteration 6367: 0.04260494115824367\n",
      "Train Loss at iteration 6368: 0.04260483043719987\n",
      "Train Loss at iteration 6369: 0.04260471973510582\n",
      "Train Loss at iteration 6370: 0.04260460905195795\n",
      "Train Loss at iteration 6371: 0.04260449838775269\n",
      "Train Loss at iteration 6372: 0.04260438774248647\n",
      "Train Loss at iteration 6373: 0.04260427711615571\n",
      "Train Loss at iteration 6374: 0.042604166508756855\n",
      "Train Loss at iteration 6375: 0.04260405592028633\n",
      "Train Loss at iteration 6376: 0.04260394535074057\n",
      "Train Loss at iteration 6377: 0.04260383480011602\n",
      "Train Loss at iteration 6378: 0.04260372426840911\n",
      "Train Loss at iteration 6379: 0.042603613755616276\n",
      "Train Loss at iteration 6380: 0.042603503261733955\n",
      "Train Loss at iteration 6381: 0.0426033927867586\n",
      "Train Loss at iteration 6382: 0.04260328233068663\n",
      "Train Loss at iteration 6383: 0.04260317189351448\n",
      "Train Loss at iteration 6384: 0.042603061475238624\n",
      "Train Loss at iteration 6385: 0.04260295107585548\n",
      "Train Loss at iteration 6386: 0.04260284069536151\n",
      "Train Loss at iteration 6387: 0.04260273033375313\n",
      "Train Loss at iteration 6388: 0.042602619991026804\n",
      "Train Loss at iteration 6389: 0.04260250966717898\n",
      "Train Loss at iteration 6390: 0.0426023993622061\n",
      "Train Loss at iteration 6391: 0.0426022890761046\n",
      "Train Loss at iteration 6392: 0.04260217880887095\n",
      "Train Loss at iteration 6393: 0.04260206856050159\n",
      "Train Loss at iteration 6394: 0.04260195833099298\n",
      "Train Loss at iteration 6395: 0.04260184812034154\n",
      "Train Loss at iteration 6396: 0.042601737928543756\n",
      "Train Loss at iteration 6397: 0.04260162775559606\n",
      "Train Loss at iteration 6398: 0.04260151760149493\n",
      "Train Loss at iteration 6399: 0.04260140746623679\n",
      "Train Loss at iteration 6400: 0.042601297349818115\n",
      "Train Loss at iteration 6401: 0.042601187252235356\n",
      "Train Loss at iteration 6402: 0.04260107717348498\n",
      "Train Loss at iteration 6403: 0.04260096711356344\n",
      "Train Loss at iteration 6404: 0.04260085707246718\n",
      "Train Loss at iteration 6405: 0.04260074705019267\n",
      "Train Loss at iteration 6406: 0.04260063704673639\n",
      "Train Loss at iteration 6407: 0.04260052706209478\n",
      "Train Loss at iteration 6408: 0.042600417096264304\n",
      "Train Loss at iteration 6409: 0.042600307149241436\n",
      "Train Loss at iteration 6410: 0.04260019722102262\n",
      "Train Loss at iteration 6411: 0.04260008731160436\n",
      "Train Loss at iteration 6412: 0.042599977420983086\n",
      "Train Loss at iteration 6413: 0.04259986754915527\n",
      "Train Loss at iteration 6414: 0.04259975769611739\n",
      "Train Loss at iteration 6415: 0.04259964786186592\n",
      "Train Loss at iteration 6416: 0.04259953804639733\n",
      "Train Loss at iteration 6417: 0.04259942824970807\n",
      "Train Loss at iteration 6418: 0.04259931847179462\n",
      "Train Loss at iteration 6419: 0.04259920871265348\n",
      "Train Loss at iteration 6420: 0.04259909897228107\n",
      "Train Loss at iteration 6421: 0.04259898925067391\n",
      "Train Loss at iteration 6422: 0.04259887954782846\n",
      "Train Loss at iteration 6423: 0.04259876986374121\n",
      "Train Loss at iteration 6424: 0.0425986601984086\n",
      "Train Loss at iteration 6425: 0.04259855055182714\n",
      "Train Loss at iteration 6426: 0.04259844092399329\n",
      "Train Loss at iteration 6427: 0.04259833131490355\n",
      "Train Loss at iteration 6428: 0.04259822172455437\n",
      "Train Loss at iteration 6429: 0.04259811215294228\n",
      "Train Loss at iteration 6430: 0.04259800260006371\n",
      "Train Loss at iteration 6431: 0.04259789306591517\n",
      "Train Loss at iteration 6432: 0.04259778355049313\n",
      "Train Loss at iteration 6433: 0.042597674053794096\n",
      "Train Loss at iteration 6434: 0.04259756457581454\n",
      "Train Loss at iteration 6435: 0.042597455116550935\n",
      "Train Loss at iteration 6436: 0.042597345675999806\n",
      "Train Loss at iteration 6437: 0.0425972362541576\n",
      "Train Loss at iteration 6438: 0.042597126851020836\n",
      "Train Loss at iteration 6439: 0.04259701746658597\n",
      "Train Loss at iteration 6440: 0.04259690810084954\n",
      "Train Loss at iteration 6441: 0.042596798753807986\n",
      "Train Loss at iteration 6442: 0.04259668942545785\n",
      "Train Loss at iteration 6443: 0.042596580115795604\n",
      "Train Loss at iteration 6444: 0.04259647082481772\n",
      "Train Loss at iteration 6445: 0.04259636155252072\n",
      "Train Loss at iteration 6446: 0.042596252298901106\n",
      "Train Loss at iteration 6447: 0.04259614306395535\n",
      "Train Loss at iteration 6448: 0.042596033847679966\n",
      "Train Loss at iteration 6449: 0.04259592465007145\n",
      "Train Loss at iteration 6450: 0.04259581547112629\n",
      "Train Loss at iteration 6451: 0.04259570631084101\n",
      "Train Loss at iteration 6452: 0.042595597169212095\n",
      "Train Loss at iteration 6453: 0.042595488046236044\n",
      "Train Loss at iteration 6454: 0.04259537894190937\n",
      "Train Loss at iteration 6455: 0.04259526985622857\n",
      "Train Loss at iteration 6456: 0.042595160789190165\n",
      "Train Loss at iteration 6457: 0.042595051740790625\n",
      "Train Loss at iteration 6458: 0.04259494271102649\n",
      "Train Loss at iteration 6459: 0.04259483369989426\n",
      "Train Loss at iteration 6460: 0.042594724707390424\n",
      "Train Loss at iteration 6461: 0.04259461573351152\n",
      "Train Loss at iteration 6462: 0.04259450677825403\n",
      "Train Loss at iteration 6463: 0.04259439784161449\n",
      "Train Loss at iteration 6464: 0.042594288923589387\n",
      "Train Loss at iteration 6465: 0.042594180024175254\n",
      "Train Loss at iteration 6466: 0.0425940711433686\n",
      "Train Loss at iteration 6467: 0.04259396228116592\n",
      "Train Loss at iteration 6468: 0.042593853437563764\n",
      "Train Loss at iteration 6469: 0.04259374461255862\n",
      "Train Loss at iteration 6470: 0.04259363580614701\n",
      "Train Loss at iteration 6471: 0.042593527018325465\n",
      "Train Loss at iteration 6472: 0.04259341824909048\n",
      "Train Loss at iteration 6473: 0.0425933094984386\n",
      "Train Loss at iteration 6474: 0.04259320076636632\n",
      "Train Loss at iteration 6475: 0.042593092052870186\n",
      "Train Loss at iteration 6476: 0.0425929833579467\n",
      "Train Loss at iteration 6477: 0.04259287468159239\n",
      "Train Loss at iteration 6478: 0.04259276602380379\n",
      "Train Loss at iteration 6479: 0.04259265738457741\n",
      "Train Loss at iteration 6480: 0.042592548763909784\n",
      "Train Loss at iteration 6481: 0.04259244016179744\n",
      "Train Loss at iteration 6482: 0.04259233157823689\n",
      "Train Loss at iteration 6483: 0.04259222301322468\n",
      "Train Loss at iteration 6484: 0.04259211446675734\n",
      "Train Loss at iteration 6485: 0.042592005938831376\n",
      "Train Loss at iteration 6486: 0.042591897429443346\n",
      "Train Loss at iteration 6487: 0.04259178893858975\n",
      "Train Loss at iteration 6488: 0.04259168046626716\n",
      "Train Loss at iteration 6489: 0.04259157201247208\n",
      "Train Loss at iteration 6490: 0.04259146357720105\n",
      "Train Loss at iteration 6491: 0.04259135516045061\n",
      "Train Loss at iteration 6492: 0.04259124676221729\n",
      "Train Loss at iteration 6493: 0.04259113838249764\n",
      "Train Loss at iteration 6494: 0.04259103002128817\n",
      "Train Loss at iteration 6495: 0.042590921678585435\n",
      "Train Loss at iteration 6496: 0.04259081335438596\n",
      "Train Loss at iteration 6497: 0.04259070504868632\n",
      "Train Loss at iteration 6498: 0.042590596761483024\n",
      "Train Loss at iteration 6499: 0.04259048849277264\n",
      "Train Loss at iteration 6500: 0.04259038024255168\n",
      "Train Loss at iteration 6501: 0.04259027201081669\n",
      "Train Loss at iteration 6502: 0.04259016379756424\n",
      "Train Loss at iteration 6503: 0.04259005560279086\n",
      "Train Loss at iteration 6504: 0.04258994742649308\n",
      "Train Loss at iteration 6505: 0.04258983926866748\n",
      "Train Loss at iteration 6506: 0.04258973112931058\n",
      "Train Loss at iteration 6507: 0.04258962300841893\n",
      "Train Loss at iteration 6508: 0.0425895149059891\n",
      "Train Loss at iteration 6509: 0.04258940682201763\n",
      "Train Loss at iteration 6510: 0.04258929875650106\n",
      "Train Loss at iteration 6511: 0.042589190709435946\n",
      "Train Loss at iteration 6512: 0.042589082680818846\n",
      "Train Loss at iteration 6513: 0.042588974670646304\n",
      "Train Loss at iteration 6514: 0.0425888666789149\n",
      "Train Loss at iteration 6515: 0.04258875870562116\n",
      "Train Loss at iteration 6516: 0.042588650750761646\n",
      "Train Loss at iteration 6517: 0.042588542814332934\n",
      "Train Loss at iteration 6518: 0.04258843489633156\n",
      "Train Loss at iteration 6519: 0.04258832699675408\n",
      "Train Loss at iteration 6520: 0.04258821911559707\n",
      "Train Loss at iteration 6521: 0.04258811125285708\n",
      "Train Loss at iteration 6522: 0.042588003408530674\n",
      "Train Loss at iteration 6523: 0.04258789558261442\n",
      "Train Loss at iteration 6524: 0.04258778777510487\n",
      "Train Loss at iteration 6525: 0.04258767998599859\n",
      "Train Loss at iteration 6526: 0.04258757221529214\n",
      "Train Loss at iteration 6527: 0.04258746446298211\n",
      "Train Loss at iteration 6528: 0.04258735672906503\n",
      "Train Loss at iteration 6529: 0.04258724901353749\n",
      "Train Loss at iteration 6530: 0.04258714131639605\n",
      "Train Loss at iteration 6531: 0.04258703363763729\n",
      "Train Loss at iteration 6532: 0.04258692597725775\n",
      "Train Loss at iteration 6533: 0.04258681833525404\n",
      "Train Loss at iteration 6534: 0.042586710711622705\n",
      "Train Loss at iteration 6535: 0.042586603106360324\n",
      "Train Loss at iteration 6536: 0.04258649551946346\n",
      "Train Loss at iteration 6537: 0.0425863879509287\n",
      "Train Loss at iteration 6538: 0.04258628040075262\n",
      "Train Loss at iteration 6539: 0.04258617286893178\n",
      "Train Loss at iteration 6540: 0.04258606535546277\n",
      "Train Loss at iteration 6541: 0.04258595786034216\n",
      "Train Loss at iteration 6542: 0.042585850383566534\n",
      "Train Loss at iteration 6543: 0.04258574292513246\n",
      "Train Loss at iteration 6544: 0.042585635485036535\n",
      "Train Loss at iteration 6545: 0.04258552806327532\n",
      "Train Loss at iteration 6546: 0.0425854206598454\n",
      "Train Loss at iteration 6547: 0.04258531327474336\n",
      "Train Loss at iteration 6548: 0.04258520590796581\n",
      "Train Loss at iteration 6549: 0.04258509855950927\n",
      "Train Loss at iteration 6550: 0.04258499122937038\n",
      "Train Loss at iteration 6551: 0.042584883917545696\n",
      "Train Loss at iteration 6552: 0.042584776624031824\n",
      "Train Loss at iteration 6553: 0.04258466934882534\n",
      "Train Loss at iteration 6554: 0.04258456209192285\n",
      "Train Loss at iteration 6555: 0.0425844548533209\n",
      "Train Loss at iteration 6556: 0.04258434763301613\n",
      "Train Loss at iteration 6557: 0.042584240431005095\n",
      "Train Loss at iteration 6558: 0.0425841332472844\n",
      "Train Loss at iteration 6559: 0.04258402608185062\n",
      "Train Loss at iteration 6560: 0.04258391893470038\n",
      "Train Loss at iteration 6561: 0.042583811805830235\n",
      "Train Loss at iteration 6562: 0.04258370469523682\n",
      "Train Loss at iteration 6563: 0.04258359760291671\n",
      "Train Loss at iteration 6564: 0.04258349052886647\n",
      "Train Loss at iteration 6565: 0.04258338347308277\n",
      "Train Loss at iteration 6566: 0.042583276435562145\n",
      "Train Loss at iteration 6567: 0.04258316941630123\n",
      "Train Loss at iteration 6568: 0.04258306241529659\n",
      "Train Loss at iteration 6569: 0.042582955432544846\n",
      "Train Loss at iteration 6570: 0.042582848468042604\n",
      "Train Loss at iteration 6571: 0.04258274152178645\n",
      "Train Loss at iteration 6572: 0.04258263459377299\n",
      "Train Loss at iteration 6573: 0.042582527683998846\n",
      "Train Loss at iteration 6574: 0.04258242079246062\n",
      "Train Loss at iteration 6575: 0.04258231391915488\n",
      "Train Loss at iteration 6576: 0.042582207064078274\n",
      "Train Loss at iteration 6577: 0.042582100227227394\n",
      "Train Loss at iteration 6578: 0.04258199340859884\n",
      "Train Loss at iteration 6579: 0.042581886608189234\n",
      "Train Loss at iteration 6580: 0.04258177982599517\n",
      "Train Loss at iteration 6581: 0.04258167306201326\n",
      "Train Loss at iteration 6582: 0.042581566316240146\n",
      "Train Loss at iteration 6583: 0.0425814595886724\n",
      "Train Loss at iteration 6584: 0.04258135287930665\n",
      "Train Loss at iteration 6585: 0.042581246188139514\n",
      "Train Loss at iteration 6586: 0.04258113951516759\n",
      "Train Loss at iteration 6587: 0.042581032860387516\n",
      "Train Loss at iteration 6588: 0.0425809262237959\n",
      "Train Loss at iteration 6589: 0.04258081960538934\n",
      "Train Loss at iteration 6590: 0.042580713005164486\n",
      "Train Loss at iteration 6591: 0.04258060642311792\n",
      "Train Loss at iteration 6592: 0.04258049985924629\n",
      "Train Loss at iteration 6593: 0.042580393313546204\n",
      "Train Loss at iteration 6594: 0.04258028678601428\n",
      "Train Loss at iteration 6595: 0.04258018027664716\n",
      "Train Loss at iteration 6596: 0.04258007378544144\n",
      "Train Loss at iteration 6597: 0.04257996731239375\n",
      "Train Loss at iteration 6598: 0.04257986085750073\n",
      "Train Loss at iteration 6599: 0.042579754420758974\n",
      "Train Loss at iteration 6600: 0.042579648002165134\n",
      "Train Loss at iteration 6601: 0.04257954160171584\n",
      "Train Loss at iteration 6602: 0.04257943521940769\n",
      "Train Loss at iteration 6603: 0.04257932885523734\n",
      "Train Loss at iteration 6604: 0.04257922250920142\n",
      "Train Loss at iteration 6605: 0.04257911618129653\n",
      "Train Loss at iteration 6606: 0.04257900987151933\n",
      "Train Loss at iteration 6607: 0.04257890357986644\n",
      "Train Loss at iteration 6608: 0.04257879730633449\n",
      "Train Loss at iteration 6609: 0.04257869105092012\n",
      "Train Loss at iteration 6610: 0.04257858481361996\n",
      "Train Loss at iteration 6611: 0.04257847859443063\n",
      "Train Loss at iteration 6612: 0.0425783723933488\n",
      "Train Loss at iteration 6613: 0.04257826621037107\n",
      "Train Loss at iteration 6614: 0.04257816004549411\n",
      "Train Loss at iteration 6615: 0.04257805389871453\n",
      "Train Loss at iteration 6616: 0.04257794777002899\n",
      "Train Loss at iteration 6617: 0.042577841659434096\n",
      "Train Loss at iteration 6618: 0.04257773556692654\n",
      "Train Loss at iteration 6619: 0.04257762949250292\n",
      "Train Loss at iteration 6620: 0.0425775234361599\n",
      "Train Loss at iteration 6621: 0.04257741739789411\n",
      "Train Loss at iteration 6622: 0.042577311377702196\n",
      "Train Loss at iteration 6623: 0.042577205375580816\n",
      "Train Loss at iteration 6624: 0.042577099391526596\n",
      "Train Loss at iteration 6625: 0.04257699342553619\n",
      "Train Loss at iteration 6626: 0.042576887477606254\n",
      "Train Loss at iteration 6627: 0.04257678154773343\n",
      "Train Loss at iteration 6628: 0.04257667563591434\n",
      "Train Loss at iteration 6629: 0.04257656974214569\n",
      "Train Loss at iteration 6630: 0.04257646386642408\n",
      "Train Loss at iteration 6631: 0.042576358008746165\n",
      "Train Loss at iteration 6632: 0.04257625216910864\n",
      "Train Loss at iteration 6633: 0.0425761463475081\n",
      "Train Loss at iteration 6634: 0.042576040543941235\n",
      "Train Loss at iteration 6635: 0.042575934758404675\n",
      "Train Loss at iteration 6636: 0.04257582899089511\n",
      "Train Loss at iteration 6637: 0.042575723241409166\n",
      "Train Loss at iteration 6638: 0.04257561750994351\n",
      "Train Loss at iteration 6639: 0.0425755117964948\n",
      "Train Loss at iteration 6640: 0.04257540610105969\n",
      "Train Loss at iteration 6641: 0.04257530042363484\n",
      "Train Loss at iteration 6642: 0.0425751947642169\n",
      "Train Loss at iteration 6643: 0.04257508912280255\n",
      "Train Loss at iteration 6644: 0.04257498349938845\n",
      "Train Loss at iteration 6645: 0.042574877893971255\n",
      "Train Loss at iteration 6646: 0.042574772306547604\n",
      "Train Loss at iteration 6647: 0.0425746667371142\n",
      "Train Loss at iteration 6648: 0.04257456118566769\n",
      "Train Loss at iteration 6649: 0.04257445565220474\n",
      "Train Loss at iteration 6650: 0.04257435013672202\n",
      "Train Loss at iteration 6651: 0.04257424463921619\n",
      "Train Loss at iteration 6652: 0.04257413915968392\n",
      "Train Loss at iteration 6653: 0.042574033698121884\n",
      "Train Loss at iteration 6654: 0.042573928254526744\n",
      "Train Loss at iteration 6655: 0.042573822828895165\n",
      "Train Loss at iteration 6656: 0.042573717421223845\n",
      "Train Loss at iteration 6657: 0.042573612031509424\n",
      "Train Loss at iteration 6658: 0.04257350665974859\n",
      "Train Loss at iteration 6659: 0.042573401305938015\n",
      "Train Loss at iteration 6660: 0.04257329597007438\n",
      "Train Loss at iteration 6661: 0.042573190652154334\n",
      "Train Loss at iteration 6662: 0.04257308535217458\n",
      "Train Loss at iteration 6663: 0.042572980070131786\n",
      "Train Loss at iteration 6664: 0.04257287480602264\n",
      "Train Loss at iteration 6665: 0.04257276955984379\n",
      "Train Loss at iteration 6666: 0.04257266433159194\n",
      "Train Loss at iteration 6667: 0.042572559121263766\n",
      "Train Loss at iteration 6668: 0.04257245392885595\n",
      "Train Loss at iteration 6669: 0.042572348754365164\n",
      "Train Loss at iteration 6670: 0.04257224359778809\n",
      "Train Loss at iteration 6671: 0.04257213845912142\n",
      "Train Loss at iteration 6672: 0.04257203333836183\n",
      "Train Loss at iteration 6673: 0.042571928235506006\n",
      "Train Loss at iteration 6674: 0.042571823150550636\n",
      "Train Loss at iteration 6675: 0.042571718083492416\n",
      "Train Loss at iteration 6676: 0.042571613034328014\n",
      "Train Loss at iteration 6677: 0.04257150800305413\n",
      "Train Loss at iteration 6678: 0.04257140298966745\n",
      "Train Loss at iteration 6679: 0.04257129799416466\n",
      "Train Loss at iteration 6680: 0.04257119301654244\n",
      "Train Loss at iteration 6681: 0.04257108805679751\n",
      "Train Loss at iteration 6682: 0.04257098311492653\n",
      "Train Loss at iteration 6683: 0.04257087819092622\n",
      "Train Loss at iteration 6684: 0.042570773284793244\n",
      "Train Loss at iteration 6685: 0.042570668396524326\n",
      "Train Loss at iteration 6686: 0.042570563526116136\n",
      "Train Loss at iteration 6687: 0.04257045867356539\n",
      "Train Loss at iteration 6688: 0.042570353838868764\n",
      "Train Loss at iteration 6689: 0.04257024902202297\n",
      "Train Loss at iteration 6690: 0.04257014422302471\n",
      "Train Loss at iteration 6691: 0.04257003944187066\n",
      "Train Loss at iteration 6692: 0.042569934678557526\n",
      "Train Loss at iteration 6693: 0.04256982993308204\n",
      "Train Loss at iteration 6694: 0.042569725205440875\n",
      "Train Loss at iteration 6695: 0.04256962049563072\n",
      "Train Loss at iteration 6696: 0.04256951580364831\n",
      "Train Loss at iteration 6697: 0.04256941112949032\n",
      "Train Loss at iteration 6698: 0.04256930647315348\n",
      "Train Loss at iteration 6699: 0.042569201834634474\n",
      "Train Loss at iteration 6700: 0.04256909721393002\n",
      "Train Loss at iteration 6701: 0.042568992611036825\n",
      "Train Loss at iteration 6702: 0.04256888802595158\n",
      "Train Loss at iteration 6703: 0.04256878345867102\n",
      "Train Loss at iteration 6704: 0.042568678909191825\n",
      "Train Loss at iteration 6705: 0.04256857437751072\n",
      "Train Loss at iteration 6706: 0.04256846986362442\n",
      "Train Loss at iteration 6707: 0.042568365367529636\n",
      "Train Loss at iteration 6708: 0.04256826088922307\n",
      "Train Loss at iteration 6709: 0.04256815642870143\n",
      "Train Loss at iteration 6710: 0.04256805198596144\n",
      "Train Loss at iteration 6711: 0.042567947560999825\n",
      "Train Loss at iteration 6712: 0.04256784315381329\n",
      "Train Loss at iteration 6713: 0.04256773876439853\n",
      "Train Loss at iteration 6714: 0.0425676343927523\n",
      "Train Loss at iteration 6715: 0.0425675300388713\n",
      "Train Loss at iteration 6716: 0.042567425702752244\n",
      "Train Loss at iteration 6717: 0.04256732138439185\n",
      "Train Loss at iteration 6718: 0.04256721708378685\n",
      "Train Loss at iteration 6719: 0.042567112800933955\n",
      "Train Loss at iteration 6720: 0.042567008535829884\n",
      "Train Loss at iteration 6721: 0.04256690428847137\n",
      "Train Loss at iteration 6722: 0.042566800058855124\n",
      "Train Loss at iteration 6723: 0.04256669584697788\n",
      "Train Loss at iteration 6724: 0.042566591652836355\n",
      "Train Loss at iteration 6725: 0.04256648747642729\n",
      "Train Loss at iteration 6726: 0.04256638331774738\n",
      "Train Loss at iteration 6727: 0.04256627917679339\n",
      "Train Loss at iteration 6728: 0.042566175053562\n",
      "Train Loss at iteration 6729: 0.04256607094805\n",
      "Train Loss at iteration 6730: 0.042565966860254066\n",
      "Train Loss at iteration 6731: 0.04256586279017095\n",
      "Train Loss at iteration 6732: 0.0425657587377974\n",
      "Train Loss at iteration 6733: 0.042565654703130104\n",
      "Train Loss at iteration 6734: 0.042565550686165826\n",
      "Train Loss at iteration 6735: 0.0425654466869013\n",
      "Train Loss at iteration 6736: 0.04256534270533325\n",
      "Train Loss at iteration 6737: 0.0425652387414584\n",
      "Train Loss at iteration 6738: 0.04256513479527352\n",
      "Train Loss at iteration 6739: 0.04256503086677531\n",
      "Train Loss at iteration 6740: 0.04256492695596053\n",
      "Train Loss at iteration 6741: 0.0425648230628259\n",
      "Train Loss at iteration 6742: 0.042564719187368164\n",
      "Train Loss at iteration 6743: 0.042564615329584075\n",
      "Train Loss at iteration 6744: 0.042564511489470366\n",
      "Train Loss at iteration 6745: 0.04256440766702377\n",
      "Train Loss at iteration 6746: 0.04256430386224103\n",
      "Train Loss at iteration 6747: 0.0425642000751189\n",
      "Train Loss at iteration 6748: 0.04256409630565411\n",
      "Train Loss at iteration 6749: 0.04256399255384341\n",
      "Train Loss at iteration 6750: 0.04256388881968356\n",
      "Train Loss at iteration 6751: 0.04256378510317127\n",
      "Train Loss at iteration 6752: 0.042563681404303305\n",
      "Train Loss at iteration 6753: 0.04256357772307642\n",
      "Train Loss at iteration 6754: 0.042563474059487356\n",
      "Train Loss at iteration 6755: 0.042563370413532864\n",
      "Train Loss at iteration 6756: 0.042563266785209684\n",
      "Train Loss at iteration 6757: 0.042563163174514584\n",
      "Train Loss at iteration 6758: 0.042563059581444296\n",
      "Train Loss at iteration 6759: 0.04256295600599557\n",
      "Train Loss at iteration 6760: 0.042562852448165184\n",
      "Train Loss at iteration 6761: 0.042562748907949866\n",
      "Train Loss at iteration 6762: 0.04256264538534638\n",
      "Train Loss at iteration 6763: 0.04256254188035148\n",
      "Train Loss at iteration 6764: 0.04256243839296193\n",
      "Train Loss at iteration 6765: 0.04256233492317447\n",
      "Train Loss at iteration 6766: 0.04256223147098586\n",
      "Train Loss at iteration 6767: 0.04256212803639288\n",
      "Train Loss at iteration 6768: 0.04256202461939225\n",
      "Train Loss at iteration 6769: 0.04256192121998076\n",
      "Train Loss at iteration 6770: 0.04256181783815516\n",
      "Train Loss at iteration 6771: 0.04256171447391221\n",
      "Train Loss at iteration 6772: 0.04256161112724868\n",
      "Train Loss at iteration 6773: 0.04256150779816131\n",
      "Train Loss at iteration 6774: 0.04256140448664689\n",
      "Train Loss at iteration 6775: 0.04256130119270217\n",
      "Train Loss at iteration 6776: 0.04256119791632392\n",
      "Train Loss at iteration 6777: 0.0425610946575089\n",
      "Train Loss at iteration 6778: 0.04256099141625388\n",
      "Train Loss at iteration 6779: 0.04256088819255562\n",
      "Train Loss at iteration 6780: 0.0425607849864109\n",
      "Train Loss at iteration 6781: 0.04256068179781649\n",
      "Train Loss at iteration 6782: 0.04256057862676913\n",
      "Train Loss at iteration 6783: 0.04256047547326563\n",
      "Train Loss at iteration 6784: 0.04256037233730274\n",
      "Train Loss at iteration 6785: 0.04256026921887723\n",
      "Train Loss at iteration 6786: 0.042560166117985875\n",
      "Train Loss at iteration 6787: 0.04256006303462545\n",
      "Train Loss at iteration 6788: 0.04255995996879273\n",
      "Train Loss at iteration 6789: 0.04255985692048449\n",
      "Train Loss at iteration 6790: 0.042559753889697495\n",
      "Train Loss at iteration 6791: 0.04255965087642853\n",
      "Train Loss at iteration 6792: 0.04255954788067438\n",
      "Train Loss at iteration 6793: 0.0425594449024318\n",
      "Train Loss at iteration 6794: 0.04255934194169759\n",
      "Train Loss at iteration 6795: 0.04255923899846852\n",
      "Train Loss at iteration 6796: 0.04255913607274137\n",
      "Train Loss at iteration 6797: 0.04255903316451293\n",
      "Train Loss at iteration 6798: 0.04255893027377997\n",
      "Train Loss at iteration 6799: 0.042558827400539276\n",
      "Train Loss at iteration 6800: 0.042558724544787624\n",
      "Train Loss at iteration 6801: 0.042558621706521804\n",
      "Train Loss at iteration 6802: 0.042558518885738604\n",
      "Train Loss at iteration 6803: 0.042558416082434804\n",
      "Train Loss at iteration 6804: 0.04255831329660719\n",
      "Train Loss at iteration 6805: 0.04255821052825256\n",
      "Train Loss at iteration 6806: 0.04255810777736768\n",
      "Train Loss at iteration 6807: 0.042558005043949354\n",
      "Train Loss at iteration 6808: 0.04255790232799437\n",
      "Train Loss at iteration 6809: 0.042557799629499515\n",
      "Train Loss at iteration 6810: 0.042557696948461565\n",
      "Train Loss at iteration 6811: 0.042557594284877355\n",
      "Train Loss at iteration 6812: 0.04255749163874363\n",
      "Train Loss at iteration 6813: 0.04255738901005719\n",
      "Train Loss at iteration 6814: 0.04255728639881485\n",
      "Train Loss at iteration 6815: 0.0425571838050134\n",
      "Train Loss at iteration 6816: 0.04255708122864961\n",
      "Train Loss at iteration 6817: 0.042556978669720316\n",
      "Train Loss at iteration 6818: 0.04255687612822229\n",
      "Train Loss at iteration 6819: 0.04255677360415232\n",
      "Train Loss at iteration 6820: 0.04255667109750721\n",
      "Train Loss at iteration 6821: 0.042556568608283774\n",
      "Train Loss at iteration 6822: 0.0425564661364788\n",
      "Train Loss at iteration 6823: 0.04255636368208909\n",
      "Train Loss at iteration 6824: 0.04255626124511145\n",
      "Train Loss at iteration 6825: 0.04255615882554268\n",
      "Train Loss at iteration 6826: 0.04255605642337957\n",
      "Train Loss at iteration 6827: 0.04255595403861894\n",
      "Train Loss at iteration 6828: 0.04255585167125758\n",
      "Train Loss at iteration 6829: 0.04255574932129231\n",
      "Train Loss at iteration 6830: 0.042555646988719935\n",
      "Train Loss at iteration 6831: 0.04255554467353723\n",
      "Train Loss at iteration 6832: 0.04255544237574104\n",
      "Train Loss at iteration 6833: 0.04255534009532815\n",
      "Train Loss at iteration 6834: 0.042555237832295395\n",
      "Train Loss at iteration 6835: 0.04255513558663956\n",
      "Train Loss at iteration 6836: 0.04255503335835745\n",
      "Train Loss at iteration 6837: 0.042554931147445886\n",
      "Train Loss at iteration 6838: 0.04255482895390169\n",
      "Train Loss at iteration 6839: 0.04255472677772166\n",
      "Train Loss at iteration 6840: 0.04255462461890262\n",
      "Train Loss at iteration 6841: 0.042554522477441376\n",
      "Train Loss at iteration 6842: 0.04255442035333473\n",
      "Train Loss at iteration 6843: 0.04255431824657953\n",
      "Train Loss at iteration 6844: 0.04255421615717256\n",
      "Train Loss at iteration 6845: 0.04255411408511066\n",
      "Train Loss at iteration 6846: 0.04255401203039063\n",
      "Train Loss at iteration 6847: 0.04255390999300929\n",
      "Train Loss at iteration 6848: 0.04255380797296349\n",
      "Train Loss at iteration 6849: 0.04255370597024999\n",
      "Train Loss at iteration 6850: 0.04255360398486567\n",
      "Train Loss at iteration 6851: 0.04255350201680731\n",
      "Train Loss at iteration 6852: 0.042553400066071764\n",
      "Train Loss at iteration 6853: 0.04255329813265582\n",
      "Train Loss at iteration 6854: 0.04255319621655633\n",
      "Train Loss at iteration 6855: 0.04255309431777011\n",
      "Train Loss at iteration 6856: 0.04255299243629398\n",
      "Train Loss at iteration 6857: 0.042552890572124774\n",
      "Train Loss at iteration 6858: 0.04255278872525931\n",
      "Train Loss at iteration 6859: 0.04255268689569443\n",
      "Train Loss at iteration 6860: 0.04255258508342692\n",
      "Train Loss at iteration 6861: 0.04255248328845365\n",
      "Train Loss at iteration 6862: 0.042552381510771446\n",
      "Train Loss at iteration 6863: 0.042552279750377134\n",
      "Train Loss at iteration 6864: 0.04255217800726753\n",
      "Train Loss at iteration 6865: 0.04255207628143948\n",
      "Train Loss at iteration 6866: 0.04255197457288982\n",
      "Train Loss at iteration 6867: 0.042551872881615355\n",
      "Train Loss at iteration 6868: 0.04255177120761296\n",
      "Train Loss at iteration 6869: 0.04255166955087944\n",
      "Train Loss at iteration 6870: 0.04255156791141163\n",
      "Train Loss at iteration 6871: 0.042551466289206384\n",
      "Train Loss at iteration 6872: 0.04255136468426053\n",
      "Train Loss at iteration 6873: 0.04255126309657091\n",
      "Train Loss at iteration 6874: 0.042551161526134355\n",
      "Train Loss at iteration 6875: 0.042551059972947704\n",
      "Train Loss at iteration 6876: 0.0425509584370078\n",
      "Train Loss at iteration 6877: 0.04255085691831149\n",
      "Train Loss at iteration 6878: 0.042550755416855594\n",
      "Train Loss at iteration 6879: 0.042550653932636975\n",
      "Train Loss at iteration 6880: 0.04255055246565247\n",
      "Train Loss at iteration 6881: 0.04255045101589891\n",
      "Train Loss at iteration 6882: 0.04255034958337316\n",
      "Train Loss at iteration 6883: 0.042550248168072055\n",
      "Train Loss at iteration 6884: 0.04255014676999244\n",
      "Train Loss at iteration 6885: 0.04255004538913116\n",
      "Train Loss at iteration 6886: 0.042549944025485065\n",
      "Train Loss at iteration 6887: 0.042549842679051\n",
      "Train Loss at iteration 6888: 0.0425497413498258\n",
      "Train Loss at iteration 6889: 0.04254964003780635\n",
      "Train Loss at iteration 6890: 0.04254953874298946\n",
      "Train Loss at iteration 6891: 0.042549437465372014\n",
      "Train Loss at iteration 6892: 0.04254933620495084\n",
      "Train Loss at iteration 6893: 0.042549234961722804\n",
      "Train Loss at iteration 6894: 0.042549133735684745\n",
      "Train Loss at iteration 6895: 0.042549032526833536\n",
      "Train Loss at iteration 6896: 0.04254893133516602\n",
      "Train Loss at iteration 6897: 0.042548830160679056\n",
      "Train Loss at iteration 6898: 0.04254872900336949\n",
      "Train Loss at iteration 6899: 0.04254862786323418\n",
      "Train Loss at iteration 6900: 0.04254852674027\n",
      "Train Loss at iteration 6901: 0.04254842563447378\n",
      "Train Loss at iteration 6902: 0.042548324545842404\n",
      "Train Loss at iteration 6903: 0.04254822347437273\n",
      "Train Loss at iteration 6904: 0.04254812242006161\n",
      "Train Loss at iteration 6905: 0.04254802138290589\n",
      "Train Loss at iteration 6906: 0.042547920362902454\n",
      "Train Loss at iteration 6907: 0.04254781936004815\n",
      "Train Loss at iteration 6908: 0.04254771837433986\n",
      "Train Loss at iteration 6909: 0.04254761740577445\n",
      "Train Loss at iteration 6910: 0.042547516454348754\n",
      "Train Loss at iteration 6911: 0.042547415520059646\n",
      "Train Loss at iteration 6912: 0.042547314602904014\n",
      "Train Loss at iteration 6913: 0.04254721370287872\n",
      "Train Loss at iteration 6914: 0.0425471128199806\n",
      "Train Loss at iteration 6915: 0.04254701195420656\n",
      "Train Loss at iteration 6916: 0.04254691110555344\n",
      "Train Loss at iteration 6917: 0.04254681027401813\n",
      "Train Loss at iteration 6918: 0.042546709459597494\n",
      "Train Loss at iteration 6919: 0.0425466086622884\n",
      "Train Loss at iteration 6920: 0.04254650788208773\n",
      "Train Loss at iteration 6921: 0.04254640711899234\n",
      "Train Loss at iteration 6922: 0.04254630637299912\n",
      "Train Loss at iteration 6923: 0.04254620564410493\n",
      "Train Loss at iteration 6924: 0.04254610493230664\n",
      "Train Loss at iteration 6925: 0.04254600423760115\n",
      "Train Loss at iteration 6926: 0.042545903559985325\n",
      "Train Loss at iteration 6927: 0.042545802899456024\n",
      "Train Loss at iteration 6928: 0.042545702256010155\n",
      "Train Loss at iteration 6929: 0.042545601629644575\n",
      "Train Loss at iteration 6930: 0.04254550102035616\n",
      "Train Loss at iteration 6931: 0.04254540042814182\n",
      "Train Loss at iteration 6932: 0.0425452998529984\n",
      "Train Loss at iteration 6933: 0.042545199294922786\n",
      "Train Loss at iteration 6934: 0.04254509875391188\n",
      "Train Loss at iteration 6935: 0.04254499822996256\n",
      "Train Loss at iteration 6936: 0.0425448977230717\n",
      "Train Loss at iteration 6937: 0.042544797233236176\n",
      "Train Loss at iteration 6938: 0.04254469676045289\n",
      "Train Loss at iteration 6939: 0.042544596304718726\n",
      "Train Loss at iteration 6940: 0.04254449586603057\n",
      "Train Loss at iteration 6941: 0.042544395444385284\n",
      "Train Loss at iteration 6942: 0.0425442950397798\n",
      "Train Loss at iteration 6943: 0.042544194652210966\n",
      "Train Loss at iteration 6944: 0.0425440942816757\n",
      "Train Loss at iteration 6945: 0.04254399392817086\n",
      "Train Loss at iteration 6946: 0.04254389359169337\n",
      "Train Loss at iteration 6947: 0.04254379327224011\n",
      "Train Loss at iteration 6948: 0.04254369296980797\n",
      "Train Loss at iteration 6949: 0.04254359268439384\n",
      "Train Loss at iteration 6950: 0.04254349241599461\n",
      "Train Loss at iteration 6951: 0.04254339216460718\n",
      "Train Loss at iteration 6952: 0.04254329193022845\n",
      "Train Loss at iteration 6953: 0.042543191712855304\n",
      "Train Loss at iteration 6954: 0.04254309151248464\n",
      "Train Loss at iteration 6955: 0.042542991329113364\n",
      "Train Loss at iteration 6956: 0.04254289116273838\n",
      "Train Loss at iteration 6957: 0.04254279101335656\n",
      "Train Loss at iteration 6958: 0.04254269088096483\n",
      "Train Loss at iteration 6959: 0.04254259076556007\n",
      "Train Loss at iteration 6960: 0.042542490667139186\n",
      "Train Loss at iteration 6961: 0.042542390585699086\n",
      "Train Loss at iteration 6962: 0.04254229052123667\n",
      "Train Loss at iteration 6963: 0.04254219047374884\n",
      "Train Loss at iteration 6964: 0.04254209044323249\n",
      "Train Loss at iteration 6965: 0.04254199042968453\n",
      "Train Loss at iteration 6966: 0.04254189043310188\n",
      "Train Loss at iteration 6967: 0.04254179045348142\n",
      "Train Loss at iteration 6968: 0.04254169049082007\n",
      "Train Loss at iteration 6969: 0.04254159054511475\n",
      "Train Loss at iteration 6970: 0.04254149061636235\n",
      "Train Loss at iteration 6971: 0.04254139070455976\n",
      "Train Loss at iteration 6972: 0.04254129080970394\n",
      "Train Loss at iteration 6973: 0.042541190931791754\n",
      "Train Loss at iteration 6974: 0.04254109107082013\n",
      "Train Loss at iteration 6975: 0.04254099122678597\n",
      "Train Loss at iteration 6976: 0.04254089139968621\n",
      "Train Loss at iteration 6977: 0.04254079158951774\n",
      "Train Loss at iteration 6978: 0.04254069179627748\n",
      "Train Loss at iteration 6979: 0.042540592019962346\n",
      "Train Loss at iteration 6980: 0.042540492260569246\n",
      "Train Loss at iteration 6981: 0.042540392518095105\n",
      "Train Loss at iteration 6982: 0.04254029279253683\n",
      "Train Loss at iteration 6983: 0.042540193083891356\n",
      "Train Loss at iteration 6984: 0.04254009339215558\n",
      "Train Loss at iteration 6985: 0.04253999371732642\n",
      "Train Loss at iteration 6986: 0.0425398940594008\n",
      "Train Loss at iteration 6987: 0.04253979441837566\n",
      "Train Loss at iteration 6988: 0.04253969479424789\n",
      "Train Loss at iteration 6989: 0.04253959518701443\n",
      "Train Loss at iteration 6990: 0.04253949559667218\n",
      "Train Loss at iteration 6991: 0.0425393960232181\n",
      "Train Loss at iteration 6992: 0.042539296466649085\n",
      "Train Loss at iteration 6993: 0.042539196926962064\n",
      "Train Loss at iteration 6994: 0.04253909740415396\n",
      "Train Loss at iteration 6995: 0.042538997898221693\n",
      "Train Loss at iteration 6996: 0.04253889840916221\n",
      "Train Loss at iteration 6997: 0.04253879893697242\n",
      "Train Loss at iteration 6998: 0.04253869948164926\n",
      "Train Loss at iteration 6999: 0.04253860004318965\n",
      "Train Loss at iteration 7000: 0.04253850062159052\n",
      "Train Loss at iteration 7001: 0.04253840121684881\n",
      "Train Loss at iteration 7002: 0.04253830182896145\n",
      "Train Loss at iteration 7003: 0.04253820245792535\n",
      "Train Loss at iteration 7004: 0.04253810310373745\n",
      "Train Loss at iteration 7005: 0.042538003766394696\n",
      "Train Loss at iteration 7006: 0.042537904445894004\n",
      "Train Loss at iteration 7007: 0.042537805142232335\n",
      "Train Loss at iteration 7008: 0.04253770585540659\n",
      "Train Loss at iteration 7009: 0.04253760658541372\n",
      "Train Loss at iteration 7010: 0.042537507332250674\n",
      "Train Loss at iteration 7011: 0.04253740809591436\n",
      "Train Loss at iteration 7012: 0.04253730887640174\n",
      "Train Loss at iteration 7013: 0.04253720967370974\n",
      "Train Loss at iteration 7014: 0.0425371104878353\n",
      "Train Loss at iteration 7015: 0.04253701131877536\n",
      "Train Loss at iteration 7016: 0.042536912166526865\n",
      "Train Loss at iteration 7017: 0.04253681303108674\n",
      "Train Loss at iteration 7018: 0.04253671391245194\n",
      "Train Loss at iteration 7019: 0.04253661481061942\n",
      "Train Loss at iteration 7020: 0.04253651572558609\n",
      "Train Loss at iteration 7021: 0.04253641665734891\n",
      "Train Loss at iteration 7022: 0.04253631760590484\n",
      "Train Loss at iteration 7023: 0.042536218571250804\n",
      "Train Loss at iteration 7024: 0.042536119553383746\n",
      "Train Loss at iteration 7025: 0.04253602055230064\n",
      "Train Loss at iteration 7026: 0.0425359215679984\n",
      "Train Loss at iteration 7027: 0.04253582260047399\n",
      "Train Loss at iteration 7028: 0.042535723649724355\n",
      "Train Loss at iteration 7029: 0.04253562471574645\n",
      "Train Loss at iteration 7030: 0.04253552579853722\n",
      "Train Loss at iteration 7031: 0.042535426898093615\n",
      "Train Loss at iteration 7032: 0.04253532801441258\n",
      "Train Loss at iteration 7033: 0.04253522914749108\n",
      "Train Loss at iteration 7034: 0.042535130297326064\n",
      "Train Loss at iteration 7035: 0.042535031463914484\n",
      "Train Loss at iteration 7036: 0.04253493264725329\n",
      "Train Loss at iteration 7037: 0.04253483384733943\n",
      "Train Loss at iteration 7038: 0.04253473506416988\n",
      "Train Loss at iteration 7039: 0.04253463629774159\n",
      "Train Loss at iteration 7040: 0.0425345375480515\n",
      "Train Loss at iteration 7041: 0.04253443881509659\n",
      "Train Loss at iteration 7042: 0.0425343400988738\n",
      "Train Loss at iteration 7043: 0.0425342413993801\n",
      "Train Loss at iteration 7044: 0.042534142716612454\n",
      "Train Loss at iteration 7045: 0.0425340440505678\n",
      "Train Loss at iteration 7046: 0.042533945401243126\n",
      "Train Loss at iteration 7047: 0.04253384676863538\n",
      "Train Loss at iteration 7048: 0.04253374815274152\n",
      "Train Loss at iteration 7049: 0.04253364955355851\n",
      "Train Loss at iteration 7050: 0.04253355097108333\n",
      "Train Loss at iteration 7051: 0.04253345240531293\n",
      "Train Loss at iteration 7052: 0.04253335385624427\n",
      "Train Loss at iteration 7053: 0.04253325532387433\n",
      "Train Loss at iteration 7054: 0.042533156808200065\n",
      "Train Loss at iteration 7055: 0.04253305830921846\n",
      "Train Loss at iteration 7056: 0.042532959826926474\n",
      "Train Loss at iteration 7057: 0.042532861361321055\n",
      "Train Loss at iteration 7058: 0.04253276291239919\n",
      "Train Loss at iteration 7059: 0.04253266448015786\n",
      "Train Loss at iteration 7060: 0.04253256606459401\n",
      "Train Loss at iteration 7061: 0.04253246766570464\n",
      "Train Loss at iteration 7062: 0.0425323692834867\n",
      "Train Loss at iteration 7063: 0.042532270917937175\n",
      "Train Loss at iteration 7064: 0.04253217256905303\n",
      "Train Loss at iteration 7065: 0.04253207423683124\n",
      "Train Loss at iteration 7066: 0.04253197592126878\n",
      "Train Loss at iteration 7067: 0.04253187762236264\n",
      "Train Loss at iteration 7068: 0.04253177934010977\n",
      "Train Loss at iteration 7069: 0.04253168107450717\n",
      "Train Loss at iteration 7070: 0.0425315828255518\n",
      "Train Loss at iteration 7071: 0.04253148459324065\n",
      "Train Loss at iteration 7072: 0.042531386377570694\n",
      "Train Loss at iteration 7073: 0.04253128817853891\n",
      "Train Loss at iteration 7074: 0.04253118999614229\n",
      "Train Loss at iteration 7075: 0.0425310918303778\n",
      "Train Loss at iteration 7076: 0.042530993681242434\n",
      "Train Loss at iteration 7077: 0.04253089554873317\n",
      "Train Loss at iteration 7078: 0.04253079743284697\n",
      "Train Loss at iteration 7079: 0.042530699333580856\n",
      "Train Loss at iteration 7080: 0.042530601250931784\n",
      "Train Loss at iteration 7081: 0.04253050318489675\n",
      "Train Loss at iteration 7082: 0.04253040513547273\n",
      "Train Loss at iteration 7083: 0.04253030710265673\n",
      "Train Loss at iteration 7084: 0.042530209086445704\n",
      "Train Loss at iteration 7085: 0.042530111086836686\n",
      "Train Loss at iteration 7086: 0.04253001310382663\n",
      "Train Loss at iteration 7087: 0.042529915137412534\n",
      "Train Loss at iteration 7088: 0.04252981718759139\n",
      "Train Loss at iteration 7089: 0.04252971925436019\n",
      "Train Loss at iteration 7090: 0.04252962133771592\n",
      "Train Loss at iteration 7091: 0.04252952343765557\n",
      "Train Loss at iteration 7092: 0.042529425554176145\n",
      "Train Loss at iteration 7093: 0.04252932768727461\n",
      "Train Loss at iteration 7094: 0.042529229836948\n",
      "Train Loss at iteration 7095: 0.04252913200319328\n",
      "Train Loss at iteration 7096: 0.04252903418600745\n",
      "Train Loss at iteration 7097: 0.04252893638538751\n",
      "Train Loss at iteration 7098: 0.04252883860133047\n",
      "Train Loss at iteration 7099: 0.042528740833833296\n",
      "Train Loss at iteration 7100: 0.04252864308289302\n",
      "Train Loss at iteration 7101: 0.0425285453485066\n",
      "Train Loss at iteration 7102: 0.04252844763067108\n",
      "Train Loss at iteration 7103: 0.042528349929383445\n",
      "Train Loss at iteration 7104: 0.04252825224464067\n",
      "Train Loss at iteration 7105: 0.04252815457643979\n",
      "Train Loss at iteration 7106: 0.04252805692477778\n",
      "Train Loss at iteration 7107: 0.04252795928965166\n",
      "Train Loss at iteration 7108: 0.04252786167105844\n",
      "Train Loss at iteration 7109: 0.042527764068995104\n",
      "Train Loss at iteration 7110: 0.04252766648345867\n",
      "Train Loss at iteration 7111: 0.042527568914446146\n",
      "Train Loss at iteration 7112: 0.04252747136195451\n",
      "Train Loss at iteration 7113: 0.04252737382598082\n",
      "Train Loss at iteration 7114: 0.04252727630652205\n",
      "Train Loss at iteration 7115: 0.042527178803575205\n",
      "Train Loss at iteration 7116: 0.04252708131713729\n",
      "Train Loss at iteration 7117: 0.04252698384720533\n",
      "Train Loss at iteration 7118: 0.04252688639377634\n",
      "Train Loss at iteration 7119: 0.04252678895684732\n",
      "Train Loss at iteration 7120: 0.042526691536415286\n",
      "Train Loss at iteration 7121: 0.04252659413247724\n",
      "Train Loss at iteration 7122: 0.0425264967450302\n",
      "Train Loss at iteration 7123: 0.04252639937407119\n",
      "Train Loss at iteration 7124: 0.04252630201959721\n",
      "Train Loss at iteration 7125: 0.042526204681605284\n",
      "Train Loss at iteration 7126: 0.04252610736009243\n",
      "Train Loss at iteration 7127: 0.04252601005505565\n",
      "Train Loss at iteration 7128: 0.04252591276649198\n",
      "Train Loss at iteration 7129: 0.042525815494398406\n",
      "Train Loss at iteration 7130: 0.042525718238771985\n",
      "Train Loss at iteration 7131: 0.04252562099960972\n",
      "Train Loss at iteration 7132: 0.042525523776908616\n",
      "Train Loss at iteration 7133: 0.04252542657066572\n",
      "Train Loss at iteration 7134: 0.04252532938087804\n",
      "Train Loss at iteration 7135: 0.042525232207542576\n",
      "Train Loss at iteration 7136: 0.0425251350506564\n",
      "Train Loss at iteration 7137: 0.04252503791021648\n",
      "Train Loss at iteration 7138: 0.04252494078621988\n",
      "Train Loss at iteration 7139: 0.042524843678663604\n",
      "Train Loss at iteration 7140: 0.04252474658754469\n",
      "Train Loss at iteration 7141: 0.04252464951286014\n",
      "Train Loss at iteration 7142: 0.04252455245460701\n",
      "Train Loss at iteration 7143: 0.04252445541278232\n",
      "Train Loss at iteration 7144: 0.042524358387383075\n",
      "Train Loss at iteration 7145: 0.04252426137840632\n",
      "Train Loss at iteration 7146: 0.0425241643858491\n",
      "Train Loss at iteration 7147: 0.042524067409708426\n",
      "Train Loss at iteration 7148: 0.04252397044998131\n",
      "Train Loss at iteration 7149: 0.04252387350666482\n",
      "Train Loss at iteration 7150: 0.04252377657975596\n",
      "Train Loss at iteration 7151: 0.04252367966925179\n",
      "Train Loss at iteration 7152: 0.0425235827751493\n",
      "Train Loss at iteration 7153: 0.04252348589744557\n",
      "Train Loss at iteration 7154: 0.042523389036137615\n",
      "Train Loss at iteration 7155: 0.042523292191222466\n",
      "Train Loss at iteration 7156: 0.04252319536269716\n",
      "Train Loss at iteration 7157: 0.04252309855055873\n",
      "Train Loss at iteration 7158: 0.042523001754804224\n",
      "Train Loss at iteration 7159: 0.042522904975430674\n",
      "Train Loss at iteration 7160: 0.04252280821243511\n",
      "Train Loss at iteration 7161: 0.04252271146581459\n",
      "Train Loss at iteration 7162: 0.04252261473556614\n",
      "Train Loss at iteration 7163: 0.0425225180216868\n",
      "Train Loss at iteration 7164: 0.04252242132417361\n",
      "Train Loss at iteration 7165: 0.042522324643023614\n",
      "Train Loss at iteration 7166: 0.042522227978233865\n",
      "Train Loss at iteration 7167: 0.04252213132980139\n",
      "Train Loss at iteration 7168: 0.04252203469772324\n",
      "Train Loss at iteration 7169: 0.042521938081996453\n",
      "Train Loss at iteration 7170: 0.042521841482618096\n",
      "Train Loss at iteration 7171: 0.04252174489958519\n",
      "Train Loss at iteration 7172: 0.04252164833289478\n",
      "Train Loss at iteration 7173: 0.04252155178254393\n",
      "Train Loss at iteration 7174: 0.042521455248529676\n",
      "Train Loss at iteration 7175: 0.04252135873084907\n",
      "Train Loss at iteration 7176: 0.04252126222949915\n",
      "Train Loss at iteration 7177: 0.042521165744476994\n",
      "Train Loss at iteration 7178: 0.04252106927577963\n",
      "Train Loss at iteration 7179: 0.04252097282340411\n",
      "Train Loss at iteration 7180: 0.042520876387347493\n",
      "Train Loss at iteration 7181: 0.04252077996760683\n",
      "Train Loss at iteration 7182: 0.042520683564179165\n",
      "Train Loss at iteration 7183: 0.04252058717706156\n",
      "Train Loss at iteration 7184: 0.04252049080625105\n",
      "Train Loss at iteration 7185: 0.04252039445174472\n",
      "Train Loss at iteration 7186: 0.04252029811353963\n",
      "Train Loss at iteration 7187: 0.04252020179163279\n",
      "Train Loss at iteration 7188: 0.04252010548602129\n",
      "Train Loss at iteration 7189: 0.042520009196702194\n",
      "Train Loss at iteration 7190: 0.04251991292367253\n",
      "Train Loss at iteration 7191: 0.0425198166669294\n",
      "Train Loss at iteration 7192: 0.042519720426469815\n",
      "Train Loss at iteration 7193: 0.04251962420229088\n",
      "Train Loss at iteration 7194: 0.04251952799438962\n",
      "Train Loss at iteration 7195: 0.04251943180276311\n",
      "Train Loss at iteration 7196: 0.042519335627408415\n",
      "Train Loss at iteration 7197: 0.0425192394683226\n",
      "Train Loss at iteration 7198: 0.042519143325502724\n",
      "Train Loss at iteration 7199: 0.042519047198945836\n",
      "Train Loss at iteration 7200: 0.04251895108864903\n",
      "Train Loss at iteration 7201: 0.04251885499460936\n",
      "Train Loss at iteration 7202: 0.04251875891682388\n",
      "Train Loss at iteration 7203: 0.04251866285528967\n",
      "Train Loss at iteration 7204: 0.04251856681000378\n",
      "Train Loss at iteration 7205: 0.04251847078096331\n",
      "Train Loss at iteration 7206: 0.04251837476816531\n",
      "Train Loss at iteration 7207: 0.04251827877160684\n",
      "Train Loss at iteration 7208: 0.042518182791284984\n",
      "Train Loss at iteration 7209: 0.04251808682719681\n",
      "Train Loss at iteration 7210: 0.04251799087933938\n",
      "Train Loss at iteration 7211: 0.04251789494770976\n",
      "Train Loss at iteration 7212: 0.04251779903230505\n",
      "Train Loss at iteration 7213: 0.042517703133122316\n",
      "Train Loss at iteration 7214: 0.0425176072501586\n",
      "Train Loss at iteration 7215: 0.04251751138341102\n",
      "Train Loss at iteration 7216: 0.04251741553287662\n",
      "Train Loss at iteration 7217: 0.0425173196985525\n",
      "Train Loss at iteration 7218: 0.0425172238804357\n",
      "Train Loss at iteration 7219: 0.04251712807852335\n",
      "Train Loss at iteration 7220: 0.04251703229281249\n",
      "Train Loss at iteration 7221: 0.04251693652330019\n",
      "Train Loss at iteration 7222: 0.04251684076998357\n",
      "Train Loss at iteration 7223: 0.042516745032859675\n",
      "Train Loss at iteration 7224: 0.042516649311925586\n",
      "Train Loss at iteration 7225: 0.04251655360717841\n",
      "Train Loss at iteration 7226: 0.042516457918615205\n",
      "Train Loss at iteration 7227: 0.04251636224623308\n",
      "Train Loss at iteration 7228: 0.042516266590029064\n",
      "Train Loss at iteration 7229: 0.04251617095000031\n",
      "Train Loss at iteration 7230: 0.04251607532614384\n",
      "Train Loss at iteration 7231: 0.04251597971845678\n",
      "Train Loss at iteration 7232: 0.04251588412693621\n",
      "Train Loss at iteration 7233: 0.042515788551579205\n",
      "Train Loss at iteration 7234: 0.04251569299238284\n",
      "Train Loss at iteration 7235: 0.04251559744934422\n",
      "Train Loss at iteration 7236: 0.042515501922460446\n",
      "Train Loss at iteration 7237: 0.04251540641172858\n",
      "Train Loss at iteration 7238: 0.042515310917145735\n",
      "Train Loss at iteration 7239: 0.04251521543870898\n",
      "Train Loss at iteration 7240: 0.04251511997641542\n",
      "Train Loss at iteration 7241: 0.04251502453026214\n",
      "Train Loss at iteration 7242: 0.04251492910024622\n",
      "Train Loss at iteration 7243: 0.04251483368636479\n",
      "Train Loss at iteration 7244: 0.042514738288614906\n",
      "Train Loss at iteration 7245: 0.04251464290699367\n",
      "Train Loss at iteration 7246: 0.0425145475414982\n",
      "Train Loss at iteration 7247: 0.04251445219212557\n",
      "Train Loss at iteration 7248: 0.04251435685887287\n",
      "Train Loss at iteration 7249: 0.042514261541737194\n",
      "Train Loss at iteration 7250: 0.04251416624071566\n",
      "Train Loss at iteration 7251: 0.042514070955805366\n",
      "Train Loss at iteration 7252: 0.042513975687003394\n",
      "Train Loss at iteration 7253: 0.04251388043430685\n",
      "Train Loss at iteration 7254: 0.04251378519771284\n",
      "Train Loss at iteration 7255: 0.04251368997721845\n",
      "Train Loss at iteration 7256: 0.04251359477282079\n",
      "Train Loss at iteration 7257: 0.042513499584516966\n",
      "Train Loss at iteration 7258: 0.04251340441230408\n",
      "Train Loss at iteration 7259: 0.04251330925617921\n",
      "Train Loss at iteration 7260: 0.042513214116139494\n",
      "Train Loss at iteration 7261: 0.04251311899218203\n",
      "Train Loss at iteration 7262: 0.042513023884303905\n",
      "Train Loss at iteration 7263: 0.04251292879250223\n",
      "Train Loss at iteration 7264: 0.042512833716774126\n",
      "Train Loss at iteration 7265: 0.042512738657116696\n",
      "Train Loss at iteration 7266: 0.04251264361352702\n",
      "Train Loss at iteration 7267: 0.042512548586002244\n",
      "Train Loss at iteration 7268: 0.04251245357453944\n",
      "Train Loss at iteration 7269: 0.04251235857913577\n",
      "Train Loss at iteration 7270: 0.04251226359978828\n",
      "Train Loss at iteration 7271: 0.04251216863649413\n",
      "Train Loss at iteration 7272: 0.04251207368925041\n",
      "Train Loss at iteration 7273: 0.04251197875805422\n",
      "Train Loss at iteration 7274: 0.042511883842902706\n",
      "Train Loss at iteration 7275: 0.042511788943792955\n",
      "Train Loss at iteration 7276: 0.04251169406072209\n",
      "Train Loss at iteration 7277: 0.04251159919368722\n",
      "Train Loss at iteration 7278: 0.042511504342685476\n",
      "Train Loss at iteration 7279: 0.04251140950771395\n",
      "Train Loss at iteration 7280: 0.04251131468876978\n",
      "Train Loss at iteration 7281: 0.04251121988585006\n",
      "Train Loss at iteration 7282: 0.04251112509895194\n",
      "Train Loss at iteration 7283: 0.042511030328072505\n",
      "Train Loss at iteration 7284: 0.0425109355732089\n",
      "Train Loss at iteration 7285: 0.04251084083435823\n",
      "Train Loss at iteration 7286: 0.0425107461115176\n",
      "Train Loss at iteration 7287: 0.04251065140468418\n",
      "Train Loss at iteration 7288: 0.04251055671385503\n",
      "Train Loss at iteration 7289: 0.04251046203902732\n",
      "Train Loss at iteration 7290: 0.04251036738019816\n",
      "Train Loss at iteration 7291: 0.04251027273736466\n",
      "Train Loss at iteration 7292: 0.042510178110523954\n",
      "Train Loss at iteration 7293: 0.04251008349967317\n",
      "Train Loss at iteration 7294: 0.04250998890480942\n",
      "Train Loss at iteration 7295: 0.04250989432592985\n",
      "Train Loss at iteration 7296: 0.04250979976303157\n",
      "Train Loss at iteration 7297: 0.04250970521611172\n",
      "Train Loss at iteration 7298: 0.042509610685167405\n",
      "Train Loss at iteration 7299: 0.04250951617019578\n",
      "Train Loss at iteration 7300: 0.04250942167119396\n",
      "Train Loss at iteration 7301: 0.04250932718815908\n",
      "Train Loss at iteration 7302: 0.04250923272108826\n",
      "Train Loss at iteration 7303: 0.042509138269978645\n",
      "Train Loss at iteration 7304: 0.04250904383482736\n",
      "Train Loss at iteration 7305: 0.04250894941563155\n",
      "Train Loss at iteration 7306: 0.04250885501238833\n",
      "Train Loss at iteration 7307: 0.04250876062509483\n",
      "Train Loss at iteration 7308: 0.042508666253748204\n",
      "Train Loss at iteration 7309: 0.04250857189834557\n",
      "Train Loss at iteration 7310: 0.04250847755888409\n",
      "Train Loss at iteration 7311: 0.04250838323536085\n",
      "Train Loss at iteration 7312: 0.04250828892777303\n",
      "Train Loss at iteration 7313: 0.04250819463611776\n",
      "Train Loss at iteration 7314: 0.04250810036039216\n",
      "Train Loss at iteration 7315: 0.04250800610059339\n",
      "Train Loss at iteration 7316: 0.04250791185671858\n",
      "Train Loss at iteration 7317: 0.04250781762876486\n",
      "Train Loss at iteration 7318: 0.04250772341672939\n",
      "Train Loss at iteration 7319: 0.0425076292206093\n",
      "Train Loss at iteration 7320: 0.04250753504040173\n",
      "Train Loss at iteration 7321: 0.04250744087610383\n",
      "Train Loss at iteration 7322: 0.04250734672771274\n",
      "Train Loss at iteration 7323: 0.04250725259522559\n",
      "Train Loss at iteration 7324: 0.04250715847863955\n",
      "Train Loss at iteration 7325: 0.042507064377951743\n",
      "Train Loss at iteration 7326: 0.04250697029315933\n",
      "Train Loss at iteration 7327: 0.04250687622425944\n",
      "Train Loss at iteration 7328: 0.04250678217124924\n",
      "Train Loss at iteration 7329: 0.04250668813412586\n",
      "Train Loss at iteration 7330: 0.04250659411288645\n",
      "Train Loss at iteration 7331: 0.042506500107528176\n",
      "Train Loss at iteration 7332: 0.04250640611804817\n",
      "Train Loss at iteration 7333: 0.042506312144443587\n",
      "Train Loss at iteration 7334: 0.04250621818671157\n",
      "Train Loss at iteration 7335: 0.04250612424484929\n",
      "Train Loss at iteration 7336: 0.042506030318853874\n",
      "Train Loss at iteration 7337: 0.042505936408722494\n",
      "Train Loss at iteration 7338: 0.0425058425144523\n",
      "Train Loss at iteration 7339: 0.04250574863604043\n",
      "Train Loss at iteration 7340: 0.04250565477348406\n",
      "Train Loss at iteration 7341: 0.04250556092678034\n",
      "Train Loss at iteration 7342: 0.0425054670959264\n",
      "Train Loss at iteration 7343: 0.04250537328091943\n",
      "Train Loss at iteration 7344: 0.042505279481756565\n",
      "Train Loss at iteration 7345: 0.04250518569843497\n",
      "Train Loss at iteration 7346: 0.04250509193095181\n",
      "Train Loss at iteration 7347: 0.04250499817930423\n",
      "Train Loss at iteration 7348: 0.042504904443489415\n",
      "Train Loss at iteration 7349: 0.04250481072350449\n",
      "Train Loss at iteration 7350: 0.04250471701934664\n",
      "Train Loss at iteration 7351: 0.042504623331013\n",
      "Train Loss at iteration 7352: 0.04250452965850076\n",
      "Train Loss at iteration 7353: 0.042504436001807075\n",
      "Train Loss at iteration 7354: 0.04250434236092911\n",
      "Train Loss at iteration 7355: 0.04250424873586401\n",
      "Train Loss at iteration 7356: 0.04250415512660896\n",
      "Train Loss at iteration 7357: 0.042504061533161115\n",
      "Train Loss at iteration 7358: 0.042503967955517655\n",
      "Train Loss at iteration 7359: 0.042503874393675714\n",
      "Train Loss at iteration 7360: 0.0425037808476325\n",
      "Train Loss at iteration 7361: 0.04250368731738515\n",
      "Train Loss at iteration 7362: 0.04250359380293084\n",
      "Train Loss at iteration 7363: 0.04250350030426673\n",
      "Train Loss at iteration 7364: 0.04250340682139001\n",
      "Train Loss at iteration 7365: 0.04250331335429784\n",
      "Train Loss at iteration 7366: 0.04250321990298739\n",
      "Train Loss at iteration 7367: 0.04250312646745583\n",
      "Train Loss at iteration 7368: 0.04250303304770034\n",
      "Train Loss at iteration 7369: 0.04250293964371806\n",
      "Train Loss at iteration 7370: 0.0425028462555062\n",
      "Train Loss at iteration 7371: 0.04250275288306193\n",
      "Train Loss at iteration 7372: 0.0425026595263824\n",
      "Train Loss at iteration 7373: 0.042502566185464816\n",
      "Train Loss at iteration 7374: 0.04250247286030632\n",
      "Train Loss at iteration 7375: 0.04250237955090412\n",
      "Train Loss at iteration 7376: 0.04250228625725537\n",
      "Train Loss at iteration 7377: 0.04250219297935726\n",
      "Train Loss at iteration 7378: 0.04250209971720697\n",
      "Train Loss at iteration 7379: 0.042502006470801657\n",
      "Train Loss at iteration 7380: 0.04250191324013851\n",
      "Train Loss at iteration 7381: 0.04250182002521473\n",
      "Train Loss at iteration 7382: 0.04250172682602748\n",
      "Train Loss at iteration 7383: 0.04250163364257392\n",
      "Train Loss at iteration 7384: 0.042501540474851274\n",
      "Train Loss at iteration 7385: 0.042501447322856684\n",
      "Train Loss at iteration 7386: 0.04250135418658737\n",
      "Train Loss at iteration 7387: 0.0425012610660405\n",
      "Train Loss at iteration 7388: 0.04250116796121324\n",
      "Train Loss at iteration 7389: 0.0425010748721028\n",
      "Train Loss at iteration 7390: 0.04250098179870635\n",
      "Train Loss at iteration 7391: 0.04250088874102109\n",
      "Train Loss at iteration 7392: 0.04250079569904419\n",
      "Train Loss at iteration 7393: 0.042500702672772846\n",
      "Train Loss at iteration 7394: 0.042500609662204245\n",
      "Train Loss at iteration 7395: 0.04250051666733557\n",
      "Train Loss at iteration 7396: 0.042500423688164035\n",
      "Train Loss at iteration 7397: 0.04250033072468679\n",
      "Train Loss at iteration 7398: 0.04250023777690104\n",
      "Train Loss at iteration 7399: 0.042500144844803994\n",
      "Train Loss at iteration 7400: 0.04250005192839282\n",
      "Train Loss at iteration 7401: 0.04249995902766473\n",
      "Train Loss at iteration 7402: 0.04249986614261691\n",
      "Train Loss at iteration 7403: 0.04249977327324653\n",
      "Train Loss at iteration 7404: 0.04249968041955081\n",
      "Train Loss at iteration 7405: 0.04249958758152694\n",
      "Train Loss at iteration 7406: 0.04249949475917212\n",
      "Train Loss at iteration 7407: 0.04249940195248353\n",
      "Train Loss at iteration 7408: 0.042499309161458367\n",
      "Train Loss at iteration 7409: 0.04249921638609385\n",
      "Train Loss at iteration 7410: 0.042499123626387164\n",
      "Train Loss at iteration 7411: 0.042499030882335496\n",
      "Train Loss at iteration 7412: 0.04249893815393606\n",
      "Train Loss at iteration 7413: 0.04249884544118604\n",
      "Train Loss at iteration 7414: 0.04249875274408266\n",
      "Train Loss at iteration 7415: 0.042498660062623095\n",
      "Train Loss at iteration 7416: 0.04249856739680455\n",
      "Train Loss at iteration 7417: 0.04249847474662425\n",
      "Train Loss at iteration 7418: 0.04249838211207937\n",
      "Train Loss at iteration 7419: 0.04249828949316713\n",
      "Train Loss at iteration 7420: 0.04249819688988472\n",
      "Train Loss at iteration 7421: 0.042498104302229354\n",
      "Train Loss at iteration 7422: 0.042498011730198244\n",
      "Train Loss at iteration 7423: 0.042497919173788576\n",
      "Train Loss at iteration 7424: 0.04249782663299757\n",
      "Train Loss at iteration 7425: 0.04249773410782243\n",
      "Train Loss at iteration 7426: 0.042497641598260354\n",
      "Train Loss at iteration 7427: 0.04249754910430856\n",
      "Train Loss at iteration 7428: 0.04249745662596426\n",
      "Train Loss at iteration 7429: 0.042497364163224666\n",
      "Train Loss at iteration 7430: 0.04249727171608696\n",
      "Train Loss at iteration 7431: 0.04249717928454838\n",
      "Train Loss at iteration 7432: 0.04249708686860612\n",
      "Train Loss at iteration 7433: 0.04249699446825741\n",
      "Train Loss at iteration 7434: 0.04249690208349946\n",
      "Train Loss at iteration 7435: 0.04249680971432945\n",
      "Train Loss at iteration 7436: 0.04249671736074464\n",
      "Train Loss at iteration 7437: 0.04249662502274221\n",
      "Train Loss at iteration 7438: 0.04249653270031938\n",
      "Train Loss at iteration 7439: 0.04249644039347339\n",
      "Train Loss at iteration 7440: 0.04249634810220141\n",
      "Train Loss at iteration 7441: 0.042496255826500705\n",
      "Train Loss at iteration 7442: 0.04249616356636846\n",
      "Train Loss at iteration 7443: 0.04249607132180191\n",
      "Train Loss at iteration 7444: 0.04249597909279826\n",
      "Train Loss at iteration 7445: 0.04249588687935474\n",
      "Train Loss at iteration 7446: 0.04249579468146856\n",
      "Train Loss at iteration 7447: 0.04249570249913695\n",
      "Train Loss at iteration 7448: 0.04249561033235711\n",
      "Train Loss at iteration 7449: 0.042495518181126284\n",
      "Train Loss at iteration 7450: 0.04249542604544168\n",
      "Train Loss at iteration 7451: 0.04249533392530054\n",
      "Train Loss at iteration 7452: 0.04249524182070006\n",
      "Train Loss at iteration 7453: 0.04249514973163747\n",
      "Train Loss at iteration 7454: 0.042495057658110014\n",
      "Train Loss at iteration 7455: 0.0424949656001149\n",
      "Train Loss at iteration 7456: 0.042494873557649346\n",
      "Train Loss at iteration 7457: 0.0424947815307106\n",
      "Train Loss at iteration 7458: 0.04249468951929587\n",
      "Train Loss at iteration 7459: 0.04249459752340241\n",
      "Train Loss at iteration 7460: 0.0424945055430274\n",
      "Train Loss at iteration 7461: 0.0424944135781681\n",
      "Train Loss at iteration 7462: 0.04249432162882174\n",
      "Train Loss at iteration 7463: 0.04249422969498555\n",
      "Train Loss at iteration 7464: 0.042494137776656754\n",
      "Train Loss at iteration 7465: 0.04249404587383258\n",
      "Train Loss at iteration 7466: 0.042493953986510256\n",
      "Train Loss at iteration 7467: 0.04249386211468703\n",
      "Train Loss at iteration 7468: 0.04249377025836013\n",
      "Train Loss at iteration 7469: 0.04249367841752678\n",
      "Train Loss at iteration 7470: 0.04249358659218422\n",
      "Train Loss at iteration 7471: 0.04249349478232967\n",
      "Train Loss at iteration 7472: 0.04249340298796039\n",
      "Train Loss at iteration 7473: 0.0424933112090736\n",
      "Train Loss at iteration 7474: 0.042493219445666544\n",
      "Train Loss at iteration 7475: 0.042493127697736444\n",
      "Train Loss at iteration 7476: 0.042493035965280555\n",
      "Train Loss at iteration 7477: 0.042492944248296104\n",
      "Train Loss at iteration 7478: 0.04249285254678033\n",
      "Train Loss at iteration 7479: 0.04249276086073047\n",
      "Train Loss at iteration 7480: 0.04249266919014378\n",
      "Train Loss at iteration 7481: 0.042492577535017485\n",
      "Train Loss at iteration 7482: 0.04249248589534883\n",
      "Train Loss at iteration 7483: 0.042492394271135055\n",
      "Train Loss at iteration 7484: 0.042492302662373416\n",
      "Train Loss at iteration 7485: 0.04249221106906113\n",
      "Train Loss at iteration 7486: 0.04249211949119544\n",
      "Train Loss at iteration 7487: 0.04249202792877363\n",
      "Train Loss at iteration 7488: 0.042491936381792904\n",
      "Train Loss at iteration 7489: 0.042491844850250524\n",
      "Train Loss at iteration 7490: 0.042491753334143734\n",
      "Train Loss at iteration 7491: 0.04249166183346978\n",
      "Train Loss at iteration 7492: 0.0424915703482259\n",
      "Train Loss at iteration 7493: 0.04249147887840936\n",
      "Train Loss at iteration 7494: 0.04249138742401739\n",
      "Train Loss at iteration 7495: 0.04249129598504726\n",
      "Train Loss at iteration 7496: 0.04249120456149618\n",
      "Train Loss at iteration 7497: 0.04249111315336143\n",
      "Train Loss at iteration 7498: 0.04249102176064026\n",
      "Train Loss at iteration 7499: 0.042490930383329927\n",
      "Train Loss at iteration 7500: 0.04249083902142766\n",
      "Train Loss at iteration 7501: 0.04249074767493073\n",
      "Train Loss at iteration 7502: 0.042490656343836385\n",
      "Train Loss at iteration 7503: 0.04249056502814187\n",
      "Train Loss at iteration 7504: 0.042490473727844445\n",
      "Train Loss at iteration 7505: 0.04249038244294137\n",
      "Train Loss at iteration 7506: 0.04249029117342988\n",
      "Train Loss at iteration 7507: 0.04249019991930726\n",
      "Train Loss at iteration 7508: 0.04249010868057076\n",
      "Train Loss at iteration 7509: 0.042490017457217616\n",
      "Train Loss at iteration 7510: 0.0424899262492451\n",
      "Train Loss at iteration 7511: 0.04248983505665048\n",
      "Train Loss at iteration 7512: 0.04248974387943099\n",
      "Train Loss at iteration 7513: 0.04248965271758391\n",
      "Train Loss at iteration 7514: 0.04248956157110649\n",
      "Train Loss at iteration 7515: 0.04248947043999599\n",
      "Train Loss at iteration 7516: 0.04248937932424968\n",
      "Train Loss at iteration 7517: 0.042489288223864816\n",
      "Train Loss at iteration 7518: 0.042489197138838655\n",
      "Train Loss at iteration 7519: 0.042489106069168475\n",
      "Train Loss at iteration 7520: 0.04248901501485152\n",
      "Train Loss at iteration 7521: 0.04248892397588506\n",
      "Train Loss at iteration 7522: 0.04248883295226637\n",
      "Train Loss at iteration 7523: 0.0424887419439927\n",
      "Train Loss at iteration 7524: 0.04248865095106134\n",
      "Train Loss at iteration 7525: 0.04248855997346952\n",
      "Train Loss at iteration 7526: 0.04248846901121454\n",
      "Train Loss at iteration 7527: 0.04248837806429366\n",
      "Train Loss at iteration 7528: 0.04248828713270413\n",
      "Train Loss at iteration 7529: 0.04248819621644323\n",
      "Train Loss at iteration 7530: 0.04248810531550824\n",
      "Train Loss at iteration 7531: 0.04248801442989642\n",
      "Train Loss at iteration 7532: 0.04248792355960505\n",
      "Train Loss at iteration 7533: 0.04248783270463137\n",
      "Train Loss at iteration 7534: 0.04248774186497269\n",
      "Train Loss at iteration 7535: 0.04248765104062626\n",
      "Train Loss at iteration 7536: 0.042487560231589366\n",
      "Train Loss at iteration 7537: 0.04248746943785926\n",
      "Train Loss at iteration 7538: 0.042487378659433236\n",
      "Train Loss at iteration 7539: 0.042487287896308575\n",
      "Train Loss at iteration 7540: 0.04248719714848252\n",
      "Train Loss at iteration 7541: 0.04248710641595239\n",
      "Train Loss at iteration 7542: 0.04248701569871542\n",
      "Train Loss at iteration 7543: 0.04248692499676891\n",
      "Train Loss at iteration 7544: 0.04248683431011014\n",
      "Train Loss at iteration 7545: 0.04248674363873637\n",
      "Train Loss at iteration 7546: 0.042486652982644896\n",
      "Train Loss at iteration 7547: 0.04248656234183299\n",
      "Train Loss at iteration 7548: 0.04248647171629793\n",
      "Train Loss at iteration 7549: 0.042486381106037\n",
      "Train Loss at iteration 7550: 0.04248629051104748\n",
      "Train Loss at iteration 7551: 0.04248619993132665\n",
      "Train Loss at iteration 7552: 0.0424861093668718\n",
      "Train Loss at iteration 7553: 0.04248601881768018\n",
      "Train Loss at iteration 7554: 0.04248592828374913\n",
      "Train Loss at iteration 7555: 0.04248583776507589\n",
      "Train Loss at iteration 7556: 0.04248574726165777\n",
      "Train Loss at iteration 7557: 0.042485656773492024\n",
      "Train Loss at iteration 7558: 0.04248556630057596\n",
      "Train Loss at iteration 7559: 0.04248547584290687\n",
      "Train Loss at iteration 7560: 0.04248538540048203\n",
      "Train Loss at iteration 7561: 0.042485294973298726\n",
      "Train Loss at iteration 7562: 0.04248520456135425\n",
      "Train Loss at iteration 7563: 0.042485114164645885\n",
      "Train Loss at iteration 7564: 0.042485023783170935\n",
      "Train Loss at iteration 7565: 0.04248493341692667\n",
      "Train Loss at iteration 7566: 0.04248484306591039\n",
      "Train Loss at iteration 7567: 0.04248475273011939\n",
      "Train Loss at iteration 7568: 0.042484662409550955\n",
      "Train Loss at iteration 7569: 0.042484572104202374\n",
      "Train Loss at iteration 7570: 0.042484481814070954\n",
      "Train Loss at iteration 7571: 0.042484391539153975\n",
      "Train Loss at iteration 7572: 0.04248430127944873\n",
      "Train Loss at iteration 7573: 0.042484211034952535\n",
      "Train Loss at iteration 7574: 0.042484120805662655\n",
      "Train Loss at iteration 7575: 0.042484030591576384\n",
      "Train Loss at iteration 7576: 0.04248394039269106\n",
      "Train Loss at iteration 7577: 0.04248385020900393\n",
      "Train Loss at iteration 7578: 0.04248376004051232\n",
      "Train Loss at iteration 7579: 0.04248366988721352\n",
      "Train Loss at iteration 7580: 0.042483579749104834\n",
      "Train Loss at iteration 7581: 0.042483489626183556\n",
      "Train Loss at iteration 7582: 0.04248339951844699\n",
      "Train Loss at iteration 7583: 0.04248330942589242\n",
      "Train Loss at iteration 7584: 0.04248321934851717\n",
      "Train Loss at iteration 7585: 0.04248312928631852\n",
      "Train Loss at iteration 7586: 0.042483039239293786\n",
      "Train Loss at iteration 7587: 0.042482949207440265\n",
      "Train Loss at iteration 7588: 0.04248285919075527\n",
      "Train Loss at iteration 7589: 0.04248276918923608\n",
      "Train Loss at iteration 7590: 0.042482679202880035\n",
      "Train Loss at iteration 7591: 0.0424825892316844\n",
      "Train Loss at iteration 7592: 0.042482499275646506\n",
      "Train Loss at iteration 7593: 0.04248240933476366\n",
      "Train Loss at iteration 7594: 0.04248231940903316\n",
      "Train Loss at iteration 7595: 0.04248222949845232\n",
      "Train Loss at iteration 7596: 0.042482139603018436\n",
      "Train Loss at iteration 7597: 0.042482049722728815\n",
      "Train Loss at iteration 7598: 0.04248195985758078\n",
      "Train Loss at iteration 7599: 0.04248187000757162\n",
      "Train Loss at iteration 7600: 0.042481780172698685\n",
      "Train Loss at iteration 7601: 0.04248169035295923\n",
      "Train Loss at iteration 7602: 0.04248160054835061\n",
      "Train Loss at iteration 7603: 0.04248151075887012\n",
      "Train Loss at iteration 7604: 0.042481420984515066\n",
      "Train Loss at iteration 7605: 0.042481331225282776\n",
      "Train Loss at iteration 7606: 0.04248124148117055\n",
      "Train Loss at iteration 7607: 0.042481151752175726\n",
      "Train Loss at iteration 7608: 0.04248106203829558\n",
      "Train Loss at iteration 7609: 0.04248097233952745\n",
      "Train Loss at iteration 7610: 0.04248088265586865\n",
      "Train Loss at iteration 7611: 0.0424807929873165\n",
      "Train Loss at iteration 7612: 0.04248070333386831\n",
      "Train Loss at iteration 7613: 0.042480613695521384\n",
      "Train Loss at iteration 7614: 0.04248052407227307\n",
      "Train Loss at iteration 7615: 0.04248043446412066\n",
      "Train Loss at iteration 7616: 0.042480344871061504\n",
      "Train Loss at iteration 7617: 0.0424802552930929\n",
      "Train Loss at iteration 7618: 0.042480165730212155\n",
      "Train Loss at iteration 7619: 0.0424800761824166\n",
      "Train Loss at iteration 7620: 0.042479986649703566\n",
      "Train Loss at iteration 7621: 0.042479897132070385\n",
      "Train Loss at iteration 7622: 0.042479807629514366\n",
      "Train Loss at iteration 7623: 0.04247971814203281\n",
      "Train Loss at iteration 7624: 0.042479628669623064\n",
      "Train Loss at iteration 7625: 0.04247953921228246\n",
      "Train Loss at iteration 7626: 0.04247944977000831\n",
      "Train Loss at iteration 7627: 0.04247936034279794\n",
      "Train Loss at iteration 7628: 0.04247927093064869\n",
      "Train Loss at iteration 7629: 0.04247918153355784\n",
      "Train Loss at iteration 7630: 0.042479092151522777\n",
      "Train Loss at iteration 7631: 0.0424790027845408\n",
      "Train Loss at iteration 7632: 0.042478913432609226\n",
      "Train Loss at iteration 7633: 0.04247882409572541\n",
      "Train Loss at iteration 7634: 0.04247873477388667\n",
      "Train Loss at iteration 7635: 0.042478645467090334\n",
      "Train Loss at iteration 7636: 0.04247855617533372\n",
      "Train Loss at iteration 7637: 0.04247846689861418\n",
      "Train Loss at iteration 7638: 0.04247837763692904\n",
      "Train Loss at iteration 7639: 0.04247828839027563\n",
      "Train Loss at iteration 7640: 0.04247819915865128\n",
      "Train Loss at iteration 7641: 0.042478109942053324\n",
      "Train Loss at iteration 7642: 0.0424780207404791\n",
      "Train Loss at iteration 7643: 0.04247793155392593\n",
      "Train Loss at iteration 7644: 0.04247784238239117\n",
      "Train Loss at iteration 7645: 0.042477753225872134\n",
      "Train Loss at iteration 7646: 0.04247766408436618\n",
      "Train Loss at iteration 7647: 0.04247757495787063\n",
      "Train Loss at iteration 7648: 0.04247748584638282\n",
      "Train Loss at iteration 7649: 0.04247739674990009\n",
      "Train Loss at iteration 7650: 0.04247730766841979\n",
      "Train Loss at iteration 7651: 0.04247721860193923\n",
      "Train Loss at iteration 7652: 0.04247712955045578\n",
      "Train Loss at iteration 7653: 0.042477040513966766\n",
      "Train Loss at iteration 7654: 0.042476951492469536\n",
      "Train Loss at iteration 7655: 0.042476862485961414\n",
      "Train Loss at iteration 7656: 0.04247677349443977\n",
      "Train Loss at iteration 7657: 0.04247668451790192\n",
      "Train Loss at iteration 7658: 0.042476595556345226\n",
      "Train Loss at iteration 7659: 0.04247650660976702\n",
      "Train Loss at iteration 7660: 0.04247641767816463\n",
      "Train Loss at iteration 7661: 0.04247632876153544\n",
      "Train Loss at iteration 7662: 0.04247623985987678\n",
      "Train Loss at iteration 7663: 0.04247615097318598\n",
      "Train Loss at iteration 7664: 0.04247606210146039\n",
      "Train Loss at iteration 7665: 0.04247597324469737\n",
      "Train Loss at iteration 7666: 0.04247588440289425\n",
      "Train Loss at iteration 7667: 0.0424757955760484\n",
      "Train Loss at iteration 7668: 0.04247570676415715\n",
      "Train Loss at iteration 7669: 0.04247561796721785\n",
      "Train Loss at iteration 7670: 0.04247552918522787\n",
      "Train Loss at iteration 7671: 0.04247544041818453\n",
      "Train Loss at iteration 7672: 0.04247535166608521\n",
      "Train Loss at iteration 7673: 0.04247526292892723\n",
      "Train Loss at iteration 7674: 0.042475174206707975\n",
      "Train Loss at iteration 7675: 0.042475085499424775\n",
      "Train Loss at iteration 7676: 0.042474996807074984\n",
      "Train Loss at iteration 7677: 0.042474908129655965\n",
      "Train Loss at iteration 7678: 0.042474819467165074\n",
      "Train Loss at iteration 7679: 0.04247473081959966\n",
      "Train Loss at iteration 7680: 0.04247464218695707\n",
      "Train Loss at iteration 7681: 0.04247455356923467\n",
      "Train Loss at iteration 7682: 0.042474464966429815\n",
      "Train Loss at iteration 7683: 0.04247437637853986\n",
      "Train Loss at iteration 7684: 0.04247428780556216\n",
      "Train Loss at iteration 7685: 0.04247419924749408\n",
      "Train Loss at iteration 7686: 0.042474110704332976\n",
      "Train Loss at iteration 7687: 0.04247402217607621\n",
      "Train Loss at iteration 7688: 0.042473933662721126\n",
      "Train Loss at iteration 7689: 0.0424738451642651\n",
      "Train Loss at iteration 7690: 0.04247375668070549\n",
      "Train Loss at iteration 7691: 0.042473668212039656\n",
      "Train Loss at iteration 7692: 0.04247357975826495\n",
      "Train Loss at iteration 7693: 0.042473491319378755\n",
      "Train Loss at iteration 7694: 0.042473402895378425\n",
      "Train Loss at iteration 7695: 0.042473314486261314\n",
      "Train Loss at iteration 7696: 0.0424732260920248\n",
      "Train Loss at iteration 7697: 0.04247313771266624\n",
      "Train Loss at iteration 7698: 0.042473049348182995\n",
      "Train Loss at iteration 7699: 0.04247296099857244\n",
      "Train Loss at iteration 7700: 0.04247287266383193\n",
      "Train Loss at iteration 7701: 0.042472784343958854\n",
      "Train Loss at iteration 7702: 0.042472696038950555\n",
      "Train Loss at iteration 7703: 0.04247260774880441\n",
      "Train Loss at iteration 7704: 0.042472519473517795\n",
      "Train Loss at iteration 7705: 0.042472431213088066\n",
      "Train Loss at iteration 7706: 0.04247234296751261\n",
      "Train Loss at iteration 7707: 0.04247225473678877\n",
      "Train Loss at iteration 7708: 0.04247216652091394\n",
      "Train Loss at iteration 7709: 0.042472078319885496\n",
      "Train Loss at iteration 7710: 0.04247199013370078\n",
      "Train Loss at iteration 7711: 0.04247190196235719\n",
      "Train Loss at iteration 7712: 0.04247181380585211\n",
      "Train Loss at iteration 7713: 0.04247172566418287\n",
      "Train Loss at iteration 7714: 0.04247163753734689\n",
      "Train Loss at iteration 7715: 0.04247154942534151\n",
      "Train Loss at iteration 7716: 0.04247146132816412\n",
      "Train Loss at iteration 7717: 0.04247137324581211\n",
      "Train Loss at iteration 7718: 0.04247128517828282\n",
      "Train Loss at iteration 7719: 0.042471197125573665\n",
      "Train Loss at iteration 7720: 0.042471109087682\n",
      "Train Loss at iteration 7721: 0.0424710210646052\n",
      "Train Loss at iteration 7722: 0.04247093305634066\n",
      "Train Loss at iteration 7723: 0.042470845062885745\n",
      "Train Loss at iteration 7724: 0.04247075708423785\n",
      "Train Loss at iteration 7725: 0.04247066912039435\n",
      "Train Loss at iteration 7726: 0.04247058117135261\n",
      "Train Loss at iteration 7727: 0.042470493237110024\n",
      "Train Loss at iteration 7728: 0.04247040531766398\n",
      "Train Loss at iteration 7729: 0.042470317413011854\n",
      "Train Loss at iteration 7730: 0.04247022952315102\n",
      "Train Loss at iteration 7731: 0.04247014164807887\n",
      "Train Loss at iteration 7732: 0.0424700537877928\n",
      "Train Loss at iteration 7733: 0.042469965942290175\n",
      "Train Loss at iteration 7734: 0.042469878111568375\n",
      "Train Loss at iteration 7735: 0.04246979029562482\n",
      "Train Loss at iteration 7736: 0.042469702494456855\n",
      "Train Loss at iteration 7737: 0.04246961470806189\n",
      "Train Loss at iteration 7738: 0.04246952693643731\n",
      "Train Loss at iteration 7739: 0.0424694391795805\n",
      "Train Loss at iteration 7740: 0.04246935143748885\n",
      "Train Loss at iteration 7741: 0.04246926371015975\n",
      "Train Loss at iteration 7742: 0.04246917599759059\n",
      "Train Loss at iteration 7743: 0.042469088299778766\n",
      "Train Loss at iteration 7744: 0.042469000616721646\n",
      "Train Loss at iteration 7745: 0.04246891294841664\n",
      "Train Loss at iteration 7746: 0.04246882529486114\n",
      "Train Loss at iteration 7747: 0.04246873765605252\n",
      "Train Loss at iteration 7748: 0.0424686500319882\n",
      "Train Loss at iteration 7749: 0.04246856242266556\n",
      "Train Loss at iteration 7750: 0.042468474828082\n",
      "Train Loss at iteration 7751: 0.04246838724823488\n",
      "Train Loss at iteration 7752: 0.042468299683121644\n",
      "Train Loss at iteration 7753: 0.042468212132739665\n",
      "Train Loss at iteration 7754: 0.04246812459708634\n",
      "Train Loss at iteration 7755: 0.042468037076159065\n",
      "Train Loss at iteration 7756: 0.042467949569955234\n",
      "Train Loss at iteration 7757: 0.04246786207847226\n",
      "Train Loss at iteration 7758: 0.04246777460170754\n",
      "Train Loss at iteration 7759: 0.042467687139658435\n",
      "Train Loss at iteration 7760: 0.04246759969232238\n",
      "Train Loss at iteration 7761: 0.042467512259696784\n",
      "Train Loss at iteration 7762: 0.042467424841779025\n",
      "Train Loss at iteration 7763: 0.04246733743856651\n",
      "Train Loss at iteration 7764: 0.04246725005005663\n",
      "Train Loss at iteration 7765: 0.042467162676246804\n",
      "Train Loss at iteration 7766: 0.04246707531713442\n",
      "Train Loss at iteration 7767: 0.042466987972716905\n",
      "Train Loss at iteration 7768: 0.04246690064299164\n",
      "Train Loss at iteration 7769: 0.04246681332795604\n",
      "Train Loss at iteration 7770: 0.04246672602760749\n",
      "Train Loss at iteration 7771: 0.04246663874194343\n",
      "Train Loss at iteration 7772: 0.04246655147096124\n",
      "Train Loss at iteration 7773: 0.042466464214658335\n",
      "Train Loss at iteration 7774: 0.04246637697303212\n",
      "Train Loss at iteration 7775: 0.04246628974608001\n",
      "Train Loss at iteration 7776: 0.042466202533799395\n",
      "Train Loss at iteration 7777: 0.0424661153361877\n",
      "Train Loss at iteration 7778: 0.042466028153242324\n",
      "Train Loss at iteration 7779: 0.04246594098496069\n",
      "Train Loss at iteration 7780: 0.042465853831340196\n",
      "Train Loss at iteration 7781: 0.042465766692378254\n",
      "Train Loss at iteration 7782: 0.04246567956807228\n",
      "Train Loss at iteration 7783: 0.04246559245841968\n",
      "Train Loss at iteration 7784: 0.04246550536341787\n",
      "Train Loss at iteration 7785: 0.042465418283064285\n",
      "Train Loss at iteration 7786: 0.04246533121735629\n",
      "Train Loss at iteration 7787: 0.042465244166291334\n",
      "Train Loss at iteration 7788: 0.04246515712986682\n",
      "Train Loss at iteration 7789: 0.04246507010808017\n",
      "Train Loss at iteration 7790: 0.0424649831009288\n",
      "Train Loss at iteration 7791: 0.042464896108410116\n",
      "Train Loss at iteration 7792: 0.042464809130521544\n",
      "Train Loss at iteration 7793: 0.042464722167260494\n",
      "Train Loss at iteration 7794: 0.04246463521862438\n",
      "Train Loss at iteration 7795: 0.04246454828461065\n",
      "Train Loss at iteration 7796: 0.04246446136521668\n",
      "Train Loss at iteration 7797: 0.04246437446043992\n",
      "Train Loss at iteration 7798: 0.04246428757027778\n",
      "Train Loss at iteration 7799: 0.04246420069472768\n",
      "Train Loss at iteration 7800: 0.04246411383378704\n",
      "Train Loss at iteration 7801: 0.04246402698745329\n",
      "Train Loss at iteration 7802: 0.042463940155723835\n",
      "Train Loss at iteration 7803: 0.042463853338596105\n",
      "Train Loss at iteration 7804: 0.04246376653606755\n",
      "Train Loss at iteration 7805: 0.042463679748135544\n",
      "Train Loss at iteration 7806: 0.04246359297479755\n",
      "Train Loss at iteration 7807: 0.04246350621605098\n",
      "Train Loss at iteration 7808: 0.04246341947189325\n",
      "Train Loss at iteration 7809: 0.042463332742321794\n",
      "Train Loss at iteration 7810: 0.04246324602733405\n",
      "Train Loss at iteration 7811: 0.04246315932692744\n",
      "Train Loss at iteration 7812: 0.042463072641099366\n",
      "Train Loss at iteration 7813: 0.04246298596984728\n",
      "Train Loss at iteration 7814: 0.042462899313168635\n",
      "Train Loss at iteration 7815: 0.04246281267106079\n",
      "Train Loss at iteration 7816: 0.04246272604352124\n",
      "Train Loss at iteration 7817: 0.04246263943054738\n",
      "Train Loss at iteration 7818: 0.04246255283213666\n",
      "Train Loss at iteration 7819: 0.0424624662482865\n",
      "Train Loss at iteration 7820: 0.042462379678994334\n",
      "Train Loss at iteration 7821: 0.0424622931242576\n",
      "Train Loss at iteration 7822: 0.04246220658407372\n",
      "Train Loss at iteration 7823: 0.04246212005844013\n",
      "Train Loss at iteration 7824: 0.042462033547354276\n",
      "Train Loss at iteration 7825: 0.04246194705081359\n",
      "Train Loss at iteration 7826: 0.042461860568815485\n",
      "Train Loss at iteration 7827: 0.04246177410135742\n",
      "Train Loss at iteration 7828: 0.04246168764843682\n",
      "Train Loss at iteration 7829: 0.04246160121005113\n",
      "Train Loss at iteration 7830: 0.04246151478619777\n",
      "Train Loss at iteration 7831: 0.0424614283768742\n",
      "Train Loss at iteration 7832: 0.04246134198207785\n",
      "Train Loss at iteration 7833: 0.04246125560180614\n",
      "Train Loss at iteration 7834: 0.04246116923605654\n",
      "Train Loss at iteration 7835: 0.04246108288482647\n",
      "Train Loss at iteration 7836: 0.04246099654811338\n",
      "Train Loss at iteration 7837: 0.04246091022591471\n",
      "Train Loss at iteration 7838: 0.042460823918227884\n",
      "Train Loss at iteration 7839: 0.04246073762505036\n",
      "Train Loss at iteration 7840: 0.04246065134637958\n",
      "Train Loss at iteration 7841: 0.04246056508221299\n",
      "Train Loss at iteration 7842: 0.042460478832548026\n",
      "Train Loss at iteration 7843: 0.042460392597382136\n",
      "Train Loss at iteration 7844: 0.042460306376712754\n",
      "Train Loss at iteration 7845: 0.04246022017053734\n",
      "Train Loss at iteration 7846: 0.042460133978853336\n",
      "Train Loss at iteration 7847: 0.04246004780165819\n",
      "Train Loss at iteration 7848: 0.04245996163894934\n",
      "Train Loss at iteration 7849: 0.042459875490724226\n",
      "Train Loss at iteration 7850: 0.04245978935698032\n",
      "Train Loss at iteration 7851: 0.04245970323771505\n",
      "Train Loss at iteration 7852: 0.04245961713292588\n",
      "Train Loss at iteration 7853: 0.04245953104261026\n",
      "Train Loss at iteration 7854: 0.04245944496676561\n",
      "Train Loss at iteration 7855: 0.04245935890538941\n",
      "Train Loss at iteration 7856: 0.0424592728584791\n",
      "Train Loss at iteration 7857: 0.04245918682603214\n",
      "Train Loss at iteration 7858: 0.04245910080804599\n",
      "Train Loss at iteration 7859: 0.04245901480451806\n",
      "Train Loss at iteration 7860: 0.04245892881544585\n",
      "Train Loss at iteration 7861: 0.04245884284082679\n",
      "Train Loss at iteration 7862: 0.042458756880658345\n",
      "Train Loss at iteration 7863: 0.04245867093493796\n",
      "Train Loss at iteration 7864: 0.0424585850036631\n",
      "Train Loss at iteration 7865: 0.0424584990868312\n",
      "Train Loss at iteration 7866: 0.04245841318443973\n",
      "Train Loss at iteration 7867: 0.042458327296486166\n",
      "Train Loss at iteration 7868: 0.04245824142296794\n",
      "Train Loss at iteration 7869: 0.042458155563882514\n",
      "Train Loss at iteration 7870: 0.04245806971922735\n",
      "Train Loss at iteration 7871: 0.0424579838889999\n",
      "Train Loss at iteration 7872: 0.04245789807319763\n",
      "Train Loss at iteration 7873: 0.04245781227181801\n",
      "Train Loss at iteration 7874: 0.042457726484858474\n",
      "Train Loss at iteration 7875: 0.0424576407123165\n",
      "Train Loss at iteration 7876: 0.04245755495418956\n",
      "Train Loss at iteration 7877: 0.04245746921047511\n",
      "Train Loss at iteration 7878: 0.042457383481170584\n",
      "Train Loss at iteration 7879: 0.04245729776627349\n",
      "Train Loss at iteration 7880: 0.04245721206578125\n",
      "Train Loss at iteration 7881: 0.042457126379691346\n",
      "Train Loss at iteration 7882: 0.042457040708001256\n",
      "Train Loss at iteration 7883: 0.04245695505070842\n",
      "Train Loss at iteration 7884: 0.04245686940781033\n",
      "Train Loss at iteration 7885: 0.04245678377930443\n",
      "Train Loss at iteration 7886: 0.0424566981651882\n",
      "Train Loss at iteration 7887: 0.0424566125654591\n",
      "Train Loss at iteration 7888: 0.0424565269801146\n",
      "Train Loss at iteration 7889: 0.04245644140915217\n",
      "Train Loss at iteration 7890: 0.04245635585256928\n",
      "Train Loss at iteration 7891: 0.04245627031036339\n",
      "Train Loss at iteration 7892: 0.042456184782531985\n",
      "Train Loss at iteration 7893: 0.04245609926907251\n",
      "Train Loss at iteration 7894: 0.042456013769982474\n",
      "Train Loss at iteration 7895: 0.042455928285259306\n",
      "Train Loss at iteration 7896: 0.042455842814900516\n",
      "Train Loss at iteration 7897: 0.04245575735890355\n",
      "Train Loss at iteration 7898: 0.042455671917265896\n",
      "Train Loss at iteration 7899: 0.04245558648998502\n",
      "Train Loss at iteration 7900: 0.0424555010770584\n",
      "Train Loss at iteration 7901: 0.04245541567848351\n",
      "Train Loss at iteration 7902: 0.04245533029425781\n",
      "Train Loss at iteration 7903: 0.042455244924378806\n",
      "Train Loss at iteration 7904: 0.04245515956884395\n",
      "Train Loss at iteration 7905: 0.04245507422765073\n",
      "Train Loss at iteration 7906: 0.04245498890079661\n",
      "Train Loss at iteration 7907: 0.042454903588279094\n",
      "Train Loss at iteration 7908: 0.042454818290095626\n",
      "Train Loss at iteration 7909: 0.04245473300624371\n",
      "Train Loss at iteration 7910: 0.04245464773672081\n",
      "Train Loss at iteration 7911: 0.042454562481524426\n",
      "Train Loss at iteration 7912: 0.04245447724065201\n",
      "Train Loss at iteration 7913: 0.04245439201410107\n",
      "Train Loss at iteration 7914: 0.04245430680186907\n",
      "Train Loss at iteration 7915: 0.04245422160395349\n",
      "Train Loss at iteration 7916: 0.042454136420351826\n",
      "Train Loss at iteration 7917: 0.04245405125106157\n",
      "Train Loss at iteration 7918: 0.04245396609608017\n",
      "Train Loss at iteration 7919: 0.04245388095540513\n",
      "Train Loss at iteration 7920: 0.04245379582903394\n",
      "Train Loss at iteration 7921: 0.04245371071696408\n",
      "Train Loss at iteration 7922: 0.04245362561919303\n",
      "Train Loss at iteration 7923: 0.04245354053571828\n",
      "Train Loss at iteration 7924: 0.04245345546653732\n",
      "Train Loss at iteration 7925: 0.04245337041164764\n",
      "Train Loss at iteration 7926: 0.04245328537104671\n",
      "Train Loss at iteration 7927: 0.04245320034473204\n",
      "Train Loss at iteration 7928: 0.04245311533270111\n",
      "Train Loss at iteration 7929: 0.042453030334951396\n",
      "Train Loss at iteration 7930: 0.0424529453514804\n",
      "Train Loss at iteration 7931: 0.042452860382285615\n",
      "Train Loss at iteration 7932: 0.042452775427364524\n",
      "Train Loss at iteration 7933: 0.04245269048671463\n",
      "Train Loss at iteration 7934: 0.04245260556033341\n",
      "Train Loss at iteration 7935: 0.042452520648218355\n",
      "Train Loss at iteration 7936: 0.04245243575036698\n",
      "Train Loss at iteration 7937: 0.04245235086677677\n",
      "Train Loss at iteration 7938: 0.042452265997445185\n",
      "Train Loss at iteration 7939: 0.042452181142369774\n",
      "Train Loss at iteration 7940: 0.042452096301548\n",
      "Train Loss at iteration 7941: 0.04245201147497734\n",
      "Train Loss at iteration 7942: 0.04245192666265534\n",
      "Train Loss at iteration 7943: 0.04245184186457947\n",
      "Train Loss at iteration 7944: 0.0424517570807472\n",
      "Train Loss at iteration 7945: 0.04245167231115606\n",
      "Train Loss at iteration 7946: 0.04245158755580356\n",
      "Train Loss at iteration 7947: 0.04245150281468717\n",
      "Train Loss at iteration 7948: 0.04245141808780441\n",
      "Train Loss at iteration 7949: 0.04245133337515274\n",
      "Train Loss at iteration 7950: 0.04245124867672971\n",
      "Train Loss at iteration 7951: 0.04245116399253279\n",
      "Train Loss at iteration 7952: 0.04245107932255949\n",
      "Train Loss at iteration 7953: 0.04245099466680731\n",
      "Train Loss at iteration 7954: 0.04245091002527375\n",
      "Train Loss at iteration 7955: 0.042450825397956325\n",
      "Train Loss at iteration 7956: 0.042450740784852524\n",
      "Train Loss at iteration 7957: 0.04245065618595985\n",
      "Train Loss at iteration 7958: 0.04245057160127582\n",
      "Train Loss at iteration 7959: 0.04245048703079792\n",
      "Train Loss at iteration 7960: 0.042450402474523675\n",
      "Train Loss at iteration 7961: 0.042450317932450576\n",
      "Train Loss at iteration 7962: 0.04245023340457614\n",
      "Train Loss at iteration 7963: 0.04245014889089785\n",
      "Train Loss at iteration 7964: 0.04245006439141325\n",
      "Train Loss at iteration 7965: 0.042449979906119824\n",
      "Train Loss at iteration 7966: 0.042449895435015074\n",
      "Train Loss at iteration 7967: 0.04244981097809653\n",
      "Train Loss at iteration 7968: 0.04244972653536169\n",
      "Train Loss at iteration 7969: 0.04244964210680806\n",
      "Train Loss at iteration 7970: 0.04244955769243316\n",
      "Train Loss at iteration 7971: 0.0424494732922345\n",
      "Train Loss at iteration 7972: 0.04244938890620957\n",
      "Train Loss at iteration 7973: 0.042449304534355914\n",
      "Train Loss at iteration 7974: 0.042449220176671014\n",
      "Train Loss at iteration 7975: 0.0424491358331524\n",
      "Train Loss at iteration 7976: 0.04244905150379758\n",
      "Train Loss at iteration 7977: 0.04244896718860408\n",
      "Train Loss at iteration 7978: 0.04244888288756939\n",
      "Train Loss at iteration 7979: 0.042448798600691055\n",
      "Train Loss at iteration 7980: 0.04244871432796658\n",
      "Train Loss at iteration 7981: 0.04244863006939346\n",
      "Train Loss at iteration 7982: 0.04244854582496923\n",
      "Train Loss at iteration 7983: 0.0424484615946914\n",
      "Train Loss at iteration 7984: 0.04244837737855749\n",
      "Train Loss at iteration 7985: 0.04244829317656501\n",
      "Train Loss at iteration 7986: 0.0424482089887115\n",
      "Train Loss at iteration 7987: 0.042448124814994465\n",
      "Train Loss at iteration 7988: 0.04244804065541143\n",
      "Train Loss at iteration 7989: 0.0424479565099599\n",
      "Train Loss at iteration 7990: 0.04244787237863742\n",
      "Train Loss at iteration 7991: 0.04244778826144148\n",
      "Train Loss at iteration 7992: 0.04244770415836962\n",
      "Train Loss at iteration 7993: 0.04244762006941936\n",
      "Train Loss at iteration 7994: 0.04244753599458822\n",
      "Train Loss at iteration 7995: 0.042447451933873726\n",
      "Train Loss at iteration 7996: 0.0424473678872734\n",
      "Train Loss at iteration 7997: 0.04244728385478478\n",
      "Train Loss at iteration 7998: 0.04244719983640536\n",
      "Train Loss at iteration 7999: 0.04244711583213269\n",
      "Train Loss at iteration 8000: 0.042447031841964286\n",
      "Train Loss at iteration 8001: 0.042446947865897665\n",
      "Train Loss at iteration 8002: 0.04244686390393037\n",
      "Train Loss at iteration 8003: 0.04244677995605992\n",
      "Train Loss at iteration 8004: 0.04244669602228384\n",
      "Train Loss at iteration 8005: 0.04244661210259968\n",
      "Train Loss at iteration 8006: 0.04244652819700494\n",
      "Train Loss at iteration 8007: 0.04244644430549715\n",
      "Train Loss at iteration 8008: 0.04244636042807386\n",
      "Train Loss at iteration 8009: 0.042446276564732585\n",
      "Train Loss at iteration 8010: 0.04244619271547086\n",
      "Train Loss at iteration 8011: 0.04244610888028621\n",
      "Train Loss at iteration 8012: 0.042446025059176176\n",
      "Train Loss at iteration 8013: 0.04244594125213829\n",
      "Train Loss at iteration 8014: 0.042445857459170074\n",
      "Train Loss at iteration 8015: 0.04244577368026906\n",
      "Train Loss at iteration 8016: 0.0424456899154328\n",
      "Train Loss at iteration 8017: 0.04244560616465882\n",
      "Train Loss at iteration 8018: 0.04244552242794466\n",
      "Train Loss at iteration 8019: 0.04244543870528783\n",
      "Train Loss at iteration 8020: 0.042445354996685884\n",
      "Train Loss at iteration 8021: 0.042445271302136364\n",
      "Train Loss at iteration 8022: 0.04244518762163679\n",
      "Train Loss at iteration 8023: 0.04244510395518471\n",
      "Train Loss at iteration 8024: 0.04244502030277766\n",
      "Train Loss at iteration 8025: 0.04244493666441317\n",
      "Train Loss at iteration 8026: 0.042444853040088805\n",
      "Train Loss at iteration 8027: 0.042444769429802076\n",
      "Train Loss at iteration 8028: 0.04244468583355052\n",
      "Train Loss at iteration 8029: 0.0424446022513317\n",
      "Train Loss at iteration 8030: 0.042444518683143154\n",
      "Train Loss at iteration 8031: 0.0424444351289824\n",
      "Train Loss at iteration 8032: 0.042444351588847\n",
      "Train Loss at iteration 8033: 0.04244426806273449\n",
      "Train Loss at iteration 8034: 0.04244418455064241\n",
      "Train Loss at iteration 8035: 0.04244410105256831\n",
      "Train Loss at iteration 8036: 0.042444017568509726\n",
      "Train Loss at iteration 8037: 0.04244393409846421\n",
      "Train Loss at iteration 8038: 0.042443850642429296\n",
      "Train Loss at iteration 8039: 0.04244376720040254\n",
      "Train Loss at iteration 8040: 0.042443683772381474\n",
      "Train Loss at iteration 8041: 0.04244360035836366\n",
      "Train Loss at iteration 8042: 0.04244351695834664\n",
      "Train Loss at iteration 8043: 0.04244343357232796\n",
      "Train Loss at iteration 8044: 0.042443350200305166\n",
      "Train Loss at iteration 8045: 0.042443266842275805\n",
      "Train Loss at iteration 8046: 0.04244318349823742\n",
      "Train Loss at iteration 8047: 0.042443100168187566\n",
      "Train Loss at iteration 8048: 0.042443016852123795\n",
      "Train Loss at iteration 8049: 0.04244293355004367\n",
      "Train Loss at iteration 8050: 0.04244285026194471\n",
      "Train Loss at iteration 8051: 0.04244276698782449\n",
      "Train Loss at iteration 8052: 0.04244268372768056\n",
      "Train Loss at iteration 8053: 0.04244260048151046\n",
      "Train Loss at iteration 8054: 0.04244251724931176\n",
      "Train Loss at iteration 8055: 0.04244243403108199\n",
      "Train Loss at iteration 8056: 0.04244235082681872\n",
      "Train Loss at iteration 8057: 0.042442267636519496\n",
      "Train Loss at iteration 8058: 0.042442184460181887\n",
      "Train Loss at iteration 8059: 0.042442101297803445\n",
      "Train Loss at iteration 8060: 0.04244201814938169\n",
      "Train Loss at iteration 8061: 0.04244193501491423\n",
      "Train Loss at iteration 8062: 0.04244185189439859\n",
      "Train Loss at iteration 8063: 0.04244176878783235\n",
      "Train Loss at iteration 8064: 0.04244168569521303\n",
      "Train Loss at iteration 8065: 0.042441602616538225\n",
      "Train Loss at iteration 8066: 0.04244151955180547\n",
      "Train Loss at iteration 8067: 0.04244143650101235\n",
      "Train Loss at iteration 8068: 0.04244135346415639\n",
      "Train Loss at iteration 8069: 0.04244127044123518\n",
      "Train Loss at iteration 8070: 0.042441187432246265\n",
      "Train Loss at iteration 8071: 0.04244110443718721\n",
      "Train Loss at iteration 8072: 0.04244102145605557\n",
      "Train Loss at iteration 8073: 0.04244093848884891\n",
      "Train Loss at iteration 8074: 0.042440855535564814\n",
      "Train Loss at iteration 8075: 0.04244077259620082\n",
      "Train Loss at iteration 8076: 0.0424406896707545\n",
      "Train Loss at iteration 8077: 0.042440606759223405\n",
      "Train Loss at iteration 8078: 0.04244052386160513\n",
      "Train Loss at iteration 8079: 0.042440440977897204\n",
      "Train Loss at iteration 8080: 0.04244035810809722\n",
      "Train Loss at iteration 8081: 0.04244027525220272\n",
      "Train Loss at iteration 8082: 0.0424401924102113\n",
      "Train Loss at iteration 8083: 0.042440109582120494\n",
      "Train Loss at iteration 8084: 0.04244002676792791\n",
      "Train Loss at iteration 8085: 0.04243994396763108\n",
      "Train Loss at iteration 8086: 0.04243986118122758\n",
      "Train Loss at iteration 8087: 0.042439778408714976\n",
      "Train Loss at iteration 8088: 0.042439695650090856\n",
      "Train Loss at iteration 8089: 0.04243961290535278\n",
      "Train Loss at iteration 8090: 0.04243953017449833\n",
      "Train Loss at iteration 8091: 0.04243944745752504\n",
      "Train Loss at iteration 8092: 0.042439364754430525\n",
      "Train Loss at iteration 8093: 0.04243928206521232\n",
      "Train Loss at iteration 8094: 0.04243919938986804\n",
      "Train Loss at iteration 8095: 0.04243911672839522\n",
      "Train Loss at iteration 8096: 0.042439034080791437\n",
      "Train Loss at iteration 8097: 0.04243895144705429\n",
      "Train Loss at iteration 8098: 0.04243886882718133\n",
      "Train Loss at iteration 8099: 0.04243878622117014\n",
      "Train Loss at iteration 8100: 0.0424387036290183\n",
      "Train Loss at iteration 8101: 0.042438621050723366\n",
      "Train Loss at iteration 8102: 0.04243853848628295\n",
      "Train Loss at iteration 8103: 0.04243845593569459\n",
      "Train Loss at iteration 8104: 0.0424383733989559\n",
      "Train Loss at iteration 8105: 0.04243829087606443\n",
      "Train Loss at iteration 8106: 0.04243820836701777\n",
      "Train Loss at iteration 8107: 0.0424381258718135\n",
      "Train Loss at iteration 8108: 0.042438043390449175\n",
      "Train Loss at iteration 8109: 0.042437960922922416\n",
      "Train Loss at iteration 8110: 0.04243787846923077\n",
      "Train Loss at iteration 8111: 0.04243779602937184\n",
      "Train Loss at iteration 8112: 0.042437713603343204\n",
      "Train Loss at iteration 8113: 0.04243763119114241\n",
      "Train Loss at iteration 8114: 0.0424375487927671\n",
      "Train Loss at iteration 8115: 0.04243746640821481\n",
      "Train Loss at iteration 8116: 0.042437384037483136\n",
      "Train Loss at iteration 8117: 0.042437301680569674\n",
      "Train Loss at iteration 8118: 0.042437219337471986\n",
      "Train Loss at iteration 8119: 0.04243713700818767\n",
      "Train Loss at iteration 8120: 0.042437054692714304\n",
      "Train Loss at iteration 8121: 0.04243697239104949\n",
      "Train Loss at iteration 8122: 0.0424368901031908\n",
      "Train Loss at iteration 8123: 0.04243680782913583\n",
      "Train Loss at iteration 8124: 0.042436725568882144\n",
      "Train Loss at iteration 8125: 0.042436643322427366\n",
      "Train Loss at iteration 8126: 0.04243656108976906\n",
      "Train Loss at iteration 8127: 0.04243647887090481\n",
      "Train Loss at iteration 8128: 0.04243639666583223\n",
      "Train Loss at iteration 8129: 0.042436314474548875\n",
      "Train Loss at iteration 8130: 0.042436232297052366\n",
      "Train Loss at iteration 8131: 0.04243615013334028\n",
      "Train Loss at iteration 8132: 0.0424360679834102\n",
      "Train Loss at iteration 8133: 0.04243598584725974\n",
      "Train Loss at iteration 8134: 0.04243590372488648\n",
      "Train Loss at iteration 8135: 0.042435821616288005\n",
      "Train Loss at iteration 8136: 0.042435739521461915\n",
      "Train Loss at iteration 8137: 0.04243565744040579\n",
      "Train Loss at iteration 8138: 0.04243557537311726\n",
      "Train Loss at iteration 8139: 0.04243549331959388\n",
      "Train Loss at iteration 8140: 0.042435411279833274\n",
      "Train Loss at iteration 8141: 0.04243532925383302\n",
      "Train Loss at iteration 8142: 0.042435247241590715\n",
      "Train Loss at iteration 8143: 0.04243516524310397\n",
      "Train Loss at iteration 8144: 0.04243508325837035\n",
      "Train Loss at iteration 8145: 0.042435001287387496\n",
      "Train Loss at iteration 8146: 0.04243491933015296\n",
      "Train Loss at iteration 8147: 0.04243483738666439\n",
      "Train Loss at iteration 8148: 0.04243475545691933\n",
      "Train Loss at iteration 8149: 0.04243467354091543\n",
      "Train Loss at iteration 8150: 0.042434591638650265\n",
      "Train Loss at iteration 8151: 0.04243450975012143\n",
      "Train Loss at iteration 8152: 0.04243442787532653\n",
      "Train Loss at iteration 8153: 0.04243434601426318\n",
      "Train Loss at iteration 8154: 0.04243426416692896\n",
      "Train Loss at iteration 8155: 0.042434182333321496\n",
      "Train Loss at iteration 8156: 0.04243410051343837\n",
      "Train Loss at iteration 8157: 0.04243401870727719\n",
      "Train Loss at iteration 8158: 0.04243393691483557\n",
      "Train Loss at iteration 8159: 0.04243385513611111\n",
      "Train Loss at iteration 8160: 0.042433773371101394\n",
      "Train Loss at iteration 8161: 0.042433691619804065\n",
      "Train Loss at iteration 8162: 0.0424336098822167\n",
      "Train Loss at iteration 8163: 0.04243352815833691\n",
      "Train Loss at iteration 8164: 0.04243344644816231\n",
      "Train Loss at iteration 8165: 0.04243336475169051\n",
      "Train Loss at iteration 8166: 0.042433283068919095\n",
      "Train Loss at iteration 8167: 0.04243320139984568\n",
      "Train Loss at iteration 8168: 0.042433119744467895\n",
      "Train Loss at iteration 8169: 0.04243303810278333\n",
      "Train Loss at iteration 8170: 0.0424329564747896\n",
      "Train Loss at iteration 8171: 0.04243287486048432\n",
      "Train Loss at iteration 8172: 0.04243279325986509\n",
      "Train Loss at iteration 8173: 0.04243271167292953\n",
      "Train Loss at iteration 8174: 0.04243263009967524\n",
      "Train Loss at iteration 8175: 0.04243254854009984\n",
      "Train Loss at iteration 8176: 0.042432466994200946\n",
      "Train Loss at iteration 8177: 0.042432385461976155\n",
      "Train Loss at iteration 8178: 0.0424323039434231\n",
      "Train Loss at iteration 8179: 0.042432222438539384\n",
      "Train Loss at iteration 8180: 0.04243214094732262\n",
      "Train Loss at iteration 8181: 0.04243205946977043\n",
      "Train Loss at iteration 8182: 0.04243197800588042\n",
      "Train Loss at iteration 8183: 0.042431896555650224\n",
      "Train Loss at iteration 8184: 0.04243181511907743\n",
      "Train Loss at iteration 8185: 0.04243173369615967\n",
      "Train Loss at iteration 8186: 0.04243165228689458\n",
      "Train Loss at iteration 8187: 0.042431570891279725\n",
      "Train Loss at iteration 8188: 0.042431489509312796\n",
      "Train Loss at iteration 8189: 0.04243140814099134\n",
      "Train Loss at iteration 8190: 0.04243132678631301\n",
      "Train Loss at iteration 8191: 0.042431245445275434\n",
      "Train Loss at iteration 8192: 0.04243116411787623\n",
      "Train Loss at iteration 8193: 0.04243108280411299\n",
      "Train Loss at iteration 8194: 0.04243100150398338\n",
      "Train Loss at iteration 8195: 0.042430920217484985\n",
      "Train Loss at iteration 8196: 0.04243083894461544\n",
      "Train Loss at iteration 8197: 0.04243075768537235\n",
      "Train Loss at iteration 8198: 0.042430676439753366\n",
      "Train Loss at iteration 8199: 0.04243059520775609\n",
      "Train Loss at iteration 8200: 0.04243051398937816\n",
      "Train Loss at iteration 8201: 0.042430432784617195\n",
      "Train Loss at iteration 8202: 0.04243035159347082\n",
      "Train Loss at iteration 8203: 0.04243027041593667\n",
      "Train Loss at iteration 8204: 0.04243018925201235\n",
      "Train Loss at iteration 8205: 0.04243010810169549\n",
      "Train Loss at iteration 8206: 0.04243002696498374\n",
      "Train Loss at iteration 8207: 0.04242994584187471\n",
      "Train Loss at iteration 8208: 0.04242986473236602\n",
      "Train Loss at iteration 8209: 0.04242978363645531\n",
      "Train Loss at iteration 8210: 0.0424297025541402\n",
      "Train Loss at iteration 8211: 0.04242962148541834\n",
      "Train Loss at iteration 8212: 0.04242954043028732\n",
      "Train Loss at iteration 8213: 0.04242945938874482\n",
      "Train Loss at iteration 8214: 0.042429378360788424\n",
      "Train Loss at iteration 8215: 0.04242929734641579\n",
      "Train Loss at iteration 8216: 0.04242921634562455\n",
      "Train Loss at iteration 8217: 0.04242913535841232\n",
      "Train Loss at iteration 8218: 0.04242905438477674\n",
      "Train Loss at iteration 8219: 0.042428973424715445\n",
      "Train Loss at iteration 8220: 0.042428892478226066\n",
      "Train Loss at iteration 8221: 0.04242881154530625\n",
      "Train Loss at iteration 8222: 0.04242873062595361\n",
      "Train Loss at iteration 8223: 0.042428649720165794\n",
      "Train Loss at iteration 8224: 0.042428568827940424\n",
      "Train Loss at iteration 8225: 0.042428487949275166\n",
      "Train Loss at iteration 8226: 0.042428407084167626\n",
      "Train Loss at iteration 8227: 0.042428326232615436\n",
      "Train Loss at iteration 8228: 0.04242824539461626\n",
      "Train Loss at iteration 8229: 0.04242816457016772\n",
      "Train Loss at iteration 8230: 0.04242808375926746\n",
      "Train Loss at iteration 8231: 0.04242800296191312\n",
      "Train Loss at iteration 8232: 0.04242792217810233\n",
      "Train Loss at iteration 8233: 0.04242784140783273\n",
      "Train Loss at iteration 8234: 0.042427760651101966\n",
      "Train Loss at iteration 8235: 0.04242767990790768\n",
      "Train Loss at iteration 8236: 0.0424275991782475\n",
      "Train Loss at iteration 8237: 0.042427518462119077\n",
      "Train Loss at iteration 8238: 0.04242743775952006\n",
      "Train Loss at iteration 8239: 0.042427357070448105\n",
      "Train Loss at iteration 8240: 0.04242727639490081\n",
      "Train Loss at iteration 8241: 0.042427195732875844\n",
      "Train Loss at iteration 8242: 0.04242711508437085\n",
      "Train Loss at iteration 8243: 0.04242703444938348\n",
      "Train Loss at iteration 8244: 0.042426953827911354\n",
      "Train Loss at iteration 8245: 0.042426873219952144\n",
      "Train Loss at iteration 8246: 0.04242679262550348\n",
      "Train Loss at iteration 8247: 0.04242671204456301\n",
      "Train Loss at iteration 8248: 0.04242663147712839\n",
      "Train Loss at iteration 8249: 0.042426550923197255\n",
      "Train Loss at iteration 8250: 0.04242647038276728\n",
      "Train Loss at iteration 8251: 0.04242638985583607\n",
      "Train Loss at iteration 8252: 0.04242630934240129\n",
      "Train Loss at iteration 8253: 0.042426228842460605\n",
      "Train Loss at iteration 8254: 0.042426148356011655\n",
      "Train Loss at iteration 8255: 0.04242606788305208\n",
      "Train Loss at iteration 8256: 0.04242598742357954\n",
      "Train Loss at iteration 8257: 0.042425906977591694\n",
      "Train Loss at iteration 8258: 0.04242582654508618\n",
      "Train Loss at iteration 8259: 0.04242574612606064\n",
      "Train Loss at iteration 8260: 0.04242566572051274\n",
      "Train Loss at iteration 8261: 0.04242558532844015\n",
      "Train Loss at iteration 8262: 0.042425504949840494\n",
      "Train Loss at iteration 8263: 0.04242542458471144\n",
      "Train Loss at iteration 8264: 0.04242534423305064\n",
      "Train Loss at iteration 8265: 0.04242526389485574\n",
      "Train Loss at iteration 8266: 0.042425183570124425\n",
      "Train Loss at iteration 8267: 0.042425103258854305\n",
      "Train Loss at iteration 8268: 0.042425022961043064\n",
      "Train Loss at iteration 8269: 0.04242494267668836\n",
      "Train Loss at iteration 8270: 0.04242486240578784\n",
      "Train Loss at iteration 8271: 0.042424782148339155\n",
      "Train Loss at iteration 8272: 0.04242470190434\n",
      "Train Loss at iteration 8273: 0.042424621673787986\n",
      "Train Loss at iteration 8274: 0.042424541456680794\n",
      "Train Loss at iteration 8275: 0.042424461253016084\n",
      "Train Loss at iteration 8276: 0.04242438106279151\n",
      "Train Loss at iteration 8277: 0.04242430088600473\n",
      "Train Loss at iteration 8278: 0.04242422072265343\n",
      "Train Loss at iteration 8279: 0.04242414057273523\n",
      "Train Loss at iteration 8280: 0.042424060436247836\n",
      "Train Loss at iteration 8281: 0.04242398031318886\n",
      "Train Loss at iteration 8282: 0.042423900203556006\n",
      "Train Loss at iteration 8283: 0.04242382010734692\n",
      "Train Loss at iteration 8284: 0.04242374002455927\n",
      "Train Loss at iteration 8285: 0.042423659955190714\n",
      "Train Loss at iteration 8286: 0.042423579899238915\n",
      "Train Loss at iteration 8287: 0.042423499856701574\n",
      "Train Loss at iteration 8288: 0.042423419827576296\n",
      "Train Loss at iteration 8289: 0.04242333981186078\n",
      "Train Loss at iteration 8290: 0.04242325980955268\n",
      "Train Loss at iteration 8291: 0.04242317982064969\n",
      "Train Loss at iteration 8292: 0.04242309984514945\n",
      "Train Loss at iteration 8293: 0.04242301988304963\n",
      "Train Loss at iteration 8294: 0.042422939934347904\n",
      "Train Loss at iteration 8295: 0.04242285999904195\n",
      "Train Loss at iteration 8296: 0.04242278007712943\n",
      "Train Loss at iteration 8297: 0.04242270016860801\n",
      "Train Loss at iteration 8298: 0.042422620273475355\n",
      "Train Loss at iteration 8299: 0.042422540391729144\n",
      "Train Loss at iteration 8300: 0.04242246052336705\n",
      "Train Loss at iteration 8301: 0.04242238066838674\n",
      "Train Loss at iteration 8302: 0.04242230082678587\n",
      "Train Loss at iteration 8303: 0.04242222099856214\n",
      "Train Loss at iteration 8304: 0.042422141183713215\n",
      "Train Loss at iteration 8305: 0.04242206138223676\n",
      "Train Loss at iteration 8306: 0.04242198159413045\n",
      "Train Loss at iteration 8307: 0.042421901819391976\n",
      "Train Loss at iteration 8308: 0.042421822058018976\n",
      "Train Loss at iteration 8309: 0.04242174231000917\n",
      "Train Loss at iteration 8310: 0.0424216625753602\n",
      "Train Loss at iteration 8311: 0.04242158285406975\n",
      "Train Loss at iteration 8312: 0.042421503146135506\n",
      "Train Loss at iteration 8313: 0.04242142345155515\n",
      "Train Loss at iteration 8314: 0.042421343770326324\n",
      "Train Loss at iteration 8315: 0.04242126410244674\n",
      "Train Loss at iteration 8316: 0.04242118444791407\n",
      "Train Loss at iteration 8317: 0.042421104806725986\n",
      "Train Loss at iteration 8318: 0.04242102517888016\n",
      "Train Loss at iteration 8319: 0.042420945564374306\n",
      "Train Loss at iteration 8320: 0.04242086596320607\n",
      "Train Loss at iteration 8321: 0.04242078637537314\n",
      "Train Loss at iteration 8322: 0.0424207068008732\n",
      "Train Loss at iteration 8323: 0.04242062723970393\n",
      "Train Loss at iteration 8324: 0.042420547691863014\n",
      "Train Loss at iteration 8325: 0.042420468157348144\n",
      "Train Loss at iteration 8326: 0.042420388636156985\n",
      "Train Loss at iteration 8327: 0.042420309128287224\n",
      "Train Loss at iteration 8328: 0.04242022963373655\n",
      "Train Loss at iteration 8329: 0.042420150152502646\n",
      "Train Loss at iteration 8330: 0.042420070684583214\n",
      "Train Loss at iteration 8331: 0.042419991229975904\n",
      "Train Loss at iteration 8332: 0.04241991178867843\n",
      "Train Loss at iteration 8333: 0.04241983236068846\n",
      "Train Loss at iteration 8334: 0.04241975294600368\n",
      "Train Loss at iteration 8335: 0.042419673544621825\n",
      "Train Loss at iteration 8336: 0.04241959415654051\n",
      "Train Loss at iteration 8337: 0.04241951478175746\n",
      "Train Loss at iteration 8338: 0.04241943542027037\n",
      "Train Loss at iteration 8339: 0.042419356072076926\n",
      "Train Loss at iteration 8340: 0.04241927673717479\n",
      "Train Loss at iteration 8341: 0.04241919741556169\n",
      "Train Loss at iteration 8342: 0.042419118107235296\n",
      "Train Loss at iteration 8343: 0.04241903881219329\n",
      "Train Loss at iteration 8344: 0.042418959530433384\n",
      "Train Loss at iteration 8345: 0.042418880261953264\n",
      "Train Loss at iteration 8346: 0.0424188010067506\n",
      "Train Loss at iteration 8347: 0.04241872176482311\n",
      "Train Loss at iteration 8348: 0.042418642536168484\n",
      "Train Loss at iteration 8349: 0.04241856332078441\n",
      "Train Loss at iteration 8350: 0.04241848411866859\n",
      "Train Loss at iteration 8351: 0.042418404929818715\n",
      "Train Loss at iteration 8352: 0.042418325754232475\n",
      "Train Loss at iteration 8353: 0.042418246591907544\n",
      "Train Loss at iteration 8354: 0.04241816744284166\n",
      "Train Loss at iteration 8355: 0.042418088307032496\n",
      "Train Loss at iteration 8356: 0.042418009184477766\n",
      "Train Loss at iteration 8357: 0.04241793007517513\n",
      "Train Loss at iteration 8358: 0.04241785097912233\n",
      "Train Loss at iteration 8359: 0.04241777189631704\n",
      "Train Loss at iteration 8360: 0.042417692826756954\n",
      "Train Loss at iteration 8361: 0.04241761377043978\n",
      "Train Loss at iteration 8362: 0.04241753472736322\n",
      "Train Loss at iteration 8363: 0.042417455697524975\n",
      "Train Loss at iteration 8364: 0.042417376680922735\n",
      "Train Loss at iteration 8365: 0.04241729767755421\n",
      "Train Loss at iteration 8366: 0.0424172186874171\n",
      "Train Loss at iteration 8367: 0.042417139710509096\n",
      "Train Loss at iteration 8368: 0.04241706074682792\n",
      "Train Loss at iteration 8369: 0.04241698179637126\n",
      "Train Loss at iteration 8370: 0.04241690285913682\n",
      "Train Loss at iteration 8371: 0.0424168239351223\n",
      "Train Loss at iteration 8372: 0.042416745024325415\n",
      "Train Loss at iteration 8373: 0.04241666612674387\n",
      "Train Loss at iteration 8374: 0.04241658724237537\n",
      "Train Loss at iteration 8375: 0.04241650837121759\n",
      "Train Loss at iteration 8376: 0.04241642951326829\n",
      "Train Loss at iteration 8377: 0.04241635066852513\n",
      "Train Loss at iteration 8378: 0.04241627183698584\n",
      "Train Loss at iteration 8379: 0.042416193018648116\n",
      "Train Loss at iteration 8380: 0.042416114213509666\n",
      "Train Loss at iteration 8381: 0.042416035421568216\n",
      "Train Loss at iteration 8382: 0.04241595664282145\n",
      "Train Loss at iteration 8383: 0.0424158778772671\n",
      "Train Loss at iteration 8384: 0.04241579912490285\n",
      "Train Loss at iteration 8385: 0.042415720385726434\n",
      "Train Loss at iteration 8386: 0.04241564165973554\n",
      "Train Loss at iteration 8387: 0.0424155629469279\n",
      "Train Loss at iteration 8388: 0.042415484247301224\n",
      "Train Loss at iteration 8389: 0.042415405560853194\n",
      "Train Loss at iteration 8390: 0.04241532688758156\n",
      "Train Loss at iteration 8391: 0.04241524822748401\n",
      "Train Loss at iteration 8392: 0.04241516958055828\n",
      "Train Loss at iteration 8393: 0.042415090946802056\n",
      "Train Loss at iteration 8394: 0.04241501232621306\n",
      "Train Loss at iteration 8395: 0.04241493371878902\n",
      "Train Loss at iteration 8396: 0.042414855124527646\n",
      "Train Loss at iteration 8397: 0.04241477654342663\n",
      "Train Loss at iteration 8398: 0.04241469797548374\n",
      "Train Loss at iteration 8399: 0.04241461942069664\n",
      "Train Loss at iteration 8400: 0.04241454087906306\n",
      "Train Loss at iteration 8401: 0.04241446235058074\n",
      "Train Loss at iteration 8402: 0.042414383835247364\n",
      "Train Loss at iteration 8403: 0.04241430533306068\n",
      "Train Loss at iteration 8404: 0.042414226844018396\n",
      "Train Loss at iteration 8405: 0.042414148368118215\n",
      "Train Loss at iteration 8406: 0.04241406990535787\n",
      "Train Loss at iteration 8407: 0.04241399145573511\n",
      "Train Loss at iteration 8408: 0.04241391301924759\n",
      "Train Loss at iteration 8409: 0.04241383459589309\n",
      "Train Loss at iteration 8410: 0.04241375618566929\n",
      "Train Loss at iteration 8411: 0.04241367778857394\n",
      "Train Loss at iteration 8412: 0.04241359940460474\n",
      "Train Loss at iteration 8413: 0.04241352103375944\n",
      "Train Loss at iteration 8414: 0.04241344267603574\n",
      "Train Loss at iteration 8415: 0.04241336433143137\n",
      "Train Loss at iteration 8416: 0.04241328599994407\n",
      "Train Loss at iteration 8417: 0.04241320768157153\n",
      "Train Loss at iteration 8418: 0.04241312937631149\n",
      "Train Loss at iteration 8419: 0.042413051084161685\n",
      "Train Loss at iteration 8420: 0.042412972805119846\n",
      "Train Loss at iteration 8421: 0.04241289453918367\n",
      "Train Loss at iteration 8422: 0.042412816286350914\n",
      "Train Loss at iteration 8423: 0.04241273804661928\n",
      "Train Loss at iteration 8424: 0.042412659819986535\n",
      "Train Loss at iteration 8425: 0.04241258160645036\n",
      "Train Loss at iteration 8426: 0.0424125034060085\n",
      "Train Loss at iteration 8427: 0.04241242521865869\n",
      "Train Loss at iteration 8428: 0.04241234704439866\n",
      "Train Loss at iteration 8429: 0.042412268883226134\n",
      "Train Loss at iteration 8430: 0.04241219073513884\n",
      "Train Loss at iteration 8431: 0.04241211260013452\n",
      "Train Loss at iteration 8432: 0.04241203447821089\n",
      "Train Loss at iteration 8433: 0.0424119563693657\n",
      "Train Loss at iteration 8434: 0.04241187827359666\n",
      "Train Loss at iteration 8435: 0.042411800190901516\n",
      "Train Loss at iteration 8436: 0.04241172212127801\n",
      "Train Loss at iteration 8437: 0.042411644064723864\n",
      "Train Loss at iteration 8438: 0.04241156602123679\n",
      "Train Loss at iteration 8439: 0.04241148799081457\n",
      "Train Loss at iteration 8440: 0.04241140997345489\n",
      "Train Loss at iteration 8441: 0.042411331969155516\n",
      "Train Loss at iteration 8442: 0.042411253977914176\n",
      "Train Loss at iteration 8443: 0.0424111759997286\n",
      "Train Loss at iteration 8444: 0.042411098034596535\n",
      "Train Loss at iteration 8445: 0.04241102008251572\n",
      "Train Loss at iteration 8446: 0.04241094214348387\n",
      "Train Loss at iteration 8447: 0.04241086421749875\n",
      "Train Loss at iteration 8448: 0.04241078630455808\n",
      "Train Loss at iteration 8449: 0.042410708404659596\n",
      "Train Loss at iteration 8450: 0.04241063051780106\n",
      "Train Loss at iteration 8451: 0.042410552643980186\n",
      "Train Loss at iteration 8452: 0.04241047478319473\n",
      "Train Loss at iteration 8453: 0.04241039693544242\n",
      "Train Loss at iteration 8454: 0.04241031910072102\n",
      "Train Loss at iteration 8455: 0.04241024127902824\n",
      "Train Loss at iteration 8456: 0.04241016347036185\n",
      "Train Loss at iteration 8457: 0.04241008567471957\n",
      "Train Loss at iteration 8458: 0.04241000789209915\n",
      "Train Loss at iteration 8459: 0.042409930122498336\n",
      "Train Loss at iteration 8460: 0.04240985236591487\n",
      "Train Loss at iteration 8461: 0.042409774622346505\n",
      "Train Loss at iteration 8462: 0.04240969689179097\n",
      "Train Loss at iteration 8463: 0.04240961917424604\n",
      "Train Loss at iteration 8464: 0.042409541469709405\n",
      "Train Loss at iteration 8465: 0.04240946377817885\n",
      "Train Loss at iteration 8466: 0.04240938609965211\n",
      "Train Loss at iteration 8467: 0.04240930843412696\n",
      "Train Loss at iteration 8468: 0.0424092307816011\n",
      "Train Loss at iteration 8469: 0.04240915314207231\n",
      "Train Loss at iteration 8470: 0.04240907551553833\n",
      "Train Loss at iteration 8471: 0.042408997901996905\n",
      "Train Loss at iteration 8472: 0.042408920301445795\n",
      "Train Loss at iteration 8473: 0.04240884271388272\n",
      "Train Loss at iteration 8474: 0.042408765139305464\n",
      "Train Loss at iteration 8475: 0.04240868757771176\n",
      "Train Loss at iteration 8476: 0.04240861002909936\n",
      "Train Loss at iteration 8477: 0.04240853249346602\n",
      "Train Loss at iteration 8478: 0.04240845497080949\n",
      "Train Loss at iteration 8479: 0.042408377461127515\n",
      "Train Loss at iteration 8480: 0.042408299964417855\n",
      "Train Loss at iteration 8481: 0.04240822248067824\n",
      "Train Loss at iteration 8482: 0.042408145009906466\n",
      "Train Loss at iteration 8483: 0.042408067552100276\n",
      "Train Loss at iteration 8484: 0.04240799010725739\n",
      "Train Loss at iteration 8485: 0.042407912675375595\n",
      "Train Loss at iteration 8486: 0.042407835256452635\n",
      "Train Loss at iteration 8487: 0.04240775785048627\n",
      "Train Loss at iteration 8488: 0.042407680457474235\n",
      "Train Loss at iteration 8489: 0.04240760307741433\n",
      "Train Loss at iteration 8490: 0.04240752571030428\n",
      "Train Loss at iteration 8491: 0.042407448356141825\n",
      "Train Loss at iteration 8492: 0.04240737101492476\n",
      "Train Loss at iteration 8493: 0.04240729368665083\n",
      "Train Loss at iteration 8494: 0.0424072163713178\n",
      "Train Loss at iteration 8495: 0.04240713906892341\n",
      "Train Loss at iteration 8496: 0.04240706177946543\n",
      "Train Loss at iteration 8497: 0.042406984502941635\n",
      "Train Loss at iteration 8498: 0.042406907239349756\n",
      "Train Loss at iteration 8499: 0.04240682998868757\n",
      "Train Loss at iteration 8500: 0.042406752750952854\n",
      "Train Loss at iteration 8501: 0.04240667552614334\n",
      "Train Loss at iteration 8502: 0.0424065983142568\n",
      "Train Loss at iteration 8503: 0.042406521115291\n",
      "Train Loss at iteration 8504: 0.0424064439292437\n",
      "Train Loss at iteration 8505: 0.04240636675611267\n",
      "Train Loss at iteration 8506: 0.04240628959589567\n",
      "Train Loss at iteration 8507: 0.04240621244859046\n",
      "Train Loss at iteration 8508: 0.042406135314194814\n",
      "Train Loss at iteration 8509: 0.04240605819270649\n",
      "Train Loss at iteration 8510: 0.04240598108412326\n",
      "Train Loss at iteration 8511: 0.04240590398844288\n",
      "Train Loss at iteration 8512: 0.042405826905663115\n",
      "Train Loss at iteration 8513: 0.042405749835781735\n",
      "Train Loss at iteration 8514: 0.042405672778796526\n",
      "Train Loss at iteration 8515: 0.04240559573470523\n",
      "Train Loss at iteration 8516: 0.042405518703505625\n",
      "Train Loss at iteration 8517: 0.042405441685195486\n",
      "Train Loss at iteration 8518: 0.042405364679772586\n",
      "Train Loss at iteration 8519: 0.04240528768723467\n",
      "Train Loss at iteration 8520: 0.04240521070757953\n",
      "Train Loss at iteration 8521: 0.04240513374080492\n",
      "Train Loss at iteration 8522: 0.042405056786908625\n",
      "Train Loss at iteration 8523: 0.042404979845888424\n",
      "Train Loss at iteration 8524: 0.042404902917742064\n",
      "Train Loss at iteration 8525: 0.042404826002467325\n",
      "Train Loss at iteration 8526: 0.04240474910006199\n",
      "Train Loss at iteration 8527: 0.04240467221052382\n",
      "Train Loss at iteration 8528: 0.042404595333850596\n",
      "Train Loss at iteration 8529: 0.04240451847004009\n",
      "Train Loss at iteration 8530: 0.042404441619090086\n",
      "Train Loss at iteration 8531: 0.04240436478099834\n",
      "Train Loss at iteration 8532: 0.04240428795576264\n",
      "Train Loss at iteration 8533: 0.042404211143380756\n",
      "Train Loss at iteration 8534: 0.042404134343850465\n",
      "Train Loss at iteration 8535: 0.04240405755716956\n",
      "Train Loss at iteration 8536: 0.04240398078333578\n",
      "Train Loss at iteration 8537: 0.042403904022346936\n",
      "Train Loss at iteration 8538: 0.04240382727420079\n",
      "Train Loss at iteration 8539: 0.042403750538895134\n",
      "Train Loss at iteration 8540: 0.042403673816427734\n",
      "Train Loss at iteration 8541: 0.04240359710679636\n",
      "Train Loss at iteration 8542: 0.04240352040999882\n",
      "Train Loss at iteration 8543: 0.04240344372603287\n",
      "Train Loss at iteration 8544: 0.04240336705489631\n",
      "Train Loss at iteration 8545: 0.04240329039658689\n",
      "Train Loss at iteration 8546: 0.04240321375110242\n",
      "Train Loss at iteration 8547: 0.042403137118440674\n",
      "Train Loss at iteration 8548: 0.042403060498599425\n",
      "Train Loss at iteration 8549: 0.042402983891576464\n",
      "Train Loss at iteration 8550: 0.04240290729736959\n",
      "Train Loss at iteration 8551: 0.04240283071597656\n",
      "Train Loss at iteration 8552: 0.042402754147395166\n",
      "Train Loss at iteration 8553: 0.04240267759162319\n",
      "Train Loss at iteration 8554: 0.04240260104865843\n",
      "Train Loss at iteration 8555: 0.04240252451849865\n",
      "Train Loss at iteration 8556: 0.04240244800114165\n",
      "Train Loss at iteration 8557: 0.042402371496585205\n",
      "Train Loss at iteration 8558: 0.04240229500482713\n",
      "Train Loss at iteration 8559: 0.042402218525865176\n",
      "Train Loss at iteration 8560: 0.04240214205969715\n",
      "Train Loss at iteration 8561: 0.042402065606320836\n",
      "Train Loss at iteration 8562: 0.04240198916573402\n",
      "Train Loss at iteration 8563: 0.0424019127379345\n",
      "Train Loss at iteration 8564: 0.04240183632292005\n",
      "Train Loss at iteration 8565: 0.04240175992068847\n",
      "Train Loss at iteration 8566: 0.04240168353123754\n",
      "Train Loss at iteration 8567: 0.04240160715456506\n",
      "Train Loss at iteration 8568: 0.042401530790668836\n",
      "Train Loss at iteration 8569: 0.04240145443954661\n",
      "Train Loss at iteration 8570: 0.04240137810119623\n",
      "Train Loss at iteration 8571: 0.04240130177561544\n",
      "Train Loss at iteration 8572: 0.042401225462802075\n",
      "Train Loss at iteration 8573: 0.04240114916275391\n",
      "Train Loss at iteration 8574: 0.04240107287546873\n",
      "Train Loss at iteration 8575: 0.04240099660094432\n",
      "Train Loss at iteration 8576: 0.042400920339178506\n",
      "Train Loss at iteration 8577: 0.04240084409016906\n",
      "Train Loss at iteration 8578: 0.042400767853913786\n",
      "Train Loss at iteration 8579: 0.042400691630410485\n",
      "Train Loss at iteration 8580: 0.04240061541965693\n",
      "Train Loss at iteration 8581: 0.042400539221650956\n",
      "Train Loss at iteration 8582: 0.04240046303639031\n",
      "Train Loss at iteration 8583: 0.04240038686387282\n",
      "Train Loss at iteration 8584: 0.042400310704096285\n",
      "Train Loss at iteration 8585: 0.0424002345570585\n",
      "Train Loss at iteration 8586: 0.042400158422757246\n",
      "Train Loss at iteration 8587: 0.042400082301190356\n",
      "Train Loss at iteration 8588: 0.0424000061923556\n",
      "Train Loss at iteration 8589: 0.04239993009625079\n",
      "Train Loss at iteration 8590: 0.04239985401287372\n",
      "Train Loss at iteration 8591: 0.04239977794222219\n",
      "Train Loss at iteration 8592: 0.04239970188429401\n",
      "Train Loss at iteration 8593: 0.042399625839086984\n",
      "Train Loss at iteration 8594: 0.042399549806598893\n",
      "Train Loss at iteration 8595: 0.04239947378682757\n",
      "Train Loss at iteration 8596: 0.04239939777977079\n",
      "Train Loss at iteration 8597: 0.042399321785426373\n",
      "Train Loss at iteration 8598: 0.04239924580379212\n",
      "Train Loss at iteration 8599: 0.042399169834865816\n",
      "Train Loss at iteration 8600: 0.042399093878645294\n",
      "Train Loss at iteration 8601: 0.04239901793512835\n",
      "Train Loss at iteration 8602: 0.04239894200431278\n",
      "Train Loss at iteration 8603: 0.04239886608619639\n",
      "Train Loss at iteration 8604: 0.042398790180776996\n",
      "Train Loss at iteration 8605: 0.042398714288052405\n",
      "Train Loss at iteration 8606: 0.042398638408020425\n",
      "Train Loss at iteration 8607: 0.04239856254067885\n",
      "Train Loss at iteration 8608: 0.04239848668602549\n",
      "Train Loss at iteration 8609: 0.04239841084405817\n",
      "Train Loss at iteration 8610: 0.04239833501477468\n",
      "Train Loss at iteration 8611: 0.04239825919817284\n",
      "Train Loss at iteration 8612: 0.04239818339425046\n",
      "Train Loss at iteration 8613: 0.042398107603005344\n",
      "Train Loss at iteration 8614: 0.04239803182443531\n",
      "Train Loss at iteration 8615: 0.042397956058538154\n",
      "Train Loss at iteration 8616: 0.042397880305311705\n",
      "Train Loss at iteration 8617: 0.04239780456475377\n",
      "Train Loss at iteration 8618: 0.04239772883686216\n",
      "Train Loss at iteration 8619: 0.04239765312163468\n",
      "Train Loss at iteration 8620: 0.042397577419069146\n",
      "Train Loss at iteration 8621: 0.04239750172916338\n",
      "Train Loss at iteration 8622: 0.042397426051915196\n",
      "Train Loss at iteration 8623: 0.0423973503873224\n",
      "Train Loss at iteration 8624: 0.0423972747353828\n",
      "Train Loss at iteration 8625: 0.04239719909609424\n",
      "Train Loss at iteration 8626: 0.04239712346945449\n",
      "Train Loss at iteration 8627: 0.04239704785546141\n",
      "Train Loss at iteration 8628: 0.042396972254112805\n",
      "Train Loss at iteration 8629: 0.04239689666540647\n",
      "Train Loss at iteration 8630: 0.042396821089340245\n",
      "Train Loss at iteration 8631: 0.04239674552591194\n",
      "Train Loss at iteration 8632: 0.04239666997511937\n",
      "Train Loss at iteration 8633: 0.04239659443696037\n",
      "Train Loss at iteration 8634: 0.042396518911432736\n",
      "Train Loss at iteration 8635: 0.042396443398534295\n",
      "Train Loss at iteration 8636: 0.04239636789826287\n",
      "Train Loss at iteration 8637: 0.04239629241061629\n",
      "Train Loss at iteration 8638: 0.04239621693559236\n",
      "Train Loss at iteration 8639: 0.04239614147318891\n",
      "Train Loss at iteration 8640: 0.042396066023403754\n",
      "Train Loss at iteration 8641: 0.04239599058623472\n",
      "Train Loss at iteration 8642: 0.042395915161679634\n",
      "Train Loss at iteration 8643: 0.042395839749736305\n",
      "Train Loss at iteration 8644: 0.042395764350402584\n",
      "Train Loss at iteration 8645: 0.042395688963676265\n",
      "Train Loss at iteration 8646: 0.04239561358955517\n",
      "Train Loss at iteration 8647: 0.04239553822803715\n",
      "Train Loss at iteration 8648: 0.04239546287912001\n",
      "Train Loss at iteration 8649: 0.04239538754280159\n",
      "Train Loss at iteration 8650: 0.0423953122190797\n",
      "Train Loss at iteration 8651: 0.042395236907952175\n",
      "Train Loss at iteration 8652: 0.042395161609416844\n",
      "Train Loss at iteration 8653: 0.04239508632347153\n",
      "Train Loss at iteration 8654: 0.042395011050114055\n",
      "Train Loss at iteration 8655: 0.04239493578934225\n",
      "Train Loss at iteration 8656: 0.04239486054115395\n",
      "Train Loss at iteration 8657: 0.04239478530554698\n",
      "Train Loss at iteration 8658: 0.04239471008251917\n",
      "Train Loss at iteration 8659: 0.04239463487206834\n",
      "Train Loss at iteration 8660: 0.04239455967419233\n",
      "Train Loss at iteration 8661: 0.04239448448888897\n",
      "Train Loss at iteration 8662: 0.04239440931615608\n",
      "Train Loss at iteration 8663: 0.04239433415599152\n",
      "Train Loss at iteration 8664: 0.04239425900839309\n",
      "Train Loss at iteration 8665: 0.042394183873358644\n",
      "Train Loss at iteration 8666: 0.042394108750885985\n",
      "Train Loss at iteration 8667: 0.042394033640972974\n",
      "Train Loss at iteration 8668: 0.04239395854361744\n",
      "Train Loss at iteration 8669: 0.0423938834588172\n",
      "Train Loss at iteration 8670: 0.04239380838657011\n",
      "Train Loss at iteration 8671: 0.042393733326873996\n",
      "Train Loss at iteration 8672: 0.04239365827972669\n",
      "Train Loss at iteration 8673: 0.04239358324512602\n",
      "Train Loss at iteration 8674: 0.04239350822306984\n",
      "Train Loss at iteration 8675: 0.04239343321355597\n",
      "Train Loss at iteration 8676: 0.042393358216582265\n",
      "Train Loss at iteration 8677: 0.042393283232146535\n",
      "Train Loss at iteration 8678: 0.04239320826024665\n",
      "Train Loss at iteration 8679: 0.04239313330088042\n",
      "Train Loss at iteration 8680: 0.04239305835404569\n",
      "Train Loss at iteration 8681: 0.042392983419740335\n",
      "Train Loss at iteration 8682: 0.04239290849796213\n",
      "Train Loss at iteration 8683: 0.042392833588708954\n",
      "Train Loss at iteration 8684: 0.04239275869197864\n",
      "Train Loss at iteration 8685: 0.042392683807769026\n",
      "Train Loss at iteration 8686: 0.04239260893607796\n",
      "Train Loss at iteration 8687: 0.04239253407690329\n",
      "Train Loss at iteration 8688: 0.042392459230242825\n",
      "Train Loss at iteration 8689: 0.04239238439609444\n",
      "Train Loss at iteration 8690: 0.042392309574455965\n",
      "Train Loss at iteration 8691: 0.042392234765325235\n",
      "Train Loss at iteration 8692: 0.042392159968700105\n",
      "Train Loss at iteration 8693: 0.042392085184578425\n",
      "Train Loss at iteration 8694: 0.04239201041295802\n",
      "Train Loss at iteration 8695: 0.04239193565383676\n",
      "Train Loss at iteration 8696: 0.04239186090721245\n",
      "Train Loss at iteration 8697: 0.04239178617308298\n",
      "Train Loss at iteration 8698: 0.042391711451446164\n",
      "Train Loss at iteration 8699: 0.04239163674229987\n",
      "Train Loss at iteration 8700: 0.04239156204564193\n",
      "Train Loss at iteration 8701: 0.042391487361470186\n",
      "Train Loss at iteration 8702: 0.042391412689782505\n",
      "Train Loss at iteration 8703: 0.042391338030576736\n",
      "Train Loss at iteration 8704: 0.04239126338385069\n",
      "Train Loss at iteration 8705: 0.042391188749602265\n",
      "Train Loss at iteration 8706: 0.04239111412782929\n",
      "Train Loss at iteration 8707: 0.042391039518529586\n",
      "Train Loss at iteration 8708: 0.04239096492170105\n",
      "Train Loss at iteration 8709: 0.042390890337341496\n",
      "Train Loss at iteration 8710: 0.0423908157654488\n",
      "Train Loss at iteration 8711: 0.04239074120602081\n",
      "Train Loss at iteration 8712: 0.04239066665905535\n",
      "Train Loss at iteration 8713: 0.04239059212455031\n",
      "Train Loss at iteration 8714: 0.04239051760250353\n",
      "Train Loss at iteration 8715: 0.04239044309291285\n",
      "Train Loss at iteration 8716: 0.04239036859577613\n",
      "Train Loss at iteration 8717: 0.042390294111091235\n",
      "Train Loss at iteration 8718: 0.04239021963885601\n",
      "Train Loss at iteration 8719: 0.0423901451790683\n",
      "Train Loss at iteration 8720: 0.04239007073172596\n",
      "Train Loss at iteration 8721: 0.042389996296826875\n",
      "Train Loss at iteration 8722: 0.042389921874368866\n",
      "Train Loss at iteration 8723: 0.04238984746434981\n",
      "Train Loss at iteration 8724: 0.04238977306676755\n",
      "Train Loss at iteration 8725: 0.04238969868161996\n",
      "Train Loss at iteration 8726: 0.04238962430890488\n",
      "Train Loss at iteration 8727: 0.042389549948620174\n",
      "Train Loss at iteration 8728: 0.042389475600763706\n",
      "Train Loss at iteration 8729: 0.04238940126533333\n",
      "Train Loss at iteration 8730: 0.0423893269423269\n",
      "Train Loss at iteration 8731: 0.04238925263174229\n",
      "Train Loss at iteration 8732: 0.04238917833357736\n",
      "Train Loss at iteration 8733: 0.04238910404782994\n",
      "Train Loss at iteration 8734: 0.042389029774497926\n",
      "Train Loss at iteration 8735: 0.04238895551357916\n",
      "Train Loss at iteration 8736: 0.0423888812650715\n",
      "Train Loss at iteration 8737: 0.04238880702897284\n",
      "Train Loss at iteration 8738: 0.042388732805281\n",
      "Train Loss at iteration 8739: 0.04238865859399386\n",
      "Train Loss at iteration 8740: 0.04238858439510929\n",
      "Train Loss at iteration 8741: 0.04238851020862515\n",
      "Train Loss at iteration 8742: 0.042388436034539304\n",
      "Train Loss at iteration 8743: 0.04238836187284962\n",
      "Train Loss at iteration 8744: 0.04238828772355395\n",
      "Train Loss at iteration 8745: 0.04238821358665017\n",
      "Train Loss at iteration 8746: 0.04238813946213614\n",
      "Train Loss at iteration 8747: 0.04238806535000974\n",
      "Train Loss at iteration 8748: 0.04238799125026882\n",
      "Train Loss at iteration 8749: 0.04238791716291123\n",
      "Train Loss at iteration 8750: 0.04238784308793489\n",
      "Train Loss at iteration 8751: 0.04238776902533763\n",
      "Train Loss at iteration 8752: 0.04238769497511732\n",
      "Train Loss at iteration 8753: 0.042387620937271844\n",
      "Train Loss at iteration 8754: 0.04238754691179904\n",
      "Train Loss at iteration 8755: 0.04238747289869682\n",
      "Train Loss at iteration 8756: 0.04238739889796303\n",
      "Train Loss at iteration 8757: 0.04238732490959554\n",
      "Train Loss at iteration 8758: 0.04238725093359222\n",
      "Train Loss at iteration 8759: 0.042387176969950945\n",
      "Train Loss at iteration 8760: 0.04238710301866959\n",
      "Train Loss at iteration 8761: 0.04238702907974602\n",
      "Train Loss at iteration 8762: 0.042386955153178106\n",
      "Train Loss at iteration 8763: 0.04238688123896371\n",
      "Train Loss at iteration 8764: 0.042386807337100756\n",
      "Train Loss at iteration 8765: 0.042386733447587054\n",
      "Train Loss at iteration 8766: 0.04238665957042052\n",
      "Train Loss at iteration 8767: 0.042386585705599\n",
      "Train Loss at iteration 8768: 0.04238651185312039\n",
      "Train Loss at iteration 8769: 0.04238643801298256\n",
      "Train Loss at iteration 8770: 0.04238636418518337\n",
      "Train Loss at iteration 8771: 0.04238629036972071\n",
      "Train Loss at iteration 8772: 0.04238621656659247\n",
      "Train Loss at iteration 8773: 0.0423861427757965\n",
      "Train Loss at iteration 8774: 0.04238606899733069\n",
      "Train Loss at iteration 8775: 0.042385995231192915\n",
      "Train Loss at iteration 8776: 0.04238592147738105\n",
      "Train Loss at iteration 8777: 0.04238584773589298\n",
      "Train Loss at iteration 8778: 0.0423857740067266\n",
      "Train Loss at iteration 8779: 0.04238570028987975\n",
      "Train Loss at iteration 8780: 0.04238562658535033\n",
      "Train Loss at iteration 8781: 0.04238555289313624\n",
      "Train Loss at iteration 8782: 0.0423854792132353\n",
      "Train Loss at iteration 8783: 0.04238540554564547\n",
      "Train Loss at iteration 8784: 0.04238533189036458\n",
      "Train Loss at iteration 8785: 0.042385258247390534\n",
      "Train Loss at iteration 8786: 0.042385184616721173\n",
      "Train Loss at iteration 8787: 0.042385110998354444\n",
      "Train Loss at iteration 8788: 0.042385037392288175\n",
      "Train Loss at iteration 8789: 0.04238496379852029\n",
      "Train Loss at iteration 8790: 0.04238489021704865\n",
      "Train Loss at iteration 8791: 0.04238481664787113\n",
      "Train Loss at iteration 8792: 0.04238474309098563\n",
      "Train Loss at iteration 8793: 0.04238466954639004\n",
      "Train Loss at iteration 8794: 0.04238459601408223\n",
      "Train Loss at iteration 8795: 0.04238452249406009\n",
      "Train Loss at iteration 8796: 0.04238444898632151\n",
      "Train Loss at iteration 8797: 0.04238437549086438\n",
      "Train Loss at iteration 8798: 0.04238430200768657\n",
      "Train Loss at iteration 8799: 0.042384228536786\n",
      "Train Loss at iteration 8800: 0.04238415507816053\n",
      "Train Loss at iteration 8801: 0.04238408163180805\n",
      "Train Loss at iteration 8802: 0.04238400819772646\n",
      "Train Loss at iteration 8803: 0.04238393477591364\n",
      "Train Loss at iteration 8804: 0.04238386136636748\n",
      "Train Loss at iteration 8805: 0.04238378796908588\n",
      "Train Loss at iteration 8806: 0.04238371458406672\n",
      "Train Loss at iteration 8807: 0.04238364121130788\n",
      "Train Loss at iteration 8808: 0.04238356785080727\n",
      "Train Loss at iteration 8809: 0.042383494502562784\n",
      "Train Loss at iteration 8810: 0.042383421166572295\n",
      "Train Loss at iteration 8811: 0.04238334784283371\n",
      "Train Loss at iteration 8812: 0.04238327453134492\n",
      "Train Loss at iteration 8813: 0.042383201232103804\n",
      "Train Loss at iteration 8814: 0.04238312794510827\n",
      "Train Loss at iteration 8815: 0.042383054670356204\n",
      "Train Loss at iteration 8816: 0.04238298140784553\n",
      "Train Loss at iteration 8817: 0.04238290815757408\n",
      "Train Loss at iteration 8818: 0.042382834919539814\n",
      "Train Loss at iteration 8819: 0.04238276169374058\n",
      "Train Loss at iteration 8820: 0.0423826884801743\n",
      "Train Loss at iteration 8821: 0.04238261527883886\n",
      "Train Loss at iteration 8822: 0.042382542089732154\n",
      "Train Loss at iteration 8823: 0.0423824689128521\n",
      "Train Loss at iteration 8824: 0.042382395748196564\n",
      "Train Loss at iteration 8825: 0.04238232259576347\n",
      "Train Loss at iteration 8826: 0.042382249455550705\n",
      "Train Loss at iteration 8827: 0.04238217632755617\n",
      "Train Loss at iteration 8828: 0.04238210321177775\n",
      "Train Loss at iteration 8829: 0.04238203010821337\n",
      "Train Loss at iteration 8830: 0.04238195701686092\n",
      "Train Loss at iteration 8831: 0.04238188393771829\n",
      "Train Loss at iteration 8832: 0.04238181087078338\n",
      "Train Loss at iteration 8833: 0.042381737816054116\n",
      "Train Loss at iteration 8834: 0.04238166477352837\n",
      "Train Loss at iteration 8835: 0.04238159174320406\n",
      "Train Loss at iteration 8836: 0.04238151872507908\n",
      "Train Loss at iteration 8837: 0.042381445719151346\n",
      "Train Loss at iteration 8838: 0.04238137272541874\n",
      "Train Loss at iteration 8839: 0.042381299743879185\n",
      "Train Loss at iteration 8840: 0.04238122677453057\n",
      "Train Loss at iteration 8841: 0.042381153817370815\n",
      "Train Loss at iteration 8842: 0.04238108087239781\n",
      "Train Loss at iteration 8843: 0.04238100793960948\n",
      "Train Loss at iteration 8844: 0.0423809350190037\n",
      "Train Loss at iteration 8845: 0.0423808621105784\n",
      "Train Loss at iteration 8846: 0.04238078921433148\n",
      "Train Loss at iteration 8847: 0.04238071633026084\n",
      "Train Loss at iteration 8848: 0.0423806434583644\n",
      "Train Loss at iteration 8849: 0.04238057059864005\n",
      "Train Loss at iteration 8850: 0.04238049775108572\n",
      "Train Loss at iteration 8851: 0.042380424915699294\n",
      "Train Loss at iteration 8852: 0.04238035209247869\n",
      "Train Loss at iteration 8853: 0.04238027928142183\n",
      "Train Loss at iteration 8854: 0.042380206482526606\n",
      "Train Loss at iteration 8855: 0.04238013369579095\n",
      "Train Loss at iteration 8856: 0.042380060921212735\n",
      "Train Loss at iteration 8857: 0.04237998815878991\n",
      "Train Loss at iteration 8858: 0.04237991540852037\n",
      "Train Loss at iteration 8859: 0.04237984267040201\n",
      "Train Loss at iteration 8860: 0.04237976994443277\n",
      "Train Loss at iteration 8861: 0.04237969723061055\n",
      "Train Loss at iteration 8862: 0.04237962452893328\n",
      "Train Loss at iteration 8863: 0.04237955183939882\n",
      "Train Loss at iteration 8864: 0.04237947916200514\n",
      "Train Loss at iteration 8865: 0.042379406496750135\n",
      "Train Loss at iteration 8866: 0.04237933384363171\n",
      "Train Loss at iteration 8867: 0.042379261202647796\n",
      "Train Loss at iteration 8868: 0.04237918857379629\n",
      "Train Loss at iteration 8869: 0.04237911595707513\n",
      "Train Loss at iteration 8870: 0.04237904335248221\n",
      "Train Loss at iteration 8871: 0.04237897076001545\n",
      "Train Loss at iteration 8872: 0.04237889817967278\n",
      "Train Loss at iteration 8873: 0.042378825611452106\n",
      "Train Loss at iteration 8874: 0.04237875305535134\n",
      "Train Loss at iteration 8875: 0.042378680511368415\n",
      "Train Loss at iteration 8876: 0.04237860797950125\n",
      "Train Loss at iteration 8877: 0.042378535459747735\n",
      "Train Loss at iteration 8878: 0.042378462952105816\n",
      "Train Loss at iteration 8879: 0.04237839045657341\n",
      "Train Loss at iteration 8880: 0.04237831797314844\n",
      "Train Loss at iteration 8881: 0.04237824550182881\n",
      "Train Loss at iteration 8882: 0.04237817304261244\n",
      "Train Loss at iteration 8883: 0.042378100595497285\n",
      "Train Loss at iteration 8884: 0.042378028160481224\n",
      "Train Loss at iteration 8885: 0.0423779557375622\n",
      "Train Loss at iteration 8886: 0.042377883326738135\n",
      "Train Loss at iteration 8887: 0.04237781092800695\n",
      "Train Loss at iteration 8888: 0.04237773854136656\n",
      "Train Loss at iteration 8889: 0.0423776661668149\n",
      "Train Loss at iteration 8890: 0.04237759380434988\n",
      "Train Loss at iteration 8891: 0.04237752145396943\n",
      "Train Loss at iteration 8892: 0.04237744911567149\n",
      "Train Loss at iteration 8893: 0.04237737678945397\n",
      "Train Loss at iteration 8894: 0.04237730447531479\n",
      "Train Loss at iteration 8895: 0.042377232173251886\n",
      "Train Loss at iteration 8896: 0.04237715988326319\n",
      "Train Loss at iteration 8897: 0.042377087605346614\n",
      "Train Loss at iteration 8898: 0.042377015339500085\n",
      "Train Loss at iteration 8899: 0.04237694308572154\n",
      "Train Loss at iteration 8900: 0.04237687084400891\n",
      "Train Loss at iteration 8901: 0.04237679861436012\n",
      "Train Loss at iteration 8902: 0.042376726396773076\n",
      "Train Loss at iteration 8903: 0.04237665419124574\n",
      "Train Loss at iteration 8904: 0.042376581997776025\n",
      "Train Loss at iteration 8905: 0.042376509816361846\n",
      "Train Loss at iteration 8906: 0.042376437647001164\n",
      "Train Loss at iteration 8907: 0.04237636548969189\n",
      "Train Loss at iteration 8908: 0.042376293344431964\n",
      "Train Loss at iteration 8909: 0.04237622121121931\n",
      "Train Loss at iteration 8910: 0.04237614909005186\n",
      "Train Loss at iteration 8911: 0.04237607698092756\n",
      "Train Loss at iteration 8912: 0.04237600488384432\n",
      "Train Loss at iteration 8913: 0.04237593279880008\n",
      "Train Loss at iteration 8914: 0.04237586072579279\n",
      "Train Loss at iteration 8915: 0.04237578866482037\n",
      "Train Loss at iteration 8916: 0.04237571661588075\n",
      "Train Loss at iteration 8917: 0.04237564457897187\n",
      "Train Loss at iteration 8918: 0.04237557255409167\n",
      "Train Loss at iteration 8919: 0.042375500541238065\n",
      "Train Loss at iteration 8920: 0.04237542854040903\n",
      "Train Loss at iteration 8921: 0.042375356551602454\n",
      "Train Loss at iteration 8922: 0.04237528457481629\n",
      "Train Loss at iteration 8923: 0.0423752126100485\n",
      "Train Loss at iteration 8924: 0.042375140657296986\n",
      "Train Loss at iteration 8925: 0.04237506871655971\n",
      "Train Loss at iteration 8926: 0.04237499678783461\n",
      "Train Loss at iteration 8927: 0.0423749248711196\n",
      "Train Loss at iteration 8928: 0.042374852966412635\n",
      "Train Loss at iteration 8929: 0.042374781073711655\n",
      "Train Loss at iteration 8930: 0.0423747091930146\n",
      "Train Loss at iteration 8931: 0.042374637324319404\n",
      "Train Loss at iteration 8932: 0.042374565467624005\n",
      "Train Loss at iteration 8933: 0.04237449362292636\n",
      "Train Loss at iteration 8934: 0.0423744217902244\n",
      "Train Loss at iteration 8935: 0.04237434996951607\n",
      "Train Loss at iteration 8936: 0.04237427816079929\n",
      "Train Loss at iteration 8937: 0.042374206364072034\n",
      "Train Loss at iteration 8938: 0.042374134579332236\n",
      "Train Loss at iteration 8939: 0.042374062806577815\n",
      "Train Loss at iteration 8940: 0.04237399104580675\n",
      "Train Loss at iteration 8941: 0.042373919297016956\n",
      "Train Loss at iteration 8942: 0.04237384756020639\n",
      "Train Loss at iteration 8943: 0.042373775835373\n",
      "Train Loss at iteration 8944: 0.04237370412251473\n",
      "Train Loss at iteration 8945: 0.04237363242162952\n",
      "Train Loss at iteration 8946: 0.04237356073271532\n",
      "Train Loss at iteration 8947: 0.04237348905577007\n",
      "Train Loss at iteration 8948: 0.04237341739079173\n",
      "Train Loss at iteration 8949: 0.04237334573777822\n",
      "Train Loss at iteration 8950: 0.042373274096727524\n",
      "Train Loss at iteration 8951: 0.04237320246763755\n",
      "Train Loss at iteration 8952: 0.042373130850506285\n",
      "Train Loss at iteration 8953: 0.04237305924533165\n",
      "Train Loss at iteration 8954: 0.04237298765211161\n",
      "Train Loss at iteration 8955: 0.0423729160708441\n",
      "Train Loss at iteration 8956: 0.04237284450152708\n",
      "Train Loss at iteration 8957: 0.042372772944158504\n",
      "Train Loss at iteration 8958: 0.04237270139873631\n",
      "Train Loss at iteration 8959: 0.04237262986525845\n",
      "Train Loss at iteration 8960: 0.04237255834372289\n",
      "Train Loss at iteration 8961: 0.04237248683412756\n",
      "Train Loss at iteration 8962: 0.04237241533647043\n",
      "Train Loss at iteration 8963: 0.04237234385074946\n",
      "Train Loss at iteration 8964: 0.042372272376962554\n",
      "Train Loss at iteration 8965: 0.04237220091510773\n",
      "Train Loss at iteration 8966: 0.042372129465182894\n",
      "Train Loss at iteration 8967: 0.04237205802718603\n",
      "Train Loss at iteration 8968: 0.042371986601115075\n",
      "Train Loss at iteration 8969: 0.04237191518696797\n",
      "Train Loss at iteration 8970: 0.04237184378474272\n",
      "Train Loss at iteration 8971: 0.04237177239443723\n",
      "Train Loss at iteration 8972: 0.04237170101604948\n",
      "Train Loss at iteration 8973: 0.04237162964957743\n",
      "Train Loss at iteration 8974: 0.042371558295019\n",
      "Train Loss at iteration 8975: 0.04237148695237221\n",
      "Train Loss at iteration 8976: 0.04237141562163496\n",
      "Train Loss at iteration 8977: 0.04237134430280524\n",
      "Train Loss at iteration 8978: 0.04237127299588098\n",
      "Train Loss at iteration 8979: 0.04237120170086018\n",
      "Train Loss at iteration 8980: 0.042371130417740765\n",
      "Train Loss at iteration 8981: 0.0423710591465207\n",
      "Train Loss at iteration 8982: 0.04237098788719796\n",
      "Train Loss at iteration 8983: 0.04237091663977049\n",
      "Train Loss at iteration 8984: 0.04237084540423627\n",
      "Train Loss at iteration 8985: 0.04237077418059323\n",
      "Train Loss at iteration 8986: 0.04237070296883935\n",
      "Train Loss at iteration 8987: 0.0423706317689726\n",
      "Train Loss at iteration 8988: 0.04237056058099092\n",
      "Train Loss at iteration 8989: 0.04237048940489228\n",
      "Train Loss at iteration 8990: 0.042370418240674654\n",
      "Train Loss at iteration 8991: 0.042370347088336\n",
      "Train Loss at iteration 8992: 0.04237027594787428\n",
      "Train Loss at iteration 8993: 0.04237020481928745\n",
      "Train Loss at iteration 8994: 0.04237013370257349\n",
      "Train Loss at iteration 8995: 0.04237006259773036\n",
      "Train Loss at iteration 8996: 0.04236999150475602\n",
      "Train Loss at iteration 8997: 0.04236992042364843\n",
      "Train Loss at iteration 8998: 0.042369849354405555\n",
      "Train Loss at iteration 8999: 0.04236977829702539\n",
      "Train Loss at iteration 9000: 0.042369707251505885\n",
      "Train Loss at iteration 9001: 0.04236963621784499\n",
      "Train Loss at iteration 9002: 0.042369565196040695\n",
      "Train Loss at iteration 9003: 0.042369494186090956\n",
      "Train Loss at iteration 9004: 0.042369423187993735\n",
      "Train Loss at iteration 9005: 0.04236935220174702\n",
      "Train Loss at iteration 9006: 0.04236928122734877\n",
      "Train Loss at iteration 9007: 0.042369210264796946\n",
      "Train Loss at iteration 9008: 0.04236913931408954\n",
      "Train Loss at iteration 9009: 0.0423690683752245\n",
      "Train Loss at iteration 9010: 0.04236899744819981\n",
      "Train Loss at iteration 9011: 0.042368926533013426\n",
      "Train Loss at iteration 9012: 0.04236885562966333\n",
      "Train Loss at iteration 9013: 0.042368784738147494\n",
      "Train Loss at iteration 9014: 0.042368713858463906\n",
      "Train Loss at iteration 9015: 0.042368642990610494\n",
      "Train Loss at iteration 9016: 0.042368572134585274\n",
      "Train Loss at iteration 9017: 0.04236850129038621\n",
      "Train Loss at iteration 9018: 0.04236843045801125\n",
      "Train Loss at iteration 9019: 0.04236835963745841\n",
      "Train Loss at iteration 9020: 0.04236828882872562\n",
      "Train Loss at iteration 9021: 0.042368218031810886\n",
      "Train Loss at iteration 9022: 0.04236814724671217\n",
      "Train Loss at iteration 9023: 0.042368076473427455\n",
      "Train Loss at iteration 9024: 0.042368005711954704\n",
      "Train Loss at iteration 9025: 0.04236793496229191\n",
      "Train Loss at iteration 9026: 0.042367864224437043\n",
      "Train Loss at iteration 9027: 0.04236779349838808\n",
      "Train Loss at iteration 9028: 0.042367722784143\n",
      "Train Loss at iteration 9029: 0.04236765208169976\n",
      "Train Loss at iteration 9030: 0.04236758139105637\n",
      "Train Loss at iteration 9031: 0.04236751071221079\n",
      "Train Loss at iteration 9032: 0.04236744004516102\n",
      "Train Loss at iteration 9033: 0.042367369389905006\n",
      "Train Loss at iteration 9034: 0.04236729874644075\n",
      "Train Loss at iteration 9035: 0.042367228114766224\n",
      "Train Loss at iteration 9036: 0.04236715749487942\n",
      "Train Loss at iteration 9037: 0.042367086886778295\n",
      "Train Loss at iteration 9038: 0.04236701629046086\n",
      "Train Loss at iteration 9039: 0.04236694570592507\n",
      "Train Loss at iteration 9040: 0.042366875133168926\n",
      "Train Loss at iteration 9041: 0.04236680457219041\n",
      "Train Loss at iteration 9042: 0.04236673402298749\n",
      "Train Loss at iteration 9043: 0.04236666348555816\n",
      "Train Loss at iteration 9044: 0.04236659295990041\n",
      "Train Loss at iteration 9045: 0.042366522446012196\n",
      "Train Loss at iteration 9046: 0.04236645194389153\n",
      "Train Loss at iteration 9047: 0.0423663814535364\n",
      "Train Loss at iteration 9048: 0.04236631097494476\n",
      "Train Loss at iteration 9049: 0.04236624050811463\n",
      "Train Loss at iteration 9050: 0.042366170053043974\n",
      "Train Loss at iteration 9051: 0.04236609960973078\n",
      "Train Loss at iteration 9052: 0.04236602917817305\n",
      "Train Loss at iteration 9053: 0.04236595875836875\n",
      "Train Loss at iteration 9054: 0.042365888350315885\n",
      "Train Loss at iteration 9055: 0.042365817954012445\n",
      "Train Loss at iteration 9056: 0.04236574756945638\n",
      "Train Loss at iteration 9057: 0.04236567719664573\n",
      "Train Loss at iteration 9058: 0.042365606835578454\n",
      "Train Loss at iteration 9059: 0.04236553648625255\n",
      "Train Loss at iteration 9060: 0.042365466148666\n",
      "Train Loss at iteration 9061: 0.04236539582281681\n",
      "Train Loss at iteration 9062: 0.04236532550870295\n",
      "Train Loss at iteration 9063: 0.04236525520632243\n",
      "Train Loss at iteration 9064: 0.042365184915673225\n",
      "Train Loss at iteration 9065: 0.042365114636753326\n",
      "Train Loss at iteration 9066: 0.042365044369560755\n",
      "Train Loss at iteration 9067: 0.042364974114093464\n",
      "Train Loss at iteration 9068: 0.04236490387034947\n",
      "Train Loss at iteration 9069: 0.04236483363832675\n",
      "Train Loss at iteration 9070: 0.04236476341802331\n",
      "Train Loss at iteration 9071: 0.04236469320943715\n",
      "Train Loss at iteration 9072: 0.04236462301256625\n",
      "Train Loss at iteration 9073: 0.04236455282740861\n",
      "Train Loss at iteration 9074: 0.04236448265396222\n",
      "Train Loss at iteration 9075: 0.042364412492225084\n",
      "Train Loss at iteration 9076: 0.04236434234219519\n",
      "Train Loss at iteration 9077: 0.04236427220387053\n",
      "Train Loss at iteration 9078: 0.04236420207724912\n",
      "Train Loss at iteration 9079: 0.042364131962328945\n",
      "Train Loss at iteration 9080: 0.042364061859108\n",
      "Train Loss at iteration 9081: 0.04236399176758428\n",
      "Train Loss at iteration 9082: 0.04236392168775578\n",
      "Train Loss at iteration 9083: 0.042363851619620516\n",
      "Train Loss at iteration 9084: 0.04236378156317648\n",
      "Train Loss at iteration 9085: 0.042363711518421665\n",
      "Train Loss at iteration 9086: 0.04236364148535407\n",
      "Train Loss at iteration 9087: 0.0423635714639717\n",
      "Train Loss at iteration 9088: 0.04236350145427255\n",
      "Train Loss at iteration 9089: 0.04236343145625462\n",
      "Train Loss at iteration 9090: 0.04236336146991591\n",
      "Train Loss at iteration 9091: 0.04236329149525443\n",
      "Train Loss at iteration 9092: 0.04236322153226819\n",
      "Train Loss at iteration 9093: 0.042363151580955154\n",
      "Train Loss at iteration 9094: 0.04236308164131337\n",
      "Train Loss at iteration 9095: 0.04236301171334082\n",
      "Train Loss at iteration 9096: 0.04236294179703551\n",
      "Train Loss at iteration 9097: 0.04236287189239544\n",
      "Train Loss at iteration 9098: 0.04236280199941859\n",
      "Train Loss at iteration 9099: 0.04236273211810302\n",
      "Train Loss at iteration 9100: 0.0423626622484467\n",
      "Train Loss at iteration 9101: 0.04236259239044763\n",
      "Train Loss at iteration 9102: 0.042362522544103824\n",
      "Train Loss at iteration 9103: 0.042362452709413294\n",
      "Train Loss at iteration 9104: 0.042362382886374055\n",
      "Train Loss at iteration 9105: 0.042362313074984094\n",
      "Train Loss at iteration 9106: 0.0423622432752414\n",
      "Train Loss at iteration 9107: 0.04236217348714402\n",
      "Train Loss at iteration 9108: 0.04236210371068995\n",
      "Train Loss at iteration 9109: 0.042362033945877196\n",
      "Train Loss at iteration 9110: 0.04236196419270375\n",
      "Train Loss at iteration 9111: 0.04236189445116764\n",
      "Train Loss at iteration 9112: 0.04236182472126688\n",
      "Train Loss at iteration 9113: 0.04236175500299947\n",
      "Train Loss at iteration 9114: 0.0423616852963634\n",
      "Train Loss at iteration 9115: 0.04236161560135672\n",
      "Train Loss at iteration 9116: 0.0423615459179774\n",
      "Train Loss at iteration 9117: 0.04236147624622349\n",
      "Train Loss at iteration 9118: 0.042361406586092984\n",
      "Train Loss at iteration 9119: 0.04236133693758389\n",
      "Train Loss at iteration 9120: 0.04236126730069422\n",
      "Train Loss at iteration 9121: 0.042361197675422006\n",
      "Train Loss at iteration 9122: 0.04236112806176522\n",
      "Train Loss at iteration 9123: 0.04236105845972193\n",
      "Train Loss at iteration 9124: 0.04236098886929009\n",
      "Train Loss at iteration 9125: 0.04236091929046775\n",
      "Train Loss at iteration 9126: 0.042360849723252936\n",
      "Train Loss at iteration 9127: 0.04236078016764363\n",
      "Train Loss at iteration 9128: 0.04236071062363787\n",
      "Train Loss at iteration 9129: 0.04236064109123366\n",
      "Train Loss at iteration 9130: 0.04236057157042902\n",
      "Train Loss at iteration 9131: 0.042360502061221966\n",
      "Train Loss at iteration 9132: 0.042360432563610516\n",
      "Train Loss at iteration 9133: 0.04236036307759269\n",
      "Train Loss at iteration 9134: 0.042360293603166496\n",
      "Train Loss at iteration 9135: 0.04236022414032996\n",
      "Train Loss at iteration 9136: 0.0423601546890811\n",
      "Train Loss at iteration 9137: 0.04236008524941792\n",
      "Train Loss at iteration 9138: 0.04236001582133847\n",
      "Train Loss at iteration 9139: 0.04235994640484074\n",
      "Train Loss at iteration 9140: 0.04235987699992275\n",
      "Train Loss at iteration 9141: 0.042359807606582554\n",
      "Train Loss at iteration 9142: 0.04235973822481813\n",
      "Train Loss at iteration 9143: 0.04235966885462752\n",
      "Train Loss at iteration 9144: 0.04235959949600874\n",
      "Train Loss at iteration 9145: 0.04235953014895982\n",
      "Train Loss at iteration 9146: 0.04235946081347878\n",
      "Train Loss at iteration 9147: 0.04235939148956363\n",
      "Train Loss at iteration 9148: 0.0423593221772124\n",
      "Train Loss at iteration 9149: 0.042359252876423115\n",
      "Train Loss at iteration 9150: 0.04235918358719381\n",
      "Train Loss at iteration 9151: 0.04235911430952248\n",
      "Train Loss at iteration 9152: 0.042359045043407165\n",
      "Train Loss at iteration 9153: 0.04235897578884589\n",
      "Train Loss at iteration 9154: 0.04235890654583668\n",
      "Train Loss at iteration 9155: 0.04235883731437756\n",
      "Train Loss at iteration 9156: 0.04235876809446655\n",
      "Train Loss at iteration 9157: 0.04235869888610169\n",
      "Train Loss at iteration 9158: 0.042358629689280995\n",
      "Train Loss at iteration 9159: 0.04235856050400248\n",
      "Train Loss at iteration 9160: 0.0423584913302642\n",
      "Train Loss at iteration 9161: 0.04235842216806417\n",
      "Train Loss at iteration 9162: 0.04235835301740041\n",
      "Train Loss at iteration 9163: 0.04235828387827096\n",
      "Train Loss at iteration 9164: 0.04235821475067385\n",
      "Train Loss at iteration 9165: 0.04235814563460709\n",
      "Train Loss at iteration 9166: 0.04235807653006871\n",
      "Train Loss at iteration 9167: 0.04235800743705677\n",
      "Train Loss at iteration 9168: 0.04235793835556927\n",
      "Train Loss at iteration 9169: 0.042357869285604255\n",
      "Train Loss at iteration 9170: 0.042357800227159764\n",
      "Train Loss at iteration 9171: 0.042357731180233796\n",
      "Train Loss at iteration 9172: 0.04235766214482442\n",
      "Train Loss at iteration 9173: 0.04235759312092963\n",
      "Train Loss at iteration 9174: 0.042357524108547505\n",
      "Train Loss at iteration 9175: 0.04235745510767602\n",
      "Train Loss at iteration 9176: 0.042357386118313266\n",
      "Train Loss at iteration 9177: 0.042357317140457235\n",
      "Train Loss at iteration 9178: 0.04235724817410599\n",
      "Train Loss at iteration 9179: 0.04235717921925753\n",
      "Train Loss at iteration 9180: 0.04235711027590992\n",
      "Train Loss at iteration 9181: 0.042357041344061194\n",
      "Train Loss at iteration 9182: 0.04235697242370936\n",
      "Train Loss at iteration 9183: 0.04235690351485248\n",
      "Train Loss at iteration 9184: 0.04235683461748859\n",
      "Train Loss at iteration 9185: 0.0423567657316157\n",
      "Train Loss at iteration 9186: 0.04235669685723188\n",
      "Train Loss at iteration 9187: 0.04235662799433516\n",
      "Train Loss at iteration 9188: 0.04235655914292353\n",
      "Train Loss at iteration 9189: 0.0423564903029951\n",
      "Train Loss at iteration 9190: 0.04235642147454787\n",
      "Train Loss at iteration 9191: 0.04235635265757987\n",
      "Train Loss at iteration 9192: 0.042356283852089165\n",
      "Train Loss at iteration 9193: 0.04235621505807378\n",
      "Train Loss at iteration 9194: 0.04235614627553175\n",
      "Train Loss at iteration 9195: 0.042356077504461125\n",
      "Train Loss at iteration 9196: 0.042356008744859945\n",
      "Train Loss at iteration 9197: 0.04235593999672624\n",
      "Train Loss at iteration 9198: 0.04235587126005805\n",
      "Train Loss at iteration 9199: 0.042355802534853444\n",
      "Train Loss at iteration 9200: 0.04235573382111043\n",
      "Train Loss at iteration 9201: 0.04235566511882707\n",
      "Train Loss at iteration 9202: 0.04235559642800139\n",
      "Train Loss at iteration 9203: 0.042355527748631445\n",
      "Train Loss at iteration 9204: 0.042355459080715294\n",
      "Train Loss at iteration 9205: 0.04235539042425095\n",
      "Train Loss at iteration 9206: 0.042355321779236475\n",
      "Train Loss at iteration 9207: 0.0423552531456699\n",
      "Train Loss at iteration 9208: 0.04235518452354929\n",
      "Train Loss at iteration 9209: 0.04235511591287268\n",
      "Train Loss at iteration 9210: 0.0423550473136381\n",
      "Train Loss at iteration 9211: 0.04235497872584363\n",
      "Train Loss at iteration 9212: 0.04235491014948727\n",
      "Train Loss at iteration 9213: 0.04235484158456712\n",
      "Train Loss at iteration 9214: 0.042354773031081185\n",
      "Train Loss at iteration 9215: 0.04235470448902753\n",
      "Train Loss at iteration 9216: 0.042354635958404205\n",
      "Train Loss at iteration 9217: 0.04235456743920924\n",
      "Train Loss at iteration 9218: 0.04235449893144069\n",
      "Train Loss at iteration 9219: 0.04235443043509663\n",
      "Train Loss at iteration 9220: 0.04235436195017509\n",
      "Train Loss at iteration 9221: 0.0423542934766741\n",
      "Train Loss at iteration 9222: 0.04235422501459173\n",
      "Train Loss at iteration 9223: 0.04235415656392604\n",
      "Train Loss at iteration 9224: 0.04235408812467507\n",
      "Train Loss at iteration 9225: 0.04235401969683685\n",
      "Train Loss at iteration 9226: 0.042353951280409474\n",
      "Train Loss at iteration 9227: 0.042353882875390964\n",
      "Train Loss at iteration 9228: 0.04235381448177937\n",
      "Train Loss at iteration 9229: 0.042353746099572764\n",
      "Train Loss at iteration 9230: 0.042353677728769176\n",
      "Train Loss at iteration 9231: 0.04235360936936667\n",
      "Train Loss at iteration 9232: 0.042353541021363315\n",
      "Train Loss at iteration 9233: 0.042353472684757136\n",
      "Train Loss at iteration 9234: 0.0423534043595462\n",
      "Train Loss at iteration 9235: 0.04235333604572858\n",
      "Train Loss at iteration 9236: 0.0423532677433023\n",
      "Train Loss at iteration 9237: 0.042353199452265435\n",
      "Train Loss at iteration 9238: 0.042353131172616035\n",
      "Train Loss at iteration 9239: 0.04235306290435215\n",
      "Train Loss at iteration 9240: 0.042352994647471845\n",
      "Train Loss at iteration 9241: 0.042352926401973176\n",
      "Train Loss at iteration 9242: 0.0423528581678542\n",
      "Train Loss at iteration 9243: 0.04235278994511296\n",
      "Train Loss at iteration 9244: 0.042352721733747545\n",
      "Train Loss at iteration 9245: 0.04235265353375598\n",
      "Train Loss at iteration 9246: 0.04235258534513635\n",
      "Train Loss at iteration 9247: 0.04235251716788669\n",
      "Train Loss at iteration 9248: 0.04235244900200506\n",
      "Train Loss at iteration 9249: 0.04235238084748955\n",
      "Train Loss at iteration 9250: 0.0423523127043382\n",
      "Train Loss at iteration 9251: 0.04235224457254906\n",
      "Train Loss at iteration 9252: 0.04235217645212021\n",
      "Train Loss at iteration 9253: 0.042352108343049706\n",
      "Train Loss at iteration 9254: 0.042352040245335607\n",
      "Train Loss at iteration 9255: 0.04235197215897597\n",
      "Train Loss at iteration 9256: 0.042351904083968855\n",
      "Train Loss at iteration 9257: 0.04235183602031234\n",
      "Train Loss at iteration 9258: 0.042351767968004474\n",
      "Train Loss at iteration 9259: 0.04235169992704332\n",
      "Train Loss at iteration 9260: 0.04235163189742696\n",
      "Train Loss at iteration 9261: 0.04235156387915343\n",
      "Train Loss at iteration 9262: 0.04235149587222081\n",
      "Train Loss at iteration 9263: 0.04235142787662718\n",
      "Train Loss at iteration 9264: 0.042351359892370564\n",
      "Train Loss at iteration 9265: 0.04235129191944907\n",
      "Train Loss at iteration 9266: 0.04235122395786074\n",
      "Train Loss at iteration 9267: 0.042351156007603655\n",
      "Train Loss at iteration 9268: 0.04235108806867586\n",
      "Train Loss at iteration 9269: 0.042351020141075446\n",
      "Train Loss at iteration 9270: 0.04235095222480046\n",
      "Train Loss at iteration 9271: 0.04235088431984897\n",
      "Train Loss at iteration 9272: 0.042350816426219066\n",
      "Train Loss at iteration 9273: 0.042350748543908794\n",
      "Train Loss at iteration 9274: 0.04235068067291623\n",
      "Train Loss at iteration 9275: 0.04235061281323945\n",
      "Train Loss at iteration 9276: 0.042350544964876516\n",
      "Train Loss at iteration 9277: 0.0423504771278255\n",
      "Train Loss at iteration 9278: 0.04235040930208448\n",
      "Train Loss at iteration 9279: 0.04235034148765151\n",
      "Train Loss at iteration 9280: 0.04235027368452465\n",
      "Train Loss at iteration 9281: 0.042350205892702004\n",
      "Train Loss at iteration 9282: 0.04235013811218163\n",
      "Train Loss at iteration 9283: 0.0423500703429616\n",
      "Train Loss at iteration 9284: 0.04235000258503999\n",
      "Train Loss at iteration 9285: 0.042349934838414864\n",
      "Train Loss at iteration 9286: 0.04234986710308429\n",
      "Train Loss at iteration 9287: 0.04234979937904635\n",
      "Train Loss at iteration 9288: 0.04234973166629912\n",
      "Train Loss at iteration 9289: 0.042349663964840675\n",
      "Train Loss at iteration 9290: 0.04234959627466908\n",
      "Train Loss at iteration 9291: 0.042349528595782435\n",
      "Train Loss at iteration 9292: 0.04234946092817878\n",
      "Train Loss at iteration 9293: 0.04234939327185621\n",
      "Train Loss at iteration 9294: 0.04234932562681279\n",
      "Train Loss at iteration 9295: 0.04234925799304661\n",
      "Train Loss at iteration 9296: 0.04234919037055575\n",
      "Train Loss at iteration 9297: 0.04234912275933827\n",
      "Train Loss at iteration 9298: 0.04234905515939223\n",
      "Train Loss at iteration 9299: 0.04234898757071577\n",
      "Train Loss at iteration 9300: 0.04234891999330691\n",
      "Train Loss at iteration 9301: 0.04234885242716374\n",
      "Train Loss at iteration 9302: 0.04234878487228436\n",
      "Train Loss at iteration 9303: 0.04234871732866684\n",
      "Train Loss at iteration 9304: 0.042348649796309236\n",
      "Train Loss at iteration 9305: 0.04234858227520966\n",
      "Train Loss at iteration 9306: 0.042348514765366184\n",
      "Train Loss at iteration 9307: 0.04234844726677688\n",
      "Train Loss at iteration 9308: 0.042348379779439824\n",
      "Train Loss at iteration 9309: 0.04234831230335311\n",
      "Train Loss at iteration 9310: 0.04234824483851481\n",
      "Train Loss at iteration 9311: 0.04234817738492303\n",
      "Train Loss at iteration 9312: 0.04234810994257582\n",
      "Train Loss at iteration 9313: 0.04234804251147127\n",
      "Train Loss at iteration 9314: 0.042347975091607475\n",
      "Train Loss at iteration 9315: 0.042347907682982515\n",
      "Train Loss at iteration 9316: 0.04234784028559447\n",
      "Train Loss at iteration 9317: 0.04234777289944142\n",
      "Train Loss at iteration 9318: 0.04234770552452145\n",
      "Train Loss at iteration 9319: 0.04234763816083267\n",
      "Train Loss at iteration 9320: 0.042347570808373126\n",
      "Train Loss at iteration 9321: 0.04234750346714092\n",
      "Train Loss at iteration 9322: 0.04234743613713414\n",
      "Train Loss at iteration 9323: 0.04234736881835089\n",
      "Train Loss at iteration 9324: 0.04234730151078922\n",
      "Train Loss at iteration 9325: 0.04234723421444724\n",
      "Train Loss at iteration 9326: 0.04234716692932303\n",
      "Train Loss at iteration 9327: 0.04234709965541468\n",
      "Train Loss at iteration 9328: 0.04234703239272027\n",
      "Train Loss at iteration 9329: 0.04234696514123792\n",
      "Train Loss at iteration 9330: 0.04234689790096567\n",
      "Train Loss at iteration 9331: 0.04234683067190164\n",
      "Train Loss at iteration 9332: 0.04234676345404392\n",
      "Train Loss at iteration 9333: 0.04234669624739059\n",
      "Train Loss at iteration 9334: 0.042346629051939755\n",
      "Train Loss at iteration 9335: 0.04234656186768947\n",
      "Train Loss at iteration 9336: 0.042346494694637866\n",
      "Train Loss at iteration 9337: 0.04234642753278301\n",
      "Train Loss at iteration 9338: 0.042346360382123\n",
      "Train Loss at iteration 9339: 0.04234629324265592\n",
      "Train Loss at iteration 9340: 0.04234622611437989\n",
      "Train Loss at iteration 9341: 0.04234615899729298\n",
      "Train Loss at iteration 9342: 0.042346091891393274\n",
      "Train Loss at iteration 9343: 0.04234602479667888\n",
      "Train Loss at iteration 9344: 0.042345957713147885\n",
      "Train Loss at iteration 9345: 0.042345890640798405\n",
      "Train Loss at iteration 9346: 0.0423458235796285\n",
      "Train Loss at iteration 9347: 0.042345756529636285\n",
      "Train Loss at iteration 9348: 0.042345689490819856\n",
      "Train Loss at iteration 9349: 0.0423456224631773\n",
      "Train Loss at iteration 9350: 0.04234555544670671\n",
      "Train Loss at iteration 9351: 0.0423454884414062\n",
      "Train Loss at iteration 9352: 0.04234542144727385\n",
      "Train Loss at iteration 9353: 0.04234535446430776\n",
      "Train Loss at iteration 9354: 0.04234528749250603\n",
      "Train Loss at iteration 9355: 0.042345220531866755\n",
      "Train Loss at iteration 9356: 0.04234515358238803\n",
      "Train Loss at iteration 9357: 0.04234508664406796\n",
      "Train Loss at iteration 9358: 0.042345019716904644\n",
      "Train Loss at iteration 9359: 0.04234495280089618\n",
      "Train Loss at iteration 9360: 0.042344885896040664\n",
      "Train Loss at iteration 9361: 0.04234481900233619\n",
      "Train Loss at iteration 9362: 0.04234475211978087\n",
      "Train Loss at iteration 9363: 0.04234468524837279\n",
      "Train Loss at iteration 9364: 0.042344618388110065\n",
      "Train Loss at iteration 9365: 0.042344551538990804\n",
      "Train Loss at iteration 9366: 0.042344484701013076\n",
      "Train Loss at iteration 9367: 0.042344417874175005\n",
      "Train Loss at iteration 9368: 0.0423443510584747\n",
      "Train Loss at iteration 9369: 0.04234428425391024\n",
      "Train Loss at iteration 9370: 0.042344217460479755\n",
      "Train Loss at iteration 9371: 0.04234415067818132\n",
      "Train Loss at iteration 9372: 0.042344083907013066\n",
      "Train Loss at iteration 9373: 0.04234401714697307\n",
      "Train Loss at iteration 9374: 0.04234395039805945\n",
      "Train Loss at iteration 9375: 0.04234388366027032\n",
      "Train Loss at iteration 9376: 0.04234381693360378\n",
      "Train Loss at iteration 9377: 0.04234375021805791\n",
      "Train Loss at iteration 9378: 0.04234368351363085\n",
      "Train Loss at iteration 9379: 0.04234361682032069\n",
      "Train Loss at iteration 9380: 0.04234355013812554\n",
      "Train Loss at iteration 9381: 0.04234348346704351\n",
      "Train Loss at iteration 9382: 0.04234341680707268\n",
      "Train Loss at iteration 9383: 0.0423433501582112\n",
      "Train Loss at iteration 9384: 0.04234328352045714\n",
      "Train Loss at iteration 9385: 0.042343216893808634\n",
      "Train Loss at iteration 9386: 0.04234315027826378\n",
      "Train Loss at iteration 9387: 0.04234308367382069\n",
      "Train Loss at iteration 9388: 0.04234301708047746\n",
      "Train Loss at iteration 9389: 0.04234295049823222\n",
      "Train Loss at iteration 9390: 0.04234288392708308\n",
      "Train Loss at iteration 9391: 0.04234281736702812\n",
      "Train Loss at iteration 9392: 0.04234275081806548\n",
      "Train Loss at iteration 9393: 0.04234268428019327\n",
      "Train Loss at iteration 9394: 0.042342617753409584\n",
      "Train Loss at iteration 9395: 0.04234255123771256\n",
      "Train Loss at iteration 9396: 0.04234248473310026\n",
      "Train Loss at iteration 9397: 0.04234241823957086\n",
      "Train Loss at iteration 9398: 0.042342351757122414\n",
      "Train Loss at iteration 9399: 0.04234228528575308\n",
      "Train Loss at iteration 9400: 0.042342218825460944\n",
      "Train Loss at iteration 9401: 0.04234215237624413\n",
      "Train Loss at iteration 9402: 0.04234208593810076\n",
      "Train Loss at iteration 9403: 0.04234201951102893\n",
      "Train Loss at iteration 9404: 0.04234195309502678\n",
      "Train Loss at iteration 9405: 0.04234188669009238\n",
      "Train Loss at iteration 9406: 0.042341820296223895\n",
      "Train Loss at iteration 9407: 0.04234175391341942\n",
      "Train Loss at iteration 9408: 0.04234168754167706\n",
      "Train Loss at iteration 9409: 0.04234162118099495\n",
      "Train Loss at iteration 9410: 0.0423415548313712\n",
      "Train Loss at iteration 9411: 0.042341488492803926\n",
      "Train Loss at iteration 9412: 0.042341422165291256\n",
      "Train Loss at iteration 9413: 0.04234135584883129\n",
      "Train Loss at iteration 9414: 0.04234128954342214\n",
      "Train Loss at iteration 9415: 0.04234122324906195\n",
      "Train Loss at iteration 9416: 0.04234115696574883\n",
      "Train Loss at iteration 9417: 0.042341090693480896\n",
      "Train Loss at iteration 9418: 0.04234102443225626\n",
      "Train Loss at iteration 9419: 0.04234095818207306\n",
      "Train Loss at iteration 9420: 0.04234089194292939\n",
      "Train Loss at iteration 9421: 0.04234082571482341\n",
      "Train Loss at iteration 9422: 0.042340759497753205\n",
      "Train Loss at iteration 9423: 0.04234069329171691\n",
      "Train Loss at iteration 9424: 0.04234062709671266\n",
      "Train Loss at iteration 9425: 0.04234056091273854\n",
      "Train Loss at iteration 9426: 0.042340494739792706\n",
      "Train Loss at iteration 9427: 0.042340428577873276\n",
      "Train Loss at iteration 9428: 0.04234036242697835\n",
      "Train Loss at iteration 9429: 0.04234029628710608\n",
      "Train Loss at iteration 9430: 0.042340230158254576\n",
      "Train Loss at iteration 9431: 0.042340164040421964\n",
      "Train Loss at iteration 9432: 0.04234009793360637\n",
      "Train Loss at iteration 9433: 0.04234003183780591\n",
      "Train Loss at iteration 9434: 0.042339965753018724\n",
      "Train Loss at iteration 9435: 0.042339899679242926\n",
      "Train Loss at iteration 9436: 0.04233983361647664\n",
      "Train Loss at iteration 9437: 0.042339767564718\n",
      "Train Loss at iteration 9438: 0.042339701523965136\n",
      "Train Loss at iteration 9439: 0.04233963549421616\n",
      "Train Loss at iteration 9440: 0.04233956947546923\n",
      "Train Loss at iteration 9441: 0.04233950346772243\n",
      "Train Loss at iteration 9442: 0.04233943747097392\n",
      "Train Loss at iteration 9443: 0.042339371485221815\n",
      "Train Loss at iteration 9444: 0.04233930551046425\n",
      "Train Loss at iteration 9445: 0.04233923954669933\n",
      "Train Loss at iteration 9446: 0.04233917359392522\n",
      "Train Loss at iteration 9447: 0.04233910765214004\n",
      "Train Loss at iteration 9448: 0.04233904172134191\n",
      "Train Loss at iteration 9449: 0.04233897580152896\n",
      "Train Loss at iteration 9450: 0.04233890989269933\n",
      "Train Loss at iteration 9451: 0.04233884399485114\n",
      "Train Loss at iteration 9452: 0.04233877810798253\n",
      "Train Loss at iteration 9453: 0.042338712232091624\n",
      "Train Loss at iteration 9454: 0.04233864636717657\n",
      "Train Loss at iteration 9455: 0.04233858051323548\n",
      "Train Loss at iteration 9456: 0.0423385146702665\n",
      "Train Loss at iteration 9457: 0.04233844883826776\n",
      "Train Loss at iteration 9458: 0.04233838301723739\n",
      "Train Loss at iteration 9459: 0.04233831720717353\n",
      "Train Loss at iteration 9460: 0.04233825140807429\n",
      "Train Loss at iteration 9461: 0.04233818561993785\n",
      "Train Loss at iteration 9462: 0.042338119842762305\n",
      "Train Loss at iteration 9463: 0.04233805407654582\n",
      "Train Loss at iteration 9464: 0.04233798832128649\n",
      "Train Loss at iteration 9465: 0.0423379225769825\n",
      "Train Loss at iteration 9466: 0.04233785684363195\n",
      "Train Loss at iteration 9467: 0.042337791121232995\n",
      "Train Loss at iteration 9468: 0.04233772540978375\n",
      "Train Loss at iteration 9469: 0.04233765970928238\n",
      "Train Loss at iteration 9470: 0.042337594019727004\n",
      "Train Loss at iteration 9471: 0.04233752834111577\n",
      "Train Loss at iteration 9472: 0.04233746267344682\n",
      "Train Loss at iteration 9473: 0.04233739701671827\n",
      "Train Loss at iteration 9474: 0.04233733137092827\n",
      "Train Loss at iteration 9475: 0.04233726573607498\n",
      "Train Loss at iteration 9476: 0.04233720011215651\n",
      "Train Loss at iteration 9477: 0.04233713449917102\n",
      "Train Loss at iteration 9478: 0.042337068897116636\n",
      "Train Loss at iteration 9479: 0.0423370033059915\n",
      "Train Loss at iteration 9480: 0.04233693772579378\n",
      "Train Loss at iteration 9481: 0.042336872156521564\n",
      "Train Loss at iteration 9482: 0.04233680659817304\n",
      "Train Loss at iteration 9483: 0.042336741050746335\n",
      "Train Loss at iteration 9484: 0.04233667551423958\n",
      "Train Loss at iteration 9485: 0.04233660998865096\n",
      "Train Loss at iteration 9486: 0.042336544473978555\n",
      "Train Loss at iteration 9487: 0.04233647897022055\n",
      "Train Loss at iteration 9488: 0.04233641347737508\n",
      "Train Loss at iteration 9489: 0.04233634799544028\n",
      "Train Loss at iteration 9490: 0.04233628252441431\n",
      "Train Loss at iteration 9491: 0.04233621706429531\n",
      "Train Loss at iteration 9492: 0.04233615161508142\n",
      "Train Loss at iteration 9493: 0.042336086176770774\n",
      "Train Loss at iteration 9494: 0.04233602074936155\n",
      "Train Loss at iteration 9495: 0.042335955332851866\n",
      "Train Loss at iteration 9496: 0.04233588992723987\n",
      "Train Loss at iteration 9497: 0.04233582453252373\n",
      "Train Loss at iteration 9498: 0.042335759148701566\n",
      "Train Loss at iteration 9499: 0.042335693775771545\n",
      "Train Loss at iteration 9500: 0.04233562841373181\n",
      "Train Loss at iteration 9501: 0.042335563062580496\n",
      "Train Loss at iteration 9502: 0.042335497722315776\n",
      "Train Loss at iteration 9503: 0.04233543239293579\n",
      "Train Loss at iteration 9504: 0.04233536707443867\n",
      "Train Loss at iteration 9505: 0.04233530176682258\n",
      "Train Loss at iteration 9506: 0.04233523647008568\n",
      "Train Loss at iteration 9507: 0.0423351711842261\n",
      "Train Loss at iteration 9508: 0.042335105909242\n",
      "Train Loss at iteration 9509: 0.04233504064513153\n",
      "Train Loss at iteration 9510: 0.04233497539189283\n",
      "Train Loss at iteration 9511: 0.04233491014952407\n",
      "Train Loss at iteration 9512: 0.0423348449180234\n",
      "Train Loss at iteration 9513: 0.04233477969738895\n",
      "Train Loss at iteration 9514: 0.042334714487618906\n",
      "Train Loss at iteration 9515: 0.042334649288711404\n",
      "Train Loss at iteration 9516: 0.042334584100664596\n",
      "Train Loss at iteration 9517: 0.04233451892347662\n",
      "Train Loss at iteration 9518: 0.04233445375714566\n",
      "Train Loss at iteration 9519: 0.04233438860166985\n",
      "Train Loss at iteration 9520: 0.04233432345704737\n",
      "Train Loss at iteration 9521: 0.042334258323276336\n",
      "Train Loss at iteration 9522: 0.04233419320035494\n",
      "Train Loss at iteration 9523: 0.0423341280882813\n",
      "Train Loss at iteration 9524: 0.042334062987053604\n",
      "Train Loss at iteration 9525: 0.04233399789667\n",
      "Train Loss at iteration 9526: 0.04233393281712863\n",
      "Train Loss at iteration 9527: 0.04233386774842767\n",
      "Train Loss at iteration 9528: 0.042333802690565275\n",
      "Train Loss at iteration 9529: 0.04233373764353959\n",
      "Train Loss at iteration 9530: 0.04233367260734879\n",
      "Train Loss at iteration 9531: 0.04233360758199103\n",
      "Train Loss at iteration 9532: 0.04233354256746444\n",
      "Train Loss at iteration 9533: 0.042333477563767215\n",
      "Train Loss at iteration 9534: 0.0423334125708975\n",
      "Train Loss at iteration 9535: 0.04233334758885344\n",
      "Train Loss at iteration 9536: 0.042333282617633236\n",
      "Train Loss at iteration 9537: 0.04233321765723501\n",
      "Train Loss at iteration 9538: 0.04233315270765693\n",
      "Train Loss at iteration 9539: 0.042333087768897174\n",
      "Train Loss at iteration 9540: 0.04233302284095388\n",
      "Train Loss at iteration 9541: 0.04233295792382522\n",
      "Train Loss at iteration 9542: 0.04233289301750938\n",
      "Train Loss at iteration 9543: 0.042332828122004486\n",
      "Train Loss at iteration 9544: 0.0423327632373087\n",
      "Train Loss at iteration 9545: 0.04233269836342023\n",
      "Train Loss at iteration 9546: 0.0423326335003372\n",
      "Train Loss at iteration 9547: 0.042332568648057754\n",
      "Train Loss at iteration 9548: 0.04233250380658011\n",
      "Train Loss at iteration 9549: 0.042332438975902394\n",
      "Train Loss at iteration 9550: 0.042332374156022805\n",
      "Train Loss at iteration 9551: 0.04233230934693946\n",
      "Train Loss at iteration 9552: 0.042332244548650574\n",
      "Train Loss at iteration 9553: 0.042332179761154276\n",
      "Train Loss at iteration 9554: 0.04233211498444874\n",
      "Train Loss at iteration 9555: 0.04233205021853215\n",
      "Train Loss at iteration 9556: 0.04233198546340267\n",
      "Train Loss at iteration 9557: 0.04233192071905844\n",
      "Train Loss at iteration 9558: 0.04233185598549766\n",
      "Train Loss at iteration 9559: 0.04233179126271848\n",
      "Train Loss at iteration 9560: 0.04233172655071906\n",
      "Train Loss at iteration 9561: 0.0423316618494976\n",
      "Train Loss at iteration 9562: 0.042331597159052235\n",
      "Train Loss at iteration 9563: 0.04233153247938114\n",
      "Train Loss at iteration 9564: 0.042331467810482505\n",
      "Train Loss at iteration 9565: 0.04233140315235449\n",
      "Train Loss at iteration 9566: 0.04233133850499525\n",
      "Train Loss at iteration 9567: 0.04233127386840296\n",
      "Train Loss at iteration 9568: 0.042331209242575815\n",
      "Train Loss at iteration 9569: 0.04233114462751197\n",
      "Train Loss at iteration 9570: 0.04233108002320957\n",
      "Train Loss at iteration 9571: 0.04233101542966684\n",
      "Train Loss at iteration 9572: 0.042330950846881925\n",
      "Train Loss at iteration 9573: 0.042330886274852986\n",
      "Train Loss at iteration 9574: 0.0423308217135782\n",
      "Train Loss at iteration 9575: 0.04233075716305575\n",
      "Train Loss at iteration 9576: 0.04233069262328382\n",
      "Train Loss at iteration 9577: 0.04233062809426056\n",
      "Train Loss at iteration 9578: 0.04233056357598415\n",
      "Train Loss at iteration 9579: 0.04233049906845278\n",
      "Train Loss at iteration 9580: 0.042330434571664605\n",
      "Train Loss at iteration 9581: 0.04233037008561781\n",
      "Train Loss at iteration 9582: 0.04233030561031057\n",
      "Train Loss at iteration 9583: 0.042330241145741056\n",
      "Train Loss at iteration 9584: 0.04233017669190745\n",
      "Train Loss at iteration 9585: 0.042330112248807936\n",
      "Train Loss at iteration 9586: 0.04233004781644066\n",
      "Train Loss at iteration 9587: 0.04232998339480383\n",
      "Train Loss at iteration 9588: 0.04232991898389562\n",
      "Train Loss at iteration 9589: 0.0423298545837142\n",
      "Train Loss at iteration 9590: 0.04232979019425774\n",
      "Train Loss at iteration 9591: 0.042329725815524445\n",
      "Train Loss at iteration 9592: 0.04232966144751246\n",
      "Train Loss at iteration 9593: 0.04232959709021998\n",
      "Train Loss at iteration 9594: 0.0423295327436452\n",
      "Train Loss at iteration 9595: 0.04232946840778628\n",
      "Train Loss at iteration 9596: 0.042329404082641395\n",
      "Train Loss at iteration 9597: 0.04232933976820873\n",
      "Train Loss at iteration 9598: 0.042329275464486504\n",
      "Train Loss at iteration 9599: 0.042329211171472834\n",
      "Train Loss at iteration 9600: 0.04232914688916595\n",
      "Train Loss at iteration 9601: 0.04232908261756401\n",
      "Train Loss at iteration 9602: 0.04232901835666521\n",
      "Train Loss at iteration 9603: 0.04232895410646771\n",
      "Train Loss at iteration 9604: 0.04232888986696973\n",
      "Train Loss at iteration 9605: 0.04232882563816942\n",
      "Train Loss at iteration 9606: 0.04232876142006496\n",
      "Train Loss at iteration 9607: 0.042328697212654566\n",
      "Train Loss at iteration 9608: 0.0423286330159364\n",
      "Train Loss at iteration 9609: 0.04232856882990865\n",
      "Train Loss at iteration 9610: 0.042328504654569506\n",
      "Train Loss at iteration 9611: 0.04232844048991716\n",
      "Train Loss at iteration 9612: 0.04232837633594977\n",
      "Train Loss at iteration 9613: 0.04232831219266554\n",
      "Train Loss at iteration 9614: 0.04232824806006264\n",
      "Train Loss at iteration 9615: 0.04232818393813929\n",
      "Train Loss at iteration 9616: 0.042328119826893655\n",
      "Train Loss at iteration 9617: 0.04232805572632393\n",
      "Train Loss at iteration 9618: 0.042327991636428294\n",
      "Train Loss at iteration 9619: 0.042327927557204936\n",
      "Train Loss at iteration 9620: 0.04232786348865203\n",
      "Train Loss at iteration 9621: 0.0423277994307678\n",
      "Train Loss at iteration 9622: 0.042327735383550405\n",
      "Train Loss at iteration 9623: 0.04232767134699805\n",
      "Train Loss at iteration 9624: 0.04232760732110893\n",
      "Train Loss at iteration 9625: 0.0423275433058812\n",
      "Train Loss at iteration 9626: 0.04232747930131309\n",
      "Train Loss at iteration 9627: 0.042327415307402756\n",
      "Train Loss at iteration 9628: 0.04232735132414843\n",
      "Train Loss at iteration 9629: 0.04232728735154827\n",
      "Train Loss at iteration 9630: 0.04232722338960047\n",
      "Train Loss at iteration 9631: 0.04232715943830323\n",
      "Train Loss at iteration 9632: 0.042327095497654746\n",
      "Train Loss at iteration 9633: 0.04232703156765321\n",
      "Train Loss at iteration 9634: 0.04232696764829681\n",
      "Train Loss at iteration 9635: 0.04232690373958373\n",
      "Train Loss at iteration 9636: 0.04232683984151217\n",
      "Train Loss at iteration 9637: 0.042326775954080326\n",
      "Train Loss at iteration 9638: 0.04232671207728641\n",
      "Train Loss at iteration 9639: 0.04232664821112858\n",
      "Train Loss at iteration 9640: 0.04232658435560506\n",
      "Train Loss at iteration 9641: 0.042326520510714026\n",
      "Train Loss at iteration 9642: 0.04232645667645368\n",
      "Train Loss at iteration 9643: 0.04232639285282223\n",
      "Train Loss at iteration 9644: 0.04232632903981787\n",
      "Train Loss at iteration 9645: 0.042326265237438765\n",
      "Train Loss at iteration 9646: 0.042326201445683144\n",
      "Train Loss at iteration 9647: 0.04232613766454921\n",
      "Train Loss at iteration 9648: 0.04232607389403512\n",
      "Train Loss at iteration 9649: 0.04232601013413911\n",
      "Train Loss at iteration 9650: 0.042325946384859356\n",
      "Train Loss at iteration 9651: 0.04232588264619407\n",
      "Train Loss at iteration 9652: 0.042325818918141446\n",
      "Train Loss at iteration 9653: 0.04232575520069968\n",
      "Train Loss at iteration 9654: 0.042325691493866974\n",
      "Train Loss at iteration 9655: 0.04232562779764153\n",
      "Train Loss at iteration 9656: 0.04232556411202155\n",
      "Train Loss at iteration 9657: 0.042325500437005226\n",
      "Train Loss at iteration 9658: 0.04232543677259075\n",
      "Train Loss at iteration 9659: 0.042325373118776356\n",
      "Train Loss at iteration 9660: 0.042325309475560215\n",
      "Train Loss at iteration 9661: 0.04232524584294053\n",
      "Train Loss at iteration 9662: 0.042325182220915525\n",
      "Train Loss at iteration 9663: 0.042325118609483375\n",
      "Train Loss at iteration 9664: 0.04232505500864231\n",
      "Train Loss at iteration 9665: 0.042324991418390516\n",
      "Train Loss at iteration 9666: 0.04232492783872619\n",
      "Train Loss at iteration 9667: 0.04232486426964755\n",
      "Train Loss at iteration 9668: 0.042324800711152806\n",
      "Train Loss at iteration 9669: 0.042324737163240124\n",
      "Train Loss at iteration 9670: 0.04232467362590776\n",
      "Train Loss at iteration 9671: 0.04232461009915389\n",
      "Train Loss at iteration 9672: 0.04232454658297672\n",
      "Train Loss at iteration 9673: 0.042324483077374464\n",
      "Train Loss at iteration 9674: 0.04232441958234532\n",
      "Train Loss at iteration 9675: 0.04232435609788749\n",
      "Train Loss at iteration 9676: 0.0423242926239992\n",
      "Train Loss at iteration 9677: 0.04232422916067864\n",
      "Train Loss at iteration 9678: 0.04232416570792402\n",
      "Train Loss at iteration 9679: 0.04232410226573355\n",
      "Train Loss at iteration 9680: 0.04232403883410543\n",
      "Train Loss at iteration 9681: 0.04232397541303789\n",
      "Train Loss at iteration 9682: 0.04232391200252912\n",
      "Train Loss at iteration 9683: 0.04232384860257733\n",
      "Train Loss at iteration 9684: 0.04232378521318074\n",
      "Train Loss at iteration 9685: 0.042323721834337545\n",
      "Train Loss at iteration 9686: 0.04232365846604596\n",
      "Train Loss at iteration 9687: 0.04232359510830419\n",
      "Train Loss at iteration 9688: 0.042323531761110464\n",
      "Train Loss at iteration 9689: 0.042323468424462975\n",
      "Train Loss at iteration 9690: 0.04232340509835994\n",
      "Train Loss at iteration 9691: 0.04232334178279957\n",
      "Train Loss at iteration 9692: 0.04232327847778008\n",
      "Train Loss at iteration 9693: 0.042323215183299685\n",
      "Train Loss at iteration 9694: 0.04232315189935657\n",
      "Train Loss at iteration 9695: 0.04232308862594898\n",
      "Train Loss at iteration 9696: 0.042323025363075126\n",
      "Train Loss at iteration 9697: 0.0423229621107332\n",
      "Train Loss at iteration 9698: 0.04232289886892144\n",
      "Train Loss at iteration 9699: 0.042322835637638036\n",
      "Train Loss at iteration 9700: 0.04232277241688123\n",
      "Train Loss at iteration 9701: 0.042322709206649205\n",
      "Train Loss at iteration 9702: 0.0423226460069402\n",
      "Train Loss at iteration 9703: 0.042322582817752426\n",
      "Train Loss at iteration 9704: 0.04232251963908409\n",
      "Train Loss at iteration 9705: 0.04232245647093341\n",
      "Train Loss at iteration 9706: 0.042322393313298616\n",
      "Train Loss at iteration 9707: 0.04232233016617791\n",
      "Train Loss at iteration 9708: 0.04232226702956952\n",
      "Train Loss at iteration 9709: 0.04232220390347165\n",
      "Train Loss at iteration 9710: 0.042322140787882524\n",
      "Train Loss at iteration 9711: 0.04232207768280035\n",
      "Train Loss at iteration 9712: 0.04232201458822337\n",
      "Train Loss at iteration 9713: 0.04232195150414979\n",
      "Train Loss at iteration 9714: 0.04232188843057782\n",
      "Train Loss at iteration 9715: 0.0423218253675057\n",
      "Train Loss at iteration 9716: 0.042321762314931614\n",
      "Train Loss at iteration 9717: 0.04232169927285383\n",
      "Train Loss at iteration 9718: 0.04232163624127052\n",
      "Train Loss at iteration 9719: 0.042321573220179945\n",
      "Train Loss at iteration 9720: 0.042321510209580304\n",
      "Train Loss at iteration 9721: 0.04232144720946983\n",
      "Train Loss at iteration 9722: 0.04232138421984672\n",
      "Train Loss at iteration 9723: 0.04232132124070921\n",
      "Train Loss at iteration 9724: 0.042321258272055545\n",
      "Train Loss at iteration 9725: 0.04232119531388391\n",
      "Train Loss at iteration 9726: 0.04232113236619256\n",
      "Train Loss at iteration 9727: 0.04232106942897969\n",
      "Train Loss at iteration 9728: 0.04232100650224355\n",
      "Train Loss at iteration 9729: 0.042320943585982325\n",
      "Train Loss at iteration 9730: 0.0423208806801943\n",
      "Train Loss at iteration 9731: 0.04232081778487764\n",
      "Train Loss at iteration 9732: 0.0423207549000306\n",
      "Train Loss at iteration 9733: 0.042320692025651405\n",
      "Train Loss at iteration 9734: 0.04232062916173827\n",
      "Train Loss at iteration 9735: 0.042320566308289426\n",
      "Train Loss at iteration 9736: 0.04232050346530308\n",
      "Train Loss at iteration 9737: 0.042320440632777505\n",
      "Train Loss at iteration 9738: 0.04232037781071088\n",
      "Train Loss at iteration 9739: 0.042320314999101466\n",
      "Train Loss at iteration 9740: 0.04232025219794747\n",
      "Train Loss at iteration 9741: 0.04232018940724711\n",
      "Train Loss at iteration 9742: 0.042320126626998654\n",
      "Train Loss at iteration 9743: 0.042320063857200274\n",
      "Train Loss at iteration 9744: 0.042320001097850245\n",
      "Train Loss at iteration 9745: 0.04231993834894679\n",
      "Train Loss at iteration 9746: 0.04231987561048812\n",
      "Train Loss at iteration 9747: 0.042319812882472475\n",
      "Train Loss at iteration 9748: 0.04231975016489808\n",
      "Train Loss at iteration 9749: 0.042319687457763176\n",
      "Train Loss at iteration 9750: 0.04231962476106598\n",
      "Train Loss at iteration 9751: 0.04231956207480473\n",
      "Train Loss at iteration 9752: 0.04231949939897767\n",
      "Train Loss at iteration 9753: 0.042319436733583\n",
      "Train Loss at iteration 9754: 0.04231937407861898\n",
      "Train Loss at iteration 9755: 0.042319311434083816\n",
      "Train Loss at iteration 9756: 0.04231924879997576\n",
      "Train Loss at iteration 9757: 0.042319186176293065\n",
      "Train Loss at iteration 9758: 0.04231912356303392\n",
      "Train Loss at iteration 9759: 0.04231906096019658\n",
      "Train Loss at iteration 9760: 0.04231899836777929\n",
      "Train Loss at iteration 9761: 0.042318935785780255\n",
      "Train Loss at iteration 9762: 0.04231887321419773\n",
      "Train Loss at iteration 9763: 0.04231881065302994\n",
      "Train Loss at iteration 9764: 0.04231874810227513\n",
      "Train Loss at iteration 9765: 0.04231868556193153\n",
      "Train Loss at iteration 9766: 0.04231862303199739\n",
      "Train Loss at iteration 9767: 0.04231856051247092\n",
      "Train Loss at iteration 9768: 0.04231849800335037\n",
      "Train Loss at iteration 9769: 0.04231843550463398\n",
      "Train Loss at iteration 9770: 0.04231837301631997\n",
      "Train Loss at iteration 9771: 0.04231831053840661\n",
      "Train Loss at iteration 9772: 0.0423182480708921\n",
      "Train Loss at iteration 9773: 0.04231818561377471\n",
      "Train Loss at iteration 9774: 0.04231812316705265\n",
      "Train Loss at iteration 9775: 0.04231806073072417\n",
      "Train Loss at iteration 9776: 0.042317998304787524\n",
      "Train Loss at iteration 9777: 0.042317935889240925\n",
      "Train Loss at iteration 9778: 0.04231787348408264\n",
      "Train Loss at iteration 9779: 0.04231781108931088\n",
      "Train Loss at iteration 9780: 0.042317748704923906\n",
      "Train Loss at iteration 9781: 0.042317686330919944\n",
      "Train Loss at iteration 9782: 0.04231762396729727\n",
      "Train Loss at iteration 9783: 0.042317561614054075\n",
      "Train Loss at iteration 9784: 0.04231749927118863\n",
      "Train Loss at iteration 9785: 0.04231743693869916\n",
      "Train Loss at iteration 9786: 0.042317374616583935\n",
      "Train Loss at iteration 9787: 0.04231731230484116\n",
      "Train Loss at iteration 9788: 0.042317250003469115\n",
      "Train Loss at iteration 9789: 0.04231718771246602\n",
      "Train Loss at iteration 9790: 0.04231712543183012\n",
      "Train Loss at iteration 9791: 0.04231706316155966\n",
      "Train Loss at iteration 9792: 0.042317000901652885\n",
      "Train Loss at iteration 9793: 0.04231693865210805\n",
      "Train Loss at iteration 9794: 0.042316876412923386\n",
      "Train Loss at iteration 9795: 0.04231681418409714\n",
      "Train Loss at iteration 9796: 0.042316751965627564\n",
      "Train Loss at iteration 9797: 0.04231668975751289\n",
      "Train Loss at iteration 9798: 0.04231662755975138\n",
      "Train Loss at iteration 9799: 0.042316565372341276\n",
      "Train Loss at iteration 9800: 0.04231650319528082\n",
      "Train Loss at iteration 9801: 0.04231644102856826\n",
      "Train Loss at iteration 9802: 0.04231637887220184\n",
      "Train Loss at iteration 9803: 0.04231631672617982\n",
      "Train Loss at iteration 9804: 0.04231625459050044\n",
      "Train Loss at iteration 9805: 0.042316192465161946\n",
      "Train Loss at iteration 9806: 0.042316130350162566\n",
      "Train Loss at iteration 9807: 0.0423160682455006\n",
      "Train Loss at iteration 9808: 0.04231600615117425\n",
      "Train Loss at iteration 9809: 0.042315944067181796\n",
      "Train Loss at iteration 9810: 0.04231588199352145\n",
      "Train Loss at iteration 9811: 0.04231581993019151\n",
      "Train Loss at iteration 9812: 0.0423157578771902\n",
      "Train Loss at iteration 9813: 0.04231569583451576\n",
      "Train Loss at iteration 9814: 0.04231563380216647\n",
      "Train Loss at iteration 9815: 0.042315571780140546\n",
      "Train Loss at iteration 9816: 0.042315509768436274\n",
      "Train Loss at iteration 9817: 0.04231544776705189\n",
      "Train Loss at iteration 9818: 0.04231538577598565\n",
      "Train Loss at iteration 9819: 0.04231532379523581\n",
      "Train Loss at iteration 9820: 0.04231526182480061\n",
      "Train Loss at iteration 9821: 0.04231519986467829\n",
      "Train Loss at iteration 9822: 0.04231513791486715\n",
      "Train Loss at iteration 9823: 0.042315075975365414\n",
      "Train Loss at iteration 9824: 0.04231501404617133\n",
      "Train Loss at iteration 9825: 0.04231495212728317\n",
      "Train Loss at iteration 9826: 0.042314890218699185\n",
      "Train Loss at iteration 9827: 0.04231482832041762\n",
      "Train Loss at iteration 9828: 0.042314766432436736\n",
      "Train Loss at iteration 9829: 0.042314704554754785\n",
      "Train Loss at iteration 9830: 0.04231464268737003\n",
      "Train Loss at iteration 9831: 0.04231458083028073\n",
      "Train Loss at iteration 9832: 0.04231451898348512\n",
      "Train Loss at iteration 9833: 0.042314457146981496\n",
      "Train Loss at iteration 9834: 0.04231439532076808\n",
      "Train Loss at iteration 9835: 0.04231433350484314\n",
      "Train Loss at iteration 9836: 0.04231427169920495\n",
      "Train Loss at iteration 9837: 0.04231420990385174\n",
      "Train Loss at iteration 9838: 0.042314148118781786\n",
      "Train Loss at iteration 9839: 0.04231408634399335\n",
      "Train Loss at iteration 9840: 0.04231402457948469\n",
      "Train Loss at iteration 9841: 0.04231396282525404\n",
      "Train Loss at iteration 9842: 0.0423139010812997\n",
      "Train Loss at iteration 9843: 0.0423138393476199\n",
      "Train Loss at iteration 9844: 0.042313777624212914\n",
      "Train Loss at iteration 9845: 0.04231371591107701\n",
      "Train Loss at iteration 9846: 0.042313654208210406\n",
      "Train Loss at iteration 9847: 0.04231359251561142\n",
      "Train Loss at iteration 9848: 0.042313530833278296\n",
      "Train Loss at iteration 9849: 0.042313469161209274\n",
      "Train Loss at iteration 9850: 0.04231340749940265\n",
      "Train Loss at iteration 9851: 0.042313345847856655\n",
      "Train Loss at iteration 9852: 0.04231328420656957\n",
      "Train Loss at iteration 9853: 0.04231322257553965\n",
      "Train Loss at iteration 9854: 0.042313160954765165\n",
      "Train Loss at iteration 9855: 0.042313099344244376\n",
      "Train Loss at iteration 9856: 0.04231303774397555\n",
      "Train Loss at iteration 9857: 0.042312976153956953\n",
      "Train Loss at iteration 9858: 0.042312914574186844\n",
      "Train Loss at iteration 9859: 0.04231285300466349\n",
      "Train Loss at iteration 9860: 0.04231279144538515\n",
      "Train Loss at iteration 9861: 0.042312729896350106\n",
      "Train Loss at iteration 9862: 0.04231266835755661\n",
      "Train Loss at iteration 9863: 0.04231260682900294\n",
      "Train Loss at iteration 9864: 0.04231254531068735\n",
      "Train Loss at iteration 9865: 0.04231248380260811\n",
      "Train Loss at iteration 9866: 0.0423124223047635\n",
      "Train Loss at iteration 9867: 0.04231236081715177\n",
      "Train Loss at iteration 9868: 0.0423122993397712\n",
      "Train Loss at iteration 9869: 0.04231223787262005\n",
      "Train Loss at iteration 9870: 0.04231217641569659\n",
      "Train Loss at iteration 9871: 0.0423121149689991\n",
      "Train Loss at iteration 9872: 0.04231205353252584\n",
      "Train Loss at iteration 9873: 0.042311992106275086\n",
      "Train Loss at iteration 9874: 0.0423119306902451\n",
      "Train Loss at iteration 9875: 0.04231186928443415\n",
      "Train Loss at iteration 9876: 0.04231180788884051\n",
      "Train Loss at iteration 9877: 0.042311746503462457\n",
      "Train Loss at iteration 9878: 0.042311685128298256\n",
      "Train Loss at iteration 9879: 0.04231162376334618\n",
      "Train Loss at iteration 9880: 0.0423115624086045\n",
      "Train Loss at iteration 9881: 0.042311501064071486\n",
      "Train Loss at iteration 9882: 0.04231143972974542\n",
      "Train Loss at iteration 9883: 0.042311378405624565\n",
      "Train Loss at iteration 9884: 0.04231131709170719\n",
      "Train Loss at iteration 9885: 0.04231125578799159\n",
      "Train Loss at iteration 9886: 0.042311194494476005\n",
      "Train Loss at iteration 9887: 0.04231113321115873\n",
      "Train Loss at iteration 9888: 0.04231107193803804\n",
      "Train Loss at iteration 9889: 0.042311010675112205\n",
      "Train Loss at iteration 9890: 0.042310949422379514\n",
      "Train Loss at iteration 9891: 0.04231088817983822\n",
      "Train Loss at iteration 9892: 0.042310826947486606\n",
      "Train Loss at iteration 9893: 0.04231076572532294\n",
      "Train Loss at iteration 9894: 0.04231070451334553\n",
      "Train Loss at iteration 9895: 0.04231064331155261\n",
      "Train Loss at iteration 9896: 0.04231058211994249\n",
      "Train Loss at iteration 9897: 0.042310520938513424\n",
      "Train Loss at iteration 9898: 0.042310459767263706\n",
      "Train Loss at iteration 9899: 0.0423103986061916\n",
      "Train Loss at iteration 9900: 0.0423103374552954\n",
      "Train Loss at iteration 9901: 0.042310276314573375\n",
      "Train Loss at iteration 9902: 0.0423102151840238\n",
      "Train Loss at iteration 9903: 0.04231015406364495\n",
      "Train Loss at iteration 9904: 0.042310092953435116\n",
      "Train Loss at iteration 9905: 0.042310031853392584\n",
      "Train Loss at iteration 9906: 0.04230997076351562\n",
      "Train Loss at iteration 9907: 0.04230990968380251\n",
      "Train Loss at iteration 9908: 0.04230984861425152\n",
      "Train Loss at iteration 9909: 0.04230978755486095\n",
      "Train Loss at iteration 9910: 0.042309726505629065\n",
      "Train Loss at iteration 9911: 0.04230966546655416\n",
      "Train Loss at iteration 9912: 0.042309604437634526\n",
      "Train Loss at iteration 9913: 0.0423095434188684\n",
      "Train Loss at iteration 9914: 0.04230948241025412\n",
      "Train Loss at iteration 9915: 0.04230942141178994\n",
      "Train Loss at iteration 9916: 0.04230936042347415\n",
      "Train Loss at iteration 9917: 0.042309299445305024\n",
      "Train Loss at iteration 9918: 0.042309238477280846\n",
      "Train Loss at iteration 9919: 0.042309177519399915\n",
      "Train Loss at iteration 9920: 0.0423091165716605\n",
      "Train Loss at iteration 9921: 0.0423090556340609\n",
      "Train Loss at iteration 9922: 0.04230899470659938\n",
      "Train Loss at iteration 9923: 0.042308933789274236\n",
      "Train Loss at iteration 9924: 0.04230887288208376\n",
      "Train Loss at iteration 9925: 0.042308811985026236\n",
      "Train Loss at iteration 9926: 0.042308751098099937\n",
      "Train Loss at iteration 9927: 0.04230869022130315\n",
      "Train Loss at iteration 9928: 0.04230862935463418\n",
      "Train Loss at iteration 9929: 0.042308568498091295\n",
      "Train Loss at iteration 9930: 0.04230850765167281\n",
      "Train Loss at iteration 9931: 0.04230844681537696\n",
      "Train Loss at iteration 9932: 0.04230838598920209\n",
      "Train Loss at iteration 9933: 0.04230832517314647\n",
      "Train Loss at iteration 9934: 0.04230826436720837\n",
      "Train Loss at iteration 9935: 0.042308203571386094\n",
      "Train Loss at iteration 9936: 0.04230814278567793\n",
      "Train Loss at iteration 9937: 0.04230808201008216\n",
      "Train Loss at iteration 9938: 0.04230802124459709\n",
      "Train Loss at iteration 9939: 0.04230796048922099\n",
      "Train Loss at iteration 9940: 0.04230789974395217\n",
      "Train Loss at iteration 9941: 0.04230783900878891\n",
      "Train Loss at iteration 9942: 0.0423077782837295\n",
      "Train Loss at iteration 9943: 0.042307717568772225\n",
      "Train Loss at iteration 9944: 0.04230765686391539\n",
      "Train Loss at iteration 9945: 0.04230759616915729\n",
      "Train Loss at iteration 9946: 0.042307535484496187\n",
      "Train Loss at iteration 9947: 0.04230747480993042\n",
      "Train Loss at iteration 9948: 0.042307414145458244\n",
      "Train Loss at iteration 9949: 0.04230735349107796\n",
      "Train Loss at iteration 9950: 0.04230729284678788\n",
      "Train Loss at iteration 9951: 0.042307232212586275\n",
      "Train Loss at iteration 9952: 0.042307171588471464\n",
      "Train Loss at iteration 9953: 0.04230711097444171\n",
      "Train Loss at iteration 9954: 0.042307050370495335\n",
      "Train Loss at iteration 9955: 0.04230698977663061\n",
      "Train Loss at iteration 9956: 0.04230692919284586\n",
      "Train Loss at iteration 9957: 0.04230686861913934\n",
      "Train Loss at iteration 9958: 0.04230680805550939\n",
      "Train Loss at iteration 9959: 0.042306747501954275\n",
      "Train Loss at iteration 9960: 0.042306686958472305\n",
      "Train Loss at iteration 9961: 0.04230662642506177\n",
      "Train Loss at iteration 9962: 0.04230656590172097\n",
      "Train Loss at iteration 9963: 0.04230650538844822\n",
      "Train Loss at iteration 9964: 0.04230644488524179\n",
      "Train Loss at iteration 9965: 0.04230638439209999\n",
      "Train Loss at iteration 9966: 0.04230632390902111\n",
      "Train Loss at iteration 9967: 0.04230626343600346\n",
      "Train Loss at iteration 9968: 0.04230620297304535\n",
      "Train Loss at iteration 9969: 0.04230614252014505\n",
      "Train Loss at iteration 9970: 0.042306082077300876\n",
      "Train Loss at iteration 9971: 0.04230602164451113\n",
      "Train Loss at iteration 9972: 0.0423059612217741\n",
      "Train Loss at iteration 9973: 0.042305900809088096\n",
      "Train Loss at iteration 9974: 0.04230584040645144\n",
      "Train Loss at iteration 9975: 0.04230578001386238\n",
      "Train Loss at iteration 9976: 0.042305719631319266\n",
      "Train Loss at iteration 9977: 0.04230565925882037\n",
      "Train Loss at iteration 9978: 0.04230559889636401\n",
      "Train Loss at iteration 9979: 0.04230553854394848\n",
      "Train Loss at iteration 9980: 0.04230547820157209\n",
      "Train Loss at iteration 9981: 0.04230541786923315\n",
      "Train Loss at iteration 9982: 0.04230535754692994\n",
      "Train Loss at iteration 9983: 0.04230529723466079\n",
      "Train Loss at iteration 9984: 0.042305236932423974\n",
      "Train Loss at iteration 9985: 0.04230517664021782\n",
      "Train Loss at iteration 9986: 0.04230511635804062\n",
      "Train Loss at iteration 9987: 0.04230505608589069\n",
      "Train Loss at iteration 9988: 0.04230499582376633\n",
      "Train Loss at iteration 9989: 0.04230493557166584\n",
      "Train Loss at iteration 9990: 0.04230487532958754\n",
      "Train Loss at iteration 9991: 0.042304815097529716\n",
      "Train Loss at iteration 9992: 0.04230475487549069\n",
      "Train Loss at iteration 9993: 0.042304694663468756\n",
      "Train Loss at iteration 9994: 0.04230463446146223\n",
      "Train Loss at iteration 9995: 0.04230457426946942\n",
      "Train Loss at iteration 9996: 0.042304514087488615\n",
      "Train Loss at iteration 9997: 0.042304453915518146\n",
      "Train Loss at iteration 9998: 0.04230439375355633\n",
      "Train Loss at iteration 9999: 0.04230433360160144\n",
      "Train Loss at iteration 10000: 0.04230427345965181\n",
      "Train Loss at iteration 10001: 0.04230421332770576\n",
      "Train Loss at iteration 10002: 0.04230415320576155\n",
      "Train Loss at iteration 10003: 0.04230409309381754\n",
      "Train Loss at iteration 10004: 0.04230403299187201\n",
      "Train Loss at iteration 10005: 0.0423039728999233\n",
      "Train Loss at iteration 10006: 0.04230391281796969\n",
      "Train Loss at iteration 10007: 0.0423038527460095\n",
      "Train Loss at iteration 10008: 0.04230379268404105\n",
      "Train Loss at iteration 10009: 0.04230373263206265\n",
      "Train Loss at iteration 10010: 0.042303672590072595\n",
      "Train Loss at iteration 10011: 0.04230361255806922\n",
      "Train Loss at iteration 10012: 0.04230355253605082\n",
      "Train Loss at iteration 10013: 0.04230349252401573\n",
      "Train Loss at iteration 10014: 0.042303432521962225\n",
      "Train Loss at iteration 10015: 0.04230337252988865\n",
      "Train Loss at iteration 10016: 0.04230331254779332\n",
      "Train Loss at iteration 10017: 0.04230325257567453\n",
      "Train Loss at iteration 10018: 0.0423031926135306\n",
      "Train Loss at iteration 10019: 0.04230313266135985\n",
      "Train Loss at iteration 10020: 0.042303072719160596\n",
      "Train Loss at iteration 10021: 0.04230301278693115\n",
      "Train Loss at iteration 10022: 0.04230295286466981\n",
      "Train Loss at iteration 10023: 0.04230289295237493\n",
      "Train Loss at iteration 10024: 0.0423028330500448\n",
      "Train Loss at iteration 10025: 0.04230277315767772\n",
      "Train Loss at iteration 10026: 0.04230271327527205\n",
      "Train Loss at iteration 10027: 0.04230265340282607\n",
      "Train Loss at iteration 10028: 0.04230259354033812\n",
      "Train Loss at iteration 10029: 0.042302533687806496\n",
      "Train Loss at iteration 10030: 0.04230247384522954\n",
      "Train Loss at iteration 10031: 0.04230241401260556\n",
      "Train Loss at iteration 10032: 0.04230235418993287\n",
      "Train Loss at iteration 10033: 0.04230229437720979\n",
      "Train Loss at iteration 10034: 0.04230223457443463\n",
      "Train Loss at iteration 10035: 0.042302174781605736\n",
      "Train Loss at iteration 10036: 0.04230211499872142\n",
      "Train Loss at iteration 10037: 0.042302055225779976\n",
      "Train Loss at iteration 10038: 0.042301995462779754\n",
      "Train Loss at iteration 10039: 0.04230193570971905\n",
      "Train Loss at iteration 10040: 0.042301875966596215\n",
      "Train Loss at iteration 10041: 0.042301816233409534\n",
      "Train Loss at iteration 10042: 0.04230175651015736\n",
      "Train Loss at iteration 10043: 0.04230169679683799\n",
      "Train Loss at iteration 10044: 0.04230163709344977\n",
      "Train Loss at iteration 10045: 0.042301577399991006\n",
      "Train Loss at iteration 10046: 0.042301517716460034\n",
      "Train Loss at iteration 10047: 0.04230145804285515\n",
      "Train Loss at iteration 10048: 0.04230139837917471\n",
      "Train Loss at iteration 10049: 0.04230133872541701\n",
      "Train Loss at iteration 10050: 0.0423012790815804\n",
      "Train Loss at iteration 10051: 0.04230121944766318\n",
      "Train Loss at iteration 10052: 0.042301159823663684\n",
      "Train Loss at iteration 10053: 0.04230110020958025\n",
      "Train Loss at iteration 10054: 0.04230104060541118\n",
      "Train Loss at iteration 10055: 0.04230098101115481\n",
      "Train Loss at iteration 10056: 0.04230092142680946\n",
      "Train Loss at iteration 10057: 0.04230086185237347\n",
      "Train Loss at iteration 10058: 0.04230080228784516\n",
      "Train Loss at iteration 10059: 0.04230074273322286\n",
      "Train Loss at iteration 10060: 0.04230068318850489\n",
      "Train Loss at iteration 10061: 0.04230062365368957\n",
      "Train Loss at iteration 10062: 0.042300564128775246\n",
      "Train Loss at iteration 10063: 0.04230050461376022\n",
      "Train Loss at iteration 10064: 0.04230044510864286\n",
      "Train Loss at iteration 10065: 0.04230038561342144\n",
      "Train Loss at iteration 10066: 0.04230032612809434\n",
      "Train Loss at iteration 10067: 0.04230026665265987\n",
      "Train Loss at iteration 10068: 0.04230020718711635\n",
      "Train Loss at iteration 10069: 0.042300147731462116\n",
      "Train Loss at iteration 10070: 0.042300088285695495\n",
      "Train Loss at iteration 10071: 0.04230002884981484\n",
      "Train Loss at iteration 10072: 0.04229996942381843\n",
      "Train Loss at iteration 10073: 0.04229991000770466\n",
      "Train Loss at iteration 10074: 0.04229985060147181\n",
      "Train Loss at iteration 10075: 0.04229979120511824\n",
      "Train Loss at iteration 10076: 0.042299731818642265\n",
      "Train Loss at iteration 10077: 0.04229967244204223\n",
      "Train Loss at iteration 10078: 0.042299613075316454\n",
      "Train Loss at iteration 10079: 0.04229955371846329\n",
      "Train Loss at iteration 10080: 0.04229949437148104\n",
      "Train Loss at iteration 10081: 0.04229943503436807\n",
      "Train Loss at iteration 10082: 0.0422993757071227\n",
      "Train Loss at iteration 10083: 0.042299316389743255\n",
      "Train Loss at iteration 10084: 0.04229925708222808\n",
      "Train Loss at iteration 10085: 0.04229919778457549\n",
      "Train Loss at iteration 10086: 0.04229913849678386\n",
      "Train Loss at iteration 10087: 0.04229907921885148\n",
      "Train Loss at iteration 10088: 0.04229901995077671\n",
      "Train Loss at iteration 10089: 0.04229896069255788\n",
      "Train Loss at iteration 10090: 0.042298901444193335\n",
      "Train Loss at iteration 10091: 0.0422988422056814\n",
      "Train Loss at iteration 10092: 0.0422987829770204\n",
      "Train Loss at iteration 10093: 0.042298723758208714\n",
      "Train Loss at iteration 10094: 0.04229866454924463\n",
      "Train Loss at iteration 10095: 0.04229860535012651\n",
      "Train Loss at iteration 10096: 0.042298546160852685\n",
      "Train Loss at iteration 10097: 0.0422984869814215\n",
      "Train Loss at iteration 10098: 0.042298427811831296\n",
      "Train Loss at iteration 10099: 0.0422983686520804\n",
      "Train Loss at iteration 10100: 0.04229830950216714\n",
      "Train Loss at iteration 10101: 0.04229825036208989\n",
      "Train Loss at iteration 10102: 0.042298191231846964\n",
      "Train Loss at iteration 10103: 0.0422981321114367\n",
      "Train Loss at iteration 10104: 0.04229807300085746\n",
      "Train Loss at iteration 10105: 0.04229801390010756\n",
      "Train Loss at iteration 10106: 0.04229795480918536\n",
      "Train Loss at iteration 10107: 0.04229789572808919\n",
      "Train Loss at iteration 10108: 0.04229783665681738\n",
      "Train Loss at iteration 10109: 0.04229777759536829\n",
      "Train Loss at iteration 10110: 0.04229771854374026\n",
      "Train Loss at iteration 10111: 0.04229765950193162\n",
      "Train Loss at iteration 10112: 0.04229760046994072\n",
      "Train Loss at iteration 10113: 0.042297541447765936\n",
      "Train Loss at iteration 10114: 0.04229748243540554\n",
      "Train Loss at iteration 10115: 0.042297423432857924\n",
      "Train Loss at iteration 10116: 0.042297364440121415\n",
      "Train Loss at iteration 10117: 0.04229730545719437\n",
      "Train Loss at iteration 10118: 0.042297246484075124\n",
      "Train Loss at iteration 10119: 0.04229718752076203\n",
      "Train Loss at iteration 10120: 0.04229712856725343\n",
      "Train Loss at iteration 10121: 0.042297069623547644\n",
      "Train Loss at iteration 10122: 0.04229701068964306\n",
      "Train Loss at iteration 10123: 0.04229695176553799\n",
      "Train Loss at iteration 10124: 0.0422968928512308\n",
      "Train Loss at iteration 10125: 0.04229683394671981\n",
      "Train Loss at iteration 10126: 0.04229677505200342\n",
      "Train Loss at iteration 10127: 0.042296716167079904\n",
      "Train Loss at iteration 10128: 0.04229665729194766\n",
      "Train Loss at iteration 10129: 0.042296598426605034\n",
      "Train Loss at iteration 10130: 0.042296539571050354\n",
      "Train Loss at iteration 10131: 0.04229648072528198\n",
      "Train Loss at iteration 10132: 0.04229642188929824\n",
      "Train Loss at iteration 10133: 0.04229636306309751\n",
      "Train Loss at iteration 10134: 0.042296304246678136\n",
      "Train Loss at iteration 10135: 0.042296245440038446\n",
      "Train Loss at iteration 10136: 0.0422961866431768\n",
      "Train Loss at iteration 10137: 0.04229612785609157\n",
      "Train Loss at iteration 10138: 0.04229606907878107\n",
      "Train Loss at iteration 10139: 0.04229601031124368\n",
      "Train Loss at iteration 10140: 0.04229595155347773\n",
      "Train Loss at iteration 10141: 0.04229589280548157\n",
      "Train Loss at iteration 10142: 0.04229583406725356\n",
      "Train Loss at iteration 10143: 0.04229577533879206\n",
      "Train Loss at iteration 10144: 0.04229571662009542\n",
      "Train Loss at iteration 10145: 0.042295657911161966\n",
      "Train Loss at iteration 10146: 0.04229559921199008\n",
      "Train Loss at iteration 10147: 0.0422955405225781\n",
      "Train Loss at iteration 10148: 0.042295481842924386\n",
      "Train Loss at iteration 10149: 0.0422954231730273\n",
      "Train Loss at iteration 10150: 0.04229536451288516\n",
      "Train Loss at iteration 10151: 0.04229530586249636\n",
      "Train Loss at iteration 10152: 0.04229524722185924\n",
      "Train Loss at iteration 10153: 0.04229518859097214\n",
      "Train Loss at iteration 10154: 0.042295129969833443\n",
      "Train Loss at iteration 10155: 0.042295071358441474\n",
      "Train Loss at iteration 10156: 0.04229501275679463\n",
      "Train Loss at iteration 10157: 0.04229495416489121\n",
      "Train Loss at iteration 10158: 0.04229489558272961\n",
      "Train Loss at iteration 10159: 0.04229483701030817\n",
      "Train Loss at iteration 10160: 0.04229477844762526\n",
      "Train Loss at iteration 10161: 0.042294719894679235\n",
      "Train Loss at iteration 10162: 0.042294661351468435\n",
      "Train Loss at iteration 10163: 0.04229460281799123\n",
      "Train Loss at iteration 10164: 0.042294544294245985\n",
      "Train Loss at iteration 10165: 0.042294485780231046\n",
      "Train Loss at iteration 10166: 0.042294427275944776\n",
      "Train Loss at iteration 10167: 0.04229436878138553\n",
      "Train Loss at iteration 10168: 0.04229431029655166\n",
      "Train Loss at iteration 10169: 0.04229425182144156\n",
      "Train Loss at iteration 10170: 0.04229419335605354\n",
      "Train Loss at iteration 10171: 0.04229413490038599\n",
      "Train Loss at iteration 10172: 0.04229407645443727\n",
      "Train Loss at iteration 10173: 0.04229401801820573\n",
      "Train Loss at iteration 10174: 0.04229395959168974\n",
      "Train Loss at iteration 10175: 0.04229390117488765\n",
      "Train Loss at iteration 10176: 0.042293842767797823\n",
      "Train Loss at iteration 10177: 0.04229378437041864\n",
      "Train Loss at iteration 10178: 0.04229372598274844\n",
      "Train Loss at iteration 10179: 0.0422936676047856\n",
      "Train Loss at iteration 10180: 0.04229360923652846\n",
      "Train Loss at iteration 10181: 0.04229355087797541\n",
      "Train Loss at iteration 10182: 0.042293492529124774\n",
      "Train Loss at iteration 10183: 0.04229343418997497\n",
      "Train Loss at iteration 10184: 0.04229337586052433\n",
      "Train Loss at iteration 10185: 0.04229331754077122\n",
      "Train Loss at iteration 10186: 0.04229325923071401\n",
      "Train Loss at iteration 10187: 0.042293200930351056\n",
      "Train Loss at iteration 10188: 0.04229314263968072\n",
      "Train Loss at iteration 10189: 0.04229308435870138\n",
      "Train Loss at iteration 10190: 0.0422930260874114\n",
      "Train Loss at iteration 10191: 0.04229296782580913\n",
      "Train Loss at iteration 10192: 0.04229290957389296\n",
      "Train Loss at iteration 10193: 0.04229285133166123\n",
      "Train Loss at iteration 10194: 0.04229279309911233\n",
      "Train Loss at iteration 10195: 0.04229273487624462\n",
      "Train Loss at iteration 10196: 0.04229267666305645\n",
      "Train Loss at iteration 10197: 0.042292618459546204\n",
      "Train Loss at iteration 10198: 0.04229256026571225\n",
      "Train Loss at iteration 10199: 0.04229250208155296\n",
      "Train Loss at iteration 10200: 0.04229244390706668\n",
      "Train Loss at iteration 10201: 0.0422923857422518\n",
      "Train Loss at iteration 10202: 0.04229232758710669\n",
      "Train Loss at iteration 10203: 0.04229226944162969\n",
      "Train Loss at iteration 10204: 0.04229221130581921\n",
      "Train Loss at iteration 10205: 0.0422921531796736\n",
      "Train Loss at iteration 10206: 0.042292095063191225\n",
      "Train Loss at iteration 10207: 0.04229203695637045\n",
      "Train Loss at iteration 10208: 0.04229197885920966\n",
      "Train Loss at iteration 10209: 0.042291920771707237\n",
      "Train Loss at iteration 10210: 0.04229186269386153\n",
      "Train Loss at iteration 10211: 0.04229180462567092\n",
      "Train Loss at iteration 10212: 0.04229174656713376\n",
      "Train Loss at iteration 10213: 0.04229168851824846\n",
      "Train Loss at iteration 10214: 0.04229163047901336\n",
      "Train Loss at iteration 10215: 0.04229157244942684\n",
      "Train Loss at iteration 10216: 0.042291514429487295\n",
      "Train Loss at iteration 10217: 0.04229145641919306\n",
      "Train Loss at iteration 10218: 0.04229139841854254\n",
      "Train Loss at iteration 10219: 0.04229134042753409\n",
      "Train Loss at iteration 10220: 0.04229128244616609\n",
      "Train Loss at iteration 10221: 0.042291224474436916\n",
      "Train Loss at iteration 10222: 0.04229116651234494\n",
      "Train Loss at iteration 10223: 0.042291108559888546\n",
      "Train Loss at iteration 10224: 0.0422910506170661\n",
      "Train Loss at iteration 10225: 0.04229099268387597\n",
      "Train Loss at iteration 10226: 0.042290934760316555\n",
      "Train Loss at iteration 10227: 0.0422908768463862\n",
      "Train Loss at iteration 10228: 0.042290818942083315\n",
      "Train Loss at iteration 10229: 0.042290761047406256\n",
      "Train Loss at iteration 10230: 0.0422907031623534\n",
      "Train Loss at iteration 10231: 0.04229064528692314\n",
      "Train Loss at iteration 10232: 0.04229058742111383\n",
      "Train Loss at iteration 10233: 0.04229052956492386\n",
      "Train Loss at iteration 10234: 0.04229047171835161\n",
      "Train Loss at iteration 10235: 0.04229041388139547\n",
      "Train Loss at iteration 10236: 0.04229035605405378\n",
      "Train Loss at iteration 10237: 0.042290298236324964\n",
      "Train Loss at iteration 10238: 0.04229024042820737\n",
      "Train Loss at iteration 10239: 0.0422901826296994\n",
      "Train Loss at iteration 10240: 0.04229012484079942\n",
      "Train Loss at iteration 10241: 0.0422900670615058\n",
      "Train Loss at iteration 10242: 0.04229000929181695\n",
      "Train Loss at iteration 10243: 0.04228995153173123\n",
      "Train Loss at iteration 10244: 0.042289893781247014\n",
      "Train Loss at iteration 10245: 0.042289836040362704\n",
      "Train Loss at iteration 10246: 0.042289778309076674\n",
      "Train Loss at iteration 10247: 0.042289720587387294\n",
      "Train Loss at iteration 10248: 0.04228966287529297\n",
      "Train Loss at iteration 10249: 0.042289605172792066\n",
      "Train Loss at iteration 10250: 0.042289547479882963\n",
      "Train Loss at iteration 10251: 0.04228948979656406\n",
      "Train Loss at iteration 10252: 0.04228943212283372\n",
      "Train Loss at iteration 10253: 0.04228937445869034\n",
      "Train Loss at iteration 10254: 0.042289316804132296\n",
      "Train Loss at iteration 10255: 0.042289259159157974\n",
      "Train Loss at iteration 10256: 0.04228920152376578\n",
      "Train Loss at iteration 10257: 0.04228914389795406\n",
      "Train Loss at iteration 10258: 0.04228908628172125\n",
      "Train Loss at iteration 10259: 0.04228902867506567\n",
      "Train Loss at iteration 10260: 0.04228897107798576\n",
      "Train Loss at iteration 10261: 0.04228891349047988\n",
      "Train Loss at iteration 10262: 0.04228885591254642\n",
      "Train Loss at iteration 10263: 0.042288798344183774\n",
      "Train Loss at iteration 10264: 0.04228874078539033\n",
      "Train Loss at iteration 10265: 0.04228868323616446\n",
      "Train Loss at iteration 10266: 0.04228862569650456\n",
      "Train Loss at iteration 10267: 0.04228856816640902\n",
      "Train Loss at iteration 10268: 0.04228851064587621\n",
      "Train Loss at iteration 10269: 0.042288453134904556\n",
      "Train Loss at iteration 10270: 0.04228839563349241\n",
      "Train Loss at iteration 10271: 0.04228833814163818\n",
      "Train Loss at iteration 10272: 0.04228828065934026\n",
      "Train Loss at iteration 10273: 0.04228822318659701\n",
      "Train Loss at iteration 10274: 0.042288165723406844\n",
      "Train Loss at iteration 10275: 0.042288108269768164\n",
      "Train Loss at iteration 10276: 0.04228805082567931\n",
      "Train Loss at iteration 10277: 0.04228799339113875\n",
      "Train Loss at iteration 10278: 0.0422879359661448\n",
      "Train Loss at iteration 10279: 0.042287878550695886\n",
      "Train Loss at iteration 10280: 0.04228782114479039\n",
      "Train Loss at iteration 10281: 0.042287763748426715\n",
      "Train Loss at iteration 10282: 0.04228770636160325\n",
      "Train Loss at iteration 10283: 0.042287648984318374\n",
      "Train Loss at iteration 10284: 0.0422875916165705\n",
      "Train Loss at iteration 10285: 0.04228753425835801\n",
      "Train Loss at iteration 10286: 0.04228747690967928\n",
      "Train Loss at iteration 10287: 0.04228741957053273\n",
      "Train Loss at iteration 10288: 0.04228736224091675\n",
      "Train Loss at iteration 10289: 0.04228730492082972\n",
      "Train Loss at iteration 10290: 0.04228724761027004\n",
      "Train Loss at iteration 10291: 0.042287190309236125\n",
      "Train Loss at iteration 10292: 0.04228713301772633\n",
      "Train Loss at iteration 10293: 0.04228707573573908\n",
      "Train Loss at iteration 10294: 0.042287018463272756\n",
      "Train Loss at iteration 10295: 0.04228696120032576\n",
      "Train Loss at iteration 10296: 0.04228690394689649\n",
      "Train Loss at iteration 10297: 0.04228684670298334\n",
      "Train Loss at iteration 10298: 0.04228678946858471\n",
      "Train Loss at iteration 10299: 0.042286732243698985\n",
      "Train Loss at iteration 10300: 0.04228667502832457\n",
      "Train Loss at iteration 10301: 0.04228661782245986\n",
      "Train Loss at iteration 10302: 0.04228656062610327\n",
      "Train Loss at iteration 10303: 0.042286503439253165\n",
      "Train Loss at iteration 10304: 0.042286446261907966\n",
      "Train Loss at iteration 10305: 0.04228638909406607\n",
      "Train Loss at iteration 10306: 0.042286331935725875\n",
      "Train Loss at iteration 10307: 0.042286274786885764\n",
      "Train Loss at iteration 10308: 0.04228621764754415\n",
      "Train Loss at iteration 10309: 0.04228616051769945\n",
      "Train Loss at iteration 10310: 0.042286103397350026\n",
      "Train Loss at iteration 10311: 0.04228604628649431\n",
      "Train Loss at iteration 10312: 0.042285989185130685\n",
      "Train Loss at iteration 10313: 0.04228593209325755\n",
      "Train Loss at iteration 10314: 0.04228587501087333\n",
      "Train Loss at iteration 10315: 0.0422858179379764\n",
      "Train Loss at iteration 10316: 0.04228576087456517\n",
      "Train Loss at iteration 10317: 0.04228570382063805\n",
      "Train Loss at iteration 10318: 0.04228564677619343\n",
      "Train Loss at iteration 10319: 0.04228558974122972\n",
      "Train Loss at iteration 10320: 0.04228553271574532\n",
      "Train Loss at iteration 10321: 0.04228547569973864\n",
      "Train Loss at iteration 10322: 0.04228541869320806\n",
      "Train Loss at iteration 10323: 0.04228536169615202\n",
      "Train Loss at iteration 10324: 0.042285304708568894\n",
      "Train Loss at iteration 10325: 0.042285247730457096\n",
      "Train Loss at iteration 10326: 0.04228519076181503\n",
      "Train Loss at iteration 10327: 0.0422851338026411\n",
      "Train Loss at iteration 10328: 0.042285076852933715\n",
      "Train Loss at iteration 10329: 0.042285019912691284\n",
      "Train Loss at iteration 10330: 0.04228496298191221\n",
      "Train Loss at iteration 10331: 0.04228490606059489\n",
      "Train Loss at iteration 10332: 0.04228484914873772\n",
      "Train Loss at iteration 10333: 0.04228479224633914\n",
      "Train Loss at iteration 10334: 0.04228473535339754\n",
      "Train Loss at iteration 10335: 0.042284678469911324\n",
      "Train Loss at iteration 10336: 0.04228462159587888\n",
      "Train Loss at iteration 10337: 0.042284564731298664\n",
      "Train Loss at iteration 10338: 0.04228450787616905\n",
      "Train Loss at iteration 10339: 0.04228445103048844\n",
      "Train Loss at iteration 10340: 0.04228439419425527\n",
      "Train Loss at iteration 10341: 0.04228433736746792\n",
      "Train Loss at iteration 10342: 0.042284280550124814\n",
      "Train Loss at iteration 10343: 0.04228422374222437\n",
      "Train Loss at iteration 10344: 0.04228416694376498\n",
      "Train Loss at iteration 10345: 0.042284110154745064\n",
      "Train Loss at iteration 10346: 0.04228405337516303\n",
      "Train Loss at iteration 10347: 0.042283996605017284\n",
      "Train Loss at iteration 10348: 0.042283939844306236\n",
      "Train Loss at iteration 10349: 0.04228388309302831\n",
      "Train Loss at iteration 10350: 0.04228382635118192\n",
      "Train Loss at iteration 10351: 0.04228376961876544\n",
      "Train Loss at iteration 10352: 0.04228371289577732\n",
      "Train Loss at iteration 10353: 0.04228365618221597\n",
      "Train Loss at iteration 10354: 0.04228359947807978\n",
      "Train Loss at iteration 10355: 0.04228354278336718\n",
      "Train Loss at iteration 10356: 0.04228348609807657\n",
      "Train Loss at iteration 10357: 0.042283429422206385\n",
      "Train Loss at iteration 10358: 0.042283372755755014\n",
      "Train Loss at iteration 10359: 0.04228331609872088\n",
      "Train Loss at iteration 10360: 0.04228325945110241\n",
      "Train Loss at iteration 10361: 0.042283202812897984\n",
      "Train Loss at iteration 10362: 0.042283146184106064\n",
      "Train Loss at iteration 10363: 0.042283089564725033\n",
      "Train Loss at iteration 10364: 0.0422830329547533\n",
      "Train Loss at iteration 10365: 0.0422829763541893\n",
      "Train Loss at iteration 10366: 0.04228291976303145\n",
      "Train Loss at iteration 10367: 0.042282863181278156\n",
      "Train Loss at iteration 10368: 0.04228280660892783\n",
      "Train Loss at iteration 10369: 0.0422827500459789\n",
      "Train Loss at iteration 10370: 0.04228269349242979\n",
      "Train Loss at iteration 10371: 0.04228263694827889\n",
      "Train Loss at iteration 10372: 0.04228258041352464\n",
      "Train Loss at iteration 10373: 0.042282523888165446\n",
      "Train Loss at iteration 10374: 0.04228246737219974\n",
      "Train Loss at iteration 10375: 0.04228241086562592\n",
      "Train Loss at iteration 10376: 0.04228235436844241\n",
      "Train Loss at iteration 10377: 0.04228229788064764\n",
      "Train Loss at iteration 10378: 0.04228224140224003\n",
      "Train Loss at iteration 10379: 0.04228218493321799\n",
      "Train Loss at iteration 10380: 0.04228212847357994\n",
      "Train Loss at iteration 10381: 0.04228207202332431\n",
      "Train Loss at iteration 10382: 0.042282015582449504\n",
      "Train Loss at iteration 10383: 0.04228195915095395\n",
      "Train Loss at iteration 10384: 0.04228190272883607\n",
      "Train Loss at iteration 10385: 0.042281846316094295\n",
      "Train Loss at iteration 10386: 0.042281789912727026\n",
      "Train Loss at iteration 10387: 0.0422817335187327\n",
      "Train Loss at iteration 10388: 0.04228167713410973\n",
      "Train Loss at iteration 10389: 0.04228162075885654\n",
      "Train Loss at iteration 10390: 0.04228156439297155\n",
      "Train Loss at iteration 10391: 0.04228150803645319\n",
      "Train Loss at iteration 10392: 0.04228145168929989\n",
      "Train Loss at iteration 10393: 0.04228139535151007\n",
      "Train Loss at iteration 10394: 0.04228133902308213\n",
      "Train Loss at iteration 10395: 0.04228128270401451\n",
      "Train Loss at iteration 10396: 0.042281226394305635\n",
      "Train Loss at iteration 10397: 0.04228117009395395\n",
      "Train Loss at iteration 10398: 0.04228111380295784\n",
      "Train Loss at iteration 10399: 0.042281057521315744\n",
      "Train Loss at iteration 10400: 0.0422810012490261\n",
      "Train Loss at iteration 10401: 0.04228094498608734\n",
      "Train Loss at iteration 10402: 0.04228088873249786\n",
      "Train Loss at iteration 10403: 0.04228083248825609\n",
      "Train Loss at iteration 10404: 0.04228077625336049\n",
      "Train Loss at iteration 10405: 0.042280720027809464\n",
      "Train Loss at iteration 10406: 0.04228066381160142\n",
      "Train Loss at iteration 10407: 0.04228060760473483\n",
      "Train Loss at iteration 10408: 0.04228055140720808\n",
      "Train Loss at iteration 10409: 0.042280495219019616\n",
      "Train Loss at iteration 10410: 0.042280439040167865\n",
      "Train Loss at iteration 10411: 0.042280382870651265\n",
      "Train Loss at iteration 10412: 0.04228032671046822\n",
      "Train Loss at iteration 10413: 0.042280270559617174\n",
      "Train Loss at iteration 10414: 0.04228021441809657\n",
      "Train Loss at iteration 10415: 0.042280158285904805\n",
      "Train Loss at iteration 10416: 0.04228010216304034\n",
      "Train Loss at iteration 10417: 0.04228004604950157\n",
      "Train Loss at iteration 10418: 0.04227998994528698\n",
      "Train Loss at iteration 10419: 0.04227993385039494\n",
      "Train Loss at iteration 10420: 0.04227987776482392\n",
      "Train Loss at iteration 10421: 0.04227982168857234\n",
      "Train Loss at iteration 10422: 0.042279765621638626\n",
      "Train Loss at iteration 10423: 0.04227970956402121\n",
      "Train Loss at iteration 10424: 0.04227965351571854\n",
      "Train Loss at iteration 10425: 0.04227959747672902\n",
      "Train Loss at iteration 10426: 0.0422795414470511\n",
      "Train Loss at iteration 10427: 0.04227948542668322\n",
      "Train Loss at iteration 10428: 0.04227942941562381\n",
      "Train Loss at iteration 10429: 0.042279373413871287\n",
      "Train Loss at iteration 10430: 0.0422793174214241\n",
      "Train Loss at iteration 10431: 0.04227926143828068\n",
      "Train Loss at iteration 10432: 0.04227920546443945\n",
      "Train Loss at iteration 10433: 0.04227914949989887\n",
      "Train Loss at iteration 10434: 0.04227909354465735\n",
      "Train Loss at iteration 10435: 0.04227903759871333\n",
      "Train Loss at iteration 10436: 0.04227898166206525\n",
      "Train Loss at iteration 10437: 0.04227892573471155\n",
      "Train Loss at iteration 10438: 0.04227886981665067\n",
      "Train Loss at iteration 10439: 0.04227881390788101\n",
      "Train Loss at iteration 10440: 0.04227875800840104\n",
      "Train Loss at iteration 10441: 0.04227870211820921\n",
      "Train Loss at iteration 10442: 0.04227864623730391\n",
      "Train Loss at iteration 10443: 0.04227859036568361\n",
      "Train Loss at iteration 10444: 0.04227853450334675\n",
      "Train Loss at iteration 10445: 0.04227847865029176\n",
      "Train Loss at iteration 10446: 0.04227842280651707\n",
      "Train Loss at iteration 10447: 0.04227836697202112\n",
      "Train Loss at iteration 10448: 0.04227831114680237\n",
      "Train Loss at iteration 10449: 0.04227825533085923\n",
      "Train Loss at iteration 10450: 0.04227819952419015\n",
      "Train Loss at iteration 10451: 0.04227814372679359\n",
      "Train Loss at iteration 10452: 0.04227808793866794\n",
      "Train Loss at iteration 10453: 0.0422780321598117\n",
      "Train Loss at iteration 10454: 0.04227797639022326\n",
      "Train Loss at iteration 10455: 0.04227792062990109\n",
      "Train Loss at iteration 10456: 0.04227786487884362\n",
      "Train Loss at iteration 10457: 0.042277809137049295\n",
      "Train Loss at iteration 10458: 0.042277753404516547\n",
      "Train Loss at iteration 10459: 0.042277697681243835\n",
      "Train Loss at iteration 10460: 0.0422776419672296\n",
      "Train Loss at iteration 10461: 0.04227758626247226\n",
      "Train Loss at iteration 10462: 0.04227753056697027\n",
      "Train Loss at iteration 10463: 0.04227747488072208\n",
      "Train Loss at iteration 10464: 0.04227741920372614\n",
      "Train Loss at iteration 10465: 0.04227736353598087\n",
      "Train Loss at iteration 10466: 0.04227730787748472\n",
      "Train Loss at iteration 10467: 0.04227725222823615\n",
      "Train Loss at iteration 10468: 0.04227719658823358\n",
      "Train Loss at iteration 10469: 0.04227714095747548\n",
      "Train Loss at iteration 10470: 0.04227708533596028\n",
      "Train Loss at iteration 10471: 0.04227702972368644\n",
      "Train Loss at iteration 10472: 0.04227697412065236\n",
      "Train Loss at iteration 10473: 0.04227691852685655\n",
      "Train Loss at iteration 10474: 0.042276862942297405\n",
      "Train Loss at iteration 10475: 0.04227680736697339\n",
      "Train Loss at iteration 10476: 0.04227675180088296\n",
      "Train Loss at iteration 10477: 0.04227669624402455\n",
      "Train Loss at iteration 10478: 0.04227664069639659\n",
      "Train Loss at iteration 10479: 0.04227658515799758\n",
      "Train Loss at iteration 10480: 0.0422765296288259\n",
      "Train Loss at iteration 10481: 0.04227647410888006\n",
      "Train Loss at iteration 10482: 0.04227641859815846\n",
      "Train Loss at iteration 10483: 0.042276363096659574\n",
      "Train Loss at iteration 10484: 0.04227630760438185\n",
      "Train Loss at iteration 10485: 0.04227625212132373\n",
      "Train Loss at iteration 10486: 0.04227619664748365\n",
      "Train Loss at iteration 10487: 0.04227614118286008\n",
      "Train Loss at iteration 10488: 0.04227608572745147\n",
      "Train Loss at iteration 10489: 0.042276030281256254\n",
      "Train Loss at iteration 10490: 0.0422759748442729\n",
      "Train Loss at iteration 10491: 0.04227591941649984\n",
      "Train Loss at iteration 10492: 0.04227586399793554\n",
      "Train Loss at iteration 10493: 0.04227580858857845\n",
      "Train Loss at iteration 10494: 0.042275753188426995\n",
      "Train Loss at iteration 10495: 0.04227569779747966\n",
      "Train Loss at iteration 10496: 0.04227564241573489\n",
      "Train Loss at iteration 10497: 0.04227558704319112\n",
      "Train Loss at iteration 10498: 0.04227553167984683\n",
      "Train Loss at iteration 10499: 0.04227547632570044\n",
      "Train Loss at iteration 10500: 0.04227542098075042\n",
      "Train Loss at iteration 10501: 0.04227536564499524\n",
      "Train Loss at iteration 10502: 0.04227531031843332\n",
      "Train Loss at iteration 10503: 0.042275255001063126\n",
      "Train Loss at iteration 10504: 0.04227519969288312\n",
      "Train Loss at iteration 10505: 0.04227514439389176\n",
      "Train Loss at iteration 10506: 0.04227508910408749\n",
      "Train Loss at iteration 10507: 0.04227503382346875\n",
      "Train Loss at iteration 10508: 0.04227497855203403\n",
      "Train Loss at iteration 10509: 0.042274923289781746\n",
      "Train Loss at iteration 10510: 0.0422748680367104\n",
      "Train Loss at iteration 10511: 0.04227481279281841\n",
      "Train Loss at iteration 10512: 0.042274757558104246\n",
      "Train Loss at iteration 10513: 0.04227470233256636\n",
      "Train Loss at iteration 10514: 0.042274647116203226\n",
      "Train Loss at iteration 10515: 0.042274591909013275\n",
      "Train Loss at iteration 10516: 0.04227453671099497\n",
      "Train Loss at iteration 10517: 0.04227448152214678\n",
      "Train Loss at iteration 10518: 0.04227442634246716\n",
      "Train Loss at iteration 10519: 0.042274371171954574\n",
      "Train Loss at iteration 10520: 0.04227431601060746\n",
      "Train Loss at iteration 10521: 0.0422742608584243\n",
      "Train Loss at iteration 10522: 0.04227420571540353\n",
      "Train Loss at iteration 10523: 0.04227415058154362\n",
      "Train Loss at iteration 10524: 0.04227409545684302\n",
      "Train Loss at iteration 10525: 0.042274040341300216\n",
      "Train Loss at iteration 10526: 0.042273985234913644\n",
      "Train Loss at iteration 10527: 0.04227393013768178\n",
      "Train Loss at iteration 10528: 0.04227387504960306\n",
      "Train Loss at iteration 10529: 0.042273819970675974\n",
      "Train Loss at iteration 10530: 0.042273764900898965\n",
      "Train Loss at iteration 10531: 0.04227370984027051\n",
      "Train Loss at iteration 10532: 0.04227365478878905\n",
      "Train Loss at iteration 10533: 0.04227359974645305\n",
      "Train Loss at iteration 10534: 0.04227354471326099\n",
      "Train Loss at iteration 10535: 0.04227348968921132\n",
      "Train Loss at iteration 10536: 0.042273434674302494\n",
      "Train Loss at iteration 10537: 0.042273379668533\n",
      "Train Loss at iteration 10538: 0.04227332467190127\n",
      "Train Loss at iteration 10539: 0.04227326968440579\n",
      "Train Loss at iteration 10540: 0.04227321470604502\n",
      "Train Loss at iteration 10541: 0.04227315973681742\n",
      "Train Loss at iteration 10542: 0.04227310477672146\n",
      "Train Loss at iteration 10543: 0.04227304982575559\n",
      "Train Loss at iteration 10544: 0.04227299488391829\n",
      "Train Loss at iteration 10545: 0.04227293995120802\n",
      "Train Loss at iteration 10546: 0.04227288502762325\n",
      "Train Loss at iteration 10547: 0.04227283011316243\n",
      "Train Loss at iteration 10548: 0.042272775207824034\n",
      "Train Loss at iteration 10549: 0.042272720311606546\n",
      "Train Loss at iteration 10550: 0.04227266542450841\n",
      "Train Loss at iteration 10551: 0.0422726105465281\n",
      "Train Loss at iteration 10552: 0.04227255567766408\n",
      "Train Loss at iteration 10553: 0.04227250081791483\n",
      "Train Loss at iteration 10554: 0.0422724459672788\n",
      "Train Loss at iteration 10555: 0.04227239112575447\n",
      "Train Loss at iteration 10556: 0.0422723362933403\n",
      "Train Loss at iteration 10557: 0.04227228147003477\n",
      "Train Loss at iteration 10558: 0.04227222665583633\n",
      "Train Loss at iteration 10559: 0.042272171850743466\n",
      "Train Loss at iteration 10560: 0.04227211705475464\n",
      "Train Loss at iteration 10561: 0.04227206226786832\n",
      "Train Loss at iteration 10562: 0.04227200749008299\n",
      "Train Loss at iteration 10563: 0.0422719527213971\n",
      "Train Loss at iteration 10564: 0.04227189796180912\n",
      "Train Loss at iteration 10565: 0.04227184321131754\n",
      "Train Loss at iteration 10566: 0.04227178846992082\n",
      "Train Loss at iteration 10567: 0.042271733737617424\n",
      "Train Loss at iteration 10568: 0.04227167901440583\n",
      "Train Loss at iteration 10569: 0.04227162430028451\n",
      "Train Loss at iteration 10570: 0.04227156959525194\n",
      "Train Loss at iteration 10571: 0.042271514899306584\n",
      "Train Loss at iteration 10572: 0.042271460212446925\n",
      "Train Loss at iteration 10573: 0.042271405534671416\n",
      "Train Loss at iteration 10574: 0.04227135086597855\n",
      "Train Loss at iteration 10575: 0.04227129620636679\n",
      "Train Loss at iteration 10576: 0.042271241555834616\n",
      "Train Loss at iteration 10577: 0.0422711869143805\n",
      "Train Loss at iteration 10578: 0.0422711322820029\n",
      "Train Loss at iteration 10579: 0.042271077658700314\n",
      "Train Loss at iteration 10580: 0.042271023044471216\n",
      "Train Loss at iteration 10581: 0.04227096843931405\n",
      "Train Loss at iteration 10582: 0.04227091384322733\n",
      "Train Loss at iteration 10583: 0.04227085925620951\n",
      "Train Loss at iteration 10584: 0.04227080467825907\n",
      "Train Loss at iteration 10585: 0.04227075010937448\n",
      "Train Loss at iteration 10586: 0.04227069554955422\n",
      "Train Loss at iteration 10587: 0.04227064099879677\n",
      "Train Loss at iteration 10588: 0.042270586457100606\n",
      "Train Loss at iteration 10589: 0.04227053192446422\n",
      "Train Loss at iteration 10590: 0.04227047740088605\n",
      "Train Loss at iteration 10591: 0.0422704228863646\n",
      "Train Loss at iteration 10592: 0.04227036838089835\n",
      "Train Loss at iteration 10593: 0.04227031388448577\n",
      "Train Loss at iteration 10594: 0.04227025939712534\n",
      "Train Loss at iteration 10595: 0.04227020491881553\n",
      "Train Loss at iteration 10596: 0.04227015044955486\n",
      "Train Loss at iteration 10597: 0.04227009598934175\n",
      "Train Loss at iteration 10598: 0.04227004153817471\n",
      "Train Loss at iteration 10599: 0.04226998709605222\n",
      "Train Loss at iteration 10600: 0.04226993266297275\n",
      "Train Loss at iteration 10601: 0.042269878238934785\n",
      "Train Loss at iteration 10602: 0.04226982382393681\n",
      "Train Loss at iteration 10603: 0.0422697694179773\n",
      "Train Loss at iteration 10604: 0.04226971502105475\n",
      "Train Loss at iteration 10605: 0.04226966063316762\n",
      "Train Loss at iteration 10606: 0.0422696062543144\n",
      "Train Loss at iteration 10607: 0.04226955188449358\n",
      "Train Loss at iteration 10608: 0.04226949752370362\n",
      "Train Loss at iteration 10609: 0.04226944317194303\n",
      "Train Loss at iteration 10610: 0.042269388829210275\n",
      "Train Loss at iteration 10611: 0.042269334495503845\n",
      "Train Loss at iteration 10612: 0.042269280170822224\n",
      "Train Loss at iteration 10613: 0.042269225855163885\n",
      "Train Loss at iteration 10614: 0.04226917154852732\n",
      "Train Loss at iteration 10615: 0.04226911725091101\n",
      "Train Loss at iteration 10616: 0.04226906296231344\n",
      "Train Loss at iteration 10617: 0.0422690086827331\n",
      "Train Loss at iteration 10618: 0.04226895441216848\n",
      "Train Loss at iteration 10619: 0.04226890015061803\n",
      "Train Loss at iteration 10620: 0.042268845898080276\n",
      "Train Loss at iteration 10621: 0.04226879165455368\n",
      "Train Loss at iteration 10622: 0.042268737420036756\n",
      "Train Loss at iteration 10623: 0.042268683194527944\n",
      "Train Loss at iteration 10624: 0.04226862897802577\n",
      "Train Loss at iteration 10625: 0.04226857477052869\n",
      "Train Loss at iteration 10626: 0.04226852057203523\n",
      "Train Loss at iteration 10627: 0.042268466382543855\n",
      "Train Loss at iteration 10628: 0.04226841220205303\n",
      "Train Loss at iteration 10629: 0.04226835803056127\n",
      "Train Loss at iteration 10630: 0.04226830386806706\n",
      "Train Loss at iteration 10631: 0.0422682497145689\n",
      "Train Loss at iteration 10632: 0.042268195570065246\n",
      "Train Loss at iteration 10633: 0.04226814143455461\n",
      "Train Loss at iteration 10634: 0.042268087308035475\n",
      "Train Loss at iteration 10635: 0.042268033190506336\n",
      "Train Loss at iteration 10636: 0.042267979081965674\n",
      "Train Loss at iteration 10637: 0.04226792498241198\n",
      "Train Loss at iteration 10638: 0.042267870891843744\n",
      "Train Loss at iteration 10639: 0.04226781681025946\n",
      "Train Loss at iteration 10640: 0.04226776273765763\n",
      "Train Loss at iteration 10641: 0.04226770867403671\n",
      "Train Loss at iteration 10642: 0.042267654619395224\n",
      "Train Loss at iteration 10643: 0.042267600573731655\n",
      "Train Loss at iteration 10644: 0.04226754653704449\n",
      "Train Loss at iteration 10645: 0.04226749250933222\n",
      "Train Loss at iteration 10646: 0.04226743849059334\n",
      "Train Loss at iteration 10647: 0.04226738448082634\n",
      "Train Loss at iteration 10648: 0.042267330480029716\n",
      "Train Loss at iteration 10649: 0.04226727648820197\n",
      "Train Loss at iteration 10650: 0.042267222505341574\n",
      "Train Loss at iteration 10651: 0.04226716853144704\n",
      "Train Loss at iteration 10652: 0.04226711456651685\n",
      "Train Loss at iteration 10653: 0.04226706061054951\n",
      "Train Loss at iteration 10654: 0.04226700666354349\n",
      "Train Loss at iteration 10655: 0.04226695272549732\n",
      "Train Loss at iteration 10656: 0.04226689879640946\n",
      "Train Loss at iteration 10657: 0.04226684487627843\n",
      "Train Loss at iteration 10658: 0.04226679096510272\n",
      "Train Loss at iteration 10659: 0.04226673706288082\n",
      "Train Loss at iteration 10660: 0.04226668316961122\n",
      "Train Loss at iteration 10661: 0.04226662928529243\n",
      "Train Loss at iteration 10662: 0.042266575409922943\n",
      "Train Loss at iteration 10663: 0.042266521543501245\n",
      "Train Loss at iteration 10664: 0.04226646768602585\n",
      "Train Loss at iteration 10665: 0.04226641383749524\n",
      "Train Loss at iteration 10666: 0.042266359997907926\n",
      "Train Loss at iteration 10667: 0.042266306167262395\n",
      "Train Loss at iteration 10668: 0.04226625234555715\n",
      "Train Loss at iteration 10669: 0.042266198532790684\n",
      "Train Loss at iteration 10670: 0.0422661447289615\n",
      "Train Loss at iteration 10671: 0.0422660909340681\n",
      "Train Loss at iteration 10672: 0.04226603714810897\n",
      "Train Loss at iteration 10673: 0.04226598337108263\n",
      "Train Loss at iteration 10674: 0.042265929602987556\n",
      "Train Loss at iteration 10675: 0.04226587584382227\n",
      "Train Loss at iteration 10676: 0.04226582209358525\n",
      "Train Loss at iteration 10677: 0.042265768352275024\n",
      "Train Loss at iteration 10678: 0.04226571461989006\n",
      "Train Loss at iteration 10679: 0.042265660896428886\n",
      "Train Loss at iteration 10680: 0.042265607181889986\n",
      "Train Loss at iteration 10681: 0.04226555347627188\n",
      "Train Loss at iteration 10682: 0.04226549977957304\n",
      "Train Loss at iteration 10683: 0.042265446091792\n",
      "Train Loss at iteration 10684: 0.04226539241292724\n",
      "Train Loss at iteration 10685: 0.04226533874297727\n",
      "Train Loss at iteration 10686: 0.04226528508194059\n",
      "Train Loss at iteration 10687: 0.04226523142981572\n",
      "Train Loss at iteration 10688: 0.042265177786601146\n",
      "Train Loss at iteration 10689: 0.04226512415229537\n",
      "Train Loss at iteration 10690: 0.042265070526896896\n",
      "Train Loss at iteration 10691: 0.04226501691040424\n",
      "Train Loss at iteration 10692: 0.04226496330281589\n",
      "Train Loss at iteration 10693: 0.04226490970413037\n",
      "Train Loss at iteration 10694: 0.042264856114346175\n",
      "Train Loss at iteration 10695: 0.04226480253346181\n",
      "Train Loss at iteration 10696: 0.042264748961475765\n",
      "Train Loss at iteration 10697: 0.04226469539838657\n",
      "Train Loss at iteration 10698: 0.04226464184419272\n",
      "Train Loss at iteration 10699: 0.04226458829889272\n",
      "Train Loss at iteration 10700: 0.04226453476248509\n",
      "Train Loss at iteration 10701: 0.042264481234968315\n",
      "Train Loss at iteration 10702: 0.042264427716340924\n",
      "Train Loss at iteration 10703: 0.04226437420660139\n",
      "Train Loss at iteration 10704: 0.04226432070574825\n",
      "Train Loss at iteration 10705: 0.04226426721378002\n",
      "Train Loss at iteration 10706: 0.04226421373069518\n",
      "Train Loss at iteration 10707: 0.04226416025649225\n",
      "Train Loss at iteration 10708: 0.042264106791169744\n",
      "Train Loss at iteration 10709: 0.04226405333472616\n",
      "Train Loss at iteration 10710: 0.04226399988716002\n",
      "Train Loss at iteration 10711: 0.04226394644846983\n",
      "Train Loss at iteration 10712: 0.04226389301865409\n",
      "Train Loss at iteration 10713: 0.042263839597711315\n",
      "Train Loss at iteration 10714: 0.042263786185640015\n",
      "Train Loss at iteration 10715: 0.04226373278243871\n",
      "Train Loss at iteration 10716: 0.042263679388105886\n",
      "Train Loss at iteration 10717: 0.04226362600264008\n",
      "Train Loss at iteration 10718: 0.04226357262603979\n",
      "Train Loss at iteration 10719: 0.042263519258303524\n",
      "Train Loss at iteration 10720: 0.04226346589942981\n",
      "Train Loss at iteration 10721: 0.04226341254941715\n",
      "Train Loss at iteration 10722: 0.042263359208264045\n",
      "Train Loss at iteration 10723: 0.04226330587596902\n",
      "Train Loss at iteration 10724: 0.042263252552530585\n",
      "Train Loss at iteration 10725: 0.04226319923794726\n",
      "Train Loss at iteration 10726: 0.04226314593221755\n",
      "Train Loss at iteration 10727: 0.04226309263533998\n",
      "Train Loss at iteration 10728: 0.042263039347313035\n",
      "Train Loss at iteration 10729: 0.04226298606813525\n",
      "Train Loss at iteration 10730: 0.04226293279780515\n",
      "Train Loss at iteration 10731: 0.04226287953632123\n",
      "Train Loss at iteration 10732: 0.04226282628368201\n",
      "Train Loss at iteration 10733: 0.04226277303988601\n",
      "Train Loss at iteration 10734: 0.042262719804931737\n",
      "Train Loss at iteration 10735: 0.04226266657881772\n",
      "Train Loss at iteration 10736: 0.04226261336154245\n",
      "Train Loss at iteration 10737: 0.04226256015310447\n",
      "Train Loss at iteration 10738: 0.04226250695350229\n",
      "Train Loss at iteration 10739: 0.04226245376273442\n",
      "Train Loss at iteration 10740: 0.04226240058079937\n",
      "Train Loss at iteration 10741: 0.042262347407695675\n",
      "Train Loss at iteration 10742: 0.04226229424342184\n",
      "Train Loss at iteration 10743: 0.042262241087976375\n",
      "Train Loss at iteration 10744: 0.04226218794135782\n",
      "Train Loss at iteration 10745: 0.04226213480356468\n",
      "Train Loss at iteration 10746: 0.042262081674595484\n",
      "Train Loss at iteration 10747: 0.04226202855444872\n",
      "Train Loss at iteration 10748: 0.042261975443122955\n",
      "Train Loss at iteration 10749: 0.04226192234061666\n",
      "Train Loss at iteration 10750: 0.042261869246928395\n",
      "Train Loss at iteration 10751: 0.042261816162056653\n",
      "Train Loss at iteration 10752: 0.04226176308599996\n",
      "Train Loss at iteration 10753: 0.04226171001875683\n",
      "Train Loss at iteration 10754: 0.04226165696032581\n",
      "Train Loss at iteration 10755: 0.04226160391070539\n",
      "Train Loss at iteration 10756: 0.04226155086989412\n",
      "Train Loss at iteration 10757: 0.04226149783789048\n",
      "Train Loss at iteration 10758: 0.042261444814693024\n",
      "Train Loss at iteration 10759: 0.04226139180030028\n",
      "Train Loss at iteration 10760: 0.04226133879471075\n",
      "Train Loss at iteration 10761: 0.04226128579792295\n",
      "Train Loss at iteration 10762: 0.04226123280993542\n",
      "Train Loss at iteration 10763: 0.04226117983074669\n",
      "Train Loss at iteration 10764: 0.04226112686035526\n",
      "Train Loss at iteration 10765: 0.04226107389875967\n",
      "Train Loss at iteration 10766: 0.04226102094595843\n",
      "Train Loss at iteration 10767: 0.042260968001950075\n",
      "Train Loss at iteration 10768: 0.04226091506673313\n",
      "Train Loss at iteration 10769: 0.04226086214030611\n",
      "Train Loss at iteration 10770: 0.042260809222667534\n",
      "Train Loss at iteration 10771: 0.042260756313815956\n",
      "Train Loss at iteration 10772: 0.04226070341374988\n",
      "Train Loss at iteration 10773: 0.04226065052246783\n",
      "Train Loss at iteration 10774: 0.04226059763996833\n",
      "Train Loss at iteration 10775: 0.042260544766249915\n",
      "Train Loss at iteration 10776: 0.04226049190131112\n",
      "Train Loss at iteration 10777: 0.042260439045150436\n",
      "Train Loss at iteration 10778: 0.04226038619776643\n",
      "Train Loss at iteration 10779: 0.042260333359157604\n",
      "Train Loss at iteration 10780: 0.04226028052932251\n",
      "Train Loss at iteration 10781: 0.042260227708259636\n",
      "Train Loss at iteration 10782: 0.042260174895967534\n",
      "Train Loss at iteration 10783: 0.04226012209244474\n",
      "Train Loss at iteration 10784: 0.04226006929768976\n",
      "Train Loss at iteration 10785: 0.04226001651170115\n",
      "Train Loss at iteration 10786: 0.042259963734477414\n",
      "Train Loss at iteration 10787: 0.04225991096601709\n",
      "Train Loss at iteration 10788: 0.04225985820631872\n",
      "Train Loss at iteration 10789: 0.04225980545538081\n",
      "Train Loss at iteration 10790: 0.0422597527132019\n",
      "Train Loss at iteration 10791: 0.04225969997978053\n",
      "Train Loss at iteration 10792: 0.04225964725511522\n",
      "Train Loss at iteration 10793: 0.042259594539204484\n",
      "Train Loss at iteration 10794: 0.042259541832046894\n",
      "Train Loss at iteration 10795: 0.042259489133640936\n",
      "Train Loss at iteration 10796: 0.042259436443985186\n",
      "Train Loss at iteration 10797: 0.04225938376307814\n",
      "Train Loss at iteration 10798: 0.042259331090918346\n",
      "Train Loss at iteration 10799: 0.04225927842750433\n",
      "Train Loss at iteration 10800: 0.04225922577283462\n",
      "Train Loss at iteration 10801: 0.042259173126907766\n",
      "Train Loss at iteration 10802: 0.04225912048972229\n",
      "Train Loss at iteration 10803: 0.04225906786127672\n",
      "Train Loss at iteration 10804: 0.042259015241569595\n",
      "Train Loss at iteration 10805: 0.04225896263059946\n",
      "Train Loss at iteration 10806: 0.04225891002836483\n",
      "Train Loss at iteration 10807: 0.04225885743486424\n",
      "Train Loss at iteration 10808: 0.04225880485009623\n",
      "Train Loss at iteration 10809: 0.04225875227405935\n",
      "Train Loss at iteration 10810: 0.042258699706752105\n",
      "Train Loss at iteration 10811: 0.04225864714817306\n",
      "Train Loss at iteration 10812: 0.04225859459832072\n",
      "Train Loss at iteration 10813: 0.04225854205719365\n",
      "Train Loss at iteration 10814: 0.04225848952479037\n",
      "Train Loss at iteration 10815: 0.042258437001109414\n",
      "Train Loss at iteration 10816: 0.04225838448614933\n",
      "Train Loss at iteration 10817: 0.042258331979908643\n",
      "Train Loss at iteration 10818: 0.0422582794823859\n",
      "Train Loss at iteration 10819: 0.04225822699357962\n",
      "Train Loss at iteration 10820: 0.04225817451348836\n",
      "Train Loss at iteration 10821: 0.04225812204211066\n",
      "Train Loss at iteration 10822: 0.04225806957944505\n",
      "Train Loss at iteration 10823: 0.04225801712549005\n",
      "Train Loss at iteration 10824: 0.04225796468024423\n",
      "Train Loss at iteration 10825: 0.042257912243706106\n",
      "Train Loss at iteration 10826: 0.04225785981587424\n",
      "Train Loss at iteration 10827: 0.042257807396747143\n",
      "Train Loss at iteration 10828: 0.042257754986323386\n",
      "Train Loss at iteration 10829: 0.04225770258460147\n",
      "Train Loss at iteration 10830: 0.04225765019157997\n",
      "Train Loss at iteration 10831: 0.042257597807257415\n",
      "Train Loss at iteration 10832: 0.04225754543163233\n",
      "Train Loss at iteration 10833: 0.04225749306470328\n",
      "Train Loss at iteration 10834: 0.042257440706468787\n",
      "Train Loss at iteration 10835: 0.042257388356927404\n",
      "Train Loss at iteration 10836: 0.042257336016077666\n",
      "Train Loss at iteration 10837: 0.04225728368391812\n",
      "Train Loss at iteration 10838: 0.04225723136044731\n",
      "Train Loss at iteration 10839: 0.04225717904566376\n",
      "Train Loss at iteration 10840: 0.04225712673956604\n",
      "Train Loss at iteration 10841: 0.04225707444215268\n",
      "Train Loss at iteration 10842: 0.042257022153422225\n",
      "Train Loss at iteration 10843: 0.04225696987337321\n",
      "Train Loss at iteration 10844: 0.04225691760200418\n",
      "Train Loss at iteration 10845: 0.04225686533931369\n",
      "Train Loss at iteration 10846: 0.042256813085300285\n",
      "Train Loss at iteration 10847: 0.042256760839962486\n",
      "Train Loss at iteration 10848: 0.042256708603298866\n",
      "Train Loss at iteration 10849: 0.04225665637530795\n",
      "Train Loss at iteration 10850: 0.04225660415598829\n",
      "Train Loss at iteration 10851: 0.042256551945338436\n",
      "Train Loss at iteration 10852: 0.04225649974335692\n",
      "Train Loss at iteration 10853: 0.04225644755004231\n",
      "Train Loss at iteration 10854: 0.042256395365393135\n",
      "Train Loss at iteration 10855: 0.04225634318940795\n",
      "Train Loss at iteration 10856: 0.042256291022085286\n",
      "Train Loss at iteration 10857: 0.0422562388634237\n",
      "Train Loss at iteration 10858: 0.04225618671342176\n",
      "Train Loss at iteration 10859: 0.042256134572077976\n",
      "Train Loss at iteration 10860: 0.04225608243939093\n",
      "Train Loss at iteration 10861: 0.04225603031535914\n",
      "Train Loss at iteration 10862: 0.04225597819998116\n",
      "Train Loss at iteration 10863: 0.04225592609325557\n",
      "Train Loss at iteration 10864: 0.04225587399518089\n",
      "Train Loss at iteration 10865: 0.04225582190575566\n",
      "Train Loss at iteration 10866: 0.042255769824978445\n",
      "Train Loss at iteration 10867: 0.042255717752847796\n",
      "Train Loss at iteration 10868: 0.04225566568936226\n",
      "Train Loss at iteration 10869: 0.04225561363452039\n",
      "Train Loss at iteration 10870: 0.04225556158832073\n",
      "Train Loss at iteration 10871: 0.04225550955076183\n",
      "Train Loss at iteration 10872: 0.04225545752184224\n",
      "Train Loss at iteration 10873: 0.04225540550156052\n",
      "Train Loss at iteration 10874: 0.042255353489915215\n",
      "Train Loss at iteration 10875: 0.04225530148690488\n",
      "Train Loss at iteration 10876: 0.042255249492528055\n",
      "Train Loss at iteration 10877: 0.042255197506783304\n",
      "Train Loss at iteration 10878: 0.04225514552966918\n",
      "Train Loss at iteration 10879: 0.042255093561184225\n",
      "Train Loss at iteration 10880: 0.042255041601327\n",
      "Train Loss at iteration 10881: 0.04225498965009606\n",
      "Train Loss at iteration 10882: 0.042254937707489955\n",
      "Train Loss at iteration 10883: 0.04225488577350723\n",
      "Train Loss at iteration 10884: 0.04225483384814645\n",
      "Train Loss at iteration 10885: 0.04225478193140617\n",
      "Train Loss at iteration 10886: 0.042254730023284934\n",
      "Train Loss at iteration 10887: 0.04225467812378131\n",
      "Train Loss at iteration 10888: 0.04225462623289384\n",
      "Train Loss at iteration 10889: 0.042254574350621084\n",
      "Train Loss at iteration 10890: 0.042254522476961605\n",
      "Train Loss at iteration 10891: 0.04225447061191394\n",
      "Train Loss at iteration 10892: 0.04225441875547665\n",
      "Train Loss at iteration 10893: 0.042254366907648305\n",
      "Train Loss at iteration 10894: 0.04225431506842745\n",
      "Train Loss at iteration 10895: 0.04225426323781265\n",
      "Train Loss at iteration 10896: 0.042254211415802455\n",
      "Train Loss at iteration 10897: 0.04225415960239542\n",
      "Train Loss at iteration 10898: 0.04225410779759011\n",
      "Train Loss at iteration 10899: 0.04225405600138508\n",
      "Train Loss at iteration 10900: 0.042254004213778874\n",
      "Train Loss at iteration 10901: 0.042253952434770065\n",
      "Train Loss at iteration 10902: 0.042253900664357205\n",
      "Train Loss at iteration 10903: 0.04225384890253887\n",
      "Train Loss at iteration 10904: 0.0422537971493136\n",
      "Train Loss at iteration 10905: 0.04225374540467995\n",
      "Train Loss at iteration 10906: 0.04225369366863649\n",
      "Train Loss at iteration 10907: 0.04225364194118178\n",
      "Train Loss at iteration 10908: 0.042253590222314376\n",
      "Train Loss at iteration 10909: 0.04225353851203283\n",
      "Train Loss at iteration 10910: 0.04225348681033573\n",
      "Train Loss at iteration 10911: 0.042253435117221606\n",
      "Train Loss at iteration 10912: 0.042253383432689036\n",
      "Train Loss at iteration 10913: 0.04225333175673657\n",
      "Train Loss at iteration 10914: 0.04225328008936277\n",
      "Train Loss at iteration 10915: 0.042253228430566205\n",
      "Train Loss at iteration 10916: 0.04225317678034544\n",
      "Train Loss at iteration 10917: 0.042253125138699034\n",
      "Train Loss at iteration 10918: 0.042253073505625544\n",
      "Train Loss at iteration 10919: 0.04225302188112353\n",
      "Train Loss at iteration 10920: 0.04225297026519156\n",
      "Train Loss at iteration 10921: 0.042252918657828195\n",
      "Train Loss at iteration 10922: 0.04225286705903201\n",
      "Train Loss at iteration 10923: 0.04225281546880155\n",
      "Train Loss at iteration 10924: 0.0422527638871354\n",
      "Train Loss at iteration 10925: 0.042252712314032104\n",
      "Train Loss at iteration 10926: 0.04225266074949023\n",
      "Train Loss at iteration 10927: 0.04225260919350835\n",
      "Train Loss at iteration 10928: 0.04225255764608502\n",
      "Train Loss at iteration 10929: 0.04225250610721881\n",
      "Train Loss at iteration 10930: 0.0422524545769083\n",
      "Train Loss at iteration 10931: 0.04225240305515203\n",
      "Train Loss at iteration 10932: 0.042252351541948584\n",
      "Train Loss at iteration 10933: 0.042252300037296506\n",
      "Train Loss at iteration 10934: 0.042252248541194394\n",
      "Train Loss at iteration 10935: 0.04225219705364079\n",
      "Train Loss at iteration 10936: 0.04225214557463426\n",
      "Train Loss at iteration 10937: 0.042252094104173396\n",
      "Train Loss at iteration 10938: 0.04225204264225674\n",
      "Train Loss at iteration 10939: 0.04225199118888288\n",
      "Train Loss at iteration 10940: 0.04225193974405036\n",
      "Train Loss at iteration 10941: 0.04225188830775776\n",
      "Train Loss at iteration 10942: 0.042251836880003654\n",
      "Train Loss at iteration 10943: 0.0422517854607866\n",
      "Train Loss at iteration 10944: 0.042251734050105176\n",
      "Train Loss at iteration 10945: 0.04225168264795794\n",
      "Train Loss at iteration 10946: 0.042251631254343495\n",
      "Train Loss at iteration 10947: 0.04225157986926035\n",
      "Train Loss at iteration 10948: 0.042251528492707115\n",
      "Train Loss at iteration 10949: 0.04225147712468237\n",
      "Train Loss at iteration 10950: 0.04225142576518467\n",
      "Train Loss at iteration 10951: 0.04225137441421257\n",
      "Train Loss at iteration 10952: 0.04225132307176466\n",
      "Train Loss at iteration 10953: 0.04225127173783951\n",
      "Train Loss at iteration 10954: 0.04225122041243568\n",
      "Train Loss at iteration 10955: 0.04225116909555176\n",
      "Train Loss at iteration 10956: 0.04225111778718631\n",
      "Train Loss at iteration 10957: 0.0422510664873379\n",
      "Train Loss at iteration 10958: 0.0422510151960051\n",
      "Train Loss at iteration 10959: 0.042250963913186494\n",
      "Train Loss at iteration 10960: 0.04225091263888066\n",
      "Train Loss at iteration 10961: 0.04225086137308614\n",
      "Train Loss at iteration 10962: 0.04225081011580153\n",
      "Train Loss at iteration 10963: 0.042250758867025395\n",
      "Train Loss at iteration 10964: 0.04225070762675633\n",
      "Train Loss at iteration 10965: 0.04225065639499289\n",
      "Train Loss at iteration 10966: 0.04225060517173366\n",
      "Train Loss at iteration 10967: 0.042250553956977195\n",
      "Train Loss at iteration 10968: 0.042250502750722084\n",
      "Train Loss at iteration 10969: 0.042250451552966896\n",
      "Train Loss at iteration 10970: 0.04225040036371022\n",
      "Train Loss at iteration 10971: 0.04225034918295062\n",
      "Train Loss at iteration 10972: 0.04225029801068666\n",
      "Train Loss at iteration 10973: 0.04225024684691694\n",
      "Train Loss at iteration 10974: 0.04225019569164003\n",
      "Train Loss at iteration 10975: 0.04225014454485449\n",
      "Train Loss at iteration 10976: 0.04225009340655892\n",
      "Train Loss at iteration 10977: 0.04225004227675189\n",
      "Train Loss at iteration 10978: 0.04224999115543196\n",
      "Train Loss at iteration 10979: 0.04224994004259774\n",
      "Train Loss at iteration 10980: 0.04224988893824777\n",
      "Train Loss at iteration 10981: 0.04224983784238066\n",
      "Train Loss at iteration 10982: 0.04224978675499496\n",
      "Train Loss at iteration 10983: 0.04224973567608928\n",
      "Train Loss at iteration 10984: 0.04224968460566217\n",
      "Train Loss at iteration 10985: 0.04224963354371222\n",
      "Train Loss at iteration 10986: 0.04224958249023802\n",
      "Train Loss at iteration 10987: 0.04224953144523812\n",
      "Train Loss at iteration 10988: 0.04224948040871115\n",
      "Train Loss at iteration 10989: 0.042249429380655644\n",
      "Train Loss at iteration 10990: 0.04224937836107019\n",
      "Train Loss at iteration 10991: 0.04224932734995338\n",
      "Train Loss at iteration 10992: 0.042249276347303806\n",
      "Train Loss at iteration 10993: 0.04224922535312002\n",
      "Train Loss at iteration 10994: 0.042249174367400624\n",
      "Train Loss at iteration 10995: 0.04224912339014419\n",
      "Train Loss at iteration 10996: 0.0422490724213493\n",
      "Train Loss at iteration 10997: 0.04224902146101454\n",
      "Train Loss at iteration 10998: 0.042248970509138485\n",
      "Train Loss at iteration 10999: 0.04224891956571972\n",
      "Train Loss at iteration 11000: 0.042248868630756845\n",
      "Train Loss at iteration 11001: 0.04224881770424842\n",
      "Train Loss at iteration 11002: 0.042248766786193034\n",
      "Train Loss at iteration 11003: 0.04224871587658928\n",
      "Train Loss at iteration 11004: 0.04224866497543573\n",
      "Train Loss at iteration 11005: 0.042248614082730986\n",
      "Train Loss at iteration 11006: 0.042248563198473595\n",
      "Train Loss at iteration 11007: 0.042248512322662184\n",
      "Train Loss at iteration 11008: 0.042248461455295316\n",
      "Train Loss at iteration 11009: 0.04224841059637157\n",
      "Train Loss at iteration 11010: 0.04224835974588956\n",
      "Train Loss at iteration 11011: 0.04224830890384782\n",
      "Train Loss at iteration 11012: 0.04224825807024499\n",
      "Train Loss at iteration 11013: 0.04224820724507963\n",
      "Train Loss at iteration 11014: 0.04224815642835032\n",
      "Train Loss at iteration 11015: 0.04224810562005567\n",
      "Train Loss at iteration 11016: 0.042248054820194246\n",
      "Train Loss at iteration 11017: 0.04224800402876464\n",
      "Train Loss at iteration 11018: 0.04224795324576545\n",
      "Train Loss at iteration 11019: 0.04224790247119523\n",
      "Train Loss at iteration 11020: 0.0422478517050526\n",
      "Train Loss at iteration 11021: 0.042247800947336156\n",
      "Train Loss at iteration 11022: 0.04224775019804445\n",
      "Train Loss at iteration 11023: 0.0422476994571761\n",
      "Train Loss at iteration 11024: 0.04224764872472968\n",
      "Train Loss at iteration 11025: 0.04224759800070379\n",
      "Train Loss at iteration 11026: 0.042247547285097005\n",
      "Train Loss at iteration 11027: 0.04224749657790791\n",
      "Train Loss at iteration 11028: 0.04224744587913512\n",
      "Train Loss at iteration 11029: 0.042247395188777216\n",
      "Train Loss at iteration 11030: 0.04224734450683278\n",
      "Train Loss at iteration 11031: 0.04224729383330039\n",
      "Train Loss at iteration 11032: 0.042247243168178676\n",
      "Train Loss at iteration 11033: 0.042247192511466185\n",
      "Train Loss at iteration 11034: 0.042247141863161544\n",
      "Train Loss at iteration 11035: 0.04224709122326332\n",
      "Train Loss at iteration 11036: 0.04224704059177011\n",
      "Train Loss at iteration 11037: 0.04224698996868051\n",
      "Train Loss at iteration 11038: 0.0422469393539931\n",
      "Train Loss at iteration 11039: 0.04224688874770649\n",
      "Train Loss at iteration 11040: 0.04224683814981927\n",
      "Train Loss at iteration 11041: 0.04224678756033003\n",
      "Train Loss at iteration 11042: 0.04224673697923736\n",
      "Train Loss at iteration 11043: 0.04224668640653985\n",
      "Train Loss at iteration 11044: 0.0422466358422361\n",
      "Train Loss at iteration 11045: 0.042246585286324696\n",
      "Train Loss at iteration 11046: 0.042246534738804235\n",
      "Train Loss at iteration 11047: 0.04224648419967333\n",
      "Train Loss at iteration 11048: 0.04224643366893054\n",
      "Train Loss at iteration 11049: 0.04224638314657449\n",
      "Train Loss at iteration 11050: 0.042246332632603766\n",
      "Train Loss at iteration 11051: 0.042246282127016956\n",
      "Train Loss at iteration 11052: 0.04224623162981266\n",
      "Train Loss at iteration 11053: 0.04224618114098947\n",
      "Train Loss at iteration 11054: 0.042246130660546\n",
      "Train Loss at iteration 11055: 0.042246080188480827\n",
      "Train Loss at iteration 11056: 0.04224602972479254\n",
      "Train Loss at iteration 11057: 0.04224597926947977\n",
      "Train Loss at iteration 11058: 0.042245928822541065\n",
      "Train Loss at iteration 11059: 0.042245878383975075\n",
      "Train Loss at iteration 11060: 0.04224582795378036\n",
      "Train Loss at iteration 11061: 0.04224577753195554\n",
      "Train Loss at iteration 11062: 0.04224572711849919\n",
      "Train Loss at iteration 11063: 0.042245676713409924\n",
      "Train Loss at iteration 11064: 0.04224562631668634\n",
      "Train Loss at iteration 11065: 0.042245575928327034\n",
      "Train Loss at iteration 11066: 0.042245525548330595\n",
      "Train Loss at iteration 11067: 0.04224547517669564\n",
      "Train Loss at iteration 11068: 0.04224542481342076\n",
      "Train Loss at iteration 11069: 0.04224537445850455\n",
      "Train Loss at iteration 11070: 0.04224532411194562\n",
      "Train Loss at iteration 11071: 0.04224527377374256\n",
      "Train Loss at iteration 11072: 0.04224522344389398\n",
      "Train Loss at iteration 11073: 0.04224517312239848\n",
      "Train Loss at iteration 11074: 0.042245122809254645\n",
      "Train Loss at iteration 11075: 0.04224507250446109\n",
      "Train Loss at iteration 11076: 0.04224502220801641\n",
      "Train Loss at iteration 11077: 0.04224497191991921\n",
      "Train Loss at iteration 11078: 0.04224492164016811\n",
      "Train Loss at iteration 11079: 0.04224487136876168\n",
      "Train Loss at iteration 11080: 0.042244821105698535\n",
      "Train Loss at iteration 11081: 0.04224477085097728\n",
      "Train Loss at iteration 11082: 0.042244720604596525\n",
      "Train Loss at iteration 11083: 0.04224467036655487\n",
      "Train Loss at iteration 11084: 0.04224462013685089\n",
      "Train Loss at iteration 11085: 0.042244569915483234\n",
      "Train Loss at iteration 11086: 0.04224451970245046\n",
      "Train Loss at iteration 11087: 0.04224446949775121\n",
      "Train Loss at iteration 11088: 0.04224441930138407\n",
      "Train Loss at iteration 11089: 0.04224436911334765\n",
      "Train Loss at iteration 11090: 0.04224431893364055\n",
      "Train Loss at iteration 11091: 0.042244268762261374\n",
      "Train Loss at iteration 11092: 0.04224421859920873\n",
      "Train Loss at iteration 11093: 0.04224416844448122\n",
      "Train Loss at iteration 11094: 0.04224411829807746\n",
      "Train Loss at iteration 11095: 0.04224406815999606\n",
      "Train Loss at iteration 11096: 0.04224401803023561\n",
      "Train Loss at iteration 11097: 0.04224396790879473\n",
      "Train Loss at iteration 11098: 0.04224391779567199\n",
      "Train Loss at iteration 11099: 0.04224386769086604\n",
      "Train Loss at iteration 11100: 0.04224381759437549\n",
      "Train Loss at iteration 11101: 0.042243767506198904\n",
      "Train Loss at iteration 11102: 0.04224371742633494\n",
      "Train Loss at iteration 11103: 0.04224366735478217\n",
      "Train Loss at iteration 11104: 0.04224361729153921\n",
      "Train Loss at iteration 11105: 0.04224356723660468\n",
      "Train Loss at iteration 11106: 0.04224351718997716\n",
      "Train Loss at iteration 11107: 0.0422434671516553\n",
      "Train Loss at iteration 11108: 0.042243417121637684\n",
      "Train Loss at iteration 11109: 0.04224336709992293\n",
      "Train Loss at iteration 11110: 0.04224331708650964\n",
      "Train Loss at iteration 11111: 0.042243267081396425\n",
      "Train Loss at iteration 11112: 0.0422432170845819\n",
      "Train Loss at iteration 11113: 0.042243167096064675\n",
      "Train Loss at iteration 11114: 0.04224311711584336\n",
      "Train Loss at iteration 11115: 0.04224306714391655\n",
      "Train Loss at iteration 11116: 0.04224301718028288\n",
      "Train Loss at iteration 11117: 0.04224296722494094\n",
      "Train Loss at iteration 11118: 0.042242917277889376\n",
      "Train Loss at iteration 11119: 0.04224286733912674\n",
      "Train Loss at iteration 11120: 0.042242817408651705\n",
      "Train Loss at iteration 11121: 0.04224276748646285\n",
      "Train Loss at iteration 11122: 0.0422427175725588\n",
      "Train Loss at iteration 11123: 0.04224266766693816\n",
      "Train Loss at iteration 11124: 0.04224261776959956\n",
      "Train Loss at iteration 11125: 0.042242567880541586\n",
      "Train Loss at iteration 11126: 0.04224251799976286\n",
      "Train Loss at iteration 11127: 0.04224246812726202\n",
      "Train Loss at iteration 11128: 0.04224241826303764\n",
      "Train Loss at iteration 11129: 0.042242368407088365\n",
      "Train Loss at iteration 11130: 0.042242318559412795\n",
      "Train Loss at iteration 11131: 0.042242268720009545\n",
      "Train Loss at iteration 11132: 0.04224221888887724\n",
      "Train Loss at iteration 11133: 0.04224216906601449\n",
      "Train Loss at iteration 11134: 0.04224211925141991\n",
      "Train Loss at iteration 11135: 0.0422420694450921\n",
      "Train Loss at iteration 11136: 0.042242019647029706\n",
      "Train Loss at iteration 11137: 0.042241969857231304\n",
      "Train Loss at iteration 11138: 0.04224192007569556\n",
      "Train Loss at iteration 11139: 0.042241870302421054\n",
      "Train Loss at iteration 11140: 0.042241820537406424\n",
      "Train Loss at iteration 11141: 0.042241770780650265\n",
      "Train Loss at iteration 11142: 0.04224172103215121\n",
      "Train Loss at iteration 11143: 0.042241671291907874\n",
      "Train Loss at iteration 11144: 0.04224162155991887\n",
      "Train Loss at iteration 11145: 0.04224157183618281\n",
      "Train Loss at iteration 11146: 0.04224152212069834\n",
      "Train Loss at iteration 11147: 0.04224147241346404\n",
      "Train Loss at iteration 11148: 0.04224142271447856\n",
      "Train Loss at iteration 11149: 0.0422413730237405\n",
      "Train Loss at iteration 11150: 0.0422413233412485\n",
      "Train Loss at iteration 11151: 0.04224127366700115\n",
      "Train Loss at iteration 11152: 0.042241224000997085\n",
      "Train Loss at iteration 11153: 0.042241174343234954\n",
      "Train Loss at iteration 11154: 0.04224112469371332\n",
      "Train Loss at iteration 11155: 0.04224107505243085\n",
      "Train Loss at iteration 11156: 0.04224102541938613\n",
      "Train Loss at iteration 11157: 0.042240975794577806\n",
      "Train Loss at iteration 11158: 0.042240926178004484\n",
      "Train Loss at iteration 11159: 0.04224087656966479\n",
      "Train Loss at iteration 11160: 0.04224082696955736\n",
      "Train Loss at iteration 11161: 0.04224077737768079\n",
      "Train Loss at iteration 11162: 0.042240727794033726\n",
      "Train Loss at iteration 11163: 0.04224067821861478\n",
      "Train Loss at iteration 11164: 0.042240628651422564\n",
      "Train Loss at iteration 11165: 0.04224057909245571\n",
      "Train Loss at iteration 11166: 0.04224052954171285\n",
      "Train Loss at iteration 11167: 0.04224047999919258\n",
      "Train Loss at iteration 11168: 0.04224043046489356\n",
      "Train Loss at iteration 11169: 0.042240380938814395\n",
      "Train Loss at iteration 11170: 0.04224033142095371\n",
      "Train Loss at iteration 11171: 0.04224028191131013\n",
      "Train Loss at iteration 11172: 0.04224023240988227\n",
      "Train Loss at iteration 11173: 0.04224018291666877\n",
      "Train Loss at iteration 11174: 0.04224013343166825\n",
      "Train Loss at iteration 11175: 0.042240083954879326\n",
      "Train Loss at iteration 11176: 0.04224003448630064\n",
      "Train Loss at iteration 11177: 0.04223998502593079\n",
      "Train Loss at iteration 11178: 0.042239935573768435\n",
      "Train Loss at iteration 11179: 0.04223988612981218\n",
      "Train Loss at iteration 11180: 0.042239836694060666\n",
      "Train Loss at iteration 11181: 0.04223978726651251\n",
      "Train Loss at iteration 11182: 0.04223973784716632\n",
      "Train Loss at iteration 11183: 0.04223968843602076\n",
      "Train Loss at iteration 11184: 0.04223963903307444\n",
      "Train Loss at iteration 11185: 0.042239589638325976\n",
      "Train Loss at iteration 11186: 0.042239540251774024\n",
      "Train Loss at iteration 11187: 0.04223949087341718\n",
      "Train Loss at iteration 11188: 0.0422394415032541\n",
      "Train Loss at iteration 11189: 0.042239392141283395\n",
      "Train Loss at iteration 11190: 0.0422393427875037\n",
      "Train Loss at iteration 11191: 0.04223929344191365\n",
      "Train Loss at iteration 11192: 0.04223924410451185\n",
      "Train Loss at iteration 11193: 0.042239194775296945\n",
      "Train Loss at iteration 11194: 0.04223914545426758\n",
      "Train Loss at iteration 11195: 0.042239096141422364\n",
      "Train Loss at iteration 11196: 0.042239046836759934\n",
      "Train Loss at iteration 11197: 0.04223899754027892\n",
      "Train Loss at iteration 11198: 0.042238948251977965\n",
      "Train Loss at iteration 11199: 0.04223889897185569\n",
      "Train Loss at iteration 11200: 0.0422388496999107\n",
      "Train Loss at iteration 11201: 0.04223880043614165\n",
      "Train Loss at iteration 11202: 0.04223875118054718\n",
      "Train Loss at iteration 11203: 0.04223870193312592\n",
      "Train Loss at iteration 11204: 0.04223865269387649\n",
      "Train Loss at iteration 11205: 0.042238603462797536\n",
      "Train Loss at iteration 11206: 0.04223855423988768\n",
      "Train Loss at iteration 11207: 0.04223850502514555\n",
      "Train Loss at iteration 11208: 0.042238455818569796\n",
      "Train Loss at iteration 11209: 0.04223840662015902\n",
      "Train Loss at iteration 11210: 0.04223835742991189\n",
      "Train Loss at iteration 11211: 0.04223830824782703\n",
      "Train Loss at iteration 11212: 0.04223825907390307\n",
      "Train Loss at iteration 11213: 0.04223820990813863\n",
      "Train Loss at iteration 11214: 0.042238160750532376\n",
      "Train Loss at iteration 11215: 0.042238111601082914\n",
      "Train Loss at iteration 11216: 0.0422380624597889\n",
      "Train Loss at iteration 11217: 0.042238013326648956\n",
      "Train Loss at iteration 11218: 0.04223796420166172\n",
      "Train Loss at iteration 11219: 0.04223791508482582\n",
      "Train Loss at iteration 11220: 0.042237865976139904\n",
      "Train Loss at iteration 11221: 0.04223781687560261\n",
      "Train Loss at iteration 11222: 0.04223776778321257\n",
      "Train Loss at iteration 11223: 0.04223771869896842\n",
      "Train Loss at iteration 11224: 0.04223766962286879\n",
      "Train Loss at iteration 11225: 0.042237620554912314\n",
      "Train Loss at iteration 11226: 0.04223757149509765\n",
      "Train Loss at iteration 11227: 0.042237522443423416\n",
      "Train Loss at iteration 11228: 0.042237473399888266\n",
      "Train Loss at iteration 11229: 0.04223742436449083\n",
      "Train Loss at iteration 11230: 0.04223737533722973\n",
      "Train Loss at iteration 11231: 0.04223732631810362\n",
      "Train Loss at iteration 11232: 0.04223727730711115\n",
      "Train Loss at iteration 11233: 0.04223722830425095\n",
      "Train Loss at iteration 11234: 0.042237179309521644\n",
      "Train Loss at iteration 11235: 0.042237130322921895\n",
      "Train Loss at iteration 11236: 0.04223708134445032\n",
      "Train Loss at iteration 11237: 0.042237032374105574\n",
      "Train Loss at iteration 11238: 0.0422369834118863\n",
      "Train Loss at iteration 11239: 0.04223693445779113\n",
      "Train Loss at iteration 11240: 0.0422368855118187\n",
      "Train Loss at iteration 11241: 0.04223683657396766\n",
      "Train Loss at iteration 11242: 0.04223678764423665\n",
      "Train Loss at iteration 11243: 0.0422367387226243\n",
      "Train Loss at iteration 11244: 0.04223668980912926\n",
      "Train Loss at iteration 11245: 0.04223664090375017\n",
      "Train Loss at iteration 11246: 0.04223659200648568\n",
      "Train Loss at iteration 11247: 0.04223654311733443\n",
      "Train Loss at iteration 11248: 0.04223649423629505\n",
      "Train Loss at iteration 11249: 0.0422364453633662\n",
      "Train Loss at iteration 11250: 0.042236396498546505\n",
      "Train Loss at iteration 11251: 0.04223634764183463\n",
      "Train Loss at iteration 11252: 0.04223629879322919\n",
      "Train Loss at iteration 11253: 0.04223624995272886\n",
      "Train Loss at iteration 11254: 0.04223620112033225\n",
      "Train Loss at iteration 11255: 0.042236152296038026\n",
      "Train Loss at iteration 11256: 0.04223610347984484\n",
      "Train Loss at iteration 11257: 0.042236054671751316\n",
      "Train Loss at iteration 11258: 0.04223600587175612\n",
      "Train Loss at iteration 11259: 0.042235957079857875\n",
      "Train Loss at iteration 11260: 0.042235908296055225\n",
      "Train Loss at iteration 11261: 0.042235859520346836\n",
      "Train Loss at iteration 11262: 0.04223581075273134\n",
      "Train Loss at iteration 11263: 0.042235761993207395\n",
      "Train Loss at iteration 11264: 0.04223571324177364\n",
      "Train Loss at iteration 11265: 0.0422356644984287\n",
      "Train Loss at iteration 11266: 0.042235615763171266\n",
      "Train Loss at iteration 11267: 0.04223556703599994\n",
      "Train Loss at iteration 11268: 0.04223551831691341\n",
      "Train Loss at iteration 11269: 0.0422354696059103\n",
      "Train Loss at iteration 11270: 0.04223542090298925\n",
      "Train Loss at iteration 11271: 0.042235372208148915\n",
      "Train Loss at iteration 11272: 0.04223532352138796\n",
      "Train Loss at iteration 11273: 0.04223527484270502\n",
      "Train Loss at iteration 11274: 0.042235226172098725\n",
      "Train Loss at iteration 11275: 0.04223517750956776\n",
      "Train Loss at iteration 11276: 0.04223512885511075\n",
      "Train Loss at iteration 11277: 0.04223508020872636\n",
      "Train Loss at iteration 11278: 0.042235031570413216\n",
      "Train Loss at iteration 11279: 0.04223498294016999\n",
      "Train Loss at iteration 11280: 0.04223493431799532\n",
      "Train Loss at iteration 11281: 0.04223488570388786\n",
      "Train Loss at iteration 11282: 0.04223483709784626\n",
      "Train Loss at iteration 11283: 0.042234788499869176\n",
      "Train Loss at iteration 11284: 0.042234739909955236\n",
      "Train Loss at iteration 11285: 0.04223469132810312\n",
      "Train Loss at iteration 11286: 0.04223464275431147\n",
      "Train Loss at iteration 11287: 0.04223459418857893\n",
      "Train Loss at iteration 11288: 0.042234545630904154\n",
      "Train Loss at iteration 11289: 0.042234497081285814\n",
      "Train Loss at iteration 11290: 0.042234448539722524\n",
      "Train Loss at iteration 11291: 0.042234400006212966\n",
      "Train Loss at iteration 11292: 0.04223435148075579\n",
      "Train Loss at iteration 11293: 0.04223430296334964\n",
      "Train Loss at iteration 11294: 0.04223425445399317\n",
      "Train Loss at iteration 11295: 0.04223420595268503\n",
      "Train Loss at iteration 11296: 0.04223415745942388\n",
      "Train Loss at iteration 11297: 0.04223410897420839\n",
      "Train Loss at iteration 11298: 0.04223406049703718\n",
      "Train Loss at iteration 11299: 0.042234012027908927\n",
      "Train Loss at iteration 11300: 0.04223396356682229\n",
      "Train Loss at iteration 11301: 0.0422339151137759\n",
      "Train Loss at iteration 11302: 0.042233866668768424\n",
      "Train Loss at iteration 11303: 0.04223381823179854\n",
      "Train Loss at iteration 11304: 0.04223376980286486\n",
      "Train Loss at iteration 11305: 0.04223372138196607\n",
      "Train Loss at iteration 11306: 0.04223367296910083\n",
      "Train Loss at iteration 11307: 0.04223362456426777\n",
      "Train Loss at iteration 11308: 0.042233576167465564\n",
      "Train Loss at iteration 11309: 0.04223352777869288\n",
      "Train Loss at iteration 11310: 0.042233479397948345\n",
      "Train Loss at iteration 11311: 0.04223343102523064\n",
      "Train Loss at iteration 11312: 0.04223338266053841\n",
      "Train Loss at iteration 11313: 0.042233334303870315\n",
      "Train Loss at iteration 11314: 0.04223328595522501\n",
      "Train Loss at iteration 11315: 0.04223323761460117\n",
      "Train Loss at iteration 11316: 0.04223318928199744\n",
      "Train Loss at iteration 11317: 0.042233140957412464\n",
      "Train Loss at iteration 11318: 0.042233092640844926\n",
      "Train Loss at iteration 11319: 0.04223304433229348\n",
      "Train Loss at iteration 11320: 0.04223299603175678\n",
      "Train Loss at iteration 11321: 0.04223294773923348\n",
      "Train Loss at iteration 11322: 0.04223289945472224\n",
      "Train Loss at iteration 11323: 0.04223285117822174\n",
      "Train Loss at iteration 11324: 0.042232802909730624\n",
      "Train Loss at iteration 11325: 0.04223275464924754\n",
      "Train Loss at iteration 11326: 0.042232706396771166\n",
      "Train Loss at iteration 11327: 0.042232658152300165\n",
      "Train Loss at iteration 11328: 0.04223260991583319\n",
      "Train Loss at iteration 11329: 0.04223256168736891\n",
      "Train Loss at iteration 11330: 0.04223251346690597\n",
      "Train Loss at iteration 11331: 0.04223246525444306\n",
      "Train Loss at iteration 11332: 0.04223241704997882\n",
      "Train Loss at iteration 11333: 0.0422323688535119\n",
      "Train Loss at iteration 11334: 0.042232320665041\n",
      "Train Loss at iteration 11335: 0.04223227248456475\n",
      "Train Loss at iteration 11336: 0.04223222431208183\n",
      "Train Loss at iteration 11337: 0.04223217614759088\n",
      "Train Loss at iteration 11338: 0.04223212799109061\n",
      "Train Loss at iteration 11339: 0.042232079842579644\n",
      "Train Loss at iteration 11340: 0.04223203170205666\n",
      "Train Loss at iteration 11341: 0.04223198356952032\n",
      "Train Loss at iteration 11342: 0.042231935444969275\n",
      "Train Loss at iteration 11343: 0.04223188732840222\n",
      "Train Loss at iteration 11344: 0.04223183921981778\n",
      "Train Loss at iteration 11345: 0.04223179111921466\n",
      "Train Loss at iteration 11346: 0.0422317430265915\n",
      "Train Loss at iteration 11347: 0.042231694941946965\n",
      "Train Loss at iteration 11348: 0.04223164686527974\n",
      "Train Loss at iteration 11349: 0.042231598796588464\n",
      "Train Loss at iteration 11350: 0.04223155073587183\n",
      "Train Loss at iteration 11351: 0.042231502683128495\n",
      "Train Loss at iteration 11352: 0.04223145463835712\n",
      "Train Loss at iteration 11353: 0.042231406601556366\n",
      "Train Loss at iteration 11354: 0.04223135857272493\n",
      "Train Loss at iteration 11355: 0.04223131055186144\n",
      "Train Loss at iteration 11356: 0.04223126253896459\n",
      "Train Loss at iteration 11357: 0.042231214534033036\n",
      "Train Loss at iteration 11358: 0.04223116653706544\n",
      "Train Loss at iteration 11359: 0.04223111854806049\n",
      "Train Loss at iteration 11360: 0.042231070567016846\n",
      "Train Loss at iteration 11361: 0.04223102259393317\n",
      "Train Loss at iteration 11362: 0.042230974628808135\n",
      "Train Loss at iteration 11363: 0.04223092667164041\n",
      "Train Loss at iteration 11364: 0.042230878722428665\n",
      "Train Loss at iteration 11365: 0.04223083078117157\n",
      "Train Loss at iteration 11366: 0.0422307828478678\n",
      "Train Loss at iteration 11367: 0.042230734922516014\n",
      "Train Loss at iteration 11368: 0.042230687005114884\n",
      "Train Loss at iteration 11369: 0.042230639095663096\n",
      "Train Loss at iteration 11370: 0.04223059119415931\n",
      "Train Loss at iteration 11371: 0.04223054330060219\n",
      "Train Loss at iteration 11372: 0.04223049541499041\n",
      "Train Loss at iteration 11373: 0.04223044753732265\n",
      "Train Loss at iteration 11374: 0.04223039966759756\n",
      "Train Loss at iteration 11375: 0.042230351805813846\n",
      "Train Loss at iteration 11376: 0.04223030395197015\n",
      "Train Loss at iteration 11377: 0.04223025610606517\n",
      "Train Loss at iteration 11378: 0.042230208268097556\n",
      "Train Loss at iteration 11379: 0.042230160438066\n",
      "Train Loss at iteration 11380: 0.04223011261596915\n",
      "Train Loss at iteration 11381: 0.04223006480180571\n",
      "Train Loss at iteration 11382: 0.04223001699557434\n",
      "Train Loss at iteration 11383: 0.042229969197273697\n",
      "Train Loss at iteration 11384: 0.04222992140690247\n",
      "Train Loss at iteration 11385: 0.04222987362445933\n",
      "Train Loss at iteration 11386: 0.042229825849942955\n",
      "Train Loss at iteration 11387: 0.042229778083352036\n",
      "Train Loss at iteration 11388: 0.04222973032468521\n",
      "Train Loss at iteration 11389: 0.04222968257394118\n",
      "Train Loss at iteration 11390: 0.04222963483111861\n",
      "Train Loss at iteration 11391: 0.042229587096216194\n",
      "Train Loss at iteration 11392: 0.042229539369232585\n",
      "Train Loss at iteration 11393: 0.042229491650166456\n",
      "Train Loss at iteration 11394: 0.0422294439390165\n",
      "Train Loss at iteration 11395: 0.042229396235781404\n",
      "Train Loss at iteration 11396: 0.042229348540459816\n",
      "Train Loss at iteration 11397: 0.04222930085305041\n",
      "Train Loss at iteration 11398: 0.04222925317355191\n",
      "Train Loss at iteration 11399: 0.04222920550196294\n",
      "Train Loss at iteration 11400: 0.042229157838282204\n",
      "Train Loss at iteration 11401: 0.04222911018250836\n",
      "Train Loss at iteration 11402: 0.04222906253464013\n",
      "Train Loss at iteration 11403: 0.04222901489467615\n",
      "Train Loss at iteration 11404: 0.04222896726261511\n",
      "Train Loss at iteration 11405: 0.04222891963845571\n",
      "Train Loss at iteration 11406: 0.042228872022196585\n",
      "Train Loss at iteration 11407: 0.04222882441383645\n",
      "Train Loss at iteration 11408: 0.04222877681337396\n",
      "Train Loss at iteration 11409: 0.04222872922080783\n",
      "Train Loss at iteration 11410: 0.04222868163613671\n",
      "Train Loss at iteration 11411: 0.04222863405935929\n",
      "Train Loss at iteration 11412: 0.04222858649047423\n",
      "Train Loss at iteration 11413: 0.042228538929480246\n",
      "Train Loss at iteration 11414: 0.042228491376376\n",
      "Train Loss at iteration 11415: 0.04222844383116018\n",
      "Train Loss at iteration 11416: 0.04222839629383145\n",
      "Train Loss at iteration 11417: 0.04222834876438852\n",
      "Train Loss at iteration 11418: 0.04222830124283004\n",
      "Train Loss at iteration 11419: 0.04222825372915471\n",
      "Train Loss at iteration 11420: 0.04222820622336121\n",
      "Train Loss at iteration 11421: 0.042228158725448224\n",
      "Train Loss at iteration 11422: 0.04222811123541444\n",
      "Train Loss at iteration 11423: 0.042228063753258514\n",
      "Train Loss at iteration 11424: 0.04222801627897915\n",
      "Train Loss at iteration 11425: 0.04222796881257504\n",
      "Train Loss at iteration 11426: 0.04222792135404486\n",
      "Train Loss at iteration 11427: 0.04222787390338729\n",
      "Train Loss at iteration 11428: 0.042227826460601005\n",
      "Train Loss at iteration 11429: 0.042227779025684704\n",
      "Train Loss at iteration 11430: 0.042227731598637054\n",
      "Train Loss at iteration 11431: 0.04222768417945678\n",
      "Train Loss at iteration 11432: 0.04222763676814252\n",
      "Train Loss at iteration 11433: 0.04222758936469298\n",
      "Train Loss at iteration 11434: 0.042227541969106844\n",
      "Train Loss at iteration 11435: 0.04222749458138279\n",
      "Train Loss at iteration 11436: 0.042227447201519525\n",
      "Train Loss at iteration 11437: 0.042227399829515705\n",
      "Train Loss at iteration 11438: 0.04222735246537004\n",
      "Train Loss at iteration 11439: 0.04222730510908121\n",
      "Train Loss at iteration 11440: 0.042227257760647896\n",
      "Train Loss at iteration 11441: 0.04222721042006879\n",
      "Train Loss at iteration 11442: 0.04222716308734259\n",
      "Train Loss at iteration 11443: 0.04222711576246795\n",
      "Train Loss at iteration 11444: 0.042227068445443584\n",
      "Train Loss at iteration 11445: 0.04222702113626818\n",
      "Train Loss at iteration 11446: 0.04222697383494042\n",
      "Train Loss at iteration 11447: 0.042226926541458994\n",
      "Train Loss at iteration 11448: 0.042226879255822594\n",
      "Train Loss at iteration 11449: 0.04222683197802989\n",
      "Train Loss at iteration 11450: 0.0422267847080796\n",
      "Train Loss at iteration 11451: 0.042226737445970386\n",
      "Train Loss at iteration 11452: 0.04222669019170096\n",
      "Train Loss at iteration 11453: 0.04222664294526999\n",
      "Train Loss at iteration 11454: 0.042226595706676195\n",
      "Train Loss at iteration 11455: 0.04222654847591824\n",
      "Train Loss at iteration 11456: 0.04222650125299482\n",
      "Train Loss at iteration 11457: 0.042226454037904634\n",
      "Train Loss at iteration 11458: 0.042226406830646344\n",
      "Train Loss at iteration 11459: 0.04222635963121869\n",
      "Train Loss at iteration 11460: 0.04222631243962033\n",
      "Train Loss at iteration 11461: 0.04222626525584996\n",
      "Train Loss at iteration 11462: 0.042226218079906265\n",
      "Train Loss at iteration 11463: 0.04222617091178796\n",
      "Train Loss at iteration 11464: 0.04222612375149372\n",
      "Train Loss at iteration 11465: 0.04222607659902224\n",
      "Train Loss at iteration 11466: 0.04222602945437221\n",
      "Train Loss at iteration 11467: 0.04222598231754232\n",
      "Train Loss at iteration 11468: 0.04222593518853129\n",
      "Train Loss at iteration 11469: 0.04222588806733777\n",
      "Train Loss at iteration 11470: 0.04222584095396049\n",
      "Train Loss at iteration 11471: 0.04222579384839812\n",
      "Train Loss at iteration 11472: 0.042225746750649364\n",
      "Train Loss at iteration 11473: 0.04222569966071292\n",
      "Train Loss at iteration 11474: 0.04222565257858748\n",
      "Train Loss at iteration 11475: 0.042225605504271735\n",
      "Train Loss at iteration 11476: 0.04222555843776437\n",
      "Train Loss at iteration 11477: 0.0422255113790641\n",
      "Train Loss at iteration 11478: 0.04222546432816962\n",
      "Train Loss at iteration 11479: 0.04222541728507961\n",
      "Train Loss at iteration 11480: 0.04222537024979276\n",
      "Train Loss at iteration 11481: 0.04222532322230779\n",
      "Train Loss at iteration 11482: 0.04222527620262338\n",
      "Train Loss at iteration 11483: 0.04222522919073822\n",
      "Train Loss at iteration 11484: 0.04222518218665104\n",
      "Train Loss at iteration 11485: 0.04222513519036051\n",
      "Train Loss at iteration 11486: 0.04222508820186532\n",
      "Train Loss at iteration 11487: 0.04222504122116418\n",
      "Train Loss at iteration 11488: 0.04222499424825581\n",
      "Train Loss at iteration 11489: 0.042224947283138865\n",
      "Train Loss at iteration 11490: 0.042224900325812054\n",
      "Train Loss at iteration 11491: 0.042224853376274095\n",
      "Train Loss at iteration 11492: 0.04222480643452367\n",
      "Train Loss at iteration 11493: 0.042224759500559476\n",
      "Train Loss at iteration 11494: 0.042224712574380235\n",
      "Train Loss at iteration 11495: 0.042224665655984615\n",
      "Train Loss at iteration 11496: 0.04222461874537133\n",
      "Train Loss at iteration 11497: 0.042224571842539076\n",
      "Train Loss at iteration 11498: 0.04222452494748656\n",
      "Train Loss at iteration 11499: 0.042224478060212484\n",
      "Train Loss at iteration 11500: 0.042224431180715546\n",
      "Train Loss at iteration 11501: 0.042224384308994424\n",
      "Train Loss at iteration 11502: 0.042224337445047846\n",
      "Train Loss at iteration 11503: 0.0422242905888745\n",
      "Train Loss at iteration 11504: 0.0422242437404731\n",
      "Train Loss at iteration 11505: 0.04222419689984233\n",
      "Train Loss at iteration 11506: 0.042224150066980895\n",
      "Train Loss at iteration 11507: 0.04222410324188751\n",
      "Train Loss at iteration 11508: 0.04222405642456087\n",
      "Train Loss at iteration 11509: 0.04222400961499967\n",
      "Train Loss at iteration 11510: 0.04222396281320262\n",
      "Train Loss at iteration 11511: 0.04222391601916841\n",
      "Train Loss at iteration 11512: 0.04222386923289577\n",
      "Train Loss at iteration 11513: 0.04222382245438338\n",
      "Train Loss at iteration 11514: 0.042223775683629935\n",
      "Train Loss at iteration 11515: 0.042223728920634175\n",
      "Train Loss at iteration 11516: 0.04222368216539479\n",
      "Train Loss at iteration 11517: 0.042223635417910445\n",
      "Train Loss at iteration 11518: 0.042223588678179894\n",
      "Train Loss at iteration 11519: 0.042223541946201815\n",
      "Train Loss at iteration 11520: 0.042223495221974924\n",
      "Train Loss at iteration 11521: 0.04222344850549793\n",
      "Train Loss at iteration 11522: 0.04222340179676952\n",
      "Train Loss at iteration 11523: 0.042223355095788416\n",
      "Train Loss at iteration 11524: 0.042223308402553304\n",
      "Train Loss at iteration 11525: 0.042223261717062914\n",
      "Train Loss at iteration 11526: 0.04222321503931594\n",
      "Train Loss at iteration 11527: 0.0422231683693111\n",
      "Train Loss at iteration 11528: 0.042223121707047076\n",
      "Train Loss at iteration 11529: 0.04222307505252259\n",
      "Train Loss at iteration 11530: 0.04222302840573634\n",
      "Train Loss at iteration 11531: 0.042222981766687054\n",
      "Train Loss at iteration 11532: 0.042222935135373424\n",
      "Train Loss at iteration 11533: 0.042222888511794156\n",
      "Train Loss at iteration 11534: 0.04222284189594795\n",
      "Train Loss at iteration 11535: 0.04222279528783355\n",
      "Train Loss at iteration 11536: 0.042222748687449616\n",
      "Train Loss at iteration 11537: 0.04222270209479489\n",
      "Train Loss at iteration 11538: 0.04222265550986806\n",
      "Train Loss at iteration 11539: 0.04222260893266787\n",
      "Train Loss at iteration 11540: 0.042222562363192974\n",
      "Train Loss at iteration 11541: 0.04222251580144213\n",
      "Train Loss at iteration 11542: 0.04222246924741402\n",
      "Train Loss at iteration 11543: 0.04222242270110737\n",
      "Train Loss at iteration 11544: 0.04222237616252089\n",
      "Train Loss at iteration 11545: 0.042222329631653274\n",
      "Train Loss at iteration 11546: 0.042222283108503234\n",
      "Train Loss at iteration 11547: 0.0422222365930695\n",
      "Train Loss at iteration 11548: 0.04222219008535076\n",
      "Train Loss at iteration 11549: 0.04222214358534574\n",
      "Train Loss at iteration 11550: 0.04222209709305316\n",
      "Train Loss at iteration 11551: 0.04222205060847171\n",
      "Train Loss at iteration 11552: 0.042222004131600106\n",
      "Train Loss at iteration 11553: 0.04222195766243707\n",
      "Train Loss at iteration 11554: 0.0422219112009813\n",
      "Train Loss at iteration 11555: 0.042221864747231524\n",
      "Train Loss at iteration 11556: 0.042221818301186445\n",
      "Train Loss at iteration 11557: 0.04222177186284479\n",
      "Train Loss at iteration 11558: 0.04222172543220525\n",
      "Train Loss at iteration 11559: 0.04222167900926655\n",
      "Train Loss at iteration 11560: 0.04222163259402741\n",
      "Train Loss at iteration 11561: 0.04222158618648654\n",
      "Train Loss at iteration 11562: 0.04222153978664264\n",
      "Train Loss at iteration 11563: 0.042221493394494436\n",
      "Train Loss at iteration 11564: 0.042221447010040644\n",
      "Train Loss at iteration 11565: 0.042221400633279976\n",
      "Train Loss at iteration 11566: 0.042221354264211156\n",
      "Train Loss at iteration 11567: 0.042221307902832864\n",
      "Train Loss at iteration 11568: 0.04222126154914386\n",
      "Train Loss at iteration 11569: 0.042221215203142835\n",
      "Train Loss at iteration 11570: 0.042221168864828525\n",
      "Train Loss at iteration 11571: 0.042221122534199615\n",
      "Train Loss at iteration 11572: 0.042221076211254836\n",
      "Train Loss at iteration 11573: 0.04222102989599292\n",
      "Train Loss at iteration 11574: 0.04222098358841255\n",
      "Train Loss at iteration 11575: 0.042220937288512485\n",
      "Train Loss at iteration 11576: 0.04222089099629141\n",
      "Train Loss at iteration 11577: 0.042220844711748036\n",
      "Train Loss at iteration 11578: 0.04222079843488112\n",
      "Train Loss at iteration 11579: 0.04222075216568935\n",
      "Train Loss at iteration 11580: 0.04222070590417144\n",
      "Train Loss at iteration 11581: 0.04222065965032613\n",
      "Train Loss at iteration 11582: 0.04222061340415212\n",
      "Train Loss at iteration 11583: 0.042220567165648135\n",
      "Train Loss at iteration 11584: 0.0422205209348129\n",
      "Train Loss at iteration 11585: 0.042220474711645126\n",
      "Train Loss at iteration 11586: 0.04222042849614352\n",
      "Train Loss at iteration 11587: 0.04222038228830683\n",
      "Train Loss at iteration 11588: 0.04222033608813377\n",
      "Train Loss at iteration 11589: 0.04222028989562304\n",
      "Train Loss at iteration 11590: 0.042220243710773375\n",
      "Train Loss at iteration 11591: 0.042220197533583484\n",
      "Train Loss at iteration 11592: 0.042220151364052105\n",
      "Train Loss at iteration 11593: 0.04222010520217794\n",
      "Train Loss at iteration 11594: 0.04222005904795973\n",
      "Train Loss at iteration 11595: 0.042220012901396174\n",
      "Train Loss at iteration 11596: 0.04221996676248602\n",
      "Train Loss at iteration 11597: 0.042219920631227974\n",
      "Train Loss at iteration 11598: 0.042219874507620755\n",
      "Train Loss at iteration 11599: 0.04221982839166308\n",
      "Train Loss at iteration 11600: 0.04221978228335369\n",
      "Train Loss at iteration 11601: 0.04221973618269129\n",
      "Train Loss at iteration 11602: 0.04221969008967462\n",
      "Train Loss at iteration 11603: 0.042219644004302385\n",
      "Train Loss at iteration 11604: 0.04221959792657332\n",
      "Train Loss at iteration 11605: 0.04221955185648615\n",
      "Train Loss at iteration 11606: 0.042219505794039595\n",
      "Train Loss at iteration 11607: 0.04221945973923238\n",
      "Train Loss at iteration 11608: 0.0422194136920632\n",
      "Train Loss at iteration 11609: 0.04221936765253083\n",
      "Train Loss at iteration 11610: 0.04221932162063397\n",
      "Train Loss at iteration 11611: 0.04221927559637134\n",
      "Train Loss at iteration 11612: 0.04221922957974168\n",
      "Train Loss at iteration 11613: 0.0422191835707437\n",
      "Train Loss at iteration 11614: 0.04221913756937613\n",
      "Train Loss at iteration 11615: 0.042219091575637704\n",
      "Train Loss at iteration 11616: 0.04221904558952713\n",
      "Train Loss at iteration 11617: 0.04221899961104314\n",
      "Train Loss at iteration 11618: 0.04221895364018448\n",
      "Train Loss at iteration 11619: 0.04221890767694986\n",
      "Train Loss at iteration 11620: 0.042218861721338\n",
      "Train Loss at iteration 11621: 0.04221881577334764\n",
      "Train Loss at iteration 11622: 0.04221876983297751\n",
      "Train Loss at iteration 11623: 0.04221872390022632\n",
      "Train Loss at iteration 11624: 0.042218677975092794\n",
      "Train Loss at iteration 11625: 0.0422186320575757\n",
      "Train Loss at iteration 11626: 0.04221858614767372\n",
      "Train Loss at iteration 11627: 0.04221854024538562\n",
      "Train Loss at iteration 11628: 0.042218494350710095\n",
      "Train Loss at iteration 11629: 0.0422184484636459\n",
      "Train Loss at iteration 11630: 0.04221840258419174\n",
      "Train Loss at iteration 11631: 0.04221835671234636\n",
      "Train Loss at iteration 11632: 0.04221831084810849\n",
      "Train Loss at iteration 11633: 0.042218264991476864\n",
      "Train Loss at iteration 11634: 0.04221821914245019\n",
      "Train Loss at iteration 11635: 0.04221817330102722\n",
      "Train Loss at iteration 11636: 0.04221812746720667\n",
      "Train Loss at iteration 11637: 0.042218081640987275\n",
      "Train Loss at iteration 11638: 0.04221803582236777\n",
      "Train Loss at iteration 11639: 0.04221799001134689\n",
      "Train Loss at iteration 11640: 0.04221794420792334\n",
      "Train Loss at iteration 11641: 0.04221789841209589\n",
      "Train Loss at iteration 11642: 0.04221785262386324\n",
      "Train Loss at iteration 11643: 0.04221780684322414\n",
      "Train Loss at iteration 11644: 0.04221776107017732\n",
      "Train Loss at iteration 11645: 0.0422177153047215\n",
      "Train Loss at iteration 11646: 0.04221766954685542\n",
      "Train Loss at iteration 11647: 0.04221762379657781\n",
      "Train Loss at iteration 11648: 0.04221757805388741\n",
      "Train Loss at iteration 11649: 0.04221753231878295\n",
      "Train Loss at iteration 11650: 0.04221748659126316\n",
      "Train Loss at iteration 11651: 0.042217440871326777\n",
      "Train Loss at iteration 11652: 0.04221739515897253\n",
      "Train Loss at iteration 11653: 0.04221734945419915\n",
      "Train Loss at iteration 11654: 0.04221730375700538\n",
      "Train Loss at iteration 11655: 0.04221725806738997\n",
      "Train Loss at iteration 11656: 0.04221721238535162\n",
      "Train Loss at iteration 11657: 0.042217166710889084\n",
      "Train Loss at iteration 11658: 0.04221712104400109\n",
      "Train Loss at iteration 11659: 0.04221707538468638\n",
      "Train Loss at iteration 11660: 0.042217029732943674\n",
      "Train Loss at iteration 11661: 0.042216984088771746\n",
      "Train Loss at iteration 11662: 0.042216938452169284\n",
      "Train Loss at iteration 11663: 0.04221689282313505\n",
      "Train Loss at iteration 11664: 0.04221684720166779\n",
      "Train Loss at iteration 11665: 0.042216801587766216\n",
      "Train Loss at iteration 11666: 0.042216755981429085\n",
      "Train Loss at iteration 11667: 0.04221671038265512\n",
      "Train Loss at iteration 11668: 0.042216664791443065\n",
      "Train Loss at iteration 11669: 0.04221661920779164\n",
      "Train Loss at iteration 11670: 0.04221657363169962\n",
      "Train Loss at iteration 11671: 0.042216528063165705\n",
      "Train Loss at iteration 11672: 0.04221648250218865\n",
      "Train Loss at iteration 11673: 0.04221643694876719\n",
      "Train Loss at iteration 11674: 0.04221639140290009\n",
      "Train Loss at iteration 11675: 0.04221634586458603\n",
      "Train Loss at iteration 11676: 0.0422163003338238\n",
      "Train Loss at iteration 11677: 0.04221625481061213\n",
      "Train Loss at iteration 11678: 0.04221620929494974\n",
      "Train Loss at iteration 11679: 0.04221616378683539\n",
      "Train Loss at iteration 11680: 0.0422161182862678\n",
      "Train Loss at iteration 11681: 0.042216072793245726\n",
      "Train Loss at iteration 11682: 0.042216027307767905\n",
      "Train Loss at iteration 11683: 0.04221598182983307\n",
      "Train Loss at iteration 11684: 0.04221593635943998\n",
      "Train Loss at iteration 11685: 0.042215890896587345\n",
      "Train Loss at iteration 11686: 0.042215845441273936\n",
      "Train Loss at iteration 11687: 0.04221579999349848\n",
      "Train Loss at iteration 11688: 0.04221575455325973\n",
      "Train Loss at iteration 11689: 0.04221570912055641\n",
      "Train Loss at iteration 11690: 0.042215663695387276\n",
      "Train Loss at iteration 11691: 0.04221561827775105\n",
      "Train Loss at iteration 11692: 0.0422155728676465\n",
      "Train Loss at iteration 11693: 0.04221552746507236\n",
      "Train Loss at iteration 11694: 0.042215482070027364\n",
      "Train Loss at iteration 11695: 0.042215436682510266\n",
      "Train Loss at iteration 11696: 0.04221539130251981\n",
      "Train Loss at iteration 11697: 0.042215345930054715\n",
      "Train Loss at iteration 11698: 0.04221530056511377\n",
      "Train Loss at iteration 11699: 0.04221525520769568\n",
      "Train Loss at iteration 11700: 0.0422152098577992\n",
      "Train Loss at iteration 11701: 0.042215164515423086\n",
      "Train Loss at iteration 11702: 0.042215119180566064\n",
      "Train Loss at iteration 11703: 0.0422150738532269\n",
      "Train Loss at iteration 11704: 0.04221502853340431\n",
      "Train Loss at iteration 11705: 0.04221498322109705\n",
      "Train Loss at iteration 11706: 0.04221493791630389\n",
      "Train Loss at iteration 11707: 0.04221489261902354\n",
      "Train Loss at iteration 11708: 0.04221484732925478\n",
      "Train Loss at iteration 11709: 0.04221480204699633\n",
      "Train Loss at iteration 11710: 0.04221475677224694\n",
      "Train Loss at iteration 11711: 0.04221471150500537\n",
      "Train Loss at iteration 11712: 0.04221466624527036\n",
      "Train Loss at iteration 11713: 0.042214620993040644\n",
      "Train Loss at iteration 11714: 0.04221457574831499\n",
      "Train Loss at iteration 11715: 0.04221453051109213\n",
      "Train Loss at iteration 11716: 0.04221448528137081\n",
      "Train Loss at iteration 11717: 0.042214440059149795\n",
      "Train Loss at iteration 11718: 0.04221439484442782\n",
      "Train Loss at iteration 11719: 0.042214349637203635\n",
      "Train Loss at iteration 11720: 0.04221430443747599\n",
      "Train Loss at iteration 11721: 0.04221425924524364\n",
      "Train Loss at iteration 11722: 0.04221421406050531\n",
      "Train Loss at iteration 11723: 0.04221416888325977\n",
      "Train Loss at iteration 11724: 0.04221412371350576\n",
      "Train Loss at iteration 11725: 0.04221407855124205\n",
      "Train Loss at iteration 11726: 0.042214033396467364\n",
      "Train Loss at iteration 11727: 0.04221398824918046\n",
      "Train Loss at iteration 11728: 0.042213943109380106\n",
      "Train Loss at iteration 11729: 0.04221389797706502\n",
      "Train Loss at iteration 11730: 0.042213852852233974\n",
      "Train Loss at iteration 11731: 0.04221380773488571\n",
      "Train Loss at iteration 11732: 0.042213762625019\n",
      "Train Loss at iteration 11733: 0.04221371752263255\n",
      "Train Loss at iteration 11734: 0.04221367242772515\n",
      "Train Loss at iteration 11735: 0.042213627340295545\n",
      "Train Loss at iteration 11736: 0.04221358226034248\n",
      "Train Loss at iteration 11737: 0.04221353718786471\n",
      "Train Loss at iteration 11738: 0.042213492122861\n",
      "Train Loss at iteration 11739: 0.04221344706533006\n",
      "Train Loss at iteration 11740: 0.04221340201527069\n",
      "Train Loss at iteration 11741: 0.04221335697268163\n",
      "Train Loss at iteration 11742: 0.04221331193756162\n",
      "Train Loss at iteration 11743: 0.042213266909909425\n",
      "Train Loss at iteration 11744: 0.04221322188972379\n",
      "Train Loss at iteration 11745: 0.04221317687700348\n",
      "Train Loss at iteration 11746: 0.04221313187174724\n",
      "Train Loss at iteration 11747: 0.04221308687395382\n",
      "Train Loss at iteration 11748: 0.04221304188362198\n",
      "Train Loss at iteration 11749: 0.04221299690075049\n",
      "Train Loss at iteration 11750: 0.04221295192533807\n",
      "Train Loss at iteration 11751: 0.042212906957383514\n",
      "Train Loss at iteration 11752: 0.042212861996885565\n",
      "Train Loss at iteration 11753: 0.04221281704384295\n",
      "Train Loss at iteration 11754: 0.042212772098254456\n",
      "Train Loss at iteration 11755: 0.04221272716011883\n",
      "Train Loss at iteration 11756: 0.04221268222943483\n",
      "Train Loss at iteration 11757: 0.04221263730620121\n",
      "Train Loss at iteration 11758: 0.04221259239041672\n",
      "Train Loss at iteration 11759: 0.04221254748208013\n",
      "Train Loss at iteration 11760: 0.04221250258119019\n",
      "Train Loss at iteration 11761: 0.042212457687745654\n",
      "Train Loss at iteration 11762: 0.04221241280174529\n",
      "Train Loss at iteration 11763: 0.04221236792318784\n",
      "Train Loss at iteration 11764: 0.04221232305207208\n",
      "Train Loss at iteration 11765: 0.04221227818839675\n",
      "Train Loss at iteration 11766: 0.04221223333216062\n",
      "Train Loss at iteration 11767: 0.04221218848336244\n",
      "Train Loss at iteration 11768: 0.04221214364200098\n",
      "Train Loss at iteration 11769: 0.042212098808074985\n",
      "Train Loss at iteration 11770: 0.04221205398158323\n",
      "Train Loss at iteration 11771: 0.042212009162524454\n",
      "Train Loss at iteration 11772: 0.04221196435089745\n",
      "Train Loss at iteration 11773: 0.042211919546700935\n",
      "Train Loss at iteration 11774: 0.042211874749933695\n",
      "Train Loss at iteration 11775: 0.0422118299605945\n",
      "Train Loss at iteration 11776: 0.04221178517868208\n",
      "Train Loss at iteration 11777: 0.04221174040419522\n",
      "Train Loss at iteration 11778: 0.04221169563713267\n",
      "Train Loss at iteration 11779: 0.04221165087749318\n",
      "Train Loss at iteration 11780: 0.042211606125275525\n",
      "Train Loss at iteration 11781: 0.04221156138047848\n",
      "Train Loss at iteration 11782: 0.042211516643100785\n",
      "Train Loss at iteration 11783: 0.0422114719131412\n",
      "Train Loss at iteration 11784: 0.04221142719059851\n",
      "Train Loss at iteration 11785: 0.042211382475471466\n",
      "Train Loss at iteration 11786: 0.042211337767758816\n",
      "Train Loss at iteration 11787: 0.042211293067459334\n",
      "Train Loss at iteration 11788: 0.04221124837457178\n",
      "Train Loss at iteration 11789: 0.04221120368909494\n",
      "Train Loss at iteration 11790: 0.04221115901102753\n",
      "Train Loss at iteration 11791: 0.042211114340368366\n",
      "Train Loss at iteration 11792: 0.04221106967711618\n",
      "Train Loss at iteration 11793: 0.042211025021269735\n",
      "Train Loss at iteration 11794: 0.04221098037282781\n",
      "Train Loss at iteration 11795: 0.042210935731789155\n",
      "Train Loss at iteration 11796: 0.04221089109815254\n",
      "Train Loss at iteration 11797: 0.042210846471916745\n",
      "Train Loss at iteration 11798: 0.042210801853080515\n",
      "Train Loss at iteration 11799: 0.04221075724164263\n",
      "Train Loss at iteration 11800: 0.04221071263760184\n",
      "Train Loss at iteration 11801: 0.04221066804095692\n",
      "Train Loss at iteration 11802: 0.04221062345170662\n",
      "Train Loss at iteration 11803: 0.04221057886984974\n",
      "Train Loss at iteration 11804: 0.04221053429538501\n",
      "Train Loss at iteration 11805: 0.04221048972831123\n",
      "Train Loss at iteration 11806: 0.04221044516862713\n",
      "Train Loss at iteration 11807: 0.04221040061633151\n",
      "Train Loss at iteration 11808: 0.04221035607142312\n",
      "Train Loss at iteration 11809: 0.042210311533900736\n",
      "Train Loss at iteration 11810: 0.04221026700376311\n",
      "Train Loss at iteration 11811: 0.042210222481009016\n",
      "Train Loss at iteration 11812: 0.042210177965637245\n",
      "Train Loss at iteration 11813: 0.04221013345764653\n",
      "Train Loss at iteration 11814: 0.042210088957035666\n",
      "Train Loss at iteration 11815: 0.0422100444638034\n",
      "Train Loss at iteration 11816: 0.042209999977948526\n",
      "Train Loss at iteration 11817: 0.042209955499469785\n",
      "Train Loss at iteration 11818: 0.04220991102836597\n",
      "Train Loss at iteration 11819: 0.04220986656463583\n",
      "Train Loss at iteration 11820: 0.04220982210827816\n",
      "Train Loss at iteration 11821: 0.04220977765929171\n",
      "Train Loss at iteration 11822: 0.042209733217675245\n",
      "Train Loss at iteration 11823: 0.042209688783427554\n",
      "Train Loss at iteration 11824: 0.0422096443565474\n",
      "Train Loss at iteration 11825: 0.04220959993703356\n",
      "Train Loss at iteration 11826: 0.042209555524884784\n",
      "Train Loss at iteration 11827: 0.04220951112009986\n",
      "Train Loss at iteration 11828: 0.04220946672267755\n",
      "Train Loss at iteration 11829: 0.04220942233261665\n",
      "Train Loss at iteration 11830: 0.042209377949915895\n",
      "Train Loss at iteration 11831: 0.042209333574574084\n",
      "Train Loss at iteration 11832: 0.042209289206589985\n",
      "Train Loss at iteration 11833: 0.04220924484596236\n",
      "Train Loss at iteration 11834: 0.042209200492689995\n",
      "Train Loss at iteration 11835: 0.04220915614677165\n",
      "Train Loss at iteration 11836: 0.04220911180820611\n",
      "Train Loss at iteration 11837: 0.042209067476992115\n",
      "Train Loss at iteration 11838: 0.042209023153128494\n",
      "Train Loss at iteration 11839: 0.04220897883661398\n",
      "Train Loss at iteration 11840: 0.04220893452744736\n",
      "Train Loss at iteration 11841: 0.0422088902256274\n",
      "Train Loss at iteration 11842: 0.0422088459311529\n",
      "Train Loss at iteration 11843: 0.042208801644022596\n",
      "Train Loss at iteration 11844: 0.042208757364235294\n",
      "Train Loss at iteration 11845: 0.042208713091789744\n",
      "Train Loss at iteration 11846: 0.042208668826684746\n",
      "Train Loss at iteration 11847: 0.04220862456891907\n",
      "Train Loss at iteration 11848: 0.04220858031849148\n",
      "Train Loss at iteration 11849: 0.04220853607540074\n",
      "Train Loss at iteration 11850: 0.04220849183964565\n",
      "Train Loss at iteration 11851: 0.04220844761122498\n",
      "Train Loss at iteration 11852: 0.04220840339013752\n",
      "Train Loss at iteration 11853: 0.04220835917638201\n",
      "Train Loss at iteration 11854: 0.04220831496995726\n",
      "Train Loss at iteration 11855: 0.042208270770862026\n",
      "Train Loss at iteration 11856: 0.042208226579095114\n",
      "Train Loss at iteration 11857: 0.042208182394655264\n",
      "Train Loss at iteration 11858: 0.04220813821754128\n",
      "Train Loss at iteration 11859: 0.042208094047751915\n",
      "Train Loss at iteration 11860: 0.04220804988528599\n",
      "Train Loss at iteration 11861: 0.04220800573014224\n",
      "Train Loss at iteration 11862: 0.042207961582319466\n",
      "Train Loss at iteration 11863: 0.04220791744181644\n",
      "Train Loss at iteration 11864: 0.042207873308631946\n",
      "Train Loss at iteration 11865: 0.04220782918276476\n",
      "Train Loss at iteration 11866: 0.042207785064213646\n",
      "Train Loss at iteration 11867: 0.042207740952977406\n",
      "Train Loss at iteration 11868: 0.042207696849054815\n",
      "Train Loss at iteration 11869: 0.04220765275244464\n",
      "Train Loss at iteration 11870: 0.04220760866314568\n",
      "Train Loss at iteration 11871: 0.0422075645811567\n",
      "Train Loss at iteration 11872: 0.04220752050647649\n",
      "Train Loss at iteration 11873: 0.04220747643910384\n",
      "Train Loss at iteration 11874: 0.04220743237903749\n",
      "Train Loss at iteration 11875: 0.04220738832627627\n",
      "Train Loss at iteration 11876: 0.04220734428081893\n",
      "Train Loss at iteration 11877: 0.04220730024266427\n",
      "Train Loss at iteration 11878: 0.04220725621181106\n",
      "Train Loss at iteration 11879: 0.042207212188258084\n",
      "Train Loss at iteration 11880: 0.04220716817200412\n",
      "Train Loss at iteration 11881: 0.04220712416304798\n",
      "Train Loss at iteration 11882: 0.04220708016138841\n",
      "Train Loss at iteration 11883: 0.042207036167024196\n",
      "Train Loss at iteration 11884: 0.04220699217995414\n",
      "Train Loss at iteration 11885: 0.04220694820017702\n",
      "Train Loss at iteration 11886: 0.042206904227691605\n",
      "Train Loss at iteration 11887: 0.04220686026249671\n",
      "Train Loss at iteration 11888: 0.04220681630459108\n",
      "Train Loss at iteration 11889: 0.04220677235397353\n",
      "Train Loss at iteration 11890: 0.04220672841064281\n",
      "Train Loss at iteration 11891: 0.042206684474597744\n",
      "Train Loss at iteration 11892: 0.04220664054583711\n",
      "Train Loss at iteration 11893: 0.04220659662435965\n",
      "Train Loss at iteration 11894: 0.04220655271016422\n",
      "Train Loss at iteration 11895: 0.04220650880324954\n",
      "Train Loss at iteration 11896: 0.042206464903614425\n",
      "Train Loss at iteration 11897: 0.04220642101125765\n",
      "Train Loss at iteration 11898: 0.04220637712617803\n",
      "Train Loss at iteration 11899: 0.04220633324837433\n",
      "Train Loss at iteration 11900: 0.04220628937784532\n",
      "Train Loss at iteration 11901: 0.04220624551458981\n",
      "Train Loss at iteration 11902: 0.04220620165860657\n",
      "Train Loss at iteration 11903: 0.04220615780989441\n",
      "Train Loss at iteration 11904: 0.04220611396845209\n",
      "Train Loss at iteration 11905: 0.04220607013427842\n",
      "Train Loss at iteration 11906: 0.042206026307372184\n",
      "Train Loss at iteration 11907: 0.042205982487732156\n",
      "Train Loss at iteration 11908: 0.04220593867535714\n",
      "Train Loss at iteration 11909: 0.042205894870245904\n",
      "Train Loss at iteration 11910: 0.04220585107239726\n",
      "Train Loss at iteration 11911: 0.04220580728180998\n",
      "Train Loss at iteration 11912: 0.042205763498482866\n",
      "Train Loss at iteration 11913: 0.042205719722414686\n",
      "Train Loss at iteration 11914: 0.04220567595360426\n",
      "Train Loss at iteration 11915: 0.042205632192050355\n",
      "Train Loss at iteration 11916: 0.04220558843775177\n",
      "Train Loss at iteration 11917: 0.042205544690707274\n",
      "Train Loss at iteration 11918: 0.042205500950915686\n",
      "Train Loss at iteration 11919: 0.04220545721837579\n",
      "Train Loss at iteration 11920: 0.04220541349308636\n",
      "Train Loss at iteration 11921: 0.042205369775046214\n",
      "Train Loss at iteration 11922: 0.04220532606425411\n",
      "Train Loss at iteration 11923: 0.04220528236070886\n",
      "Train Loss at iteration 11924: 0.04220523866440926\n",
      "Train Loss at iteration 11925: 0.0422051949753541\n",
      "Train Loss at iteration 11926: 0.04220515129354214\n",
      "Train Loss at iteration 11927: 0.042205107618972205\n",
      "Train Loss at iteration 11928: 0.04220506395164308\n",
      "Train Loss at iteration 11929: 0.042205020291553555\n",
      "Train Loss at iteration 11930: 0.04220497663870243\n",
      "Train Loss at iteration 11931: 0.04220493299308849\n",
      "Train Loss at iteration 11932: 0.04220488935471053\n",
      "Train Loss at iteration 11933: 0.042204845723567336\n",
      "Train Loss at iteration 11934: 0.0422048020996577\n",
      "Train Loss at iteration 11935: 0.042204758482980435\n",
      "Train Loss at iteration 11936: 0.042204714873534326\n",
      "Train Loss at iteration 11937: 0.04220467127131816\n",
      "Train Loss at iteration 11938: 0.042204627676330736\n",
      "Train Loss at iteration 11939: 0.04220458408857084\n",
      "Train Loss at iteration 11940: 0.04220454050803729\n",
      "Train Loss at iteration 11941: 0.042204496934728865\n",
      "Train Loss at iteration 11942: 0.04220445336864436\n",
      "Train Loss at iteration 11943: 0.04220440980978255\n",
      "Train Loss at iteration 11944: 0.04220436625814228\n",
      "Train Loss at iteration 11945: 0.04220432271372229\n",
      "Train Loss at iteration 11946: 0.04220427917652143\n",
      "Train Loss at iteration 11947: 0.042204235646538454\n",
      "Train Loss at iteration 11948: 0.04220419212377217\n",
      "Train Loss at iteration 11949: 0.04220414860822138\n",
      "Train Loss at iteration 11950: 0.04220410509988488\n",
      "Train Loss at iteration 11951: 0.04220406159876146\n",
      "Train Loss at iteration 11952: 0.04220401810484993\n",
      "Train Loss at iteration 11953: 0.04220397461814907\n",
      "Train Loss at iteration 11954: 0.042203931138657697\n",
      "Train Loss at iteration 11955: 0.04220388766637459\n",
      "Train Loss at iteration 11956: 0.04220384420129857\n",
      "Train Loss at iteration 11957: 0.042203800743428416\n",
      "Train Loss at iteration 11958: 0.042203757292762926\n",
      "Train Loss at iteration 11959: 0.042203713849300904\n",
      "Train Loss at iteration 11960: 0.042203670413041156\n",
      "Train Loss at iteration 11961: 0.042203626983982466\n",
      "Train Loss at iteration 11962: 0.04220358356212364\n",
      "Train Loss at iteration 11963: 0.04220354014746348\n",
      "Train Loss at iteration 11964: 0.04220349674000079\n",
      "Train Loss at iteration 11965: 0.04220345333973436\n",
      "Train Loss at iteration 11966: 0.04220340994666299\n",
      "Train Loss at iteration 11967: 0.04220336656078548\n",
      "Train Loss at iteration 11968: 0.042203323182100634\n",
      "Train Loss at iteration 11969: 0.04220327981060726\n",
      "Train Loss at iteration 11970: 0.04220323644630415\n",
      "Train Loss at iteration 11971: 0.0422031930891901\n",
      "Train Loss at iteration 11972: 0.04220314973926392\n",
      "Train Loss at iteration 11973: 0.042203106396524415\n",
      "Train Loss at iteration 11974: 0.042203063060970374\n",
      "Train Loss at iteration 11975: 0.04220301973260061\n",
      "Train Loss at iteration 11976: 0.042202976411413906\n",
      "Train Loss at iteration 11977: 0.0422029330974091\n",
      "Train Loss at iteration 11978: 0.04220288979058496\n",
      "Train Loss at iteration 11979: 0.042202846490940304\n",
      "Train Loss at iteration 11980: 0.04220280319847393\n",
      "Train Loss at iteration 11981: 0.04220275991318465\n",
      "Train Loss at iteration 11982: 0.042202716635071254\n",
      "Train Loss at iteration 11983: 0.04220267336413256\n",
      "Train Loss at iteration 11984: 0.04220263010036736\n",
      "Train Loss at iteration 11985: 0.04220258684377447\n",
      "Train Loss at iteration 11986: 0.042202543594352666\n",
      "Train Loss at iteration 11987: 0.04220250035210079\n",
      "Train Loss at iteration 11988: 0.042202457117017624\n",
      "Train Loss at iteration 11989: 0.042202413889101974\n",
      "Train Loss at iteration 11990: 0.04220237066835264\n",
      "Train Loss at iteration 11991: 0.04220232745476845\n",
      "Train Loss at iteration 11992: 0.04220228424834819\n",
      "Train Loss at iteration 11993: 0.042202241049090664\n",
      "Train Loss at iteration 11994: 0.0422021978569947\n",
      "Train Loss at iteration 11995: 0.042202154672059074\n",
      "Train Loss at iteration 11996: 0.0422021114942826\n",
      "Train Loss at iteration 11997: 0.042202068323664105\n",
      "Train Loss at iteration 11998: 0.04220202516020236\n",
      "Train Loss at iteration 11999: 0.042201982003896216\n",
      "Train Loss at iteration 12000: 0.04220193885474444\n",
      "Train Loss at iteration 12001: 0.042201895712745865\n",
      "Train Loss at iteration 12002: 0.042201852577899275\n",
      "Train Loss at iteration 12003: 0.04220180945020351\n",
      "Train Loss at iteration 12004: 0.042201766329657345\n",
      "Train Loss at iteration 12005: 0.04220172321625961\n",
      "Train Loss at iteration 12006: 0.042201680110009104\n",
      "Train Loss at iteration 12007: 0.042201637010904625\n",
      "Train Loss at iteration 12008: 0.04220159391894501\n",
      "Train Loss at iteration 12009: 0.042201550834129034\n",
      "Train Loss at iteration 12010: 0.042201507756455536\n",
      "Train Loss at iteration 12011: 0.042201464685923305\n",
      "Train Loss at iteration 12012: 0.042201421622531154\n",
      "Train Loss at iteration 12013: 0.0422013785662779\n",
      "Train Loss at iteration 12014: 0.042201335517162354\n",
      "Train Loss at iteration 12015: 0.042201292475183304\n",
      "Train Loss at iteration 12016: 0.042201249440339594\n",
      "Train Loss at iteration 12017: 0.042201206412630005\n",
      "Train Loss at iteration 12018: 0.04220116339205336\n",
      "Train Loss at iteration 12019: 0.04220112037860848\n",
      "Train Loss at iteration 12020: 0.04220107737229416\n",
      "Train Loss at iteration 12021: 0.04220103437310922\n",
      "Train Loss at iteration 12022: 0.04220099138105246\n",
      "Train Loss at iteration 12023: 0.0422009483961227\n",
      "Train Loss at iteration 12024: 0.04220090541831875\n",
      "Train Loss at iteration 12025: 0.04220086244763943\n",
      "Train Loss at iteration 12026: 0.04220081948408354\n",
      "Train Loss at iteration 12027: 0.0422007765276499\n",
      "Train Loss at iteration 12028: 0.04220073357833732\n",
      "Train Loss at iteration 12029: 0.04220069063614461\n",
      "Train Loss at iteration 12030: 0.04220064770107059\n",
      "Train Loss at iteration 12031: 0.04220060477311407\n",
      "Train Loss at iteration 12032: 0.04220056185227387\n",
      "Train Loss at iteration 12033: 0.04220051893854878\n",
      "Train Loss at iteration 12034: 0.04220047603193764\n",
      "Train Loss at iteration 12035: 0.04220043313243926\n",
      "Train Loss at iteration 12036: 0.042200390240052434\n",
      "Train Loss at iteration 12037: 0.04220034735477599\n",
      "Train Loss at iteration 12038: 0.04220030447660875\n",
      "Train Loss at iteration 12039: 0.04220026160554953\n",
      "Train Loss at iteration 12040: 0.042200218741597136\n",
      "Train Loss at iteration 12041: 0.042200175884750384\n",
      "Train Loss at iteration 12042: 0.04220013303500809\n",
      "Train Loss at iteration 12043: 0.04220009019236906\n",
      "Train Loss at iteration 12044: 0.042200047356832124\n",
      "Train Loss at iteration 12045: 0.0422000045283961\n",
      "Train Loss at iteration 12046: 0.04219996170705978\n",
      "Train Loss at iteration 12047: 0.04219991889282202\n",
      "Train Loss at iteration 12048: 0.0421998760856816\n",
      "Train Loss at iteration 12049: 0.04219983328563737\n",
      "Train Loss at iteration 12050: 0.04219979049268811\n",
      "Train Loss at iteration 12051: 0.042199747706832665\n",
      "Train Loss at iteration 12052: 0.04219970492806985\n",
      "Train Loss at iteration 12053: 0.042199662156398456\n",
      "Train Loss at iteration 12054: 0.04219961939181734\n",
      "Train Loss at iteration 12055: 0.042199576634325296\n",
      "Train Loss at iteration 12056: 0.04219953388392114\n",
      "Train Loss at iteration 12057: 0.04219949114060371\n",
      "Train Loss at iteration 12058: 0.0421994484043718\n",
      "Train Loss at iteration 12059: 0.042199405675224254\n",
      "Train Loss at iteration 12060: 0.04219936295315986\n",
      "Train Loss at iteration 12061: 0.04219932023817747\n",
      "Train Loss at iteration 12062: 0.0421992775302759\n",
      "Train Loss at iteration 12063: 0.04219923482945393\n",
      "Train Loss at iteration 12064: 0.04219919213571043\n",
      "Train Loss at iteration 12065: 0.042199149449044196\n",
      "Train Loss at iteration 12066: 0.04219910676945405\n",
      "Train Loss at iteration 12067: 0.042199064096938806\n",
      "Train Loss at iteration 12068: 0.0421990214314973\n",
      "Train Loss at iteration 12069: 0.04219897877312835\n",
      "Train Loss at iteration 12070: 0.04219893612183077\n",
      "Train Loss at iteration 12071: 0.042198893477603375\n",
      "Train Loss at iteration 12072: 0.042198850840444994\n",
      "Train Loss at iteration 12073: 0.04219880821035445\n",
      "Train Loss at iteration 12074: 0.04219876558733057\n",
      "Train Loss at iteration 12075: 0.04219872297137217\n",
      "Train Loss at iteration 12076: 0.04219868036247807\n",
      "Train Loss at iteration 12077: 0.042198637760647105\n",
      "Train Loss at iteration 12078: 0.04219859516587808\n",
      "Train Loss at iteration 12079: 0.04219855257816984\n",
      "Train Loss at iteration 12080: 0.04219850999752117\n",
      "Train Loss at iteration 12081: 0.042198467423930935\n",
      "Train Loss at iteration 12082: 0.04219842485739793\n",
      "Train Loss at iteration 12083: 0.042198382297921\n",
      "Train Loss at iteration 12084: 0.04219833974549894\n",
      "Train Loss at iteration 12085: 0.04219829720013061\n",
      "Train Loss at iteration 12086: 0.042198254661814816\n",
      "Train Loss at iteration 12087: 0.042198212130550375\n",
      "Train Loss at iteration 12088: 0.04219816960633613\n",
      "Train Loss at iteration 12089: 0.04219812708917088\n",
      "Train Loss at iteration 12090: 0.042198084579053476\n",
      "Train Loss at iteration 12091: 0.04219804207598274\n",
      "Train Loss at iteration 12092: 0.04219799957995748\n",
      "Train Loss at iteration 12093: 0.04219795709097655\n",
      "Train Loss at iteration 12094: 0.04219791460903873\n",
      "Train Loss at iteration 12095: 0.042197872134142896\n",
      "Train Loss at iteration 12096: 0.04219782966628785\n",
      "Train Loss at iteration 12097: 0.042197787205472403\n",
      "Train Loss at iteration 12098: 0.04219774475169541\n",
      "Train Loss at iteration 12099: 0.042197702304955696\n",
      "Train Loss at iteration 12100: 0.04219765986525208\n",
      "Train Loss at iteration 12101: 0.04219761743258338\n",
      "Train Loss at iteration 12102: 0.042197575006948436\n",
      "Train Loss at iteration 12103: 0.042197532588346076\n",
      "Train Loss at iteration 12104: 0.04219749017677512\n",
      "Train Loss at iteration 12105: 0.0421974477722344\n",
      "Train Loss at iteration 12106: 0.04219740537472274\n",
      "Train Loss at iteration 12107: 0.042197362984238984\n",
      "Train Loss at iteration 12108: 0.04219732060078194\n",
      "Train Loss at iteration 12109: 0.04219727822435045\n",
      "Train Loss at iteration 12110: 0.04219723585494334\n",
      "Train Loss at iteration 12111: 0.042197193492559446\n",
      "Train Loss at iteration 12112: 0.04219715113719759\n",
      "Train Loss at iteration 12113: 0.042197108788856595\n",
      "Train Loss at iteration 12114: 0.04219706644753529\n",
      "Train Loss at iteration 12115: 0.04219702411323253\n",
      "Train Loss at iteration 12116: 0.04219698178594713\n",
      "Train Loss at iteration 12117: 0.0421969394656779\n",
      "Train Loss at iteration 12118: 0.042196897152423696\n",
      "Train Loss at iteration 12119: 0.04219685484618334\n",
      "Train Loss at iteration 12120: 0.04219681254695568\n",
      "Train Loss at iteration 12121: 0.04219677025473953\n",
      "Train Loss at iteration 12122: 0.042196727969533714\n",
      "Train Loss at iteration 12123: 0.04219668569133708\n",
      "Train Loss at iteration 12124: 0.042196643420148464\n",
      "Train Loss at iteration 12125: 0.04219660115596667\n",
      "Train Loss at iteration 12126: 0.042196558898790544\n",
      "Train Loss at iteration 12127: 0.04219651664861894\n",
      "Train Loss at iteration 12128: 0.042196474405450664\n",
      "Train Loss at iteration 12129: 0.04219643216928456\n",
      "Train Loss at iteration 12130: 0.042196389940119455\n",
      "Train Loss at iteration 12131: 0.0421963477179542\n",
      "Train Loss at iteration 12132: 0.042196305502787614\n",
      "Train Loss at iteration 12133: 0.04219626329461851\n",
      "Train Loss at iteration 12134: 0.042196221093445764\n",
      "Train Loss at iteration 12135: 0.04219617889926818\n",
      "Train Loss at iteration 12136: 0.042196136712084616\n",
      "Train Loss at iteration 12137: 0.04219609453189388\n",
      "Train Loss at iteration 12138: 0.04219605235869482\n",
      "Train Loss at iteration 12139: 0.04219601019248627\n",
      "Train Loss at iteration 12140: 0.04219596803326707\n",
      "Train Loss at iteration 12141: 0.04219592588103604\n",
      "Train Loss at iteration 12142: 0.04219588373579203\n",
      "Train Loss at iteration 12143: 0.042195841597533874\n",
      "Train Loss at iteration 12144: 0.042195799466260404\n",
      "Train Loss at iteration 12145: 0.042195757341970466\n",
      "Train Loss at iteration 12146: 0.042195715224662866\n",
      "Train Loss at iteration 12147: 0.04219567311433648\n",
      "Train Loss at iteration 12148: 0.04219563101099012\n",
      "Train Loss at iteration 12149: 0.042195588914622634\n",
      "Train Loss at iteration 12150: 0.04219554682523284\n",
      "Train Loss at iteration 12151: 0.042195504742819606\n",
      "Train Loss at iteration 12152: 0.04219546266738174\n",
      "Train Loss at iteration 12153: 0.0421954205989181\n",
      "Train Loss at iteration 12154: 0.042195378537427525\n",
      "Train Loss at iteration 12155: 0.04219533648290884\n",
      "Train Loss at iteration 12156: 0.04219529443536088\n",
      "Train Loss at iteration 12157: 0.042195252394782494\n",
      "Train Loss at iteration 12158: 0.04219521036117252\n",
      "Train Loss at iteration 12159: 0.042195168334529784\n",
      "Train Loss at iteration 12160: 0.04219512631485315\n",
      "Train Loss at iteration 12161: 0.04219508430214144\n",
      "Train Loss at iteration 12162: 0.042195042296393495\n",
      "Train Loss at iteration 12163: 0.04219500029760814\n",
      "Train Loss at iteration 12164: 0.04219495830578426\n",
      "Train Loss at iteration 12165: 0.04219491632092065\n",
      "Train Loss at iteration 12166: 0.042194874343016164\n",
      "Train Loss at iteration 12167: 0.04219483237206965\n",
      "Train Loss at iteration 12168: 0.042194790408079934\n",
      "Train Loss at iteration 12169: 0.04219474845104587\n",
      "Train Loss at iteration 12170: 0.042194706500966295\n",
      "Train Loss at iteration 12171: 0.042194664557840045\n",
      "Train Loss at iteration 12172: 0.04219462262166598\n",
      "Train Loss at iteration 12173: 0.04219458069244292\n",
      "Train Loss at iteration 12174: 0.04219453877016971\n",
      "Train Loss at iteration 12175: 0.0421944968548452\n",
      "Train Loss at iteration 12176: 0.04219445494646823\n",
      "Train Loss at iteration 12177: 0.04219441304503764\n",
      "Train Loss at iteration 12178: 0.04219437115055228\n",
      "Train Loss at iteration 12179: 0.04219432926301098\n",
      "Train Loss at iteration 12180: 0.04219428738241259\n",
      "Train Loss at iteration 12181: 0.04219424550875595\n",
      "Train Loss at iteration 12182: 0.04219420364203991\n",
      "Train Loss at iteration 12183: 0.0421941617822633\n",
      "Train Loss at iteration 12184: 0.04219411992942498\n",
      "Train Loss at iteration 12185: 0.042194078083523795\n",
      "Train Loss at iteration 12186: 0.04219403624455857\n",
      "Train Loss at iteration 12187: 0.04219399441252816\n",
      "Train Loss at iteration 12188: 0.042193952587431416\n",
      "Train Loss at iteration 12189: 0.04219391076926718\n",
      "Train Loss at iteration 12190: 0.042193868958034286\n",
      "Train Loss at iteration 12191: 0.04219382715373159\n",
      "Train Loss at iteration 12192: 0.042193785356357934\n",
      "Train Loss at iteration 12193: 0.04219374356591216\n",
      "Train Loss at iteration 12194: 0.04219370178239313\n",
      "Train Loss at iteration 12195: 0.04219366000579966\n",
      "Train Loss at iteration 12196: 0.042193618236130624\n",
      "Train Loss at iteration 12197: 0.04219357647338486\n",
      "Train Loss at iteration 12198: 0.0421935347175612\n",
      "Train Loss at iteration 12199: 0.04219349296865852\n",
      "Train Loss at iteration 12200: 0.04219345122667565\n",
      "Train Loss at iteration 12201: 0.04219340949161142\n",
      "Train Loss at iteration 12202: 0.042193367763464704\n",
      "Train Loss at iteration 12203: 0.04219332604223434\n",
      "Train Loss at iteration 12204: 0.042193284327919175\n",
      "Train Loss at iteration 12205: 0.042193242620518046\n",
      "Train Loss at iteration 12206: 0.04219320092002983\n",
      "Train Loss at iteration 12207: 0.04219315922645335\n",
      "Train Loss at iteration 12208: 0.04219311753978746\n",
      "Train Loss at iteration 12209: 0.042193075860031\n",
      "Train Loss at iteration 12210: 0.04219303418718285\n",
      "Train Loss at iteration 12211: 0.042192992521241826\n",
      "Train Loss at iteration 12212: 0.04219295086220679\n",
      "Train Loss at iteration 12213: 0.042192909210076596\n",
      "Train Loss at iteration 12214: 0.04219286756485009\n",
      "Train Loss at iteration 12215: 0.04219282592652612\n",
      "Train Loss at iteration 12216: 0.042192784295103536\n",
      "Train Loss at iteration 12217: 0.042192742670581186\n",
      "Train Loss at iteration 12218: 0.04219270105295793\n",
      "Train Loss at iteration 12219: 0.04219265944223262\n",
      "Train Loss at iteration 12220: 0.04219261783840408\n",
      "Train Loss at iteration 12221: 0.042192576241471186\n",
      "Train Loss at iteration 12222: 0.042192534651432786\n",
      "Train Loss at iteration 12223: 0.04219249306828773\n",
      "Train Loss at iteration 12224: 0.042192451492034856\n",
      "Train Loss at iteration 12225: 0.04219240992267303\n",
      "Train Loss at iteration 12226: 0.04219236836020111\n",
      "Train Loss at iteration 12227: 0.04219232680461794\n",
      "Train Loss at iteration 12228: 0.04219228525592237\n",
      "Train Loss at iteration 12229: 0.04219224371411325\n",
      "Train Loss at iteration 12230: 0.04219220217918945\n",
      "Train Loss at iteration 12231: 0.042192160651149786\n",
      "Train Loss at iteration 12232: 0.04219211912999316\n",
      "Train Loss at iteration 12233: 0.042192077615718394\n",
      "Train Loss at iteration 12234: 0.04219203610832435\n",
      "Train Loss at iteration 12235: 0.04219199460780988\n",
      "Train Loss at iteration 12236: 0.042191953114173834\n",
      "Train Loss at iteration 12237: 0.04219191162741508\n",
      "Train Loss at iteration 12238: 0.04219187014753246\n",
      "Train Loss at iteration 12239: 0.042191828674524826\n",
      "Train Loss at iteration 12240: 0.04219178720839104\n",
      "Train Loss at iteration 12241: 0.04219174574912994\n",
      "Train Loss at iteration 12242: 0.04219170429674042\n",
      "Train Loss at iteration 12243: 0.04219166285122131\n",
      "Train Loss at iteration 12244: 0.04219162141257145\n",
      "Train Loss at iteration 12245: 0.04219157998078971\n",
      "Train Loss at iteration 12246: 0.04219153855587497\n",
      "Train Loss at iteration 12247: 0.042191497137826064\n",
      "Train Loss at iteration 12248: 0.04219145572664183\n",
      "Train Loss at iteration 12249: 0.042191414322321145\n",
      "Train Loss at iteration 12250: 0.04219137292486287\n",
      "Train Loss at iteration 12251: 0.042191331534265866\n",
      "Train Loss at iteration 12252: 0.04219129015052896\n",
      "Train Loss at iteration 12253: 0.04219124877365104\n",
      "Train Loss at iteration 12254: 0.042191207403630956\n",
      "Train Loss at iteration 12255: 0.04219116604046755\n",
      "Train Loss at iteration 12256: 0.0421911246841597\n",
      "Train Loss at iteration 12257: 0.04219108333470625\n",
      "Train Loss at iteration 12258: 0.042191041992106076\n",
      "Train Loss at iteration 12259: 0.04219100065635801\n",
      "Train Loss at iteration 12260: 0.042190959327460933\n",
      "Train Loss at iteration 12261: 0.0421909180054137\n",
      "Train Loss at iteration 12262: 0.042190876690215165\n",
      "Train Loss at iteration 12263: 0.042190835381864175\n",
      "Train Loss at iteration 12264: 0.04219079408035962\n",
      "Train Loss at iteration 12265: 0.042190752785700315\n",
      "Train Loss at iteration 12266: 0.042190711497885156\n",
      "Train Loss at iteration 12267: 0.04219067021691302\n",
      "Train Loss at iteration 12268: 0.04219062894278272\n",
      "Train Loss at iteration 12269: 0.04219058767549314\n",
      "Train Loss at iteration 12270: 0.042190546415043134\n",
      "Train Loss at iteration 12271: 0.042190505161431566\n",
      "Train Loss at iteration 12272: 0.0421904639146573\n",
      "Train Loss at iteration 12273: 0.042190422674719195\n",
      "Train Loss at iteration 12274: 0.04219038144161611\n",
      "Train Loss at iteration 12275: 0.04219034021534692\n",
      "Train Loss at iteration 12276: 0.04219029899591046\n",
      "Train Loss at iteration 12277: 0.04219025778330561\n",
      "Train Loss at iteration 12278: 0.04219021657753124\n",
      "Train Loss at iteration 12279: 0.04219017537858619\n",
      "Train Loss at iteration 12280: 0.04219013418646932\n",
      "Train Loss at iteration 12281: 0.04219009300117953\n",
      "Train Loss at iteration 12282: 0.04219005182271565\n",
      "Train Loss at iteration 12283: 0.042190010651076554\n",
      "Train Loss at iteration 12284: 0.042189969486261095\n",
      "Train Loss at iteration 12285: 0.04218992832826816\n",
      "Train Loss at iteration 12286: 0.04218988717709659\n",
      "Train Loss at iteration 12287: 0.042189846032745254\n",
      "Train Loss at iteration 12288: 0.04218980489521302\n",
      "Train Loss at iteration 12289: 0.04218976376449875\n",
      "Train Loss at iteration 12290: 0.042189722640601314\n",
      "Train Loss at iteration 12291: 0.04218968152351956\n",
      "Train Loss at iteration 12292: 0.04218964041325237\n",
      "Train Loss at iteration 12293: 0.04218959930979861\n",
      "Train Loss at iteration 12294: 0.04218955821315714\n",
      "Train Loss at iteration 12295: 0.04218951712332682\n",
      "Train Loss at iteration 12296: 0.04218947604030651\n",
      "Train Loss at iteration 12297: 0.0421894349640951\n",
      "Train Loss at iteration 12298: 0.04218939389469143\n",
      "Train Loss at iteration 12299: 0.042189352832094386\n",
      "Train Loss at iteration 12300: 0.042189311776302814\n",
      "Train Loss at iteration 12301: 0.0421892707273156\n",
      "Train Loss at iteration 12302: 0.042189229685131606\n",
      "Train Loss at iteration 12303: 0.04218918864974969\n",
      "Train Loss at iteration 12304: 0.04218914762116873\n",
      "Train Loss at iteration 12305: 0.04218910659938757\n",
      "Train Loss at iteration 12306: 0.04218906558440511\n",
      "Train Loss at iteration 12307: 0.04218902457622021\n",
      "Train Loss at iteration 12308: 0.04218898357483173\n",
      "Train Loss at iteration 12309: 0.04218894258023854\n",
      "Train Loss at iteration 12310: 0.042188901592439494\n",
      "Train Loss at iteration 12311: 0.04218886061143348\n",
      "Train Loss at iteration 12312: 0.04218881963721936\n",
      "Train Loss at iteration 12313: 0.042188778669796013\n",
      "Train Loss at iteration 12314: 0.04218873770916229\n",
      "Train Loss at iteration 12315: 0.04218869675531707\n",
      "Train Loss at iteration 12316: 0.042188655808259215\n",
      "Train Loss at iteration 12317: 0.042188614867987605\n",
      "Train Loss at iteration 12318: 0.04218857393450111\n",
      "Train Loss at iteration 12319: 0.042188533007798595\n",
      "Train Loss at iteration 12320: 0.04218849208787893\n",
      "Train Loss at iteration 12321: 0.04218845117474098\n",
      "Train Loss at iteration 12322: 0.04218841026838363\n",
      "Train Loss at iteration 12323: 0.04218836936880573\n",
      "Train Loss at iteration 12324: 0.042188328476006176\n",
      "Train Loss at iteration 12325: 0.042188287589983824\n",
      "Train Loss at iteration 12326: 0.04218824671073754\n",
      "Train Loss at iteration 12327: 0.04218820583826621\n",
      "Train Loss at iteration 12328: 0.042188164972568706\n",
      "Train Loss at iteration 12329: 0.04218812411364387\n",
      "Train Loss at iteration 12330: 0.04218808326149062\n",
      "Train Loss at iteration 12331: 0.04218804241610779\n",
      "Train Loss at iteration 12332: 0.04218800157749428\n",
      "Train Loss at iteration 12333: 0.04218796074564895\n",
      "Train Loss at iteration 12334: 0.042187919920570666\n",
      "Train Loss at iteration 12335: 0.04218787910225831\n",
      "Train Loss at iteration 12336: 0.04218783829071075\n",
      "Train Loss at iteration 12337: 0.04218779748592686\n",
      "Train Loss at iteration 12338: 0.04218775668790553\n",
      "Train Loss at iteration 12339: 0.042187715896645626\n",
      "Train Loss at iteration 12340: 0.04218767511214599\n",
      "Train Loss at iteration 12341: 0.04218763433440554\n",
      "Train Loss at iteration 12342: 0.04218759356342313\n",
      "Train Loss at iteration 12343: 0.042187552799197635\n",
      "Train Loss at iteration 12344: 0.042187512041727916\n",
      "Train Loss at iteration 12345: 0.042187471291012905\n",
      "Train Loss at iteration 12346: 0.04218743054705141\n",
      "Train Loss at iteration 12347: 0.04218738980984234\n",
      "Train Loss at iteration 12348: 0.04218734907938455\n",
      "Train Loss at iteration 12349: 0.042187308355676946\n",
      "Train Loss at iteration 12350: 0.04218726763871838\n",
      "Train Loss at iteration 12351: 0.04218722692850773\n",
      "Train Loss at iteration 12352: 0.04218718622504389\n",
      "Train Loss at iteration 12353: 0.04218714552832572\n",
      "Train Loss at iteration 12354: 0.0421871048383521\n",
      "Train Loss at iteration 12355: 0.04218706415512189\n",
      "Train Loss at iteration 12356: 0.04218702347863401\n",
      "Train Loss at iteration 12357: 0.0421869828088873\n",
      "Train Loss at iteration 12358: 0.042186942145880646\n",
      "Train Loss at iteration 12359: 0.04218690148961294\n",
      "Train Loss at iteration 12360: 0.04218686084008304\n",
      "Train Loss at iteration 12361: 0.04218682019728983\n",
      "Train Loss at iteration 12362: 0.04218677956123219\n",
      "Train Loss at iteration 12363: 0.04218673893190901\n",
      "Train Loss at iteration 12364: 0.04218669830931916\n",
      "Train Loss at iteration 12365: 0.0421866576934615\n",
      "Train Loss at iteration 12366: 0.04218661708433493\n",
      "Train Loss at iteration 12367: 0.04218657648193834\n",
      "Train Loss at iteration 12368: 0.04218653588627058\n",
      "Train Loss at iteration 12369: 0.04218649529733055\n",
      "Train Loss at iteration 12370: 0.04218645471511711\n",
      "Train Loss at iteration 12371: 0.04218641413962917\n",
      "Train Loss at iteration 12372: 0.04218637357086558\n",
      "Train Loss at iteration 12373: 0.042186333008825246\n",
      "Train Loss at iteration 12374: 0.04218629245350703\n",
      "Train Loss at iteration 12375: 0.04218625190490982\n",
      "Train Loss at iteration 12376: 0.042186211363032494\n",
      "Train Loss at iteration 12377: 0.04218617082787394\n",
      "Train Loss at iteration 12378: 0.042186130299433036\n",
      "Train Loss at iteration 12379: 0.04218608977770865\n",
      "Train Loss at iteration 12380: 0.04218604926269968\n",
      "Train Loss at iteration 12381: 0.042186008754405\n",
      "Train Loss at iteration 12382: 0.0421859682528235\n",
      "Train Loss at iteration 12383: 0.04218592775795405\n",
      "Train Loss at iteration 12384: 0.04218588726979556\n",
      "Train Loss at iteration 12385: 0.042185846788346866\n",
      "Train Loss at iteration 12386: 0.04218580631360688\n",
      "Train Loss at iteration 12387: 0.042185765845574506\n",
      "Train Loss at iteration 12388: 0.042185725384248585\n",
      "Train Loss at iteration 12389: 0.04218568492962801\n",
      "Train Loss at iteration 12390: 0.042185644481711686\n",
      "Train Loss at iteration 12391: 0.042185604040498495\n",
      "Train Loss at iteration 12392: 0.042185563605987283\n",
      "Train Loss at iteration 12393: 0.042185523178176976\n",
      "Train Loss at iteration 12394: 0.04218548275706644\n",
      "Train Loss at iteration 12395: 0.04218544234265457\n",
      "Train Loss at iteration 12396: 0.04218540193494023\n",
      "Train Loss at iteration 12397: 0.04218536153392232\n",
      "Train Loss at iteration 12398: 0.04218532113959972\n",
      "Train Loss at iteration 12399: 0.042185280751971324\n",
      "Train Loss at iteration 12400: 0.042185240371036006\n",
      "Train Loss at iteration 12401: 0.04218519999679266\n",
      "Train Loss at iteration 12402: 0.042185159629240174\n",
      "Train Loss at iteration 12403: 0.04218511926837743\n",
      "Train Loss at iteration 12404: 0.042185078914203304\n",
      "Train Loss at iteration 12405: 0.042185038566716695\n",
      "Train Loss at iteration 12406: 0.04218499822591649\n",
      "Train Loss at iteration 12407: 0.042184957891801565\n",
      "Train Loss at iteration 12408: 0.042184917564370816\n",
      "Train Loss at iteration 12409: 0.04218487724362313\n",
      "Train Loss at iteration 12410: 0.042184836929557386\n",
      "Train Loss at iteration 12411: 0.042184796622172484\n",
      "Train Loss at iteration 12412: 0.04218475632146731\n",
      "Train Loss at iteration 12413: 0.04218471602744073\n",
      "Train Loss at iteration 12414: 0.042184675740091666\n",
      "Train Loss at iteration 12415: 0.04218463545941898\n",
      "Train Loss at iteration 12416: 0.042184595185421565\n",
      "Train Loss at iteration 12417: 0.04218455491809834\n",
      "Train Loss at iteration 12418: 0.042184514657448144\n",
      "Train Loss at iteration 12419: 0.0421844744034699\n",
      "Train Loss at iteration 12420: 0.04218443415616248\n",
      "Train Loss at iteration 12421: 0.04218439391552478\n",
      "Train Loss at iteration 12422: 0.0421843536815557\n",
      "Train Loss at iteration 12423: 0.04218431345425412\n",
      "Train Loss at iteration 12424: 0.042184273233618924\n",
      "Train Loss at iteration 12425: 0.04218423301964901\n",
      "Train Loss at iteration 12426: 0.042184192812343264\n",
      "Train Loss at iteration 12427: 0.042184152611700575\n",
      "Train Loss at iteration 12428: 0.042184112417719855\n",
      "Train Loss at iteration 12429: 0.04218407223039996\n",
      "Train Loss at iteration 12430: 0.04218403204973981\n",
      "Train Loss at iteration 12431: 0.042183991875738285\n",
      "Train Loss at iteration 12432: 0.04218395170839426\n",
      "Train Loss at iteration 12433: 0.042183911547706655\n",
      "Train Loss at iteration 12434: 0.04218387139367436\n",
      "Train Loss at iteration 12435: 0.042183831246296245\n",
      "Train Loss at iteration 12436: 0.04218379110557121\n",
      "Train Loss at iteration 12437: 0.042183750971498166\n",
      "Train Loss at iteration 12438: 0.04218371084407599\n",
      "Train Loss at iteration 12439: 0.042183670723303555\n",
      "Train Loss at iteration 12440: 0.042183630609179804\n",
      "Train Loss at iteration 12441: 0.042183590501703574\n",
      "Train Loss at iteration 12442: 0.0421835504008738\n",
      "Train Loss at iteration 12443: 0.04218351030668935\n",
      "Train Loss at iteration 12444: 0.04218347021914914\n",
      "Train Loss at iteration 12445: 0.04218343013825205\n",
      "Train Loss at iteration 12446: 0.042183390063996976\n",
      "Train Loss at iteration 12447: 0.04218334999638282\n",
      "Train Loss at iteration 12448: 0.04218330993540846\n",
      "Train Loss at iteration 12449: 0.042183269881072794\n",
      "Train Loss at iteration 12450: 0.04218322983337474\n",
      "Train Loss at iteration 12451: 0.042183189792313155\n",
      "Train Loss at iteration 12452: 0.042183149757886965\n",
      "Train Loss at iteration 12453: 0.042183109730095064\n",
      "Train Loss at iteration 12454: 0.04218306970893634\n",
      "Train Loss at iteration 12455: 0.042183029694409666\n",
      "Train Loss at iteration 12456: 0.042182989686513975\n",
      "Train Loss at iteration 12457: 0.04218294968524814\n",
      "Train Loss at iteration 12458: 0.04218290969061107\n",
      "Train Loss at iteration 12459: 0.042182869702601676\n",
      "Train Loss at iteration 12460: 0.0421828297212188\n",
      "Train Loss at iteration 12461: 0.0421827897464614\n",
      "Train Loss at iteration 12462: 0.04218274977832833\n",
      "Train Loss at iteration 12463: 0.042182709816818514\n",
      "Train Loss at iteration 12464: 0.042182669861930824\n",
      "Train Loss at iteration 12465: 0.0421826299136642\n",
      "Train Loss at iteration 12466: 0.0421825899720175\n",
      "Train Loss at iteration 12467: 0.04218255003698964\n",
      "Train Loss at iteration 12468: 0.042182510108579506\n",
      "Train Loss at iteration 12469: 0.042182470186786\n",
      "Train Loss at iteration 12470: 0.04218243027160805\n",
      "Train Loss at iteration 12471: 0.0421823903630445\n",
      "Train Loss at iteration 12472: 0.042182350461094295\n",
      "Train Loss at iteration 12473: 0.04218231056575631\n",
      "Train Loss at iteration 12474: 0.04218227067702946\n",
      "Train Loss at iteration 12475: 0.04218223079491264\n",
      "Train Loss at iteration 12476: 0.04218219091940472\n",
      "Train Loss at iteration 12477: 0.04218215105050466\n",
      "Train Loss at iteration 12478: 0.0421821111882113\n",
      "Train Loss at iteration 12479: 0.04218207133252357\n",
      "Train Loss at iteration 12480: 0.04218203148344038\n",
      "Train Loss at iteration 12481: 0.042181991640960606\n",
      "Train Loss at iteration 12482: 0.04218195180508317\n",
      "Train Loss at iteration 12483: 0.04218191197580695\n",
      "Train Loss at iteration 12484: 0.042181872153130864\n",
      "Train Loss at iteration 12485: 0.042181832337053825\n",
      "Train Loss at iteration 12486: 0.042181792527574695\n",
      "Train Loss at iteration 12487: 0.04218175272469241\n",
      "Train Loss at iteration 12488: 0.04218171292840587\n",
      "Train Loss at iteration 12489: 0.04218167313871396\n",
      "Train Loss at iteration 12490: 0.04218163335561558\n",
      "Train Loss at iteration 12491: 0.04218159357910966\n",
      "Train Loss at iteration 12492: 0.04218155380919509\n",
      "Train Loss at iteration 12493: 0.04218151404587076\n",
      "Train Loss at iteration 12494: 0.04218147428913559\n",
      "Train Loss at iteration 12495: 0.04218143453898847\n",
      "Train Loss at iteration 12496: 0.04218139479542832\n",
      "Train Loss at iteration 12497: 0.042181355058454016\n",
      "Train Loss at iteration 12498: 0.042181315328064475\n",
      "Train Loss at iteration 12499: 0.04218127560425862\n",
      "Train Loss at iteration 12500: 0.042181235887035325\n",
      "Train Loss at iteration 12501: 0.04218119617639352\n",
      "Train Loss at iteration 12502: 0.04218115647233209\n",
      "Train Loss at iteration 12503: 0.04218111677484996\n",
      "Train Loss at iteration 12504: 0.042181077083946014\n",
      "Train Loss at iteration 12505: 0.04218103739961916\n",
      "Train Loss at iteration 12506: 0.042180997721868334\n",
      "Train Loss at iteration 12507: 0.0421809580506924\n",
      "Train Loss at iteration 12508: 0.04218091838609027\n",
      "Train Loss at iteration 12509: 0.042180878728060874\n",
      "Train Loss at iteration 12510: 0.042180839076603104\n",
      "Train Loss at iteration 12511: 0.042180799431715855\n",
      "Train Loss at iteration 12512: 0.042180759793398054\n",
      "Train Loss at iteration 12513: 0.0421807201616486\n",
      "Train Loss at iteration 12514: 0.0421806805364664\n",
      "Train Loss at iteration 12515: 0.04218064091785036\n",
      "Train Loss at iteration 12516: 0.04218060130579938\n",
      "Train Loss at iteration 12517: 0.04218056170031237\n",
      "Train Loss at iteration 12518: 0.04218052210138824\n",
      "Train Loss at iteration 12519: 0.04218048250902591\n",
      "Train Loss at iteration 12520: 0.042180442923224266\n",
      "Train Loss at iteration 12521: 0.04218040334398224\n",
      "Train Loss at iteration 12522: 0.04218036377129872\n",
      "Train Loss at iteration 12523: 0.04218032420517262\n",
      "Train Loss at iteration 12524: 0.04218028464560284\n",
      "Train Loss at iteration 12525: 0.04218024509258832\n",
      "Train Loss at iteration 12526: 0.042180205546127926\n",
      "Train Loss at iteration 12527: 0.042180166006220605\n",
      "Train Loss at iteration 12528: 0.042180126472865245\n",
      "Train Loss at iteration 12529: 0.04218008694606076\n",
      "Train Loss at iteration 12530: 0.042180047425806055\n",
      "Train Loss at iteration 12531: 0.042180007912100054\n",
      "Train Loss at iteration 12532: 0.042179968404941655\n",
      "Train Loss at iteration 12533: 0.04217992890432978\n",
      "Train Loss at iteration 12534: 0.042179889410263315\n",
      "Train Loss at iteration 12535: 0.042179849922741194\n",
      "Train Loss at iteration 12536: 0.042179810441762326\n",
      "Train Loss at iteration 12537: 0.04217977096732561\n",
      "Train Loss at iteration 12538: 0.04217973149942996\n",
      "Train Loss at iteration 12539: 0.04217969203807429\n",
      "Train Loss at iteration 12540: 0.04217965258325753\n",
      "Train Loss at iteration 12541: 0.04217961313497857\n",
      "Train Loss at iteration 12542: 0.04217957369323631\n",
      "Train Loss at iteration 12543: 0.042179534258029686\n",
      "Train Loss at iteration 12544: 0.04217949482935761\n",
      "Train Loss at iteration 12545: 0.042179455407218985\n",
      "Train Loss at iteration 12546: 0.04217941599161273\n",
      "Train Loss at iteration 12547: 0.042179376582537745\n",
      "Train Loss at iteration 12548: 0.042179337179992955\n",
      "Train Loss at iteration 12549: 0.04217929778397726\n",
      "Train Loss at iteration 12550: 0.042179258394489605\n",
      "Train Loss at iteration 12551: 0.04217921901152886\n",
      "Train Loss at iteration 12552: 0.042179179635093975\n",
      "Train Loss at iteration 12553: 0.042179140265183844\n",
      "Train Loss at iteration 12554: 0.0421791009017974\n",
      "Train Loss at iteration 12555: 0.04217906154493352\n",
      "Train Loss at iteration 12556: 0.04217902219459116\n",
      "Train Loss at iteration 12557: 0.04217898285076921\n",
      "Train Loss at iteration 12558: 0.0421789435134666\n",
      "Train Loss at iteration 12559: 0.04217890418268223\n",
      "Train Loss at iteration 12560: 0.04217886485841504\n",
      "Train Loss at iteration 12561: 0.0421788255406639\n",
      "Train Loss at iteration 12562: 0.04217878622942777\n",
      "Train Loss at iteration 12563: 0.04217874692470554\n",
      "Train Loss at iteration 12564: 0.04217870762649614\n",
      "Train Loss at iteration 12565: 0.04217866833479848\n",
      "Train Loss at iteration 12566: 0.04217862904961149\n",
      "Train Loss at iteration 12567: 0.04217858977093406\n",
      "Train Loss at iteration 12568: 0.04217855049876512\n",
      "Train Loss at iteration 12569: 0.042178511233103615\n",
      "Train Loss at iteration 12570: 0.0421784719739484\n",
      "Train Loss at iteration 12571: 0.042178432721298446\n",
      "Train Loss at iteration 12572: 0.04217839347515265\n",
      "Train Loss at iteration 12573: 0.04217835423550993\n",
      "Train Loss at iteration 12574: 0.04217831500236921\n",
      "Train Loss at iteration 12575: 0.04217827577572939\n",
      "Train Loss at iteration 12576: 0.04217823655558941\n",
      "Train Loss at iteration 12577: 0.042178197341948186\n",
      "Train Loss at iteration 12578: 0.04217815813480462\n",
      "Train Loss at iteration 12579: 0.04217811893415764\n",
      "Train Loss at iteration 12580: 0.04217807974000619\n",
      "Train Loss at iteration 12581: 0.04217804055234913\n",
      "Train Loss at iteration 12582: 0.04217800137118543\n",
      "Train Loss at iteration 12583: 0.04217796219651399\n",
      "Train Loss at iteration 12584: 0.04217792302833374\n",
      "Train Loss at iteration 12585: 0.042177883866643597\n",
      "Train Loss at iteration 12586: 0.04217784471144246\n",
      "Train Loss at iteration 12587: 0.042177805562729286\n",
      "Train Loss at iteration 12588: 0.04217776642050297\n",
      "Train Loss at iteration 12589: 0.042177727284762434\n",
      "Train Loss at iteration 12590: 0.0421776881555066\n",
      "Train Loss at iteration 12591: 0.042177649032734396\n",
      "Train Loss at iteration 12592: 0.04217760991644474\n",
      "Train Loss at iteration 12593: 0.042177570806636554\n",
      "Train Loss at iteration 12594: 0.042177531703308756\n",
      "Train Loss at iteration 12595: 0.042177492606460275\n",
      "Train Loss at iteration 12596: 0.042177453516090015\n",
      "Train Loss at iteration 12597: 0.04217741443219692\n",
      "Train Loss at iteration 12598: 0.04217737535477989\n",
      "Train Loss at iteration 12599: 0.04217733628383787\n",
      "Train Loss at iteration 12600: 0.04217729721936978\n",
      "Train Loss at iteration 12601: 0.04217725816137453\n",
      "Train Loss at iteration 12602: 0.04217721910985104\n",
      "Train Loss at iteration 12603: 0.04217718006479825\n",
      "Train Loss at iteration 12604: 0.04217714102621508\n",
      "Train Loss at iteration 12605: 0.042177101994100435\n",
      "Train Loss at iteration 12606: 0.042177062968453256\n",
      "Train Loss at iteration 12607: 0.04217702394927245\n",
      "Train Loss at iteration 12608: 0.04217698493655698\n",
      "Train Loss at iteration 12609: 0.04217694593030574\n",
      "Train Loss at iteration 12610: 0.042176906930517635\n",
      "Train Loss at iteration 12611: 0.04217686793719162\n",
      "Train Loss at iteration 12612: 0.04217682895032662\n",
      "Train Loss at iteration 12613: 0.04217678996992156\n",
      "Train Loss at iteration 12614: 0.04217675099597536\n",
      "Train Loss at iteration 12615: 0.04217671202848691\n",
      "Train Loss at iteration 12616: 0.0421766730674552\n",
      "Train Loss at iteration 12617: 0.04217663411287911\n",
      "Train Loss at iteration 12618: 0.04217659516475759\n",
      "Train Loss at iteration 12619: 0.04217655622308955\n",
      "Train Loss at iteration 12620: 0.04217651728787392\n",
      "Train Loss at iteration 12621: 0.042176478359109634\n",
      "Train Loss at iteration 12622: 0.04217643943679561\n",
      "Train Loss at iteration 12623: 0.042176400520930774\n",
      "Train Loss at iteration 12624: 0.042176361611514064\n",
      "Train Loss at iteration 12625: 0.04217632270854439\n",
      "Train Loss at iteration 12626: 0.042176283812020696\n",
      "Train Loss at iteration 12627: 0.042176244921941906\n",
      "Train Loss at iteration 12628: 0.04217620603830693\n",
      "Train Loss at iteration 12629: 0.042176167161114726\n",
      "Train Loss at iteration 12630: 0.04217612829036419\n",
      "Train Loss at iteration 12631: 0.042176089426054275\n",
      "Train Loss at iteration 12632: 0.042176050568183895\n",
      "Train Loss at iteration 12633: 0.042176011716752004\n",
      "Train Loss at iteration 12634: 0.04217597287175749\n",
      "Train Loss at iteration 12635: 0.04217593403319932\n",
      "Train Loss at iteration 12636: 0.0421758952010764\n",
      "Train Loss at iteration 12637: 0.042175856375387656\n",
      "Train Loss at iteration 12638: 0.042175817556132034\n",
      "Train Loss at iteration 12639: 0.042175778743308454\n",
      "Train Loss at iteration 12640: 0.042175739936915854\n",
      "Train Loss at iteration 12641: 0.042175701136953146\n",
      "Train Loss at iteration 12642: 0.04217566234341928\n",
      "Train Loss at iteration 12643: 0.042175623556313185\n",
      "Train Loss at iteration 12644: 0.042175584775633775\n",
      "Train Loss at iteration 12645: 0.04217554600138\n",
      "Train Loss at iteration 12646: 0.04217550723355078\n",
      "Train Loss at iteration 12647: 0.04217546847214503\n",
      "Train Loss at iteration 12648: 0.042175429717161726\n",
      "Train Loss at iteration 12649: 0.042175390968599756\n",
      "Train Loss at iteration 12650: 0.042175352226458074\n",
      "Train Loss at iteration 12651: 0.04217531349073562\n",
      "Train Loss at iteration 12652: 0.04217527476143129\n",
      "Train Loss at iteration 12653: 0.04217523603854405\n",
      "Train Loss at iteration 12654: 0.04217519732207282\n",
      "Train Loss at iteration 12655: 0.042175158612016525\n",
      "Train Loss at iteration 12656: 0.042175119908374116\n",
      "Train Loss at iteration 12657: 0.042175081211144515\n",
      "Train Loss at iteration 12658: 0.042175042520326646\n",
      "Train Loss at iteration 12659: 0.042175003835919475\n",
      "Train Loss at iteration 12660: 0.04217496515792189\n",
      "Train Loss at iteration 12661: 0.04217492648633286\n",
      "Train Loss at iteration 12662: 0.0421748878211513\n",
      "Train Loss at iteration 12663: 0.04217484916237617\n",
      "Train Loss at iteration 12664: 0.04217481051000636\n",
      "Train Loss at iteration 12665: 0.04217477186404082\n",
      "Train Loss at iteration 12666: 0.04217473322447852\n",
      "Train Loss at iteration 12667: 0.042174694591318354\n",
      "Train Loss at iteration 12668: 0.04217465596455927\n",
      "Train Loss at iteration 12669: 0.04217461734420022\n",
      "Train Loss at iteration 12670: 0.0421745787302401\n",
      "Train Loss at iteration 12671: 0.04217454012267788\n",
      "Train Loss at iteration 12672: 0.04217450152151248\n",
      "Train Loss at iteration 12673: 0.04217446292674285\n",
      "Train Loss at iteration 12674: 0.04217442433836791\n",
      "Train Loss at iteration 12675: 0.04217438575638661\n",
      "Train Loss at iteration 12676: 0.042174347180797864\n",
      "Train Loss at iteration 12677: 0.04217430861160064\n",
      "Train Loss at iteration 12678: 0.04217427004879384\n",
      "Train Loss at iteration 12679: 0.042174231492376424\n",
      "Train Loss at iteration 12680: 0.04217419294234733\n",
      "Train Loss at iteration 12681: 0.042174154398705484\n",
      "Train Loss at iteration 12682: 0.04217411586144983\n",
      "Train Loss at iteration 12683: 0.0421740773305793\n",
      "Train Loss at iteration 12684: 0.042174038806092856\n",
      "Train Loss at iteration 12685: 0.042174000287989385\n",
      "Train Loss at iteration 12686: 0.04217396177626787\n",
      "Train Loss at iteration 12687: 0.042173923270927234\n",
      "Train Loss at iteration 12688: 0.042173884771966416\n",
      "Train Loss at iteration 12689: 0.04217384627938436\n",
      "Train Loss at iteration 12690: 0.042173807793179995\n",
      "Train Loss at iteration 12691: 0.04217376931335226\n",
      "Train Loss at iteration 12692: 0.042173730839900114\n",
      "Train Loss at iteration 12693: 0.04217369237282247\n",
      "Train Loss at iteration 12694: 0.04217365391211828\n",
      "Train Loss at iteration 12695: 0.04217361545778649\n",
      "Train Loss at iteration 12696: 0.042173577009826026\n",
      "Train Loss at iteration 12697: 0.04217353856823584\n",
      "Train Loss at iteration 12698: 0.04217350013301486\n",
      "Train Loss at iteration 12699: 0.04217346170416203\n",
      "Train Loss at iteration 12700: 0.042173423281676316\n",
      "Train Loss at iteration 12701: 0.042173384865556614\n",
      "Train Loss at iteration 12702: 0.0421733464558019\n",
      "Train Loss at iteration 12703: 0.042173308052411096\n",
      "Train Loss at iteration 12704: 0.042173269655383164\n",
      "Train Loss at iteration 12705: 0.04217323126471702\n",
      "Train Loss at iteration 12706: 0.042173192880411624\n",
      "Train Loss at iteration 12707: 0.0421731545024659\n",
      "Train Loss at iteration 12708: 0.04217311613087882\n",
      "Train Loss at iteration 12709: 0.042173077765649304\n",
      "Train Loss at iteration 12710: 0.04217303940677629\n",
      "Train Loss at iteration 12711: 0.04217300105425873\n",
      "Train Loss at iteration 12712: 0.04217296270809558\n",
      "Train Loss at iteration 12713: 0.04217292436828575\n",
      "Train Loss at iteration 12714: 0.0421728860348282\n",
      "Train Loss at iteration 12715: 0.042172847707721894\n",
      "Train Loss at iteration 12716: 0.04217280938696575\n",
      "Train Loss at iteration 12717: 0.04217277107255872\n",
      "Train Loss at iteration 12718: 0.04217273276449974\n",
      "Train Loss at iteration 12719: 0.04217269446278776\n",
      "Train Loss at iteration 12720: 0.042172656167421725\n",
      "Train Loss at iteration 12721: 0.042172617878400584\n",
      "Train Loss at iteration 12722: 0.042172579595723285\n",
      "Train Loss at iteration 12723: 0.04217254131938874\n",
      "Train Loss at iteration 12724: 0.04217250304939595\n",
      "Train Loss at iteration 12725: 0.0421724647857438\n",
      "Train Loss at iteration 12726: 0.04217242652843127\n",
      "Train Loss at iteration 12727: 0.04217238827745731\n",
      "Train Loss at iteration 12728: 0.04217235003282084\n",
      "Train Loss at iteration 12729: 0.042172311794520835\n",
      "Train Loss at iteration 12730: 0.04217227356255623\n",
      "Train Loss at iteration 12731: 0.042172235336925955\n",
      "Train Loss at iteration 12732: 0.04217219711762897\n",
      "Train Loss at iteration 12733: 0.04217215890466423\n",
      "Train Loss at iteration 12734: 0.04217212069803066\n",
      "Train Loss at iteration 12735: 0.04217208249772723\n",
      "Train Loss at iteration 12736: 0.04217204430375287\n",
      "Train Loss at iteration 12737: 0.04217200611610654\n",
      "Train Loss at iteration 12738: 0.04217196793478717\n",
      "Train Loss at iteration 12739: 0.04217192975979372\n",
      "Train Loss at iteration 12740: 0.04217189159112514\n",
      "Train Loss at iteration 12741: 0.04217185342878038\n",
      "Train Loss at iteration 12742: 0.04217181527275837\n",
      "Train Loss at iteration 12743: 0.042171777123058085\n",
      "Train Loss at iteration 12744: 0.04217173897967846\n",
      "Train Loss at iteration 12745: 0.042171700842618426\n",
      "Train Loss at iteration 12746: 0.042171662711876966\n",
      "Train Loss at iteration 12747: 0.042171624587453\n",
      "Train Loss at iteration 12748: 0.04217158646934549\n",
      "Train Loss at iteration 12749: 0.04217154835755339\n",
      "Train Loss at iteration 12750: 0.04217151025207565\n",
      "Train Loss at iteration 12751: 0.0421714721529112\n",
      "Train Loss at iteration 12752: 0.042171434060059014\n",
      "Train Loss at iteration 12753: 0.042171395973518024\n",
      "Train Loss at iteration 12754: 0.04217135789328719\n",
      "Train Loss at iteration 12755: 0.04217131981936547\n",
      "Train Loss at iteration 12756: 0.04217128175175179\n",
      "Train Loss at iteration 12757: 0.04217124369044511\n",
      "Train Loss at iteration 12758: 0.042171205635444395\n",
      "Train Loss at iteration 12759: 0.042171167586748604\n",
      "Train Loss at iteration 12760: 0.04217112954435665\n",
      "Train Loss at iteration 12761: 0.04217109150826752\n",
      "Train Loss at iteration 12762: 0.042171053478480144\n",
      "Train Loss at iteration 12763: 0.04217101545499349\n",
      "Train Loss at iteration 12764: 0.0421709774378065\n",
      "Train Loss at iteration 12765: 0.04217093942691813\n",
      "Train Loss at iteration 12766: 0.042170901422327316\n",
      "Train Loss at iteration 12767: 0.04217086342403305\n",
      "Train Loss at iteration 12768: 0.04217082543203424\n",
      "Train Loss at iteration 12769: 0.04217078744632988\n",
      "Train Loss at iteration 12770: 0.04217074946691888\n",
      "Train Loss at iteration 12771: 0.042170711493800236\n",
      "Train Loss at iteration 12772: 0.04217067352697287\n",
      "Train Loss at iteration 12773: 0.04217063556643574\n",
      "Train Loss at iteration 12774: 0.04217059761218782\n",
      "Train Loss at iteration 12775: 0.04217055966422803\n",
      "Train Loss at iteration 12776: 0.042170521722555365\n",
      "Train Loss at iteration 12777: 0.042170483787168755\n",
      "Train Loss at iteration 12778: 0.04217044585806715\n",
      "Train Loss at iteration 12779: 0.04217040793524952\n",
      "Train Loss at iteration 12780: 0.042170370018714806\n",
      "Train Loss at iteration 12781: 0.04217033210846198\n",
      "Train Loss at iteration 12782: 0.042170294204489985\n",
      "Train Loss at iteration 12783: 0.04217025630679777\n",
      "Train Loss at iteration 12784: 0.042170218415384306\n",
      "Train Loss at iteration 12785: 0.04217018053024855\n",
      "Train Loss at iteration 12786: 0.042170142651389436\n",
      "Train Loss at iteration 12787: 0.042170104778805936\n",
      "Train Loss at iteration 12788: 0.04217006691249701\n",
      "Train Loss at iteration 12789: 0.0421700290524616\n",
      "Train Loss at iteration 12790: 0.04216999119869868\n",
      "Train Loss at iteration 12791: 0.04216995335120719\n",
      "Train Loss at iteration 12792: 0.042169915509986104\n",
      "Train Loss at iteration 12793: 0.04216987767503438\n",
      "Train Loss at iteration 12794: 0.04216983984635093\n",
      "Train Loss at iteration 12795: 0.04216980202393477\n",
      "Train Loss at iteration 12796: 0.042169764207784836\n",
      "Train Loss at iteration 12797: 0.042169726397900086\n",
      "Train Loss at iteration 12798: 0.04216968859427947\n",
      "Train Loss at iteration 12799: 0.04216965079692196\n",
      "Train Loss at iteration 12800: 0.0421696130058265\n",
      "Train Loss at iteration 12801: 0.04216957522099206\n",
      "Train Loss at iteration 12802: 0.0421695374424176\n",
      "Train Loss at iteration 12803: 0.04216949967010206\n",
      "Train Loss at iteration 12804: 0.04216946190404442\n",
      "Train Loss at iteration 12805: 0.042169424144243635\n",
      "Train Loss at iteration 12806: 0.04216938639069866\n",
      "Train Loss at iteration 12807: 0.04216934864340845\n",
      "Train Loss at iteration 12808: 0.04216931090237197\n",
      "Train Loss at iteration 12809: 0.04216927316758819\n",
      "Train Loss at iteration 12810: 0.042169235439056055\n",
      "Train Loss at iteration 12811: 0.04216919771677453\n",
      "Train Loss at iteration 12812: 0.042169160000742584\n",
      "Train Loss at iteration 12813: 0.04216912229095916\n",
      "Train Loss at iteration 12814: 0.04216908458742324\n",
      "Train Loss at iteration 12815: 0.04216904689013376\n",
      "Train Loss at iteration 12816: 0.04216900919908971\n",
      "Train Loss at iteration 12817: 0.04216897151429003\n",
      "Train Loss at iteration 12818: 0.04216893383573369\n",
      "Train Loss at iteration 12819: 0.04216889616341965\n",
      "Train Loss at iteration 12820: 0.04216885849734686\n",
      "Train Loss at iteration 12821: 0.04216882083751431\n",
      "Train Loss at iteration 12822: 0.04216878318392095\n",
      "Train Loss at iteration 12823: 0.04216874553656572\n",
      "Train Loss at iteration 12824: 0.04216870789544761\n",
      "Train Loss at iteration 12825: 0.04216867026056558\n",
      "Train Loss at iteration 12826: 0.042168632631918584\n",
      "Train Loss at iteration 12827: 0.04216859500950558\n",
      "Train Loss at iteration 12828: 0.04216855739332555\n",
      "Train Loss at iteration 12829: 0.04216851978337744\n",
      "Train Loss at iteration 12830: 0.04216848217966022\n",
      "Train Loss at iteration 12831: 0.042168444582172865\n",
      "Train Loss at iteration 12832: 0.04216840699091432\n",
      "Train Loss at iteration 12833: 0.04216836940588357\n",
      "Train Loss at iteration 12834: 0.042168331827079554\n",
      "Train Loss at iteration 12835: 0.042168294254501255\n",
      "Train Loss at iteration 12836: 0.04216825668814762\n",
      "Train Loss at iteration 12837: 0.04216821912801764\n",
      "Train Loss at iteration 12838: 0.04216818157411026\n",
      "Train Loss at iteration 12839: 0.04216814402642445\n",
      "Train Loss at iteration 12840: 0.04216810648495919\n",
      "Train Loss at iteration 12841: 0.04216806894971342\n",
      "Train Loss at iteration 12842: 0.04216803142068612\n",
      "Train Loss at iteration 12843: 0.04216799389787625\n",
      "Train Loss at iteration 12844: 0.042167956381282776\n",
      "Train Loss at iteration 12845: 0.04216791887090469\n",
      "Train Loss at iteration 12846: 0.04216788136674092\n",
      "Train Loss at iteration 12847: 0.042167843868790446\n",
      "Train Loss at iteration 12848: 0.04216780637705225\n",
      "Train Loss at iteration 12849: 0.042167768891525285\n",
      "Train Loss at iteration 12850: 0.042167731412208524\n",
      "Train Loss at iteration 12851: 0.04216769393910092\n",
      "Train Loss at iteration 12852: 0.04216765647220145\n",
      "Train Loss at iteration 12853: 0.04216761901150909\n",
      "Train Loss at iteration 12854: 0.0421675815570228\n",
      "Train Loss at iteration 12855: 0.04216754410874156\n",
      "Train Loss at iteration 12856: 0.04216750666666431\n",
      "Train Loss at iteration 12857: 0.04216746923079004\n",
      "Train Loss at iteration 12858: 0.042167431801117705\n",
      "Train Loss at iteration 12859: 0.0421673943776463\n",
      "Train Loss at iteration 12860: 0.04216735696037476\n",
      "Train Loss at iteration 12861: 0.042167319549302085\n",
      "Train Loss at iteration 12862: 0.04216728214442723\n",
      "Train Loss at iteration 12863: 0.042167244745749156\n",
      "Train Loss at iteration 12864: 0.042167207353266845\n",
      "Train Loss at iteration 12865: 0.04216716996697926\n",
      "Train Loss at iteration 12866: 0.04216713258688537\n",
      "Train Loss at iteration 12867: 0.04216709521298416\n",
      "Train Loss at iteration 12868: 0.04216705784527458\n",
      "Train Loss at iteration 12869: 0.0421670204837556\n",
      "Train Loss at iteration 12870: 0.042166983128426226\n",
      "Train Loss at iteration 12871: 0.042166945779285384\n",
      "Train Loss at iteration 12872: 0.042166908436332064\n",
      "Train Loss at iteration 12873: 0.04216687109956524\n",
      "Train Loss at iteration 12874: 0.04216683376898388\n",
      "Train Loss at iteration 12875: 0.04216679644458696\n",
      "Train Loss at iteration 12876: 0.04216675912637344\n",
      "Train Loss at iteration 12877: 0.042166721814342306\n",
      "Train Loss at iteration 12878: 0.042166684508492515\n",
      "Train Loss at iteration 12879: 0.04216664720882305\n",
      "Train Loss at iteration 12880: 0.04216660991533289\n",
      "Train Loss at iteration 12881: 0.04216657262802099\n",
      "Train Loss at iteration 12882: 0.04216653534688634\n",
      "Train Loss at iteration 12883: 0.042166498071927884\n",
      "Train Loss at iteration 12884: 0.04216646080314463\n",
      "Train Loss at iteration 12885: 0.042166423540535534\n",
      "Train Loss at iteration 12886: 0.04216638628409957\n",
      "Train Loss at iteration 12887: 0.04216634903383571\n",
      "Train Loss at iteration 12888: 0.04216631178974294\n",
      "Train Loss at iteration 12889: 0.042166274551820225\n",
      "Train Loss at iteration 12890: 0.04216623732006653\n",
      "Train Loss at iteration 12891: 0.04216620009448085\n",
      "Train Loss at iteration 12892: 0.042166162875062134\n",
      "Train Loss at iteration 12893: 0.04216612566180937\n",
      "Train Loss at iteration 12894: 0.04216608845472155\n",
      "Train Loss at iteration 12895: 0.042166051253797625\n",
      "Train Loss at iteration 12896: 0.04216601405903656\n",
      "Train Loss at iteration 12897: 0.042165976870437355\n",
      "Train Loss at iteration 12898: 0.04216593968799899\n",
      "Train Loss at iteration 12899: 0.042165902511720414\n",
      "Train Loss at iteration 12900: 0.042165865341600633\n",
      "Train Loss at iteration 12901: 0.04216582817763861\n",
      "Train Loss at iteration 12902: 0.0421657910198333\n",
      "Train Loss at iteration 12903: 0.042165753868183706\n",
      "Train Loss at iteration 12904: 0.0421657167226888\n",
      "Train Loss at iteration 12905: 0.04216567958334756\n",
      "Train Loss at iteration 12906: 0.04216564245015895\n",
      "Train Loss at iteration 12907: 0.04216560532312195\n",
      "Train Loss at iteration 12908: 0.042165568202235555\n",
      "Train Loss at iteration 12909: 0.04216553108749872\n",
      "Train Loss at iteration 12910: 0.04216549397891043\n",
      "Train Loss at iteration 12911: 0.04216545687646968\n",
      "Train Loss at iteration 12912: 0.04216541978017541\n",
      "Train Loss at iteration 12913: 0.04216538269002664\n",
      "Train Loss at iteration 12914: 0.04216534560602232\n",
      "Train Loss at iteration 12915: 0.042165308528161444\n",
      "Train Loss at iteration 12916: 0.042165271456442985\n",
      "Train Loss at iteration 12917: 0.04216523439086591\n",
      "Train Loss at iteration 12918: 0.04216519733142922\n",
      "Train Loss at iteration 12919: 0.04216516027813188\n",
      "Train Loss at iteration 12920: 0.04216512323097287\n",
      "Train Loss at iteration 12921: 0.04216508618995116\n",
      "Train Loss at iteration 12922: 0.042165049155065755\n",
      "Train Loss at iteration 12923: 0.042165012126315626\n",
      "Train Loss at iteration 12924: 0.042164975103699744\n",
      "Train Loss at iteration 12925: 0.04216493808721709\n",
      "Train Loss at iteration 12926: 0.042164901076866636\n",
      "Train Loss at iteration 12927: 0.0421648640726474\n",
      "Train Loss at iteration 12928: 0.04216482707455832\n",
      "Train Loss at iteration 12929: 0.0421647900825984\n",
      "Train Loss at iteration 12930: 0.04216475309676661\n",
      "Train Loss at iteration 12931: 0.04216471611706194\n",
      "Train Loss at iteration 12932: 0.04216467914348337\n",
      "Train Loss at iteration 12933: 0.04216464217602986\n",
      "Train Loss at iteration 12934: 0.04216460521470044\n",
      "Train Loss at iteration 12935: 0.04216456825949403\n",
      "Train Loss at iteration 12936: 0.04216453131040967\n",
      "Train Loss at iteration 12937: 0.04216449436744629\n",
      "Train Loss at iteration 12938: 0.04216445743060291\n",
      "Train Loss at iteration 12939: 0.04216442049987851\n",
      "Train Loss at iteration 12940: 0.04216438357527206\n",
      "Train Loss at iteration 12941: 0.04216434665678254\n",
      "Train Loss at iteration 12942: 0.04216430974440894\n",
      "Train Loss at iteration 12943: 0.04216427283815026\n",
      "Train Loss at iteration 12944: 0.04216423593800545\n",
      "Train Loss at iteration 12945: 0.04216419904397351\n",
      "Train Loss at iteration 12946: 0.04216416215605344\n",
      "Train Loss at iteration 12947: 0.042164125274244194\n",
      "Train Loss at iteration 12948: 0.04216408839854477\n",
      "Train Loss at iteration 12949: 0.04216405152895415\n",
      "Train Loss at iteration 12950: 0.04216401466547132\n",
      "Train Loss at iteration 12951: 0.042163977808095277\n",
      "Train Loss at iteration 12952: 0.04216394095682498\n",
      "Train Loss at iteration 12953: 0.04216390411165944\n",
      "Train Loss at iteration 12954: 0.042163867272597626\n",
      "Train Loss at iteration 12955: 0.04216383043963852\n",
      "Train Loss at iteration 12956: 0.04216379361278112\n",
      "Train Loss at iteration 12957: 0.04216375679202441\n",
      "Train Loss at iteration 12958: 0.04216371997736738\n",
      "Train Loss at iteration 12959: 0.042163683168808974\n",
      "Train Loss at iteration 12960: 0.04216364636634824\n",
      "Train Loss at iteration 12961: 0.042163609569984134\n",
      "Train Loss at iteration 12962: 0.04216357277971564\n",
      "Train Loss at iteration 12963: 0.042163535995541755\n",
      "Train Loss at iteration 12964: 0.04216349921746146\n",
      "Train Loss at iteration 12965: 0.04216346244547373\n",
      "Train Loss at iteration 12966: 0.04216342567957758\n",
      "Train Loss at iteration 12967: 0.04216338891977198\n",
      "Train Loss at iteration 12968: 0.04216335216605591\n",
      "Train Loss at iteration 12969: 0.042163315418428375\n",
      "Train Loss at iteration 12970: 0.042163278676888356\n",
      "Train Loss at iteration 12971: 0.04216324194143483\n",
      "Train Loss at iteration 12972: 0.042163205212066804\n",
      "Train Loss at iteration 12973: 0.04216316848878326\n",
      "Train Loss at iteration 12974: 0.04216313177158318\n",
      "Train Loss at iteration 12975: 0.04216309506046556\n",
      "Train Loss at iteration 12976: 0.04216305835542938\n",
      "Train Loss at iteration 12977: 0.04216302165647362\n",
      "Train Loss at iteration 12978: 0.042162984963597314\n",
      "Train Loss at iteration 12979: 0.04216294827679941\n",
      "Train Loss at iteration 12980: 0.042162911596078906\n",
      "Train Loss at iteration 12981: 0.04216287492143478\n",
      "Train Loss at iteration 12982: 0.04216283825286606\n",
      "Train Loss at iteration 12983: 0.0421628015903717\n",
      "Train Loss at iteration 12984: 0.04216276493395072\n",
      "Train Loss at iteration 12985: 0.04216272828360207\n",
      "Train Loss at iteration 12986: 0.04216269163932479\n",
      "Train Loss at iteration 12987: 0.04216265500111782\n",
      "Train Loss at iteration 12988: 0.04216261836898018\n",
      "Train Loss at iteration 12989: 0.042162581742910855\n",
      "Train Loss at iteration 12990: 0.042162545122908845\n",
      "Train Loss at iteration 12991: 0.042162508508973136\n",
      "Train Loss at iteration 12992: 0.04216247190110272\n",
      "Train Loss at iteration 12993: 0.04216243529929658\n",
      "Train Loss at iteration 12994: 0.04216239870355371\n",
      "Train Loss at iteration 12995: 0.04216236211387312\n",
      "Train Loss at iteration 12996: 0.04216232553025378\n",
      "Train Loss at iteration 12997: 0.04216228895269468\n",
      "Train Loss at iteration 12998: 0.04216225238119484\n",
      "Train Loss at iteration 12999: 0.04216221581575324\n",
      "Train Loss at iteration 13000: 0.04216217925636886\n",
      "Train Loss at iteration 13001: 0.04216214270304071\n",
      "Train Loss at iteration 13002: 0.042162106155767765\n",
      "Train Loss at iteration 13003: 0.04216206961454903\n",
      "Train Loss at iteration 13004: 0.04216203307938351\n",
      "Train Loss at iteration 13005: 0.04216199655027018\n",
      "Train Loss at iteration 13006: 0.04216196002720805\n",
      "Train Loss at iteration 13007: 0.042161923510196084\n",
      "Train Loss at iteration 13008: 0.04216188699923332\n",
      "Train Loss at iteration 13009: 0.04216185049431872\n",
      "Train Loss at iteration 13010: 0.04216181399545129\n",
      "Train Loss at iteration 13011: 0.04216177750263002\n",
      "Train Loss at iteration 13012: 0.04216174101585391\n",
      "Train Loss at iteration 13013: 0.04216170453512195\n",
      "Train Loss at iteration 13014: 0.042161668060433144\n",
      "Train Loss at iteration 13015: 0.042161631591786486\n",
      "Train Loss at iteration 13016: 0.04216159512918096\n",
      "Train Loss at iteration 13017: 0.042161558672615576\n",
      "Train Loss at iteration 13018: 0.04216152222208931\n",
      "Train Loss at iteration 13019: 0.04216148577760119\n",
      "Train Loss at iteration 13020: 0.04216144933915019\n",
      "Train Loss at iteration 13021: 0.04216141290673531\n",
      "Train Loss at iteration 13022: 0.04216137648035555\n",
      "Train Loss at iteration 13023: 0.042161340060009884\n",
      "Train Loss at iteration 13024: 0.04216130364569736\n",
      "Train Loss at iteration 13025: 0.04216126723741692\n",
      "Train Loss at iteration 13026: 0.0421612308351676\n",
      "Train Loss at iteration 13027: 0.042161194438948384\n",
      "Train Loss at iteration 13028: 0.04216115804875826\n",
      "Train Loss at iteration 13029: 0.04216112166459624\n",
      "Train Loss at iteration 13030: 0.04216108528646133\n",
      "Train Loss at iteration 13031: 0.0421610489143525\n",
      "Train Loss at iteration 13032: 0.04216101254826876\n",
      "Train Loss at iteration 13033: 0.042160976188209125\n",
      "Train Loss at iteration 13034: 0.04216093983417258\n",
      "Train Loss at iteration 13035: 0.04216090348615811\n",
      "Train Loss at iteration 13036: 0.04216086714416473\n",
      "Train Loss at iteration 13037: 0.042160830808191455\n",
      "Train Loss at iteration 13038: 0.042160794478237264\n",
      "Train Loss at iteration 13039: 0.04216075815430115\n",
      "Train Loss at iteration 13040: 0.04216072183638212\n",
      "Train Loss at iteration 13041: 0.04216068552447919\n",
      "Train Loss at iteration 13042: 0.04216064921859135\n",
      "Train Loss at iteration 13043: 0.04216061291871758\n",
      "Train Loss at iteration 13044: 0.04216057662485691\n",
      "Train Loss at iteration 13045: 0.04216054033700833\n",
      "Train Loss at iteration 13046: 0.042160504055170814\n",
      "Train Loss at iteration 13047: 0.042160467779343416\n",
      "Train Loss at iteration 13048: 0.042160431509525094\n",
      "Train Loss at iteration 13049: 0.04216039524571487\n",
      "Train Loss at iteration 13050: 0.042160358987911734\n",
      "Train Loss at iteration 13051: 0.0421603227361147\n",
      "Train Loss at iteration 13052: 0.042160286490322746\n",
      "Train Loss at iteration 13053: 0.042160250250534916\n",
      "Train Loss at iteration 13054: 0.04216021401675017\n",
      "Train Loss at iteration 13055: 0.042160177788967516\n",
      "Train Loss at iteration 13056: 0.042160141567186\n",
      "Train Loss at iteration 13057: 0.04216010535140457\n",
      "Train Loss at iteration 13058: 0.04216006914162225\n",
      "Train Loss at iteration 13059: 0.04216003293783806\n",
      "Train Loss at iteration 13060: 0.04215999674005098\n",
      "Train Loss at iteration 13061: 0.04215996054826\n",
      "Train Loss at iteration 13062: 0.04215992436246415\n",
      "Train Loss at iteration 13063: 0.04215988818266244\n",
      "Train Loss at iteration 13064: 0.04215985200885385\n",
      "Train Loss at iteration 13065: 0.0421598158410374\n",
      "Train Loss at iteration 13066: 0.04215977967921209\n",
      "Train Loss at iteration 13067: 0.04215974352337692\n",
      "Train Loss at iteration 13068: 0.0421597073735309\n",
      "Train Loss at iteration 13069: 0.04215967122967302\n",
      "Train Loss at iteration 13070: 0.04215963509180232\n",
      "Train Loss at iteration 13071: 0.04215959895991777\n",
      "Train Loss at iteration 13072: 0.04215956283401837\n",
      "Train Loss at iteration 13073: 0.042159526714103154\n",
      "Train Loss at iteration 13074: 0.042159490600171115\n",
      "Train Loss at iteration 13075: 0.04215945449222127\n",
      "Train Loss at iteration 13076: 0.04215941839025261\n",
      "Train Loss at iteration 13077: 0.04215938229426413\n",
      "Train Loss at iteration 13078: 0.04215934620425486\n",
      "Train Loss at iteration 13079: 0.0421593101202238\n",
      "Train Loss at iteration 13080: 0.042159274042169945\n",
      "Train Loss at iteration 13081: 0.042159237970092314\n",
      "Train Loss at iteration 13082: 0.04215920190398992\n",
      "Train Loss at iteration 13083: 0.04215916584386175\n",
      "Train Loss at iteration 13084: 0.042159129789706824\n",
      "Train Loss at iteration 13085: 0.04215909374152415\n",
      "Train Loss at iteration 13086: 0.042159057699312726\n",
      "Train Loss at iteration 13087: 0.04215902166307156\n",
      "Train Loss at iteration 13088: 0.04215898563279967\n",
      "Train Loss at iteration 13089: 0.04215894960849607\n",
      "Train Loss at iteration 13090: 0.04215891359015973\n",
      "Train Loss at iteration 13091: 0.042158877577789716\n",
      "Train Loss at iteration 13092: 0.04215884157138498\n",
      "Train Loss at iteration 13093: 0.04215880557094458\n",
      "Train Loss at iteration 13094: 0.04215876957646749\n",
      "Train Loss at iteration 13095: 0.042158733587952725\n",
      "Train Loss at iteration 13096: 0.04215869760539931\n",
      "Train Loss at iteration 13097: 0.042158661628806236\n",
      "Train Loss at iteration 13098: 0.04215862565817252\n",
      "Train Loss at iteration 13099: 0.04215858969349717\n",
      "Train Loss at iteration 13100: 0.042158553734779204\n",
      "Train Loss at iteration 13101: 0.042158517782017624\n",
      "Train Loss at iteration 13102: 0.042158481835211446\n",
      "Train Loss at iteration 13103: 0.042158445894359665\n",
      "Train Loss at iteration 13104: 0.04215840995946131\n",
      "Train Loss at iteration 13105: 0.04215837403051539\n",
      "Train Loss at iteration 13106: 0.04215833810752089\n",
      "Train Loss at iteration 13107: 0.04215830219047685\n",
      "Train Loss at iteration 13108: 0.04215826627938227\n",
      "Train Loss at iteration 13109: 0.042158230374236175\n",
      "Train Loss at iteration 13110: 0.042158194475037554\n",
      "Train Loss at iteration 13111: 0.042158158581785427\n",
      "Train Loss at iteration 13112: 0.04215812269447881\n",
      "Train Loss at iteration 13113: 0.0421580868131167\n",
      "Train Loss at iteration 13114: 0.042158050937698144\n",
      "Train Loss at iteration 13115: 0.04215801506822212\n",
      "Train Loss at iteration 13116: 0.04215797920468766\n",
      "Train Loss at iteration 13117: 0.04215794334709377\n",
      "Train Loss at iteration 13118: 0.042157907495439455\n",
      "Train Loss at iteration 13119: 0.04215787164972373\n",
      "Train Loss at iteration 13120: 0.04215783580994563\n",
      "Train Loss at iteration 13121: 0.04215779997610413\n",
      "Train Loss at iteration 13122: 0.04215776414819827\n",
      "Train Loss at iteration 13123: 0.04215772832622708\n",
      "Train Loss at iteration 13124: 0.04215769251018954\n",
      "Train Loss at iteration 13125: 0.042157656700084666\n",
      "Train Loss at iteration 13126: 0.04215762089591149\n",
      "Train Loss at iteration 13127: 0.04215758509766901\n",
      "Train Loss at iteration 13128: 0.042157549305356264\n",
      "Train Loss at iteration 13129: 0.04215751351897225\n",
      "Train Loss at iteration 13130: 0.04215747773851597\n",
      "Train Loss at iteration 13131: 0.04215744196398646\n",
      "Train Loss at iteration 13132: 0.04215740619538272\n",
      "Train Loss at iteration 13133: 0.04215737043270379\n",
      "Train Loss at iteration 13134: 0.042157334675948656\n",
      "Train Loss at iteration 13135: 0.04215729892511636\n",
      "Train Loss at iteration 13136: 0.0421572631802059\n",
      "Train Loss at iteration 13137: 0.04215722744121629\n",
      "Train Loss at iteration 13138: 0.042157191708146545\n",
      "Train Loss at iteration 13139: 0.042157155980995696\n",
      "Train Loss at iteration 13140: 0.04215712025976275\n",
      "Train Loss at iteration 13141: 0.042157084544446735\n",
      "Train Loss at iteration 13142: 0.042157048835046666\n",
      "Train Loss at iteration 13143: 0.04215701313156153\n",
      "Train Loss at iteration 13144: 0.042156977433990384\n",
      "Train Loss at iteration 13145: 0.04215694174233223\n",
      "Train Loss at iteration 13146: 0.04215690605658606\n",
      "Train Loss at iteration 13147: 0.04215687037675092\n",
      "Train Loss at iteration 13148: 0.04215683470282585\n",
      "Train Loss at iteration 13149: 0.04215679903480981\n",
      "Train Loss at iteration 13150: 0.04215676337270186\n",
      "Train Loss at iteration 13151: 0.04215672771650101\n",
      "Train Loss at iteration 13152: 0.04215669206620627\n",
      "Train Loss at iteration 13153: 0.042156656421816666\n",
      "Train Loss at iteration 13154: 0.04215662078333122\n",
      "Train Loss at iteration 13155: 0.04215658515074894\n",
      "Train Loss at iteration 13156: 0.04215654952406885\n",
      "Train Loss at iteration 13157: 0.04215651390328997\n",
      "Train Loss at iteration 13158: 0.04215647828841132\n",
      "Train Loss at iteration 13159: 0.04215644267943191\n",
      "Train Loss at iteration 13160: 0.04215640707635078\n",
      "Train Loss at iteration 13161: 0.042156371479166926\n",
      "Train Loss at iteration 13162: 0.04215633588787939\n",
      "Train Loss at iteration 13163: 0.04215630030248718\n",
      "Train Loss at iteration 13164: 0.04215626472298931\n",
      "Train Loss at iteration 13165: 0.042156229149384834\n",
      "Train Loss at iteration 13166: 0.04215619358167273\n",
      "Train Loss at iteration 13167: 0.04215615801985204\n",
      "Train Loss at iteration 13168: 0.04215612246392178\n",
      "Train Loss at iteration 13169: 0.04215608691388098\n",
      "Train Loss at iteration 13170: 0.04215605136972865\n",
      "Train Loss at iteration 13171: 0.04215601583146381\n",
      "Train Loss at iteration 13172: 0.0421559802990855\n",
      "Train Loss at iteration 13173: 0.04215594477259273\n",
      "Train Loss at iteration 13174: 0.04215590925198453\n",
      "Train Loss at iteration 13175: 0.04215587373725989\n",
      "Train Loss at iteration 13176: 0.042155838228417876\n",
      "Train Loss at iteration 13177: 0.04215580272545748\n",
      "Train Loss at iteration 13178: 0.042155767228377744\n",
      "Train Loss at iteration 13179: 0.04215573173717767\n",
      "Train Loss at iteration 13180: 0.04215569625185629\n",
      "Train Loss at iteration 13181: 0.042155660772412645\n",
      "Train Loss at iteration 13182: 0.04215562529884575\n",
      "Train Loss at iteration 13183: 0.04215558983115461\n",
      "Train Loss at iteration 13184: 0.042155554369338266\n",
      "Train Loss at iteration 13185: 0.042155518913395744\n",
      "Train Loss at iteration 13186: 0.04215548346332605\n",
      "Train Loss at iteration 13187: 0.042155448019128225\n",
      "Train Loss at iteration 13188: 0.042155412580801285\n",
      "Train Loss at iteration 13189: 0.04215537714834426\n",
      "Train Loss at iteration 13190: 0.04215534172175617\n",
      "Train Loss at iteration 13191: 0.04215530630103605\n",
      "Train Loss at iteration 13192: 0.0421552708861829\n",
      "Train Loss at iteration 13193: 0.04215523547719577\n",
      "Train Loss at iteration 13194: 0.042155200074073695\n",
      "Train Loss at iteration 13195: 0.04215516467681566\n",
      "Train Loss at iteration 13196: 0.04215512928542072\n",
      "Train Loss at iteration 13197: 0.04215509389988789\n",
      "Train Loss at iteration 13198: 0.042155058520216206\n",
      "Train Loss at iteration 13199: 0.042155023146404694\n",
      "Train Loss at iteration 13200: 0.04215498777845237\n",
      "Train Loss at iteration 13201: 0.04215495241635826\n",
      "Train Loss at iteration 13202: 0.0421549170601214\n",
      "Train Loss at iteration 13203: 0.04215488170974081\n",
      "Train Loss at iteration 13204: 0.04215484636521553\n",
      "Train Loss at iteration 13205: 0.04215481102654455\n",
      "Train Loss at iteration 13206: 0.042154775693726956\n",
      "Train Loss at iteration 13207: 0.04215474036676172\n",
      "Train Loss at iteration 13208: 0.04215470504564791\n",
      "Train Loss at iteration 13209: 0.04215466973038453\n",
      "Train Loss at iteration 13210: 0.042154634420970616\n",
      "Train Loss at iteration 13211: 0.04215459911740519\n",
      "Train Loss at iteration 13212: 0.04215456381968729\n",
      "Train Loss at iteration 13213: 0.04215452852781594\n",
      "Train Loss at iteration 13214: 0.042154493241790165\n",
      "Train Loss at iteration 13215: 0.042154457961609\n",
      "Train Loss at iteration 13216: 0.04215442268727147\n",
      "Train Loss at iteration 13217: 0.04215438741877661\n",
      "Train Loss at iteration 13218: 0.04215435215612344\n",
      "Train Loss at iteration 13219: 0.04215431689931099\n",
      "Train Loss at iteration 13220: 0.0421542816483383\n",
      "Train Loss at iteration 13221: 0.04215424640320439\n",
      "Train Loss at iteration 13222: 0.042154211163908294\n",
      "Train Loss at iteration 13223: 0.04215417593044904\n",
      "Train Loss at iteration 13224: 0.04215414070282566\n",
      "Train Loss at iteration 13225: 0.04215410548103718\n",
      "Train Loss at iteration 13226: 0.04215407026508263\n",
      "Train Loss at iteration 13227: 0.04215403505496106\n",
      "Train Loss at iteration 13228: 0.04215399985067148\n",
      "Train Loss at iteration 13229: 0.04215396465221293\n",
      "Train Loss at iteration 13230: 0.04215392945958444\n",
      "Train Loss at iteration 13231: 0.04215389427278503\n",
      "Train Loss at iteration 13232: 0.04215385909181374\n",
      "Train Loss at iteration 13233: 0.04215382391666961\n",
      "Train Loss at iteration 13234: 0.042153788747351664\n",
      "Train Loss at iteration 13235: 0.04215375358385893\n",
      "Train Loss at iteration 13236: 0.042153718426190444\n",
      "Train Loss at iteration 13237: 0.042153683274345245\n",
      "Train Loss at iteration 13238: 0.042153648128322355\n",
      "Train Loss at iteration 13239: 0.04215361298812081\n",
      "Train Loss at iteration 13240: 0.04215357785373964\n",
      "Train Loss at iteration 13241: 0.0421535427251779\n",
      "Train Loss at iteration 13242: 0.042153507602434595\n",
      "Train Loss at iteration 13243: 0.04215347248550877\n",
      "Train Loss at iteration 13244: 0.04215343737439946\n",
      "Train Loss at iteration 13245: 0.04215340226910569\n",
      "Train Loss at iteration 13246: 0.042153367169626504\n",
      "Train Loss at iteration 13247: 0.04215333207596093\n",
      "Train Loss at iteration 13248: 0.042153296988108\n",
      "Train Loss at iteration 13249: 0.04215326190606676\n",
      "Train Loss at iteration 13250: 0.042153226829836236\n",
      "Train Loss at iteration 13251: 0.042153191759415454\n",
      "Train Loss at iteration 13252: 0.04215315669480346\n",
      "Train Loss at iteration 13253: 0.0421531216359993\n",
      "Train Loss at iteration 13254: 0.042153086583001986\n",
      "Train Loss at iteration 13255: 0.042153051535810566\n",
      "Train Loss at iteration 13256: 0.04215301649442407\n",
      "Train Loss at iteration 13257: 0.04215298145884153\n",
      "Train Loss at iteration 13258: 0.042152946429062\n",
      "Train Loss at iteration 13259: 0.04215291140508451\n",
      "Train Loss at iteration 13260: 0.042152876386908074\n",
      "Train Loss at iteration 13261: 0.042152841374531735\n",
      "Train Loss at iteration 13262: 0.042152806367954564\n",
      "Train Loss at iteration 13263: 0.042152771367175566\n",
      "Train Loss at iteration 13264: 0.04215273637219378\n",
      "Train Loss at iteration 13265: 0.04215270138300825\n",
      "Train Loss at iteration 13266: 0.042152666399618004\n",
      "Train Loss at iteration 13267: 0.0421526314220221\n",
      "Train Loss at iteration 13268: 0.04215259645021954\n",
      "Train Loss at iteration 13269: 0.04215256148420939\n",
      "Train Loss at iteration 13270: 0.042152526523990694\n",
      "Train Loss at iteration 13271: 0.04215249156956246\n",
      "Train Loss at iteration 13272: 0.04215245662092375\n",
      "Train Loss at iteration 13273: 0.04215242167807359\n",
      "Train Loss at iteration 13274: 0.04215238674101102\n",
      "Train Loss at iteration 13275: 0.04215235180973509\n",
      "Train Loss at iteration 13276: 0.04215231688424482\n",
      "Train Loss at iteration 13277: 0.042152281964539254\n",
      "Train Loss at iteration 13278: 0.04215224705061746\n",
      "Train Loss at iteration 13279: 0.04215221214247844\n",
      "Train Loss at iteration 13280: 0.042152177240121234\n",
      "Train Loss at iteration 13281: 0.04215214234354491\n",
      "Train Loss at iteration 13282: 0.04215210745274847\n",
      "Train Loss at iteration 13283: 0.042152072567731\n",
      "Train Loss at iteration 13284: 0.0421520376884915\n",
      "Train Loss at iteration 13285: 0.04215200281502904\n",
      "Train Loss at iteration 13286: 0.042151967947342626\n",
      "Train Loss at iteration 13287: 0.042151933085431324\n",
      "Train Loss at iteration 13288: 0.042151898229294174\n",
      "Train Loss at iteration 13289: 0.04215186337893021\n",
      "Train Loss at iteration 13290: 0.04215182853433848\n",
      "Train Loss at iteration 13291: 0.042151793695518004\n",
      "Train Loss at iteration 13292: 0.042151758862467845\n",
      "Train Loss at iteration 13293: 0.042151724035187035\n",
      "Train Loss at iteration 13294: 0.042151689213674626\n",
      "Train Loss at iteration 13295: 0.04215165439792965\n",
      "Train Loss at iteration 13296: 0.04215161958795115\n",
      "Train Loss at iteration 13297: 0.042151584783738164\n",
      "Train Loss at iteration 13298: 0.042151549985289756\n",
      "Train Loss at iteration 13299: 0.04215151519260493\n",
      "Train Loss at iteration 13300: 0.042151480405682756\n",
      "Train Loss at iteration 13301: 0.0421514456245223\n",
      "Train Loss at iteration 13302: 0.042151410849122536\n",
      "Train Loss at iteration 13303: 0.04215137607948257\n",
      "Train Loss at iteration 13304: 0.042151341315601415\n",
      "Train Loss at iteration 13305: 0.04215130655747812\n",
      "Train Loss at iteration 13306: 0.04215127180511174\n",
      "Train Loss at iteration 13307: 0.0421512370585013\n",
      "Train Loss at iteration 13308: 0.04215120231764586\n",
      "Train Loss at iteration 13309: 0.042151167582544447\n",
      "Train Loss at iteration 13310: 0.04215113285319612\n",
      "Train Loss at iteration 13311: 0.04215109812959991\n",
      "Train Loss at iteration 13312: 0.042151063411754876\n",
      "Train Loss at iteration 13313: 0.04215102869966006\n",
      "Train Loss at iteration 13314: 0.042150993993314505\n",
      "Train Loss at iteration 13315: 0.04215095929271724\n",
      "Train Loss at iteration 13316: 0.04215092459786733\n",
      "Train Loss at iteration 13317: 0.042150889908763825\n",
      "Train Loss at iteration 13318: 0.04215085522540576\n",
      "Train Loss at iteration 13319: 0.04215082054779218\n",
      "Train Loss at iteration 13320: 0.04215078587592212\n",
      "Train Loss at iteration 13321: 0.04215075120979463\n",
      "Train Loss at iteration 13322: 0.04215071654940878\n",
      "Train Loss at iteration 13323: 0.0421506818947636\n",
      "Train Loss at iteration 13324: 0.04215064724585812\n",
      "Train Loss at iteration 13325: 0.04215061260269143\n",
      "Train Loss at iteration 13326: 0.042150577965262524\n",
      "Train Loss at iteration 13327: 0.042150543333570475\n",
      "Train Loss at iteration 13328: 0.042150508707614345\n",
      "Train Loss at iteration 13329: 0.04215047408739315\n",
      "Train Loss at iteration 13330: 0.042150439472905955\n",
      "Train Loss at iteration 13331: 0.04215040486415182\n",
      "Train Loss at iteration 13332: 0.04215037026112976\n",
      "Train Loss at iteration 13333: 0.042150335663838856\n",
      "Train Loss at iteration 13334: 0.042150301072278136\n",
      "Train Loss at iteration 13335: 0.042150266486446646\n",
      "Train Loss at iteration 13336: 0.04215023190634345\n",
      "Train Loss at iteration 13337: 0.042150197331967586\n",
      "Train Loss at iteration 13338: 0.042150162763318094\n",
      "Train Loss at iteration 13339: 0.04215012820039405\n",
      "Train Loss at iteration 13340: 0.042150093643194476\n",
      "Train Loss at iteration 13341: 0.04215005909171844\n",
      "Train Loss at iteration 13342: 0.04215002454596497\n",
      "Train Loss at iteration 13343: 0.04214999000593312\n",
      "Train Loss at iteration 13344: 0.04214995547162198\n",
      "Train Loss at iteration 13345: 0.04214992094303054\n",
      "Train Loss at iteration 13346: 0.04214988642015789\n",
      "Train Loss at iteration 13347: 0.04214985190300307\n",
      "Train Loss at iteration 13348: 0.04214981739156512\n",
      "Train Loss at iteration 13349: 0.0421497828858431\n",
      "Train Loss at iteration 13350: 0.04214974838583606\n",
      "Train Loss at iteration 13351: 0.042149713891543046\n",
      "Train Loss at iteration 13352: 0.04214967940296311\n",
      "Train Loss at iteration 13353: 0.04214964492009532\n",
      "Train Loss at iteration 13354: 0.042149610442938704\n",
      "Train Loss at iteration 13355: 0.04214957597149231\n",
      "Train Loss at iteration 13356: 0.04214954150575522\n",
      "Train Loss at iteration 13357: 0.04214950704572645\n",
      "Train Loss at iteration 13358: 0.042149472591405085\n",
      "Train Loss at iteration 13359: 0.04214943814279015\n",
      "Train Loss at iteration 13360: 0.042149403699880704\n",
      "Train Loss at iteration 13361: 0.04214936926267582\n",
      "Train Loss at iteration 13362: 0.04214933483117453\n",
      "Train Loss at iteration 13363: 0.04214930040537588\n",
      "Train Loss at iteration 13364: 0.04214926598527895\n",
      "Train Loss at iteration 13365: 0.04214923157088277\n",
      "Train Loss at iteration 13366: 0.04214919716218639\n",
      "Train Loss at iteration 13367: 0.042149162759188875\n",
      "Train Loss at iteration 13368: 0.04214912836188928\n",
      "Train Loss at iteration 13369: 0.042149093970286655\n",
      "Train Loss at iteration 13370: 0.042149059584380054\n",
      "Train Loss at iteration 13371: 0.04214902520416853\n",
      "Train Loss at iteration 13372: 0.042148990829651144\n",
      "Train Loss at iteration 13373: 0.042148956460826933\n",
      "Train Loss at iteration 13374: 0.04214892209769497\n",
      "Train Loss at iteration 13375: 0.0421488877402543\n",
      "Train Loss at iteration 13376: 0.04214885338850397\n",
      "Train Loss at iteration 13377: 0.04214881904244305\n",
      "Train Loss at iteration 13378: 0.0421487847020706\n",
      "Train Loss at iteration 13379: 0.04214875036738565\n",
      "Train Loss at iteration 13380: 0.042148716038387275\n",
      "Train Loss at iteration 13381: 0.04214868171507453\n",
      "Train Loss at iteration 13382: 0.04214864739744645\n",
      "Train Loss at iteration 13383: 0.042148613085502114\n",
      "Train Loss at iteration 13384: 0.042148578779240574\n",
      "Train Loss at iteration 13385: 0.042148544478660885\n",
      "Train Loss at iteration 13386: 0.04214851018376211\n",
      "Train Loss at iteration 13387: 0.04214847589454328\n",
      "Train Loss at iteration 13388: 0.04214844161100348\n",
      "Train Loss at iteration 13389: 0.04214840733314173\n",
      "Train Loss at iteration 13390: 0.042148373060957145\n",
      "Train Loss at iteration 13391: 0.04214833879444873\n",
      "Train Loss at iteration 13392: 0.042148304533615566\n",
      "Train Loss at iteration 13393: 0.04214827027845671\n",
      "Train Loss at iteration 13394: 0.04214823602897121\n",
      "Train Loss at iteration 13395: 0.04214820178515813\n",
      "Train Loss at iteration 13396: 0.04214816754701653\n",
      "Train Loss at iteration 13397: 0.042148133314545456\n",
      "Train Loss at iteration 13398: 0.04214809908774398\n",
      "Train Loss at iteration 13399: 0.04214806486661116\n",
      "Train Loss at iteration 13400: 0.04214803065114605\n",
      "Train Loss at iteration 13401: 0.0421479964413477\n",
      "Train Loss at iteration 13402: 0.042147962237215175\n",
      "Train Loss at iteration 13403: 0.042147928038747555\n",
      "Train Loss at iteration 13404: 0.04214789384594386\n",
      "Train Loss at iteration 13405: 0.04214785965880319\n",
      "Train Loss at iteration 13406: 0.04214782547732457\n",
      "Train Loss at iteration 13407: 0.04214779130150707\n",
      "Train Loss at iteration 13408: 0.04214775713134975\n",
      "Train Loss at iteration 13409: 0.042147722966851695\n",
      "Train Loss at iteration 13410: 0.04214768880801194\n",
      "Train Loss at iteration 13411: 0.04214765465482954\n",
      "Train Loss at iteration 13412: 0.042147620507303564\n",
      "Train Loss at iteration 13413: 0.04214758636543307\n",
      "Train Loss at iteration 13414: 0.04214755222921713\n",
      "Train Loss at iteration 13415: 0.04214751809865479\n",
      "Train Loss at iteration 13416: 0.04214748397374512\n",
      "Train Loss at iteration 13417: 0.04214744985448719\n",
      "Train Loss at iteration 13418: 0.04214741574088003\n",
      "Train Loss at iteration 13419: 0.042147381632922726\n",
      "Train Loss at iteration 13420: 0.04214734753061434\n",
      "Train Loss at iteration 13421: 0.042147313433953934\n",
      "Train Loss at iteration 13422: 0.04214727934294056\n",
      "Train Loss at iteration 13423: 0.04214724525757328\n",
      "Train Loss at iteration 13424: 0.042147211177851175\n",
      "Train Loss at iteration 13425: 0.04214717710377328\n",
      "Train Loss at iteration 13426: 0.04214714303533869\n",
      "Train Loss at iteration 13427: 0.04214710897254643\n",
      "Train Loss at iteration 13428: 0.042147074915395585\n",
      "Train Loss at iteration 13429: 0.04214704086388522\n",
      "Train Loss at iteration 13430: 0.04214700681801439\n",
      "Train Loss at iteration 13431: 0.04214697277778216\n",
      "Train Loss at iteration 13432: 0.042146938743187604\n",
      "Train Loss at iteration 13433: 0.04214690471422978\n",
      "Train Loss at iteration 13434: 0.042146870690907744\n",
      "Train Loss at iteration 13435: 0.042146836673220556\n",
      "Train Loss at iteration 13436: 0.0421468026611673\n",
      "Train Loss at iteration 13437: 0.04214676865474702\n",
      "Train Loss at iteration 13438: 0.0421467346539588\n",
      "Train Loss at iteration 13439: 0.042146700658801696\n",
      "Train Loss at iteration 13440: 0.04214666666927476\n",
      "Train Loss at iteration 13441: 0.042146632685377085\n",
      "Train Loss at iteration 13442: 0.04214659870710771\n",
      "Train Loss at iteration 13443: 0.0421465647344657\n",
      "Train Loss at iteration 13444: 0.04214653076745015\n",
      "Train Loss at iteration 13445: 0.042146496806060085\n",
      "Train Loss at iteration 13446: 0.042146462850294615\n",
      "Train Loss at iteration 13447: 0.04214642890015278\n",
      "Train Loss at iteration 13448: 0.04214639495563364\n",
      "Train Loss at iteration 13449: 0.042146361016736265\n",
      "Train Loss at iteration 13450: 0.04214632708345973\n",
      "Train Loss at iteration 13451: 0.04214629315580311\n",
      "Train Loss at iteration 13452: 0.04214625923376545\n",
      "Train Loss at iteration 13453: 0.04214622531734583\n",
      "Train Loss at iteration 13454: 0.0421461914065433\n",
      "Train Loss at iteration 13455: 0.04214615750135695\n",
      "Train Loss at iteration 13456: 0.04214612360178584\n",
      "Train Loss at iteration 13457: 0.04214608970782904\n",
      "Train Loss at iteration 13458: 0.042146055819485606\n",
      "Train Loss at iteration 13459: 0.04214602193675462\n",
      "Train Loss at iteration 13460: 0.042145988059635126\n",
      "Train Loss at iteration 13461: 0.04214595418812623\n",
      "Train Loss at iteration 13462: 0.04214592032222696\n",
      "Train Loss at iteration 13463: 0.04214588646193642\n",
      "Train Loss at iteration 13464: 0.04214585260725365\n",
      "Train Loss at iteration 13465: 0.04214581875817774\n",
      "Train Loss at iteration 13466: 0.04214578491470773\n",
      "Train Loss at iteration 13467: 0.04214575107684274\n",
      "Train Loss at iteration 13468: 0.04214571724458179\n",
      "Train Loss at iteration 13469: 0.04214568341792397\n",
      "Train Loss at iteration 13470: 0.04214564959686835\n",
      "Train Loss at iteration 13471: 0.04214561578141399\n",
      "Train Loss at iteration 13472: 0.04214558197155998\n",
      "Train Loss at iteration 13473: 0.04214554816730537\n",
      "Train Loss at iteration 13474: 0.042145514368649224\n",
      "Train Loss at iteration 13475: 0.04214548057559064\n",
      "Train Loss at iteration 13476: 0.04214544678812866\n",
      "Train Loss at iteration 13477: 0.042145413006262386\n",
      "Train Loss at iteration 13478: 0.042145379229990856\n",
      "Train Loss at iteration 13479: 0.04214534545931317\n",
      "Train Loss at iteration 13480: 0.04214531169422837\n",
      "Train Loss at iteration 13481: 0.042145277934735534\n",
      "Train Loss at iteration 13482: 0.04214524418083376\n",
      "Train Loss at iteration 13483: 0.0421452104325221\n",
      "Train Loss at iteration 13484: 0.04214517668979961\n",
      "Train Loss at iteration 13485: 0.042145142952665395\n",
      "Train Loss at iteration 13486: 0.042145109221118505\n",
      "Train Loss at iteration 13487: 0.04214507549515802\n",
      "Train Loss at iteration 13488: 0.04214504177478301\n",
      "Train Loss at iteration 13489: 0.042145008059992536\n",
      "Train Loss at iteration 13490: 0.04214497435078571\n",
      "Train Loss at iteration 13491: 0.04214494064716156\n",
      "Train Loss at iteration 13492: 0.04214490694911916\n",
      "Train Loss at iteration 13493: 0.04214487325665762\n",
      "Train Loss at iteration 13494: 0.04214483956977599\n",
      "Train Loss at iteration 13495: 0.04214480588847334\n",
      "Train Loss at iteration 13496: 0.042144772212748746\n",
      "Train Loss at iteration 13497: 0.04214473854260129\n",
      "Train Loss at iteration 13498: 0.04214470487803004\n",
      "Train Loss at iteration 13499: 0.04214467121903406\n",
      "Train Loss at iteration 13500: 0.04214463756561245\n",
      "Train Loss at iteration 13501: 0.04214460391776426\n",
      "Train Loss at iteration 13502: 0.04214457027548858\n",
      "Train Loss at iteration 13503: 0.04214453663878449\n",
      "Train Loss at iteration 13504: 0.04214450300765103\n",
      "Train Loss at iteration 13505: 0.04214446938208731\n",
      "Train Loss at iteration 13506: 0.04214443576209239\n",
      "Train Loss at iteration 13507: 0.04214440214766535\n",
      "Train Loss at iteration 13508: 0.042144368538805266\n",
      "Train Loss at iteration 13509: 0.04214433493551121\n",
      "Train Loss at iteration 13510: 0.04214430133778226\n",
      "Train Loss at iteration 13511: 0.042144267745617484\n",
      "Train Loss at iteration 13512: 0.04214423415901598\n",
      "Train Loss at iteration 13513: 0.04214420057797679\n",
      "Train Loss at iteration 13514: 0.04214416700249903\n",
      "Train Loss at iteration 13515: 0.04214413343258174\n",
      "Train Loss at iteration 13516: 0.042144099868224025\n",
      "Train Loss at iteration 13517: 0.042144066309424945\n",
      "Train Loss at iteration 13518: 0.042144032756183586\n",
      "Train Loss at iteration 13519: 0.042143999208499004\n",
      "Train Loss at iteration 13520: 0.04214396566637031\n",
      "Train Loss at iteration 13521: 0.042143932129796564\n",
      "Train Loss at iteration 13522: 0.04214389859877683\n",
      "Train Loss at iteration 13523: 0.042143865073310215\n",
      "Train Loss at iteration 13524: 0.04214383155339578\n",
      "Train Loss at iteration 13525: 0.0421437980390326\n",
      "Train Loss at iteration 13526: 0.04214376453021977\n",
      "Train Loss at iteration 13527: 0.042143731026956344\n",
      "Train Loss at iteration 13528: 0.042143697529241415\n",
      "Train Loss at iteration 13529: 0.04214366403707408\n",
      "Train Loss at iteration 13530: 0.042143630550453365\n",
      "Train Loss at iteration 13531: 0.042143597069378404\n",
      "Train Loss at iteration 13532: 0.042143563593848236\n",
      "Train Loss at iteration 13533: 0.042143530123861975\n",
      "Train Loss at iteration 13534: 0.04214349665941868\n",
      "Train Loss at iteration 13535: 0.04214346320051743\n",
      "Train Loss at iteration 13536: 0.04214342974715731\n",
      "Train Loss at iteration 13537: 0.04214339629933741\n",
      "Train Loss at iteration 13538: 0.042143362857056776\n",
      "Train Loss at iteration 13539: 0.04214332942031453\n",
      "Train Loss at iteration 13540: 0.04214329598910972\n",
      "Train Loss at iteration 13541: 0.042143262563441454\n",
      "Train Loss at iteration 13542: 0.04214322914330879\n",
      "Train Loss at iteration 13543: 0.042143195728710815\n",
      "Train Loss at iteration 13544: 0.042143162319646627\n",
      "Train Loss at iteration 13545: 0.042143128916115276\n",
      "Train Loss at iteration 13546: 0.042143095518115875\n",
      "Train Loss at iteration 13547: 0.04214306212564749\n",
      "Train Loss at iteration 13548: 0.0421430287387092\n",
      "Train Loss at iteration 13549: 0.042142995357300074\n",
      "Train Loss at iteration 13550: 0.04214296198141923\n",
      "Train Loss at iteration 13551: 0.04214292861106572\n",
      "Train Loss at iteration 13552: 0.04214289524623863\n",
      "Train Loss at iteration 13553: 0.042142861886937055\n",
      "Train Loss at iteration 13554: 0.042142828533160086\n",
      "Train Loss at iteration 13555: 0.042142795184906774\n",
      "Train Loss at iteration 13556: 0.042142761842176225\n",
      "Train Loss at iteration 13557: 0.04214272850496752\n",
      "Train Loss at iteration 13558: 0.042142695173279725\n",
      "Train Loss at iteration 13559: 0.042142661847111934\n",
      "Train Loss at iteration 13560: 0.04214262852646324\n",
      "Train Loss at iteration 13561: 0.04214259521133273\n",
      "Train Loss at iteration 13562: 0.042142561901719465\n",
      "Train Loss at iteration 13563: 0.04214252859762255\n",
      "Train Loss at iteration 13564: 0.04214249529904106\n",
      "Train Loss at iteration 13565: 0.04214246200597406\n",
      "Train Loss at iteration 13566: 0.04214242871842067\n",
      "Train Loss at iteration 13567: 0.04214239543637995\n",
      "Train Loss at iteration 13568: 0.042142362159851\n",
      "Train Loss at iteration 13569: 0.0421423288888329\n",
      "Train Loss at iteration 13570: 0.04214229562332472\n",
      "Train Loss at iteration 13571: 0.042142262363325585\n",
      "Train Loss at iteration 13572: 0.04214222910883453\n",
      "Train Loss at iteration 13573: 0.04214219585985067\n",
      "Train Loss at iteration 13574: 0.042142162616373074\n",
      "Train Loss at iteration 13575: 0.04214212937840085\n",
      "Train Loss at iteration 13576: 0.04214209614593307\n",
      "Train Loss at iteration 13577: 0.042142062918968826\n",
      "Train Loss at iteration 13578: 0.04214202969750719\n",
      "Train Loss at iteration 13579: 0.04214199648154726\n",
      "Train Loss at iteration 13580: 0.04214196327108813\n",
      "Train Loss at iteration 13581: 0.04214193006612887\n",
      "Train Loss at iteration 13582: 0.04214189686666858\n",
      "Train Loss at iteration 13583: 0.04214186367270634\n",
      "Train Loss at iteration 13584: 0.042141830484241236\n",
      "Train Loss at iteration 13585: 0.04214179730127236\n",
      "Train Loss at iteration 13586: 0.0421417641237988\n",
      "Train Loss at iteration 13587: 0.04214173095181963\n",
      "Train Loss at iteration 13588: 0.04214169778533396\n",
      "Train Loss at iteration 13589: 0.04214166462434087\n",
      "Train Loss at iteration 13590: 0.042141631468839445\n",
      "Train Loss at iteration 13591: 0.042141598318828764\n",
      "Train Loss at iteration 13592: 0.042141565174307935\n",
      "Train Loss at iteration 13593: 0.04214153203527603\n",
      "Train Loss at iteration 13594: 0.04214149890173214\n",
      "Train Loss at iteration 13595: 0.04214146577367536\n",
      "Train Loss at iteration 13596: 0.04214143265110478\n",
      "Train Loss at iteration 13597: 0.04214139953401949\n",
      "Train Loss at iteration 13598: 0.04214136642241857\n",
      "Train Loss at iteration 13599: 0.042141333316301106\n",
      "Train Loss at iteration 13600: 0.04214130021566621\n",
      "Train Loss at iteration 13601: 0.04214126712051294\n",
      "Train Loss at iteration 13602: 0.04214123403084043\n",
      "Train Loss at iteration 13603: 0.04214120094664772\n",
      "Train Loss at iteration 13604: 0.04214116786793394\n",
      "Train Loss at iteration 13605: 0.04214113479469816\n",
      "Train Loss at iteration 13606: 0.042141101726939474\n",
      "Train Loss at iteration 13607: 0.042141068664656965\n",
      "Train Loss at iteration 13608: 0.042141035607849736\n",
      "Train Loss at iteration 13609: 0.042141002556516885\n",
      "Train Loss at iteration 13610: 0.04214096951065748\n",
      "Train Loss at iteration 13611: 0.04214093647027063\n",
      "Train Loss at iteration 13612: 0.042140903435355426\n",
      "Train Loss at iteration 13613: 0.04214087040591095\n",
      "Train Loss at iteration 13614: 0.04214083738193629\n",
      "Train Loss at iteration 13615: 0.042140804363430553\n",
      "Train Loss at iteration 13616: 0.042140771350392826\n",
      "Train Loss at iteration 13617: 0.042140738342822205\n",
      "Train Loss at iteration 13618: 0.042140705340717774\n",
      "Train Loss at iteration 13619: 0.042140672344078624\n",
      "Train Loss at iteration 13620: 0.042140639352903846\n",
      "Train Loss at iteration 13621: 0.042140606367192544\n",
      "Train Loss at iteration 13622: 0.04214057338694381\n",
      "Train Loss at iteration 13623: 0.042140540412156736\n",
      "Train Loss at iteration 13624: 0.042140507442830405\n",
      "Train Loss at iteration 13625: 0.04214047447896392\n",
      "Train Loss at iteration 13626: 0.04214044152055637\n",
      "Train Loss at iteration 13627: 0.042140408567606855\n",
      "Train Loss at iteration 13628: 0.042140375620114454\n",
      "Train Loss at iteration 13629: 0.04214034267807829\n",
      "Train Loss at iteration 13630: 0.04214030974149741\n",
      "Train Loss at iteration 13631: 0.04214027681037097\n",
      "Train Loss at iteration 13632: 0.04214024388469802\n",
      "Train Loss at iteration 13633: 0.04214021096447766\n",
      "Train Loss at iteration 13634: 0.042140178049709\n",
      "Train Loss at iteration 13635: 0.042140145140391126\n",
      "Train Loss at iteration 13636: 0.04214011223652312\n",
      "Train Loss at iteration 13637: 0.042140079338104104\n",
      "Train Loss at iteration 13638: 0.04214004644513315\n",
      "Train Loss at iteration 13639: 0.04214001355760937\n",
      "Train Loss at iteration 13640: 0.042139980675531846\n",
      "Train Loss at iteration 13641: 0.04213994779889969\n",
      "Train Loss at iteration 13642: 0.04213991492771197\n",
      "Train Loss at iteration 13643: 0.042139882061967816\n",
      "Train Loss at iteration 13644: 0.0421398492016663\n",
      "Train Loss at iteration 13645: 0.042139816346806545\n",
      "Train Loss at iteration 13646: 0.04213978349738762\n",
      "Train Loss at iteration 13647: 0.04213975065340863\n",
      "Train Loss at iteration 13648: 0.04213971781486867\n",
      "Train Loss at iteration 13649: 0.04213968498176685\n",
      "Train Loss at iteration 13650: 0.04213965215410225\n",
      "Train Loss at iteration 13651: 0.04213961933187399\n",
      "Train Loss at iteration 13652: 0.04213958651508112\n",
      "Train Loss at iteration 13653: 0.04213955370372281\n",
      "Train Loss at iteration 13654: 0.04213952089779809\n",
      "Train Loss at iteration 13655: 0.0421394880973061\n",
      "Train Loss at iteration 13656: 0.04213945530224592\n",
      "Train Loss at iteration 13657: 0.04213942251261665\n",
      "Train Loss at iteration 13658: 0.04213938972841741\n",
      "Train Loss at iteration 13659: 0.042139356949647266\n",
      "Train Loss at iteration 13660: 0.04213932417630532\n",
      "Train Loss at iteration 13661: 0.04213929140839069\n",
      "Train Loss at iteration 13662: 0.04213925864590247\n",
      "Train Loss at iteration 13663: 0.04213922588883975\n",
      "Train Loss at iteration 13664: 0.042139193137201644\n",
      "Train Loss at iteration 13665: 0.04213916039098723\n",
      "Train Loss at iteration 13666: 0.04213912765019564\n",
      "Train Loss at iteration 13667: 0.04213909491482592\n",
      "Train Loss at iteration 13668: 0.04213906218487722\n",
      "Train Loss at iteration 13669: 0.04213902946034863\n",
      "Train Loss at iteration 13670: 0.042138996741239235\n",
      "Train Loss at iteration 13671: 0.042138964027548145\n",
      "Train Loss at iteration 13672: 0.042138931319274466\n",
      "Train Loss at iteration 13673: 0.04213889861641729\n",
      "Train Loss at iteration 13674: 0.042138865918975726\n",
      "Train Loss at iteration 13675: 0.042138833226948856\n",
      "Train Loss at iteration 13676: 0.04213880054033581\n",
      "Train Loss at iteration 13677: 0.04213876785913566\n",
      "Train Loss at iteration 13678: 0.04213873518334752\n",
      "Train Loss at iteration 13679: 0.04213870251297049\n",
      "Train Loss at iteration 13680: 0.04213866984800369\n",
      "Train Loss at iteration 13681: 0.0421386371884462\n",
      "Train Loss at iteration 13682: 0.042138604534297124\n",
      "Train Loss at iteration 13683: 0.042138571885555566\n",
      "Train Loss at iteration 13684: 0.042138539242220635\n",
      "Train Loss at iteration 13685: 0.04213850660429143\n",
      "Train Loss at iteration 13686: 0.04213847397176705\n",
      "Train Loss at iteration 13687: 0.0421384413446466\n",
      "Train Loss at iteration 13688: 0.042138408722929184\n",
      "Train Loss at iteration 13689: 0.04213837610661392\n",
      "Train Loss at iteration 13690: 0.04213834349569987\n",
      "Train Loss at iteration 13691: 0.04213831089018618\n",
      "Train Loss at iteration 13692: 0.04213827829007193\n",
      "Train Loss at iteration 13693: 0.04213824569535624\n",
      "Train Loss at iteration 13694: 0.0421382131060382\n",
      "Train Loss at iteration 13695: 0.04213818052211691\n",
      "Train Loss at iteration 13696: 0.04213814794359149\n",
      "Train Loss at iteration 13697: 0.042138115370461034\n",
      "Train Loss at iteration 13698: 0.042138082802724644\n",
      "Train Loss at iteration 13699: 0.04213805024038144\n",
      "Train Loss at iteration 13700: 0.042138017683430506\n",
      "Train Loss at iteration 13701: 0.042137985131870954\n",
      "Train Loss at iteration 13702: 0.04213795258570191\n",
      "Train Loss at iteration 13703: 0.042137920044922464\n",
      "Train Loss at iteration 13704: 0.042137887509531693\n",
      "Train Loss at iteration 13705: 0.042137854979528745\n",
      "Train Loss at iteration 13706: 0.042137822454912696\n",
      "Train Loss at iteration 13707: 0.042137789935682686\n",
      "Train Loss at iteration 13708: 0.04213775742183779\n",
      "Train Loss at iteration 13709: 0.04213772491337713\n",
      "Train Loss at iteration 13710: 0.04213769241029979\n",
      "Train Loss at iteration 13711: 0.04213765991260491\n",
      "Train Loss at iteration 13712: 0.04213762742029156\n",
      "Train Loss at iteration 13713: 0.04213759493335888\n",
      "Train Loss at iteration 13714: 0.04213756245180596\n",
      "Train Loss at iteration 13715: 0.04213752997563191\n",
      "Train Loss at iteration 13716: 0.042137497504835836\n",
      "Train Loss at iteration 13717: 0.04213746503941683\n",
      "Train Loss at iteration 13718: 0.04213743257937404\n",
      "Train Loss at iteration 13719: 0.042137400124706535\n",
      "Train Loss at iteration 13720: 0.04213736767541344\n",
      "Train Loss at iteration 13721: 0.042137335231493864\n",
      "Train Loss at iteration 13722: 0.0421373027929469\n",
      "Train Loss at iteration 13723: 0.04213727035977167\n",
      "Train Loss at iteration 13724: 0.042137237931967296\n",
      "Train Loss at iteration 13725: 0.042137205509532843\n",
      "Train Loss at iteration 13726: 0.042137173092467464\n",
      "Train Loss at iteration 13727: 0.04213714068077024\n",
      "Train Loss at iteration 13728: 0.042137108274440296\n",
      "Train Loss at iteration 13729: 0.04213707587347674\n",
      "Train Loss at iteration 13730: 0.042137043477878666\n",
      "Train Loss at iteration 13731: 0.0421370110876452\n",
      "Train Loss at iteration 13732: 0.04213697870277544\n",
      "Train Loss at iteration 13733: 0.042136946323268504\n",
      "Train Loss at iteration 13734: 0.04213691394912349\n",
      "Train Loss at iteration 13735: 0.04213688158033953\n",
      "Train Loss at iteration 13736: 0.04213684921691571\n",
      "Train Loss at iteration 13737: 0.04213681685885116\n",
      "Train Loss at iteration 13738: 0.04213678450614498\n",
      "Train Loss at iteration 13739: 0.042136752158796274\n",
      "Train Loss at iteration 13740: 0.04213671981680417\n",
      "Train Loss at iteration 13741: 0.04213668748016775\n",
      "Train Loss at iteration 13742: 0.04213665514888617\n",
      "Train Loss at iteration 13743: 0.042136622822958505\n",
      "Train Loss at iteration 13744: 0.042136590502383865\n",
      "Train Loss at iteration 13745: 0.042136558187161395\n",
      "Train Loss at iteration 13746: 0.042136525877290165\n",
      "Train Loss at iteration 13747: 0.04213649357276932\n",
      "Train Loss at iteration 13748: 0.042136461273597964\n",
      "Train Loss at iteration 13749: 0.04213642897977519\n",
      "Train Loss at iteration 13750: 0.042136396691300135\n",
      "Train Loss at iteration 13751: 0.04213636440817189\n",
      "Train Loss at iteration 13752: 0.04213633213038959\n",
      "Train Loss at iteration 13753: 0.04213629985795232\n",
      "Train Loss at iteration 13754: 0.042136267590859214\n",
      "Train Loss at iteration 13755: 0.042136235329109394\n",
      "Train Loss at iteration 13756: 0.042136203072701946\n",
      "Train Loss at iteration 13757: 0.042136170821635996\n",
      "Train Loss at iteration 13758: 0.04213613857591065\n",
      "Train Loss at iteration 13759: 0.04213610633552503\n",
      "Train Loss at iteration 13760: 0.04213607410047827\n",
      "Train Loss at iteration 13761: 0.04213604187076944\n",
      "Train Loss at iteration 13762: 0.042136009646397674\n",
      "Train Loss at iteration 13763: 0.0421359774273621\n",
      "Train Loss at iteration 13764: 0.04213594521366182\n",
      "Train Loss at iteration 13765: 0.04213591300529595\n",
      "Train Loss at iteration 13766: 0.04213588080226359\n",
      "Train Loss at iteration 13767: 0.04213584860456388\n",
      "Train Loss at iteration 13768: 0.042135816412195916\n",
      "Train Loss at iteration 13769: 0.04213578422515884\n",
      "Train Loss at iteration 13770: 0.04213575204345172\n",
      "Train Loss at iteration 13771: 0.04213571986707372\n",
      "Train Loss at iteration 13772: 0.04213568769602392\n",
      "Train Loss at iteration 13773: 0.04213565553030146\n",
      "Train Loss at iteration 13774: 0.04213562336990544\n",
      "Train Loss at iteration 13775: 0.042135591214834976\n",
      "Train Loss at iteration 13776: 0.0421355590650892\n",
      "Train Loss at iteration 13777: 0.0421355269206672\n",
      "Train Loss at iteration 13778: 0.042135494781568125\n",
      "Train Loss at iteration 13779: 0.04213546264779108\n",
      "Train Loss at iteration 13780: 0.04213543051933517\n",
      "Train Loss at iteration 13781: 0.04213539839619952\n",
      "Train Loss at iteration 13782: 0.042135366278383254\n",
      "Train Loss at iteration 13783: 0.04213533416588548\n",
      "Train Loss at iteration 13784: 0.04213530205870532\n",
      "Train Loss at iteration 13785: 0.04213526995684187\n",
      "Train Loss at iteration 13786: 0.04213523786029429\n",
      "Train Loss at iteration 13787: 0.04213520576906168\n",
      "Train Loss at iteration 13788: 0.04213517368314312\n",
      "Train Loss at iteration 13789: 0.042135141602537785\n",
      "Train Loss at iteration 13790: 0.04213510952724476\n",
      "Train Loss at iteration 13791: 0.04213507745726317\n",
      "Train Loss at iteration 13792: 0.04213504539259213\n",
      "Train Loss at iteration 13793: 0.04213501333323078\n",
      "Train Loss at iteration 13794: 0.0421349812791782\n",
      "Train Loss at iteration 13795: 0.04213494923043355\n",
      "Train Loss at iteration 13796: 0.04213491718699591\n",
      "Train Loss at iteration 13797: 0.04213488514886444\n",
      "Train Loss at iteration 13798: 0.04213485311603823\n",
      "Train Loss at iteration 13799: 0.04213482108851641\n",
      "Train Loss at iteration 13800: 0.042134789066298084\n",
      "Train Loss at iteration 13801: 0.042134757049382396\n",
      "Train Loss at iteration 13802: 0.042134725037768474\n",
      "Train Loss at iteration 13803: 0.042134693031455395\n",
      "Train Loss at iteration 13804: 0.0421346610304423\n",
      "Train Loss at iteration 13805: 0.04213462903472833\n",
      "Train Loss at iteration 13806: 0.04213459704431258\n",
      "Train Loss at iteration 13807: 0.04213456505919417\n",
      "Train Loss at iteration 13808: 0.04213453307937224\n",
      "Train Loss at iteration 13809: 0.0421345011048459\n",
      "Train Loss at iteration 13810: 0.04213446913561428\n",
      "Train Loss at iteration 13811: 0.04213443717167648\n",
      "Train Loss at iteration 13812: 0.04213440521303164\n",
      "Train Loss at iteration 13813: 0.042134373259678864\n",
      "Train Loss at iteration 13814: 0.04213434131161731\n",
      "Train Loss at iteration 13815: 0.04213430936884605\n",
      "Train Loss at iteration 13816: 0.04213427743136425\n",
      "Train Loss at iteration 13817: 0.042134245499171004\n",
      "Train Loss at iteration 13818: 0.04213421357226545\n",
      "Train Loss at iteration 13819: 0.04213418165064669\n",
      "Train Loss at iteration 13820: 0.042134149734313876\n",
      "Train Loss at iteration 13821: 0.042134117823266114\n",
      "Train Loss at iteration 13822: 0.04213408591750252\n",
      "Train Loss at iteration 13823: 0.04213405401702224\n",
      "Train Loss at iteration 13824: 0.04213402212182436\n",
      "Train Loss at iteration 13825: 0.042133990231908044\n",
      "Train Loss at iteration 13826: 0.0421339583472724\n",
      "Train Loss at iteration 13827: 0.042133926467916534\n",
      "Train Loss at iteration 13828: 0.042133894593839594\n",
      "Train Loss at iteration 13829: 0.04213386272504069\n",
      "Train Loss at iteration 13830: 0.04213383086151895\n",
      "Train Loss at iteration 13831: 0.0421337990032735\n",
      "Train Loss at iteration 13832: 0.04213376715030346\n",
      "Train Loss at iteration 13833: 0.04213373530260797\n",
      "Train Loss at iteration 13834: 0.04213370346018613\n",
      "Train Loss at iteration 13835: 0.04213367162303707\n",
      "Train Loss at iteration 13836: 0.042133639791159944\n",
      "Train Loss at iteration 13837: 0.04213360796455384\n",
      "Train Loss at iteration 13838: 0.042133576143217895\n",
      "Train Loss at iteration 13839: 0.042133544327151246\n",
      "Train Loss at iteration 13840: 0.04213351251635301\n",
      "Train Loss at iteration 13841: 0.0421334807108223\n",
      "Train Loss at iteration 13842: 0.04213344891055826\n",
      "Train Loss at iteration 13843: 0.042133417115560026\n",
      "Train Loss at iteration 13844: 0.04213338532582669\n",
      "Train Loss at iteration 13845: 0.0421333535413574\n",
      "Train Loss at iteration 13846: 0.042133321762151274\n",
      "Train Loss at iteration 13847: 0.04213328998820746\n",
      "Train Loss at iteration 13848: 0.042133258219525044\n",
      "Train Loss at iteration 13849: 0.04213322645610319\n",
      "Train Loss at iteration 13850: 0.042133194697941016\n",
      "Train Loss at iteration 13851: 0.04213316294503764\n",
      "Train Loss at iteration 13852: 0.04213313119739219\n",
      "Train Loss at iteration 13853: 0.042133099455003806\n",
      "Train Loss at iteration 13854: 0.0421330677178716\n",
      "Train Loss at iteration 13855: 0.04213303598599471\n",
      "Train Loss at iteration 13856: 0.04213300425937227\n",
      "Train Loss at iteration 13857: 0.04213297253800338\n",
      "Train Loss at iteration 13858: 0.042132940821887205\n",
      "Train Loss at iteration 13859: 0.04213290911102285\n",
      "Train Loss at iteration 13860: 0.04213287740540944\n",
      "Train Loss at iteration 13861: 0.04213284570504611\n",
      "Train Loss at iteration 13862: 0.042132814009932\n",
      "Train Loss at iteration 13863: 0.042132782320066224\n",
      "Train Loss at iteration 13864: 0.04213275063544791\n",
      "Train Loss at iteration 13865: 0.042132718956076204\n",
      "Train Loss at iteration 13866: 0.04213268728195023\n",
      "Train Loss at iteration 13867: 0.0421326556130691\n",
      "Train Loss at iteration 13868: 0.04213262394943195\n",
      "Train Loss at iteration 13869: 0.04213259229103793\n",
      "Train Loss at iteration 13870: 0.04213256063788617\n",
      "Train Loss at iteration 13871: 0.04213252898997575\n",
      "Train Loss at iteration 13872: 0.04213249734730586\n",
      "Train Loss at iteration 13873: 0.04213246570987561\n",
      "Train Loss at iteration 13874: 0.0421324340776841\n",
      "Train Loss at iteration 13875: 0.04213240245073051\n",
      "Train Loss at iteration 13876: 0.04213237082901394\n",
      "Train Loss at iteration 13877: 0.04213233921253354\n",
      "Train Loss at iteration 13878: 0.042132307601288416\n",
      "Train Loss at iteration 13879: 0.042132275995277725\n",
      "Train Loss at iteration 13880: 0.04213224439450058\n",
      "Train Loss at iteration 13881: 0.042132212798956105\n",
      "Train Loss at iteration 13882: 0.042132181208643454\n",
      "Train Loss at iteration 13883: 0.04213214962356177\n",
      "Train Loss at iteration 13884: 0.042132118043710136\n",
      "Train Loss at iteration 13885: 0.04213208646908773\n",
      "Train Loss at iteration 13886: 0.042132054899693655\n",
      "Train Loss at iteration 13887: 0.042132023335527076\n",
      "Train Loss at iteration 13888: 0.04213199177658709\n",
      "Train Loss at iteration 13889: 0.04213196022287284\n",
      "Train Loss at iteration 13890: 0.042131928674383486\n",
      "Train Loss at iteration 13891: 0.04213189713111812\n",
      "Train Loss at iteration 13892: 0.0421318655930759\n",
      "Train Loss at iteration 13893: 0.042131834060255946\n",
      "Train Loss at iteration 13894: 0.0421318025326574\n",
      "Train Loss at iteration 13895: 0.0421317710102794\n",
      "Train Loss at iteration 13896: 0.04213173949312107\n",
      "Train Loss at iteration 13897: 0.04213170798118155\n",
      "Train Loss at iteration 13898: 0.042131676474459964\n",
      "Train Loss at iteration 13899: 0.042131644972955455\n",
      "Train Loss at iteration 13900: 0.04213161347666717\n",
      "Train Loss at iteration 13901: 0.04213158198559421\n",
      "Train Loss at iteration 13902: 0.04213155049973574\n",
      "Train Loss at iteration 13903: 0.04213151901909088\n",
      "Train Loss at iteration 13904: 0.04213148754365877\n",
      "Train Loss at iteration 13905: 0.04213145607343853\n",
      "Train Loss at iteration 13906: 0.04213142460842933\n",
      "Train Loss at iteration 13907: 0.04213139314863027\n",
      "Train Loss at iteration 13908: 0.0421313616940405\n",
      "Train Loss at iteration 13909: 0.04213133024465914\n",
      "Train Loss at iteration 13910: 0.04213129880048536\n",
      "Train Loss at iteration 13911: 0.04213126736151827\n",
      "Train Loss at iteration 13912: 0.04213123592775701\n",
      "Train Loss at iteration 13913: 0.042131204499200714\n",
      "Train Loss at iteration 13914: 0.04213117307584853\n",
      "Train Loss at iteration 13915: 0.04213114165769958\n",
      "Train Loss at iteration 13916: 0.042131110244753016\n",
      "Train Loss at iteration 13917: 0.04213107883700796\n",
      "Train Loss at iteration 13918: 0.04213104743446355\n",
      "Train Loss at iteration 13919: 0.042131016037118926\n",
      "Train Loss at iteration 13920: 0.042130984644973245\n",
      "Train Loss at iteration 13921: 0.0421309532580256\n",
      "Train Loss at iteration 13922: 0.04213092187627517\n",
      "Train Loss at iteration 13923: 0.04213089049972108\n",
      "Train Loss at iteration 13924: 0.04213085912836247\n",
      "Train Loss at iteration 13925: 0.042130827762198446\n",
      "Train Loss at iteration 13926: 0.04213079640122819\n",
      "Train Loss at iteration 13927: 0.042130765045450824\n",
      "Train Loss at iteration 13928: 0.04213073369486548\n",
      "Train Loss at iteration 13929: 0.042130702349471305\n",
      "Train Loss at iteration 13930: 0.04213067100926743\n",
      "Train Loss at iteration 13931: 0.042130639674253\n",
      "Train Loss at iteration 13932: 0.04213060834442715\n",
      "Train Loss at iteration 13933: 0.04213057701978903\n",
      "Train Loss at iteration 13934: 0.04213054570033776\n",
      "Train Loss at iteration 13935: 0.04213051438607249\n",
      "Train Loss at iteration 13936: 0.04213048307699236\n",
      "Train Loss at iteration 13937: 0.0421304517730965\n",
      "Train Loss at iteration 13938: 0.04213042047438406\n",
      "Train Loss at iteration 13939: 0.042130389180854176\n",
      "Train Loss at iteration 13940: 0.042130357892506\n",
      "Train Loss at iteration 13941: 0.04213032660933865\n",
      "Train Loss at iteration 13942: 0.04213029533135129\n",
      "Train Loss at iteration 13943: 0.04213026405854304\n",
      "Train Loss at iteration 13944: 0.042130232790913054\n",
      "Train Loss at iteration 13945: 0.04213020152846046\n",
      "Train Loss at iteration 13946: 0.04213017027118442\n",
      "Train Loss at iteration 13947: 0.04213013901908406\n",
      "Train Loss at iteration 13948: 0.04213010777215851\n",
      "Train Loss at iteration 13949: 0.042130076530406936\n",
      "Train Loss at iteration 13950: 0.04213004529382845\n",
      "Train Loss at iteration 13951: 0.04213001406242224\n",
      "Train Loss at iteration 13952: 0.04212998283618739\n",
      "Train Loss at iteration 13953: 0.04212995161512308\n",
      "Train Loss at iteration 13954: 0.04212992039922844\n",
      "Train Loss at iteration 13955: 0.04212988918850264\n",
      "Train Loss at iteration 13956: 0.042129857982944774\n",
      "Train Loss at iteration 13957: 0.04212982678255401\n",
      "Train Loss at iteration 13958: 0.04212979558732949\n",
      "Train Loss at iteration 13959: 0.04212976439727036\n",
      "Train Loss at iteration 13960: 0.04212973321237575\n",
      "Train Loss at iteration 13961: 0.042129702032644815\n",
      "Train Loss at iteration 13962: 0.042129670858076694\n",
      "Train Loss at iteration 13963: 0.042129639688670534\n",
      "Train Loss at iteration 13964: 0.042129608524425474\n",
      "Train Loss at iteration 13965: 0.042129577365340655\n",
      "Train Loss at iteration 13966: 0.042129546211415236\n",
      "Train Loss at iteration 13967: 0.04212951506264833\n",
      "Train Loss at iteration 13968: 0.04212948391903911\n",
      "Train Loss at iteration 13969: 0.04212945278058672\n",
      "Train Loss at iteration 13970: 0.04212942164729028\n",
      "Train Loss at iteration 13971: 0.042129390519148956\n",
      "Train Loss at iteration 13972: 0.04212935939616188\n",
      "Train Loss at iteration 13973: 0.042129328278328194\n",
      "Train Loss at iteration 13974: 0.042129297165647075\n",
      "Train Loss at iteration 13975: 0.04212926605811763\n",
      "Train Loss at iteration 13976: 0.042129234955739024\n",
      "Train Loss at iteration 13977: 0.0421292038585104\n",
      "Train Loss at iteration 13978: 0.04212917276643089\n",
      "Train Loss at iteration 13979: 0.04212914167949966\n",
      "Train Loss at iteration 13980: 0.04212911059771584\n",
      "Train Loss at iteration 13981: 0.04212907952107859\n",
      "Train Loss at iteration 13982: 0.042129048449587043\n",
      "Train Loss at iteration 13983: 0.04212901738324034\n",
      "Train Loss at iteration 13984: 0.04212898632203765\n",
      "Train Loss at iteration 13985: 0.0421289552659781\n",
      "Train Loss at iteration 13986: 0.042128924215060856\n",
      "Train Loss at iteration 13987: 0.04212889316928503\n",
      "Train Loss at iteration 13988: 0.04212886212864981\n",
      "Train Loss at iteration 13989: 0.04212883109315433\n",
      "Train Loss at iteration 13990: 0.04212880006279771\n",
      "Train Loss at iteration 13991: 0.042128769037579124\n",
      "Train Loss at iteration 13992: 0.042128738017497724\n",
      "Train Loss at iteration 13993: 0.04212870700255265\n",
      "Train Loss at iteration 13994: 0.04212867599274303\n",
      "Train Loss at iteration 13995: 0.04212864498806804\n",
      "Train Loss at iteration 13996: 0.042128613988526804\n",
      "Train Loss at iteration 13997: 0.04212858299411851\n",
      "Train Loss at iteration 13998: 0.042128552004842265\n",
      "Train Loss at iteration 13999: 0.04212852102069723\n",
      "Train Loss at iteration 14000: 0.04212849004168256\n",
      "Train Loss at iteration 14001: 0.0421284590677974\n",
      "Train Loss at iteration 14002: 0.04212842809904089\n",
      "Train Loss at iteration 14003: 0.0421283971354122\n",
      "Train Loss at iteration 14004: 0.042128366176910466\n",
      "Train Loss at iteration 14005: 0.042128335223534825\n",
      "Train Loss at iteration 14006: 0.04212830427528445\n",
      "Train Loss at iteration 14007: 0.04212827333215849\n",
      "Train Loss at iteration 14008: 0.042128242394156065\n",
      "Train Loss at iteration 14009: 0.042128211461276356\n",
      "Train Loss at iteration 14010: 0.042128180533518494\n",
      "Train Loss at iteration 14011: 0.042128149610881654\n",
      "Train Loss at iteration 14012: 0.04212811869336494\n",
      "Train Loss at iteration 14013: 0.04212808778096756\n",
      "Train Loss at iteration 14014: 0.042128056873688625\n",
      "Train Loss at iteration 14015: 0.04212802597152731\n",
      "Train Loss at iteration 14016: 0.04212799507448273\n",
      "Train Loss at iteration 14017: 0.042127964182554066\n",
      "Train Loss at iteration 14018: 0.042127933295740466\n",
      "Train Loss at iteration 14019: 0.04212790241404108\n",
      "Train Loss at iteration 14020: 0.042127871537455064\n",
      "Train Loss at iteration 14021: 0.04212784066598155\n",
      "Train Loss at iteration 14022: 0.042127809799619706\n",
      "Train Loss at iteration 14023: 0.04212777893836867\n",
      "Train Loss at iteration 14024: 0.04212774808222762\n",
      "Train Loss at iteration 14025: 0.04212771723119568\n",
      "Train Loss at iteration 14026: 0.04212768638527203\n",
      "Train Loss at iteration 14027: 0.0421276555444558\n",
      "Train Loss at iteration 14028: 0.04212762470874614\n",
      "Train Loss at iteration 14029: 0.042127593878142224\n",
      "Train Loss at iteration 14030: 0.042127563052643195\n",
      "Train Loss at iteration 14031: 0.042127532232248195\n",
      "Train Loss at iteration 14032: 0.0421275014169564\n",
      "Train Loss at iteration 14033: 0.04212747060676693\n",
      "Train Loss at iteration 14034: 0.042127439801678976\n",
      "Train Loss at iteration 14035: 0.042127409001691665\n",
      "Train Loss at iteration 14036: 0.04212737820680416\n",
      "Train Loss at iteration 14037: 0.04212734741701561\n",
      "Train Loss at iteration 14038: 0.04212731663232519\n",
      "Train Loss at iteration 14039: 0.04212728585273202\n",
      "Train Loss at iteration 14040: 0.04212725507823528\n",
      "Train Loss at iteration 14041: 0.042127224308834116\n",
      "Train Loss at iteration 14042: 0.04212719354452767\n",
      "Train Loss at iteration 14043: 0.04212716278531513\n",
      "Train Loss at iteration 14044: 0.04212713203119563\n",
      "Train Loss at iteration 14045: 0.0421271012821683\n",
      "Train Loss at iteration 14046: 0.04212707053823234\n",
      "Train Loss at iteration 14047: 0.04212703979938689\n",
      "Train Loss at iteration 14048: 0.0421270090656311\n",
      "Train Loss at iteration 14049: 0.04212697833696411\n",
      "Train Loss at iteration 14050: 0.04212694761338511\n",
      "Train Loss at iteration 14051: 0.042126916894893236\n",
      "Train Loss at iteration 14052: 0.04212688618148763\n",
      "Train Loss at iteration 14053: 0.042126855473167475\n",
      "Train Loss at iteration 14054: 0.042126824769931924\n",
      "Train Loss at iteration 14055: 0.04212679407178012\n",
      "Train Loss at iteration 14056: 0.04212676337871122\n",
      "Train Loss at iteration 14057: 0.0421267326907244\n",
      "Train Loss at iteration 14058: 0.042126702007818796\n",
      "Train Loss at iteration 14059: 0.042126671329993565\n",
      "Train Loss at iteration 14060: 0.04212664065724788\n",
      "Train Loss at iteration 14061: 0.04212660998958089\n",
      "Train Loss at iteration 14062: 0.04212657932699175\n",
      "Train Loss at iteration 14063: 0.042126548669479624\n",
      "Train Loss at iteration 14064: 0.04212651801704365\n",
      "Train Loss at iteration 14065: 0.04212648736968302\n",
      "Train Loss at iteration 14066: 0.04212645672739686\n",
      "Train Loss at iteration 14067: 0.04212642609018435\n",
      "Train Loss at iteration 14068: 0.04212639545804463\n",
      "Train Loss at iteration 14069: 0.04212636483097686\n",
      "Train Loss at iteration 14070: 0.04212633420898022\n",
      "Train Loss at iteration 14071: 0.04212630359205386\n",
      "Train Loss at iteration 14072: 0.042126272980196916\n",
      "Train Loss at iteration 14073: 0.04212624237340859\n",
      "Train Loss at iteration 14074: 0.04212621177168799\n",
      "Train Loss at iteration 14075: 0.04212618117503432\n",
      "Train Loss at iteration 14076: 0.0421261505834467\n",
      "Train Loss at iteration 14077: 0.04212611999692431\n",
      "Train Loss at iteration 14078: 0.042126089415466327\n",
      "Train Loss at iteration 14079: 0.04212605883907188\n",
      "Train Loss at iteration 14080: 0.04212602826774015\n",
      "Train Loss at iteration 14081: 0.04212599770147029\n",
      "Train Loss at iteration 14082: 0.04212596714026147\n",
      "Train Loss at iteration 14083: 0.042125936584112816\n",
      "Train Loss at iteration 14084: 0.04212590603302351\n",
      "Train Loss at iteration 14085: 0.04212587548699274\n",
      "Train Loss at iteration 14086: 0.04212584494601963\n",
      "Train Loss at iteration 14087: 0.042125814410103345\n",
      "Train Loss at iteration 14088: 0.04212578387924306\n",
      "Train Loss at iteration 14089: 0.042125753353437934\n",
      "Train Loss at iteration 14090: 0.04212572283268711\n",
      "Train Loss at iteration 14091: 0.04212569231698978\n",
      "Train Loss at iteration 14092: 0.04212566180634507\n",
      "Train Loss at iteration 14093: 0.04212563130075218\n",
      "Train Loss at iteration 14094: 0.042125600800210244\n",
      "Train Loss at iteration 14095: 0.04212557030471844\n",
      "Train Loss at iteration 14096: 0.04212553981427592\n",
      "Train Loss at iteration 14097: 0.04212550932888184\n",
      "Train Loss at iteration 14098: 0.04212547884853537\n",
      "Train Loss at iteration 14099: 0.04212544837323569\n",
      "Train Loss at iteration 14100: 0.04212541790298193\n",
      "Train Loss at iteration 14101: 0.04212538743777328\n",
      "Train Loss at iteration 14102: 0.0421253569776089\n",
      "Train Loss at iteration 14103: 0.042125326522487946\n",
      "Train Loss at iteration 14104: 0.04212529607240957\n",
      "Train Loss at iteration 14105: 0.042125265627372954\n",
      "Train Loss at iteration 14106: 0.04212523518737724\n",
      "Train Loss at iteration 14107: 0.04212520475242162\n",
      "Train Loss at iteration 14108: 0.04212517432250525\n",
      "Train Loss at iteration 14109: 0.042125143897627275\n",
      "Train Loss at iteration 14110: 0.042125113477786864\n",
      "Train Loss at iteration 14111: 0.04212508306298321\n",
      "Train Loss at iteration 14112: 0.04212505265321545\n",
      "Train Loss at iteration 14113: 0.04212502224848275\n",
      "Train Loss at iteration 14114: 0.042124991848784286\n",
      "Train Loss at iteration 14115: 0.04212496145411921\n",
      "Train Loss at iteration 14116: 0.042124931064486705\n",
      "Train Loss at iteration 14117: 0.04212490067988592\n",
      "Train Loss at iteration 14118: 0.04212487030031602\n",
      "Train Loss at iteration 14119: 0.04212483992577617\n",
      "Train Loss at iteration 14120: 0.04212480955626555\n",
      "Train Loss at iteration 14121: 0.04212477919178332\n",
      "Train Loss at iteration 14122: 0.04212474883232865\n",
      "Train Loss at iteration 14123: 0.04212471847790068\n",
      "Train Loss at iteration 14124: 0.0421246881284986\n",
      "Train Loss at iteration 14125: 0.042124657784121576\n",
      "Train Loss at iteration 14126: 0.042124627444768754\n",
      "Train Loss at iteration 14127: 0.04212459711043933\n",
      "Train Loss at iteration 14128: 0.04212456678113246\n",
      "Train Loss at iteration 14129: 0.0421245364568473\n",
      "Train Loss at iteration 14130: 0.042124506137583013\n",
      "Train Loss at iteration 14131: 0.0421244758233388\n",
      "Train Loss at iteration 14132: 0.0421244455141138\n",
      "Train Loss at iteration 14133: 0.04212441520990717\n",
      "Train Loss at iteration 14134: 0.0421243849107181\n",
      "Train Loss at iteration 14135: 0.042124354616545755\n",
      "Train Loss at iteration 14136: 0.0421243243273893\n",
      "Train Loss at iteration 14137: 0.0421242940432479\n",
      "Train Loss at iteration 14138: 0.04212426376412072\n",
      "Train Loss at iteration 14139: 0.04212423349000694\n",
      "Train Loss at iteration 14140: 0.04212420322090571\n",
      "Train Loss at iteration 14141: 0.04212417295681622\n",
      "Train Loss at iteration 14142: 0.042124142697737614\n",
      "Train Loss at iteration 14143: 0.04212411244366909\n",
      "Train Loss at iteration 14144: 0.0421240821946098\n",
      "Train Loss at iteration 14145: 0.04212405195055891\n",
      "Train Loss at iteration 14146: 0.04212402171151558\n",
      "Train Loss at iteration 14147: 0.042123991477479014\n",
      "Train Loss at iteration 14148: 0.04212396124844836\n",
      "Train Loss at iteration 14149: 0.04212393102442278\n",
      "Train Loss at iteration 14150: 0.04212390080540145\n",
      "Train Loss at iteration 14151: 0.04212387059138354\n",
      "Train Loss at iteration 14152: 0.04212384038236823\n",
      "Train Loss at iteration 14153: 0.042123810178354676\n",
      "Train Loss at iteration 14154: 0.04212377997934205\n",
      "Train Loss at iteration 14155: 0.04212374978532953\n",
      "Train Loss at iteration 14156: 0.042123719596316285\n",
      "Train Loss at iteration 14157: 0.0421236894123015\n",
      "Train Loss at iteration 14158: 0.0421236592332843\n",
      "Train Loss at iteration 14159: 0.042123629059263916\n",
      "Train Loss at iteration 14160: 0.04212359889023946\n",
      "Train Loss at iteration 14161: 0.042123568726210134\n",
      "Train Loss at iteration 14162: 0.042123538567175114\n",
      "Train Loss at iteration 14163: 0.04212350841313357\n",
      "Train Loss at iteration 14164: 0.04212347826408466\n",
      "Train Loss at iteration 14165: 0.042123448120027565\n",
      "Train Loss at iteration 14166: 0.04212341798096146\n",
      "Train Loss at iteration 14167: 0.04212338784688551\n",
      "Train Loss at iteration 14168: 0.04212335771779889\n",
      "Train Loss at iteration 14169: 0.04212332759370077\n",
      "Train Loss at iteration 14170: 0.04212329747459033\n",
      "Train Loss at iteration 14171: 0.04212326736046674\n",
      "Train Loss at iteration 14172: 0.04212323725132915\n",
      "Train Loss at iteration 14173: 0.042123207147176774\n",
      "Train Loss at iteration 14174: 0.04212317704800877\n",
      "Train Loss at iteration 14175: 0.042123146953824284\n",
      "Train Loss at iteration 14176: 0.04212311686462252\n",
      "Train Loss at iteration 14177: 0.04212308678040264\n",
      "Train Loss at iteration 14178: 0.04212305670116383\n",
      "Train Loss at iteration 14179: 0.04212302662690524\n",
      "Train Loss at iteration 14180: 0.04212299655762606\n",
      "Train Loss at iteration 14181: 0.04212296649332547\n",
      "Train Loss at iteration 14182: 0.04212293643400264\n",
      "Train Loss at iteration 14183: 0.042122906379656724\n",
      "Train Loss at iteration 14184: 0.04212287633028691\n",
      "Train Loss at iteration 14185: 0.042122846285892385\n",
      "Train Loss at iteration 14186: 0.0421228162464723\n",
      "Train Loss at iteration 14187: 0.042122786212025864\n",
      "Train Loss at iteration 14188: 0.04212275618255221\n",
      "Train Loss at iteration 14189: 0.042122726158050544\n",
      "Train Loss at iteration 14190: 0.04212269613852003\n",
      "Train Loss at iteration 14191: 0.042122666123959845\n",
      "Train Loss at iteration 14192: 0.042122636114369165\n",
      "Train Loss at iteration 14193: 0.04212260610974717\n",
      "Train Loss at iteration 14194: 0.042122576110093034\n",
      "Train Loss at iteration 14195: 0.04212254611540591\n",
      "Train Loss at iteration 14196: 0.04212251612568501\n",
      "Train Loss at iteration 14197: 0.04212248614092949\n",
      "Train Loss at iteration 14198: 0.04212245616113853\n",
      "Train Loss at iteration 14199: 0.042122426186311304\n",
      "Train Loss at iteration 14200: 0.04212239621644699\n",
      "Train Loss at iteration 14201: 0.04212236625154478\n",
      "Train Loss at iteration 14202: 0.04212233629160381\n",
      "Train Loss at iteration 14203: 0.04212230633662332\n",
      "Train Loss at iteration 14204: 0.042122276386602424\n",
      "Train Loss at iteration 14205: 0.04212224644154033\n",
      "Train Loss at iteration 14206: 0.042122216501436216\n",
      "Train Loss at iteration 14207: 0.04212218656628925\n",
      "Train Loss at iteration 14208: 0.042122156636098626\n",
      "Train Loss at iteration 14209: 0.04212212671086351\n",
      "Train Loss at iteration 14210: 0.042122096790583076\n",
      "Train Loss at iteration 14211: 0.042122066875256506\n",
      "Train Loss at iteration 14212: 0.04212203696488298\n",
      "Train Loss at iteration 14213: 0.042122007059461675\n",
      "Train Loss at iteration 14214: 0.042121977158991776\n",
      "Train Loss at iteration 14215: 0.04212194726347245\n",
      "Train Loss at iteration 14216: 0.04212191737290288\n",
      "Train Loss at iteration 14217: 0.04212188748728225\n",
      "Train Loss at iteration 14218: 0.04212185760660974\n",
      "Train Loss at iteration 14219: 0.04212182773088453\n",
      "Train Loss at iteration 14220: 0.04212179786010578\n",
      "Train Loss at iteration 14221: 0.0421217679942727\n",
      "Train Loss at iteration 14222: 0.04212173813338444\n",
      "Train Loss at iteration 14223: 0.0421217082774402\n",
      "Train Loss at iteration 14224: 0.042121678426439144\n",
      "Train Loss at iteration 14225: 0.04212164858038047\n",
      "Train Loss at iteration 14226: 0.04212161873926335\n",
      "Train Loss at iteration 14227: 0.04212158890308696\n",
      "Train Loss at iteration 14228: 0.042121559071850494\n",
      "Train Loss at iteration 14229: 0.04212152924555312\n",
      "Train Loss at iteration 14230: 0.042121499424194006\n",
      "Train Loss at iteration 14231: 0.042121469607772355\n",
      "Train Loss at iteration 14232: 0.04212143979628735\n",
      "Train Loss at iteration 14233: 0.04212140998973816\n",
      "Train Loss at iteration 14234: 0.042121380188123975\n",
      "Train Loss at iteration 14235: 0.042121350391443955\n",
      "Train Loss at iteration 14236: 0.04212132059969731\n",
      "Train Loss at iteration 14237: 0.04212129081288321\n",
      "Train Loss at iteration 14238: 0.04212126103100084\n",
      "Train Loss at iteration 14239: 0.04212123125404937\n",
      "Train Loss at iteration 14240: 0.042121201482028\n",
      "Train Loss at iteration 14241: 0.0421211717149359\n",
      "Train Loss at iteration 14242: 0.042121141952772244\n",
      "Train Loss at iteration 14243: 0.042121112195536235\n",
      "Train Loss at iteration 14244: 0.042121082443227036\n",
      "Train Loss at iteration 14245: 0.042121052695843854\n",
      "Train Loss at iteration 14246: 0.042121022953385844\n",
      "Train Loss at iteration 14247: 0.04212099321585223\n",
      "Train Loss at iteration 14248: 0.04212096348324215\n",
      "Train Loss at iteration 14249: 0.04212093375555479\n",
      "Train Loss at iteration 14250: 0.04212090403278936\n",
      "Train Loss at iteration 14251: 0.042120874314945045\n",
      "Train Loss at iteration 14252: 0.04212084460202102\n",
      "Train Loss at iteration 14253: 0.042120814894016444\n",
      "Train Loss at iteration 14254: 0.042120785190930536\n",
      "Train Loss at iteration 14255: 0.04212075549276246\n",
      "Train Loss at iteration 14256: 0.04212072579951141\n",
      "Train Loss at iteration 14257: 0.04212069611117657\n",
      "Train Loss at iteration 14258: 0.04212066642775711\n",
      "Train Loss at iteration 14259: 0.04212063674925224\n",
      "Train Loss at iteration 14260: 0.04212060707566113\n",
      "Train Loss at iteration 14261: 0.042120577406982954\n",
      "Train Loss at iteration 14262: 0.04212054774321693\n",
      "Train Loss at iteration 14263: 0.0421205180843622\n",
      "Train Loss at iteration 14264: 0.04212048843041798\n",
      "Train Loss at iteration 14265: 0.04212045878138346\n",
      "Train Loss at iteration 14266: 0.04212042913725779\n",
      "Train Loss at iteration 14267: 0.042120399498040195\n",
      "Train Loss at iteration 14268: 0.04212036986372984\n",
      "Train Loss at iteration 14269: 0.04212034023432591\n",
      "Train Loss at iteration 14270: 0.042120310609827606\n",
      "Train Loss at iteration 14271: 0.04212028099023411\n",
      "Train Loss at iteration 14272: 0.042120251375544586\n",
      "Train Loss at iteration 14273: 0.042120221765758255\n",
      "Train Loss at iteration 14274: 0.04212019216087428\n",
      "Train Loss at iteration 14275: 0.042120162560891856\n",
      "Train Loss at iteration 14276: 0.04212013296581017\n",
      "Train Loss at iteration 14277: 0.0421201033756284\n",
      "Train Loss at iteration 14278: 0.042120073790345755\n",
      "Train Loss at iteration 14279: 0.042120044209961405\n",
      "Train Loss at iteration 14280: 0.04212001463447454\n",
      "Train Loss at iteration 14281: 0.04211998506388435\n",
      "Train Loss at iteration 14282: 0.04211995549819002\n",
      "Train Loss at iteration 14283: 0.04211992593739074\n",
      "Train Loss at iteration 14284: 0.04211989638148569\n",
      "Train Loss at iteration 14285: 0.04211986683047408\n",
      "Train Loss at iteration 14286: 0.04211983728435507\n",
      "Train Loss at iteration 14287: 0.04211980774312788\n",
      "Train Loss at iteration 14288: 0.042119778206791664\n",
      "Train Loss at iteration 14289: 0.042119748675345646\n",
      "Train Loss at iteration 14290: 0.04211971914878898\n",
      "Train Loss at iteration 14291: 0.042119689627120885\n",
      "Train Loss at iteration 14292: 0.04211966011034054\n",
      "Train Loss at iteration 14293: 0.04211963059844713\n",
      "Train Loss at iteration 14294: 0.042119601091439834\n",
      "Train Loss at iteration 14295: 0.042119571589317874\n",
      "Train Loss at iteration 14296: 0.042119542092080395\n",
      "Train Loss at iteration 14297: 0.04211951259972663\n",
      "Train Loss at iteration 14298: 0.04211948311225574\n",
      "Train Loss at iteration 14299: 0.04211945362966693\n",
      "Train Loss at iteration 14300: 0.04211942415195939\n",
      "Train Loss at iteration 14301: 0.0421193946791323\n",
      "Train Loss at iteration 14302: 0.04211936521118485\n",
      "Train Loss at iteration 14303: 0.04211933574811625\n",
      "Train Loss at iteration 14304: 0.04211930628992568\n",
      "Train Loss at iteration 14305: 0.042119276836612325\n",
      "Train Loss at iteration 14306: 0.042119247388175386\n",
      "Train Loss at iteration 14307: 0.04211921794461404\n",
      "Train Loss at iteration 14308: 0.04211918850592749\n",
      "Train Loss at iteration 14309: 0.04211915907211492\n",
      "Train Loss at iteration 14310: 0.04211912964317552\n",
      "Train Loss at iteration 14311: 0.0421191002191085\n",
      "Train Loss at iteration 14312: 0.04211907079991304\n",
      "Train Loss at iteration 14313: 0.04211904138558833\n",
      "Train Loss at iteration 14314: 0.04211901197613356\n",
      "Train Loss at iteration 14315: 0.04211898257154793\n",
      "Train Loss at iteration 14316: 0.04211895317183062\n",
      "Train Loss at iteration 14317: 0.04211892377698084\n",
      "Train Loss at iteration 14318: 0.04211889438699777\n",
      "Train Loss at iteration 14319: 0.04211886500188061\n",
      "Train Loss at iteration 14320: 0.04211883562162855\n",
      "Train Loss at iteration 14321: 0.04211880624624078\n",
      "Train Loss at iteration 14322: 0.04211877687571649\n",
      "Train Loss at iteration 14323: 0.042118747510054885\n",
      "Train Loss at iteration 14324: 0.04211871814925515\n",
      "Train Loss at iteration 14325: 0.04211868879331648\n",
      "Train Loss at iteration 14326: 0.042118659442238075\n",
      "Train Loss at iteration 14327: 0.04211863009601913\n",
      "Train Loss at iteration 14328: 0.042118600754658814\n",
      "Train Loss at iteration 14329: 0.04211857141815636\n",
      "Train Loss at iteration 14330: 0.042118542086510925\n",
      "Train Loss at iteration 14331: 0.04211851275972173\n",
      "Train Loss at iteration 14332: 0.04211848343778796\n",
      "Train Loss at iteration 14333: 0.04211845412070882\n",
      "Train Loss at iteration 14334: 0.04211842480848348\n",
      "Train Loss at iteration 14335: 0.04211839550111115\n",
      "Train Loss at iteration 14336: 0.04211836619859103\n",
      "Train Loss at iteration 14337: 0.042118336900922315\n",
      "Train Loss at iteration 14338: 0.04211830760810419\n",
      "Train Loss at iteration 14339: 0.04211827832013585\n",
      "Train Loss at iteration 14340: 0.04211824903701652\n",
      "Train Loss at iteration 14341: 0.04211821975874536\n",
      "Train Loss at iteration 14342: 0.04211819048532157\n",
      "Train Loss at iteration 14343: 0.04211816121674435\n",
      "Train Loss at iteration 14344: 0.04211813195301292\n",
      "Train Loss at iteration 14345: 0.04211810269412645\n",
      "Train Loss at iteration 14346: 0.04211807344008415\n",
      "Train Loss at iteration 14347: 0.0421180441908852\n",
      "Train Loss at iteration 14348: 0.042118014946528816\n",
      "Train Loss at iteration 14349: 0.04211798570701418\n",
      "Train Loss at iteration 14350: 0.042117956472340484\n",
      "Train Loss at iteration 14351: 0.04211792724250695\n",
      "Train Loss at iteration 14352: 0.04211789801751276\n",
      "Train Loss at iteration 14353: 0.042117868797357114\n",
      "Train Loss at iteration 14354: 0.042117839582039206\n",
      "Train Loss at iteration 14355: 0.042117810371558234\n",
      "Train Loss at iteration 14356: 0.042117781165913394\n",
      "Train Loss at iteration 14357: 0.0421177519651039\n",
      "Train Loss at iteration 14358: 0.04211772276912892\n",
      "Train Loss at iteration 14359: 0.04211769357798768\n",
      "Train Loss at iteration 14360: 0.04211766439167937\n",
      "Train Loss at iteration 14361: 0.04211763521020318\n",
      "Train Loss at iteration 14362: 0.042117606033558325\n",
      "Train Loss at iteration 14363: 0.042117576861743986\n",
      "Train Loss at iteration 14364: 0.04211754769475938\n",
      "Train Loss at iteration 14365: 0.04211751853260368\n",
      "Train Loss at iteration 14366: 0.04211748937527612\n",
      "Train Loss at iteration 14367: 0.04211746022277587\n",
      "Train Loss at iteration 14368: 0.042117431075102144\n",
      "Train Loss at iteration 14369: 0.04211740193225413\n",
      "Train Loss at iteration 14370: 0.04211737279423103\n",
      "Train Loss at iteration 14371: 0.042117343661032064\n",
      "Train Loss at iteration 14372: 0.04211731453265641\n",
      "Train Loss at iteration 14373: 0.042117285409103276\n",
      "Train Loss at iteration 14374: 0.04211725629037185\n",
      "Train Loss at iteration 14375: 0.04211722717646136\n",
      "Train Loss at iteration 14376: 0.04211719806737098\n",
      "Train Loss at iteration 14377: 0.04211716896309993\n",
      "Train Loss at iteration 14378: 0.04211713986364739\n",
      "Train Loss at iteration 14379: 0.042117110769012575\n",
      "Train Loss at iteration 14380: 0.04211708167919468\n",
      "Train Loss at iteration 14381: 0.04211705259419292\n",
      "Train Loss at iteration 14382: 0.04211702351400649\n",
      "Train Loss at iteration 14383: 0.04211699443863456\n",
      "Train Loss at iteration 14384: 0.042116965368076374\n",
      "Train Loss at iteration 14385: 0.04211693630233112\n",
      "Train Loss at iteration 14386: 0.042116907241398\n",
      "Train Loss at iteration 14387: 0.04211687818527621\n",
      "Train Loss at iteration 14388: 0.04211684913396495\n",
      "Train Loss at iteration 14389: 0.04211682008746344\n",
      "Train Loss at iteration 14390: 0.042116791045770846\n",
      "Train Loss at iteration 14391: 0.04211676200888642\n",
      "Train Loss at iteration 14392: 0.04211673297680934\n",
      "Train Loss at iteration 14393: 0.04211670394953881\n",
      "Train Loss at iteration 14394: 0.04211667492707401\n",
      "Train Loss at iteration 14395: 0.04211664590941418\n",
      "Train Loss at iteration 14396: 0.0421166168965585\n",
      "Train Loss at iteration 14397: 0.04211658788850619\n",
      "Train Loss at iteration 14398: 0.04211655888525642\n",
      "Train Loss at iteration 14399: 0.04211652988680844\n",
      "Train Loss at iteration 14400: 0.04211650089316144\n",
      "Train Loss at iteration 14401: 0.0421164719043146\n",
      "Train Loss at iteration 14402: 0.04211644292026714\n",
      "Train Loss at iteration 14403: 0.04211641394101826\n",
      "Train Loss at iteration 14404: 0.04211638496656717\n",
      "Train Loss at iteration 14405: 0.042116355996913074\n",
      "Train Loss at iteration 14406: 0.042116327032055166\n",
      "Train Loss at iteration 14407: 0.04211629807199267\n",
      "Train Loss at iteration 14408: 0.04211626911672477\n",
      "Train Loss at iteration 14409: 0.0421162401662507\n",
      "Train Loss at iteration 14410: 0.04211621122056962\n",
      "Train Loss at iteration 14411: 0.04211618227968077\n",
      "Train Loss at iteration 14412: 0.042116153343583346\n",
      "Train Loss at iteration 14413: 0.04211612441227655\n",
      "Train Loss at iteration 14414: 0.0421160954857596\n",
      "Train Loss at iteration 14415: 0.04211606656403169\n",
      "Train Loss at iteration 14416: 0.04211603764709202\n",
      "Train Loss at iteration 14417: 0.042116008734939814\n",
      "Train Loss at iteration 14418: 0.04211597982757425\n",
      "Train Loss at iteration 14419: 0.042115950924994566\n",
      "Train Loss at iteration 14420: 0.04211592202719995\n",
      "Train Loss at iteration 14421: 0.04211589313418962\n",
      "Train Loss at iteration 14422: 0.04211586424596276\n",
      "Train Loss at iteration 14423: 0.04211583536251862\n",
      "Train Loss at iteration 14424: 0.04211580648385636\n",
      "Train Loss at iteration 14425: 0.0421157776099752\n",
      "Train Loss at iteration 14426: 0.04211574874087437\n",
      "Train Loss at iteration 14427: 0.04211571987655305\n",
      "Train Loss at iteration 14428: 0.04211569101701046\n",
      "Train Loss at iteration 14429: 0.042115662162245805\n",
      "Train Loss at iteration 14430: 0.042115633312258295\n",
      "Train Loss at iteration 14431: 0.04211560446704715\n",
      "Train Loss at iteration 14432: 0.042115575626611536\n",
      "Train Loss at iteration 14433: 0.0421155467909507\n",
      "Train Loss at iteration 14434: 0.04211551796006384\n",
      "Train Loss at iteration 14435: 0.04211548913395018\n",
      "Train Loss at iteration 14436: 0.042115460312608884\n",
      "Train Loss at iteration 14437: 0.0421154314960392\n",
      "Train Loss at iteration 14438: 0.04211540268424033\n",
      "Train Loss at iteration 14439: 0.04211537387721147\n",
      "Train Loss at iteration 14440: 0.04211534507495185\n",
      "Train Loss at iteration 14441: 0.04211531627746065\n",
      "Train Loss at iteration 14442: 0.042115287484737106\n",
      "Train Loss at iteration 14443: 0.0421152586967804\n",
      "Train Loss at iteration 14444: 0.04211522991358978\n",
      "Train Loss at iteration 14445: 0.042115201135164426\n",
      "Train Loss at iteration 14446: 0.042115172361503556\n",
      "Train Loss at iteration 14447: 0.042115143592606374\n",
      "Train Loss at iteration 14448: 0.0421151148284721\n",
      "Train Loss at iteration 14449: 0.04211508606909994\n",
      "Train Loss at iteration 14450: 0.04211505731448909\n",
      "Train Loss at iteration 14451: 0.0421150285646388\n",
      "Train Loss at iteration 14452: 0.042114999819548235\n",
      "Train Loss at iteration 14453: 0.04211497107921665\n",
      "Train Loss at iteration 14454: 0.0421149423436432\n",
      "Train Loss at iteration 14455: 0.04211491361282715\n",
      "Train Loss at iteration 14456: 0.04211488488676767\n",
      "Train Loss at iteration 14457: 0.042114856165464\n",
      "Train Loss at iteration 14458: 0.04211482744891534\n",
      "Train Loss at iteration 14459: 0.042114798737120894\n",
      "Train Loss at iteration 14460: 0.04211477003007991\n",
      "Train Loss at iteration 14461: 0.04211474132779154\n",
      "Train Loss at iteration 14462: 0.04211471263025505\n",
      "Train Loss at iteration 14463: 0.042114683937469616\n",
      "Train Loss at iteration 14464: 0.042114655249434474\n",
      "Train Loss at iteration 14465: 0.04211462656614882\n",
      "Train Loss at iteration 14466: 0.04211459788761187\n",
      "Train Loss at iteration 14467: 0.04211456921382283\n",
      "Train Loss at iteration 14468: 0.04211454054478095\n",
      "Train Loss at iteration 14469: 0.04211451188048539\n",
      "Train Loss at iteration 14470: 0.042114483220935396\n",
      "Train Loss at iteration 14471: 0.04211445456613017\n",
      "Train Loss at iteration 14472: 0.04211442591606893\n",
      "Train Loss at iteration 14473: 0.0421143972707509\n",
      "Train Loss at iteration 14474: 0.042114368630175265\n",
      "Train Loss at iteration 14475: 0.042114339994341256\n",
      "Train Loss at iteration 14476: 0.04211431136324807\n",
      "Train Loss at iteration 14477: 0.04211428273689496\n",
      "Train Loss at iteration 14478: 0.042114254115281115\n",
      "Train Loss at iteration 14479: 0.042114225498405745\n",
      "Train Loss at iteration 14480: 0.04211419688626805\n",
      "Train Loss at iteration 14481: 0.04211416827886729\n",
      "Train Loss at iteration 14482: 0.04211413967620264\n",
      "Train Loss at iteration 14483: 0.04211411107827332\n",
      "Train Loss at iteration 14484: 0.042114082485078565\n",
      "Train Loss at iteration 14485: 0.04211405389661757\n",
      "Train Loss at iteration 14486: 0.04211402531288957\n",
      "Train Loss at iteration 14487: 0.04211399673389375\n",
      "Train Loss at iteration 14488: 0.04211396815962936\n",
      "Train Loss at iteration 14489: 0.04211393959009558\n",
      "Train Loss at iteration 14490: 0.042113911025291655\n",
      "Train Loss at iteration 14491: 0.04211388246521679\n",
      "Train Loss at iteration 14492: 0.0421138539098702\n",
      "Train Loss at iteration 14493: 0.042113825359251095\n",
      "Train Loss at iteration 14494: 0.042113796813358695\n",
      "Train Loss at iteration 14495: 0.04211376827219223\n",
      "Train Loss at iteration 14496: 0.04211373973575089\n",
      "Train Loss at iteration 14497: 0.04211371120403393\n",
      "Train Loss at iteration 14498: 0.0421136826770405\n",
      "Train Loss at iteration 14499: 0.0421136541547699\n",
      "Train Loss at iteration 14500: 0.042113625637221284\n",
      "Train Loss at iteration 14501: 0.0421135971243939\n",
      "Train Loss at iteration 14502: 0.042113568616286956\n",
      "Train Loss at iteration 14503: 0.04211354011289966\n",
      "Train Loss at iteration 14504: 0.04211351161423126\n",
      "Train Loss at iteration 14505: 0.042113483120280945\n",
      "Train Loss at iteration 14506: 0.04211345463104793\n",
      "Train Loss at iteration 14507: 0.042113426146531456\n",
      "Train Loss at iteration 14508: 0.04211339766673072\n",
      "Train Loss at iteration 14509: 0.04211336919164494\n",
      "Train Loss at iteration 14510: 0.04211334072127335\n",
      "Train Loss at iteration 14511: 0.04211331225561517\n",
      "Train Loss at iteration 14512: 0.04211328379466959\n",
      "Train Loss at iteration 14513: 0.042113255338435865\n",
      "Train Loss at iteration 14514: 0.04211322688691318\n",
      "Train Loss at iteration 14515: 0.04211319844010079\n",
      "Train Loss at iteration 14516: 0.04211316999799787\n",
      "Train Loss at iteration 14517: 0.042113141560603674\n",
      "Train Loss at iteration 14518: 0.04211311312791741\n",
      "Train Loss at iteration 14519: 0.042113084699938304\n",
      "Train Loss at iteration 14520: 0.04211305627666556\n",
      "Train Loss at iteration 14521: 0.04211302785809841\n",
      "Train Loss at iteration 14522: 0.04211299944423607\n",
      "Train Loss at iteration 14523: 0.04211297103507775\n",
      "Train Loss at iteration 14524: 0.042112942630622686\n",
      "Train Loss at iteration 14525: 0.0421129142308701\n",
      "Train Loss at iteration 14526: 0.04211288583581921\n",
      "Train Loss at iteration 14527: 0.042112857445469214\n",
      "Train Loss at iteration 14528: 0.04211282905981935\n",
      "Train Loss at iteration 14529: 0.04211280067886885\n",
      "Train Loss at iteration 14530: 0.04211277230261691\n",
      "Train Loss at iteration 14531: 0.04211274393106278\n",
      "Train Loss at iteration 14532: 0.04211271556420565\n",
      "Train Loss at iteration 14533: 0.04211268720204476\n",
      "Train Loss at iteration 14534: 0.04211265884457933\n",
      "Train Loss at iteration 14535: 0.04211263049180857\n",
      "Train Loss at iteration 14536: 0.04211260214373172\n",
      "Train Loss at iteration 14537: 0.04211257380034799\n",
      "Train Loss at iteration 14538: 0.0421125454616566\n",
      "Train Loss at iteration 14539: 0.042112517127656776\n",
      "Train Loss at iteration 14540: 0.04211248879834774\n",
      "Train Loss at iteration 14541: 0.04211246047372871\n",
      "Train Loss at iteration 14542: 0.042112432153798916\n",
      "Train Loss at iteration 14543: 0.04211240383855758\n",
      "Train Loss at iteration 14544: 0.04211237552800391\n",
      "Train Loss at iteration 14545: 0.04211234722213715\n",
      "Train Loss at iteration 14546: 0.04211231892095652\n",
      "Train Loss at iteration 14547: 0.04211229062446122\n",
      "Train Loss at iteration 14548: 0.042112262332650495\n",
      "Train Loss at iteration 14549: 0.04211223404552356\n",
      "Train Loss at iteration 14550: 0.04211220576307965\n",
      "Train Loss at iteration 14551: 0.04211217748531797\n",
      "Train Loss at iteration 14552: 0.04211214921223774\n",
      "Train Loss at iteration 14553: 0.042112120943838205\n",
      "Train Loss at iteration 14554: 0.04211209268011858\n",
      "Train Loss at iteration 14555: 0.04211206442107811\n",
      "Train Loss at iteration 14556: 0.04211203616671598\n",
      "Train Loss at iteration 14557: 0.04211200791703143\n",
      "Train Loss at iteration 14558: 0.042111979672023694\n",
      "Train Loss at iteration 14559: 0.04211195143169199\n",
      "Train Loss at iteration 14560: 0.04211192319603554\n",
      "Train Loss at iteration 14561: 0.04211189496505357\n",
      "Train Loss at iteration 14562: 0.0421118667387453\n",
      "Train Loss at iteration 14563: 0.04211183851710997\n",
      "Train Loss at iteration 14564: 0.0421118103001468\n",
      "Train Loss at iteration 14565: 0.042111782087855\n",
      "Train Loss at iteration 14566: 0.04211175388023381\n",
      "Train Loss at iteration 14567: 0.042111725677282455\n",
      "Train Loss at iteration 14568: 0.042111697479000156\n",
      "Train Loss at iteration 14569: 0.04211166928538614\n",
      "Train Loss at iteration 14570: 0.04211164109643963\n",
      "Train Loss at iteration 14571: 0.04211161291215987\n",
      "Train Loss at iteration 14572: 0.04211158473254606\n",
      "Train Loss at iteration 14573: 0.042111556557597435\n",
      "Train Loss at iteration 14574: 0.042111528387313246\n",
      "Train Loss at iteration 14575: 0.04211150022169268\n",
      "Train Loss at iteration 14576: 0.04211147206073499\n",
      "Train Loss at iteration 14577: 0.04211144390443939\n",
      "Train Loss at iteration 14578: 0.04211141575280512\n",
      "Train Loss at iteration 14579: 0.04211138760583139\n",
      "Train Loss at iteration 14580: 0.042111359463517434\n",
      "Train Loss at iteration 14581: 0.04211133132586249\n",
      "Train Loss at iteration 14582: 0.04211130319286578\n",
      "Train Loss at iteration 14583: 0.04211127506452653\n",
      "Train Loss at iteration 14584: 0.04211124694084396\n",
      "Train Loss at iteration 14585: 0.0421112188218173\n",
      "Train Loss at iteration 14586: 0.04211119070744578\n",
      "Train Loss at iteration 14587: 0.04211116259772865\n",
      "Train Loss at iteration 14588: 0.042111134492665105\n",
      "Train Loss at iteration 14589: 0.0421111063922544\n",
      "Train Loss at iteration 14590: 0.042111078296495734\n",
      "Train Loss at iteration 14591: 0.04211105020538836\n",
      "Train Loss at iteration 14592: 0.0421110221189315\n",
      "Train Loss at iteration 14593: 0.042110994037124386\n",
      "Train Loss at iteration 14594: 0.042110965959966246\n",
      "Train Loss at iteration 14595: 0.042110937887456304\n",
      "Train Loss at iteration 14596: 0.042110909819593784\n",
      "Train Loss at iteration 14597: 0.04211088175637793\n",
      "Train Loss at iteration 14598: 0.04211085369780796\n",
      "Train Loss at iteration 14599: 0.04211082564388311\n",
      "Train Loss at iteration 14600: 0.042110797594602616\n",
      "Train Loss at iteration 14601: 0.04211076954996569\n",
      "Train Loss at iteration 14602: 0.04211074150997158\n",
      "Train Loss at iteration 14603: 0.042110713474619504\n",
      "Train Loss at iteration 14604: 0.042110685443908696\n",
      "Train Loss at iteration 14605: 0.0421106574178384\n",
      "Train Loss at iteration 14606: 0.04211062939640783\n",
      "Train Loss at iteration 14607: 0.04211060137961621\n",
      "Train Loss at iteration 14608: 0.04211057336746278\n",
      "Train Loss at iteration 14609: 0.04211054535994679\n",
      "Train Loss at iteration 14610: 0.042110517357067445\n",
      "Train Loss at iteration 14611: 0.04211048935882398\n",
      "Train Loss at iteration 14612: 0.04211046136521562\n",
      "Train Loss at iteration 14613: 0.04211043337624164\n",
      "Train Loss at iteration 14614: 0.042110405391901216\n",
      "Train Loss at iteration 14615: 0.04211037741219362\n",
      "Train Loss at iteration 14616: 0.04211034943711805\n",
      "Train Loss at iteration 14617: 0.04211032146667375\n",
      "Train Loss at iteration 14618: 0.04211029350085997\n",
      "Train Loss at iteration 14619: 0.04211026553967594\n",
      "Train Loss at iteration 14620: 0.042110237583120866\n",
      "Train Loss at iteration 14621: 0.042110209631193996\n",
      "Train Loss at iteration 14622: 0.04211018168389456\n",
      "Train Loss at iteration 14623: 0.04211015374122181\n",
      "Train Loss at iteration 14624: 0.04211012580317494\n",
      "Train Loss at iteration 14625: 0.042110097869753234\n",
      "Train Loss at iteration 14626: 0.042110069940955876\n",
      "Train Loss at iteration 14627: 0.042110042016782125\n",
      "Train Loss at iteration 14628: 0.04211001409723121\n",
      "Train Loss at iteration 14629: 0.04210998618230238\n",
      "Train Loss at iteration 14630: 0.04210995827199482\n",
      "Train Loss at iteration 14631: 0.04210993036630782\n",
      "Train Loss at iteration 14632: 0.04210990246524059\n",
      "Train Loss at iteration 14633: 0.04210987456879236\n",
      "Train Loss at iteration 14634: 0.04210984667696236\n",
      "Train Loss at iteration 14635: 0.04210981878974983\n",
      "Train Loss at iteration 14636: 0.04210979090715402\n",
      "Train Loss at iteration 14637: 0.042109763029174144\n",
      "Train Loss at iteration 14638: 0.04210973515580946\n",
      "Train Loss at iteration 14639: 0.04210970728705918\n",
      "Train Loss at iteration 14640: 0.042109679422922534\n",
      "Train Loss at iteration 14641: 0.042109651563398784\n",
      "Train Loss at iteration 14642: 0.04210962370848714\n",
      "Train Loss at iteration 14643: 0.042109595858186855\n",
      "Train Loss at iteration 14644: 0.042109568012497156\n",
      "Train Loss at iteration 14645: 0.04210954017141728\n",
      "Train Loss at iteration 14646: 0.04210951233494647\n",
      "Train Loss at iteration 14647: 0.042109484503083945\n",
      "Train Loss at iteration 14648: 0.042109456675828945\n",
      "Train Loss at iteration 14649: 0.04210942885318073\n",
      "Train Loss at iteration 14650: 0.04210940103513851\n",
      "Train Loss at iteration 14651: 0.042109373221701524\n",
      "Train Loss at iteration 14652: 0.04210934541286901\n",
      "Train Loss at iteration 14653: 0.04210931760864023\n",
      "Train Loss at iteration 14654: 0.042109289809014364\n",
      "Train Loss at iteration 14655: 0.04210926201399071\n",
      "Train Loss at iteration 14656: 0.04210923422356847\n",
      "Train Loss at iteration 14657: 0.04210920643774689\n",
      "Train Loss at iteration 14658: 0.0421091786565252\n",
      "Train Loss at iteration 14659: 0.04210915087990267\n",
      "Train Loss at iteration 14660: 0.04210912310787849\n",
      "Train Loss at iteration 14661: 0.04210909534045192\n",
      "Train Loss at iteration 14662: 0.0421090675776222\n",
      "Train Loss at iteration 14663: 0.04210903981938857\n",
      "Train Loss at iteration 14664: 0.04210901206575026\n",
      "Train Loss at iteration 14665: 0.04210898431670651\n",
      "Train Loss at iteration 14666: 0.04210895657225656\n",
      "Train Loss at iteration 14667: 0.04210892883239965\n",
      "Train Loss at iteration 14668: 0.04210890109713502\n",
      "Train Loss at iteration 14669: 0.0421088733664619\n",
      "Train Loss at iteration 14670: 0.04210884564037953\n",
      "Train Loss at iteration 14671: 0.042108817918887154\n",
      "Train Loss at iteration 14672: 0.04210879020198401\n",
      "Train Loss at iteration 14673: 0.042108762489669355\n",
      "Train Loss at iteration 14674: 0.0421087347819424\n",
      "Train Loss at iteration 14675: 0.042108707078802377\n",
      "Train Loss at iteration 14676: 0.042108679380248565\n",
      "Train Loss at iteration 14677: 0.04210865168628017\n",
      "Train Loss at iteration 14678: 0.04210862399689644\n",
      "Train Loss at iteration 14679: 0.04210859631209663\n",
      "Train Loss at iteration 14680: 0.042108568631879964\n",
      "Train Loss at iteration 14681: 0.04210854095624569\n",
      "Train Loss at iteration 14682: 0.042108513285193036\n",
      "Train Loss at iteration 14683: 0.042108485618721256\n",
      "Train Loss at iteration 14684: 0.04210845795682958\n",
      "Train Loss at iteration 14685: 0.04210843029951726\n",
      "Train Loss at iteration 14686: 0.04210840264678353\n",
      "Train Loss at iteration 14687: 0.042108374998627636\n",
      "Train Loss at iteration 14688: 0.042108347355048814\n",
      "Train Loss at iteration 14689: 0.042108319716046304\n",
      "Train Loss at iteration 14690: 0.04210829208161935\n",
      "Train Loss at iteration 14691: 0.0421082644517672\n",
      "Train Loss at iteration 14692: 0.04210823682648908\n",
      "Train Loss at iteration 14693: 0.042108209205784244\n",
      "Train Loss at iteration 14694: 0.04210818158965193\n",
      "Train Loss at iteration 14695: 0.04210815397809138\n",
      "Train Loss at iteration 14696: 0.04210812637110184\n",
      "Train Loss at iteration 14697: 0.04210809876868253\n",
      "Train Loss at iteration 14698: 0.04210807117083273\n",
      "Train Loss at iteration 14699: 0.04210804357755166\n",
      "Train Loss at iteration 14700: 0.04210801598883856\n",
      "Train Loss at iteration 14701: 0.04210798840469268\n",
      "Train Loss at iteration 14702: 0.042107960825113255\n",
      "Train Loss at iteration 14703: 0.042107933250099545\n",
      "Train Loss at iteration 14704: 0.04210790567965078\n",
      "Train Loss at iteration 14705: 0.04210787811376621\n",
      "Train Loss at iteration 14706: 0.04210785055244507\n",
      "Train Loss at iteration 14707: 0.0421078229956866\n",
      "Train Loss at iteration 14708: 0.04210779544349006\n",
      "Train Loss at iteration 14709: 0.04210776789585468\n",
      "Train Loss at iteration 14710: 0.042107740352779706\n",
      "Train Loss at iteration 14711: 0.042107712814264384\n",
      "Train Loss at iteration 14712: 0.04210768528030796\n",
      "Train Loss at iteration 14713: 0.04210765775090968\n",
      "Train Loss at iteration 14714: 0.042107630226068775\n",
      "Train Loss at iteration 14715: 0.0421076027057845\n",
      "Train Loss at iteration 14716: 0.04210757519005612\n",
      "Train Loss at iteration 14717: 0.042107547678882834\n",
      "Train Loss at iteration 14718: 0.04210752017226392\n",
      "Train Loss at iteration 14719: 0.042107492670198615\n",
      "Train Loss at iteration 14720: 0.04210746517268616\n",
      "Train Loss at iteration 14721: 0.042107437679725815\n",
      "Train Loss at iteration 14722: 0.0421074101913168\n",
      "Train Loss at iteration 14723: 0.04210738270745838\n",
      "Train Loss at iteration 14724: 0.04210735522814979\n",
      "Train Loss at iteration 14725: 0.04210732775339029\n",
      "Train Loss at iteration 14726: 0.04210730028317911\n",
      "Train Loss at iteration 14727: 0.042107272817515494\n",
      "Train Loss at iteration 14728: 0.04210724535639872\n",
      "Train Loss at iteration 14729: 0.042107217899828\n",
      "Train Loss at iteration 14730: 0.04210719044780258\n",
      "Train Loss at iteration 14731: 0.042107163000321725\n",
      "Train Loss at iteration 14732: 0.04210713555738468\n",
      "Train Loss at iteration 14733: 0.04210710811899067\n",
      "Train Loss at iteration 14734: 0.04210708068513898\n",
      "Train Loss at iteration 14735: 0.042107053255828815\n",
      "Train Loss at iteration 14736: 0.04210702583105945\n",
      "Train Loss at iteration 14737: 0.04210699841083012\n",
      "Train Loss at iteration 14738: 0.04210697099514009\n",
      "Train Loss at iteration 14739: 0.04210694358398858\n",
      "Train Loss at iteration 14740: 0.042106916177374866\n",
      "Train Loss at iteration 14741: 0.042106888775298154\n",
      "Train Loss at iteration 14742: 0.042106861377757744\n",
      "Train Loss at iteration 14743: 0.04210683398475286\n",
      "Train Loss at iteration 14744: 0.04210680659628273\n",
      "Train Loss at iteration 14745: 0.04210677921234663\n",
      "Train Loss at iteration 14746: 0.042106751832943795\n",
      "Train Loss at iteration 14747: 0.04210672445807348\n",
      "Train Loss at iteration 14748: 0.04210669708773495\n",
      "Train Loss at iteration 14749: 0.04210666972192741\n",
      "Train Loss at iteration 14750: 0.04210664236065015\n",
      "Train Loss at iteration 14751: 0.04210661500390239\n",
      "Train Loss at iteration 14752: 0.04210658765168339\n",
      "Train Loss at iteration 14753: 0.04210656030399242\n",
      "Train Loss at iteration 14754: 0.04210653296082871\n",
      "Train Loss at iteration 14755: 0.04210650562219149\n",
      "Train Loss at iteration 14756: 0.042106478288080056\n",
      "Train Loss at iteration 14757: 0.042106450958493614\n",
      "Train Loss at iteration 14758: 0.042106423633431436\n",
      "Train Loss at iteration 14759: 0.04210639631289277\n",
      "Train Loss at iteration 14760: 0.04210636899687685\n",
      "Train Loss at iteration 14761: 0.04210634168538296\n",
      "Train Loss at iteration 14762: 0.042106314378410316\n",
      "Train Loss at iteration 14763: 0.0421062870759582\n",
      "Train Loss at iteration 14764: 0.042106259778025824\n",
      "Train Loss at iteration 14765: 0.042106232484612476\n",
      "Train Loss at iteration 14766: 0.042106205195717374\n",
      "Train Loss at iteration 14767: 0.042106177911339804\n",
      "Train Loss at iteration 14768: 0.04210615063147899\n",
      "Train Loss at iteration 14769: 0.04210612335613419\n",
      "Train Loss at iteration 14770: 0.04210609608530467\n",
      "Train Loss at iteration 14771: 0.04210606881898965\n",
      "Train Loss at iteration 14772: 0.04210604155718841\n",
      "Train Loss at iteration 14773: 0.04210601429990019\n",
      "Train Loss at iteration 14774: 0.04210598704712425\n",
      "Train Loss at iteration 14775: 0.04210595979885984\n",
      "Train Loss at iteration 14776: 0.042105932555106196\n",
      "Train Loss at iteration 14777: 0.042105905315862585\n",
      "Train Loss at iteration 14778: 0.04210587808112827\n",
      "Train Loss at iteration 14779: 0.042105850850902474\n",
      "Train Loss at iteration 14780: 0.04210582362518447\n",
      "Train Loss at iteration 14781: 0.04210579640397351\n",
      "Train Loss at iteration 14782: 0.042105769187268834\n",
      "Train Loss at iteration 14783: 0.04210574197506971\n",
      "Train Loss at iteration 14784: 0.042105714767375395\n",
      "Train Loss at iteration 14785: 0.04210568756418512\n",
      "Train Loss at iteration 14786: 0.04210566036549815\n",
      "Train Loss at iteration 14787: 0.04210563317131375\n",
      "Train Loss at iteration 14788: 0.04210560598163116\n",
      "Train Loss at iteration 14789: 0.04210557879644964\n",
      "Train Loss at iteration 14790: 0.04210555161576843\n",
      "Train Loss at iteration 14791: 0.0421055244395868\n",
      "Train Loss at iteration 14792: 0.042105497267904\n",
      "Train Loss at iteration 14793: 0.04210547010071928\n",
      "Train Loss at iteration 14794: 0.042105442938031916\n",
      "Train Loss at iteration 14795: 0.042105415779841115\n",
      "Train Loss at iteration 14796: 0.04210538862614618\n",
      "Train Loss at iteration 14797: 0.04210536147694633\n",
      "Train Loss at iteration 14798: 0.04210533433224085\n",
      "Train Loss at iteration 14799: 0.04210530719202897\n",
      "Train Loss at iteration 14800: 0.042105280056309966\n",
      "Train Loss at iteration 14801: 0.04210525292508308\n",
      "Train Loss at iteration 14802: 0.04210522579834757\n",
      "Train Loss at iteration 14803: 0.04210519867610269\n",
      "Train Loss at iteration 14804: 0.0421051715583477\n",
      "Train Loss at iteration 14805: 0.04210514444508185\n",
      "Train Loss at iteration 14806: 0.042105117336304405\n",
      "Train Loss at iteration 14807: 0.042105090232014614\n",
      "Train Loss at iteration 14808: 0.04210506313221174\n",
      "Train Loss at iteration 14809: 0.042105036036895006\n",
      "Train Loss at iteration 14810: 0.04210500894606373\n",
      "Train Loss at iteration 14811: 0.04210498185971712\n",
      "Train Loss at iteration 14812: 0.04210495477785444\n",
      "Train Loss at iteration 14813: 0.04210492770047496\n",
      "Train Loss at iteration 14814: 0.04210490062757792\n",
      "Train Loss at iteration 14815: 0.0421048735591626\n",
      "Train Loss at iteration 14816: 0.04210484649522824\n",
      "Train Loss at iteration 14817: 0.04210481943577411\n",
      "Train Loss at iteration 14818: 0.04210479238079944\n",
      "Train Loss at iteration 14819: 0.04210476533030353\n",
      "Train Loss at iteration 14820: 0.04210473828428559\n",
      "Train Loss at iteration 14821: 0.04210471124274492\n",
      "Train Loss at iteration 14822: 0.04210468420568075\n",
      "Train Loss at iteration 14823: 0.04210465717309234\n",
      "Train Loss at iteration 14824: 0.04210463014497897\n",
      "Train Loss at iteration 14825: 0.04210460312133988\n",
      "Train Loss at iteration 14826: 0.042104576102174315\n",
      "Train Loss at iteration 14827: 0.04210454908748157\n",
      "Train Loss at iteration 14828: 0.04210452207726088\n",
      "Train Loss at iteration 14829: 0.042104495071511504\n",
      "Train Loss at iteration 14830: 0.04210446807023271\n",
      "Train Loss at iteration 14831: 0.04210444107342374\n",
      "Train Loss at iteration 14832: 0.04210441408108387\n",
      "Train Loss at iteration 14833: 0.04210438709321235\n",
      "Train Loss at iteration 14834: 0.04210436010980845\n",
      "Train Loss at iteration 14835: 0.042104333130871426\n",
      "Train Loss at iteration 14836: 0.04210430615640052\n",
      "Train Loss at iteration 14837: 0.04210427918639502\n",
      "Train Loss at iteration 14838: 0.042104252220854156\n",
      "Train Loss at iteration 14839: 0.04210422525977721\n",
      "Train Loss at iteration 14840: 0.04210419830316342\n",
      "Train Loss at iteration 14841: 0.04210417135101208\n",
      "Train Loss at iteration 14842: 0.04210414440332243\n",
      "Train Loss at iteration 14843: 0.04210411746009374\n",
      "Train Loss at iteration 14844: 0.042104090521325244\n",
      "Train Loss at iteration 14845: 0.04210406358701623\n",
      "Train Loss at iteration 14846: 0.04210403665716594\n",
      "Train Loss at iteration 14847: 0.04210400973177366\n",
      "Train Loss at iteration 14848: 0.042103982810838624\n",
      "Train Loss at iteration 14849: 0.04210395589436011\n",
      "Train Loss at iteration 14850: 0.04210392898233737\n",
      "Train Loss at iteration 14851: 0.042103902074769675\n",
      "Train Loss at iteration 14852: 0.042103875171656285\n",
      "Train Loss at iteration 14853: 0.042103848272996444\n",
      "Train Loss at iteration 14854: 0.04210382137878944\n",
      "Train Loss at iteration 14855: 0.042103794489034514\n",
      "Train Loss at iteration 14856: 0.042103767603730934\n",
      "Train Loss at iteration 14857: 0.04210374072287798\n",
      "Train Loss at iteration 14858: 0.04210371384647489\n",
      "Train Loss at iteration 14859: 0.04210368697452093\n",
      "Train Loss at iteration 14860: 0.04210366010701536\n",
      "Train Loss at iteration 14861: 0.042103633243957474\n",
      "Train Loss at iteration 14862: 0.04210360638534649\n",
      "Train Loss at iteration 14863: 0.04210357953118172\n",
      "Train Loss at iteration 14864: 0.04210355268146237\n",
      "Train Loss at iteration 14865: 0.04210352583618773\n",
      "Train Loss at iteration 14866: 0.042103498995357085\n",
      "Train Loss at iteration 14867: 0.04210347215896967\n",
      "Train Loss at iteration 14868: 0.04210344532702476\n",
      "Train Loss at iteration 14869: 0.042103418499521615\n",
      "Train Loss at iteration 14870: 0.04210339167645949\n",
      "Train Loss at iteration 14871: 0.042103364857837666\n",
      "Train Loss at iteration 14872: 0.042103338043655396\n",
      "Train Loss at iteration 14873: 0.04210331123391195\n",
      "Train Loss at iteration 14874: 0.042103284428606585\n",
      "Train Loss at iteration 14875: 0.04210325762773858\n",
      "Train Loss at iteration 14876: 0.04210323083130718\n",
      "Train Loss at iteration 14877: 0.04210320403931166\n",
      "Train Loss at iteration 14878: 0.042103177251751285\n",
      "Train Loss at iteration 14879: 0.042103150468625325\n",
      "Train Loss at iteration 14880: 0.04210312368993303\n",
      "Train Loss at iteration 14881: 0.042103096915673675\n",
      "Train Loss at iteration 14882: 0.04210307014584652\n",
      "Train Loss at iteration 14883: 0.04210304338045084\n",
      "Train Loss at iteration 14884: 0.04210301661948591\n",
      "Train Loss at iteration 14885: 0.04210298986295095\n",
      "Train Loss at iteration 14886: 0.04210296311084527\n",
      "Train Loss at iteration 14887: 0.042102936363168114\n",
      "Train Loss at iteration 14888: 0.04210290961991876\n",
      "Train Loss at iteration 14889: 0.042102882881096475\n",
      "Train Loss at iteration 14890: 0.042102856146700514\n",
      "Train Loss at iteration 14891: 0.04210282941673015\n",
      "Train Loss at iteration 14892: 0.04210280269118465\n",
      "Train Loss at iteration 14893: 0.04210277597006328\n",
      "Train Loss at iteration 14894: 0.0421027492533653\n",
      "Train Loss at iteration 14895: 0.04210272254108999\n",
      "Train Loss at iteration 14896: 0.0421026958332366\n",
      "Train Loss at iteration 14897: 0.04210266912980441\n",
      "Train Loss at iteration 14898: 0.04210264243079269\n",
      "Train Loss at iteration 14899: 0.04210261573620069\n",
      "Train Loss at iteration 14900: 0.042102589046027705\n",
      "Train Loss at iteration 14901: 0.04210256236027296\n",
      "Train Loss at iteration 14902: 0.042102535678935765\n",
      "Train Loss at iteration 14903: 0.04210250900201537\n",
      "Train Loss at iteration 14904: 0.04210248232951104\n",
      "Train Loss at iteration 14905: 0.04210245566142205\n",
      "Train Loss at iteration 14906: 0.042102428997747667\n",
      "Train Loss at iteration 14907: 0.04210240233848715\n",
      "Train Loss at iteration 14908: 0.042102375683639774\n",
      "Train Loss at iteration 14909: 0.04210234903320483\n",
      "Train Loss at iteration 14910: 0.04210232238718155\n",
      "Train Loss at iteration 14911: 0.042102295745569215\n",
      "Train Loss at iteration 14912: 0.04210226910836709\n",
      "Train Loss at iteration 14913: 0.04210224247557446\n",
      "Train Loss at iteration 14914: 0.04210221584719058\n",
      "Train Loss at iteration 14915: 0.04210218922321472\n",
      "Train Loss at iteration 14916: 0.04210216260364617\n",
      "Train Loss at iteration 14917: 0.04210213598848417\n",
      "Train Loss at iteration 14918: 0.042102109377728\n",
      "Train Loss at iteration 14919: 0.04210208277137694\n",
      "Train Loss at iteration 14920: 0.04210205616943025\n",
      "Train Loss at iteration 14921: 0.042102029571887196\n",
      "Train Loss at iteration 14922: 0.042102002978747055\n",
      "Train Loss at iteration 14923: 0.042101976390009094\n",
      "Train Loss at iteration 14924: 0.0421019498056726\n",
      "Train Loss at iteration 14925: 0.04210192322573681\n",
      "Train Loss at iteration 14926: 0.04210189665020102\n",
      "Train Loss at iteration 14927: 0.0421018700790645\n",
      "Train Loss at iteration 14928: 0.0421018435123265\n",
      "Train Loss at iteration 14929: 0.04210181694998632\n",
      "Train Loss at iteration 14930: 0.04210179039204322\n",
      "Train Loss at iteration 14931: 0.04210176383849645\n",
      "Train Loss at iteration 14932: 0.04210173728934532\n",
      "Train Loss at iteration 14933: 0.042101710744589065\n",
      "Train Loss at iteration 14934: 0.04210168420422698\n",
      "Train Loss at iteration 14935: 0.04210165766825832\n",
      "Train Loss at iteration 14936: 0.04210163113668238\n",
      "Train Loss at iteration 14937: 0.042101604609498404\n",
      "Train Loss at iteration 14938: 0.042101578086705685\n",
      "Train Loss at iteration 14939: 0.04210155156830349\n",
      "Train Loss at iteration 14940: 0.04210152505429109\n",
      "Train Loss at iteration 14941: 0.04210149854466774\n",
      "Train Loss at iteration 14942: 0.04210147203943274\n",
      "Train Loss at iteration 14943: 0.042101445538585365\n",
      "Train Loss at iteration 14944: 0.04210141904212485\n",
      "Train Loss at iteration 14945: 0.0421013925500505\n",
      "Train Loss at iteration 14946: 0.04210136606236159\n",
      "Train Loss at iteration 14947: 0.04210133957905737\n",
      "Train Loss at iteration 14948: 0.042101313100137136\n",
      "Train Loss at iteration 14949: 0.04210128662560015\n",
      "Train Loss at iteration 14950: 0.04210126015544569\n",
      "Train Loss at iteration 14951: 0.042101233689673015\n",
      "Train Loss at iteration 14952: 0.04210120722828143\n",
      "Train Loss at iteration 14953: 0.04210118077127017\n",
      "Train Loss at iteration 14954: 0.04210115431863854\n",
      "Train Loss at iteration 14955: 0.04210112787038581\n",
      "Train Loss at iteration 14956: 0.04210110142651123\n",
      "Train Loss at iteration 14957: 0.04210107498701409\n",
      "Train Loss at iteration 14958: 0.04210104855189367\n",
      "Train Loss at iteration 14959: 0.042101022121149245\n",
      "Train Loss at iteration 14960: 0.04210099569478009\n",
      "Train Loss at iteration 14961: 0.042100969272785456\n",
      "Train Loss at iteration 14962: 0.04210094285516465\n",
      "Train Loss at iteration 14963: 0.042100916441916936\n",
      "Train Loss at iteration 14964: 0.04210089003304157\n",
      "Train Loss at iteration 14965: 0.04210086362853786\n",
      "Train Loss at iteration 14966: 0.04210083722840506\n",
      "Train Loss at iteration 14967: 0.04210081083264246\n",
      "Train Loss at iteration 14968: 0.04210078444124931\n",
      "Train Loss at iteration 14969: 0.04210075805422492\n",
      "Train Loss at iteration 14970: 0.04210073167156854\n",
      "Train Loss at iteration 14971: 0.04210070529327946\n",
      "Train Loss at iteration 14972: 0.04210067891935694\n",
      "Train Loss at iteration 14973: 0.04210065254980026\n",
      "Train Loss at iteration 14974: 0.04210062618460872\n",
      "Train Loss at iteration 14975: 0.042100599823781566\n",
      "Train Loss at iteration 14976: 0.04210057346731809\n",
      "Train Loss at iteration 14977: 0.04210054711521758\n",
      "Train Loss at iteration 14978: 0.04210052076747929\n",
      "Train Loss at iteration 14979: 0.04210049442410251\n",
      "Train Loss at iteration 14980: 0.042100468085086516\n",
      "Train Loss at iteration 14981: 0.04210044175043057\n",
      "Train Loss at iteration 14982: 0.042100415420133985\n",
      "Train Loss at iteration 14983: 0.04210038909419599\n",
      "Train Loss at iteration 14984: 0.04210036277261591\n",
      "Train Loss at iteration 14985: 0.042100336455392996\n",
      "Train Loss at iteration 14986: 0.04210031014252652\n",
      "Train Loss at iteration 14987: 0.04210028383401578\n",
      "Train Loss at iteration 14988: 0.04210025752986005\n",
      "Train Loss at iteration 14989: 0.04210023123005859\n",
      "Train Loss at iteration 14990: 0.0421002049346107\n",
      "Train Loss at iteration 14991: 0.04210017864351565\n",
      "Train Loss at iteration 14992: 0.04210015235677271\n",
      "Train Loss at iteration 14993: 0.04210012607438118\n",
      "Train Loss at iteration 14994: 0.04210009979634032\n",
      "Train Loss at iteration 14995: 0.042100073522649405\n",
      "Train Loss at iteration 14996: 0.042100047253307744\n",
      "Train Loss at iteration 14997: 0.04210002098831458\n",
      "Train Loss at iteration 14998: 0.04209999472766922\n",
      "Train Loss at iteration 14999: 0.042099968471370916\n",
      "Train Loss at iteration 15000: 0.042099942219418984\n",
      "Train Loss at iteration 15001: 0.04209991597181266\n",
      "Train Loss at iteration 15002: 0.042099889728551255\n",
      "Train Loss at iteration 15003: 0.04209986348963404\n",
      "Train Loss at iteration 15004: 0.04209983725506031\n",
      "Train Loss at iteration 15005: 0.04209981102482931\n",
      "Train Loss at iteration 15006: 0.04209978479894035\n",
      "Train Loss at iteration 15007: 0.042099758577392704\n",
      "Train Loss at iteration 15008: 0.042099732360185646\n",
      "Train Loss at iteration 15009: 0.04209970614731846\n",
      "Train Loss at iteration 15010: 0.04209967993879042\n",
      "Train Loss at iteration 15011: 0.04209965373460083\n",
      "Train Loss at iteration 15012: 0.04209962753474893\n",
      "Train Loss at iteration 15013: 0.04209960133923405\n",
      "Train Loss at iteration 15014: 0.042099575148055436\n",
      "Train Loss at iteration 15015: 0.042099548961212374\n",
      "Train Loss at iteration 15016: 0.04209952277870416\n",
      "Train Loss at iteration 15017: 0.04209949660053006\n",
      "Train Loss at iteration 15018: 0.04209947042668938\n",
      "Train Loss at iteration 15019: 0.04209944425718137\n",
      "Train Loss at iteration 15020: 0.04209941809200533\n",
      "Train Loss at iteration 15021: 0.04209939193116053\n",
      "Train Loss at iteration 15022: 0.04209936577464628\n",
      "Train Loss at iteration 15023: 0.042099339622461825\n",
      "Train Loss at iteration 15024: 0.04209931347460647\n",
      "Train Loss at iteration 15025: 0.042099287331079496\n",
      "Train Loss at iteration 15026: 0.042099261191880184\n",
      "Train Loss at iteration 15027: 0.04209923505700781\n",
      "Train Loss at iteration 15028: 0.04209920892646166\n",
      "Train Loss at iteration 15029: 0.042099182800241025\n",
      "Train Loss at iteration 15030: 0.04209915667834517\n",
      "Train Loss at iteration 15031: 0.0420991305607734\n",
      "Train Loss at iteration 15032: 0.042099104447524995\n",
      "Train Loss at iteration 15033: 0.04209907833859922\n",
      "Train Loss at iteration 15034: 0.04209905223399538\n",
      "Train Loss at iteration 15035: 0.04209902613371274\n",
      "Train Loss at iteration 15036: 0.042099000037750606\n",
      "Train Loss at iteration 15037: 0.04209897394610824\n",
      "Train Loss at iteration 15038: 0.04209894785878494\n",
      "Train Loss at iteration 15039: 0.042098921775779986\n",
      "Train Loss at iteration 15040: 0.042098895697092645\n",
      "Train Loss at iteration 15041: 0.04209886962272223\n",
      "Train Loss at iteration 15042: 0.04209884355266802\n",
      "Train Loss at iteration 15043: 0.04209881748692928\n",
      "Train Loss at iteration 15044: 0.042098791425505315\n",
      "Train Loss at iteration 15045: 0.04209876536839541\n",
      "Train Loss at iteration 15046: 0.04209873931559883\n",
      "Train Loss at iteration 15047: 0.042098713267114875\n",
      "Train Loss at iteration 15048: 0.042098687222942834\n",
      "Train Loss at iteration 15049: 0.04209866118308198\n",
      "Train Loss at iteration 15050: 0.0420986351475316\n",
      "Train Loss at iteration 15051: 0.042098609116291\n",
      "Train Loss at iteration 15052: 0.04209858308935943\n",
      "Train Loss at iteration 15053: 0.04209855706673623\n",
      "Train Loss at iteration 15054: 0.04209853104842062\n",
      "Train Loss at iteration 15055: 0.04209850503441195\n",
      "Train Loss at iteration 15056: 0.04209847902470944\n",
      "Train Loss at iteration 15057: 0.04209845301931242\n",
      "Train Loss at iteration 15058: 0.042098427018220176\n",
      "Train Loss at iteration 15059: 0.04209840102143198\n",
      "Train Loss at iteration 15060: 0.04209837502894713\n",
      "Train Loss at iteration 15061: 0.04209834904076489\n",
      "Train Loss at iteration 15062: 0.04209832305688459\n",
      "Train Loss at iteration 15063: 0.04209829707730548\n",
      "Train Loss at iteration 15064: 0.04209827110202685\n",
      "Train Loss at iteration 15065: 0.042098245131048005\n",
      "Train Loss at iteration 15066: 0.04209821916436823\n",
      "Train Loss at iteration 15067: 0.04209819320198679\n",
      "Train Loss at iteration 15068: 0.04209816724390298\n",
      "Train Loss at iteration 15069: 0.04209814129011611\n",
      "Train Loss at iteration 15070: 0.04209811534062546\n",
      "Train Loss at iteration 15071: 0.042098089395430305\n",
      "Train Loss at iteration 15072: 0.042098063454529926\n",
      "Train Loss at iteration 15073: 0.04209803751792364\n",
      "Train Loss at iteration 15074: 0.04209801158561072\n",
      "Train Loss at iteration 15075: 0.04209798565759046\n",
      "Train Loss at iteration 15076: 0.04209795973386212\n",
      "Train Loss at iteration 15077: 0.04209793381442503\n",
      "Train Loss at iteration 15078: 0.04209790789927847\n",
      "Train Loss at iteration 15079: 0.042097881988421695\n",
      "Train Loss at iteration 15080: 0.04209785608185403\n",
      "Train Loss at iteration 15081: 0.04209783017957477\n",
      "Train Loss at iteration 15082: 0.042097804281583155\n",
      "Train Loss at iteration 15083: 0.04209777838787854\n",
      "Train Loss at iteration 15084: 0.04209775249846015\n",
      "Train Loss at iteration 15085: 0.04209772661332732\n",
      "Train Loss at iteration 15086: 0.042097700732479336\n",
      "Train Loss at iteration 15087: 0.04209767485591545\n",
      "Train Loss at iteration 15088: 0.04209764898363501\n",
      "Train Loss at iteration 15089: 0.04209762311563726\n",
      "Train Loss at iteration 15090: 0.0420975972519215\n",
      "Train Loss at iteration 15091: 0.04209757139248703\n",
      "Train Loss at iteration 15092: 0.04209754553733314\n",
      "Train Loss at iteration 15093: 0.04209751968645912\n",
      "Train Loss at iteration 15094: 0.04209749383986426\n",
      "Train Loss at iteration 15095: 0.04209746799754783\n",
      "Train Loss at iteration 15096: 0.04209744215950915\n",
      "Train Loss at iteration 15097: 0.042097416325747504\n",
      "Train Loss at iteration 15098: 0.042097390496262176\n",
      "Train Loss at iteration 15099: 0.04209736467105246\n",
      "Train Loss at iteration 15100: 0.042097338850117655\n",
      "Train Loss at iteration 15101: 0.04209731303345705\n",
      "Train Loss at iteration 15102: 0.04209728722106992\n",
      "Train Loss at iteration 15103: 0.042097261412955576\n",
      "Train Loss at iteration 15104: 0.042097235609113305\n",
      "Train Loss at iteration 15105: 0.04209720980954238\n",
      "Train Loss at iteration 15106: 0.04209718401424213\n",
      "Train Loss at iteration 15107: 0.04209715822321181\n",
      "Train Loss at iteration 15108: 0.042097132436450746\n",
      "Train Loss at iteration 15109: 0.042097106653958216\n",
      "Train Loss at iteration 15110: 0.0420970808757335\n",
      "Train Loss at iteration 15111: 0.04209705510177591\n",
      "Train Loss at iteration 15112: 0.042097029332084715\n",
      "Train Loss at iteration 15113: 0.04209700356665925\n",
      "Train Loss at iteration 15114: 0.04209697780549875\n",
      "Train Loss at iteration 15115: 0.04209695204860257\n",
      "Train Loss at iteration 15116: 0.04209692629596995\n",
      "Train Loss at iteration 15117: 0.042096900547600215\n",
      "Train Loss at iteration 15118: 0.042096874803492654\n",
      "Train Loss at iteration 15119: 0.04209684906364655\n",
      "Train Loss at iteration 15120: 0.0420968233280612\n",
      "Train Loss at iteration 15121: 0.04209679759673591\n",
      "Train Loss at iteration 15122: 0.042096771869669966\n",
      "Train Loss at iteration 15123: 0.042096746146862644\n",
      "Train Loss at iteration 15124: 0.042096720428313285\n",
      "Train Loss at iteration 15125: 0.04209669471402113\n",
      "Train Loss at iteration 15126: 0.0420966690039855\n",
      "Train Loss at iteration 15127: 0.0420966432982057\n",
      "Train Loss at iteration 15128: 0.042096617596681\n",
      "Train Loss at iteration 15129: 0.04209659189941071\n",
      "Train Loss at iteration 15130: 0.04209656620639412\n",
      "Train Loss at iteration 15131: 0.042096540517630524\n",
      "Train Loss at iteration 15132: 0.04209651483311923\n",
      "Train Loss at iteration 15133: 0.042096489152859504\n",
      "Train Loss at iteration 15134: 0.04209646347685068\n",
      "Train Loss at iteration 15135: 0.04209643780509203\n",
      "Train Loss at iteration 15136: 0.04209641213758284\n",
      "Train Loss at iteration 15137: 0.04209638647432242\n",
      "Train Loss at iteration 15138: 0.04209636081531008\n",
      "Train Loss at iteration 15139: 0.042096335160545104\n",
      "Train Loss at iteration 15140: 0.042096309510026776\n",
      "Train Loss at iteration 15141: 0.04209628386375439\n",
      "Train Loss at iteration 15142: 0.042096258221727266\n",
      "Train Loss at iteration 15143: 0.042096232583944695\n",
      "Train Loss at iteration 15144: 0.04209620695040595\n",
      "Train Loss at iteration 15145: 0.042096181321110364\n",
      "Train Loss at iteration 15146: 0.042096155696057194\n",
      "Train Loss at iteration 15147: 0.04209613007524577\n",
      "Train Loss at iteration 15148: 0.04209610445867537\n",
      "Train Loss at iteration 15149: 0.042096078846345315\n",
      "Train Loss at iteration 15150: 0.04209605323825486\n",
      "Train Loss at iteration 15151: 0.042096027634403343\n",
      "Train Loss at iteration 15152: 0.042096002034790025\n",
      "Train Loss at iteration 15153: 0.04209597643941425\n",
      "Train Loss at iteration 15154: 0.04209595084827529\n",
      "Train Loss at iteration 15155: 0.04209592526137243\n",
      "Train Loss at iteration 15156: 0.04209589967870498\n",
      "Train Loss at iteration 15157: 0.04209587410027223\n",
      "Train Loss at iteration 15158: 0.042095848526073516\n",
      "Train Loss at iteration 15159: 0.04209582295610808\n",
      "Train Loss at iteration 15160: 0.042095797390375264\n",
      "Train Loss at iteration 15161: 0.04209577182887435\n",
      "Train Loss at iteration 15162: 0.042095746271604624\n",
      "Train Loss at iteration 15163: 0.04209572071856541\n",
      "Train Loss at iteration 15164: 0.042095695169756\n",
      "Train Loss at iteration 15165: 0.04209566962517566\n",
      "Train Loss at iteration 15166: 0.042095644084823754\n",
      "Train Loss at iteration 15167: 0.04209561854869953\n",
      "Train Loss at iteration 15168: 0.042095593016802296\n",
      "Train Loss at iteration 15169: 0.042095567489131354\n",
      "Train Loss at iteration 15170: 0.04209554196568602\n",
      "Train Loss at iteration 15171: 0.04209551644646557\n",
      "Train Loss at iteration 15172: 0.04209549093146933\n",
      "Train Loss at iteration 15173: 0.042095465420696566\n",
      "Train Loss at iteration 15174: 0.04209543991414661\n",
      "Train Loss at iteration 15175: 0.04209541441181874\n",
      "Train Loss at iteration 15176: 0.04209538891371226\n",
      "Train Loss at iteration 15177: 0.04209536341982648\n",
      "Train Loss at iteration 15178: 0.0420953379301607\n",
      "Train Loss at iteration 15179: 0.04209531244471423\n",
      "Train Loss at iteration 15180: 0.04209528696348634\n",
      "Train Loss at iteration 15181: 0.042095261486476354\n",
      "Train Loss at iteration 15182: 0.04209523601368357\n",
      "Train Loss at iteration 15183: 0.04209521054510729\n",
      "Train Loss at iteration 15184: 0.042095185080746785\n",
      "Train Loss at iteration 15185: 0.04209515962060141\n",
      "Train Loss at iteration 15186: 0.04209513416467043\n",
      "Train Loss at iteration 15187: 0.042095108712953154\n",
      "Train Loss at iteration 15188: 0.0420950832654489\n",
      "Train Loss at iteration 15189: 0.042095057822156945\n",
      "Train Loss at iteration 15190: 0.0420950323830766\n",
      "Train Loss at iteration 15191: 0.04209500694820718\n",
      "Train Loss at iteration 15192: 0.04209498151754796\n",
      "Train Loss at iteration 15193: 0.042094956091098264\n",
      "Train Loss at iteration 15194: 0.04209493066885739\n",
      "Train Loss at iteration 15195: 0.04209490525082465\n",
      "Train Loss at iteration 15196: 0.04209487983699932\n",
      "Train Loss at iteration 15197: 0.04209485442738073\n",
      "Train Loss at iteration 15198: 0.042094829021968164\n",
      "Train Loss at iteration 15199: 0.04209480362076093\n",
      "Train Loss at iteration 15200: 0.04209477822375834\n",
      "Train Loss at iteration 15201: 0.04209475283095969\n",
      "Train Loss at iteration 15202: 0.04209472744236429\n",
      "Train Loss at iteration 15203: 0.042094702057971424\n",
      "Train Loss at iteration 15204: 0.04209467667778043\n",
      "Train Loss at iteration 15205: 0.04209465130179057\n",
      "Train Loss at iteration 15206: 0.04209462593000118\n",
      "Train Loss at iteration 15207: 0.042094600562411544\n",
      "Train Loss at iteration 15208: 0.042094575199020975\n",
      "Train Loss at iteration 15209: 0.04209454983982878\n",
      "Train Loss at iteration 15210: 0.04209452448483426\n",
      "Train Loss at iteration 15211: 0.04209449913403672\n",
      "Train Loss at iteration 15212: 0.04209447378743547\n",
      "Train Loss at iteration 15213: 0.0420944484450298\n",
      "Train Loss at iteration 15214: 0.04209442310681903\n",
      "Train Loss at iteration 15215: 0.04209439777280246\n",
      "Train Loss at iteration 15216: 0.04209437244297938\n",
      "Train Loss at iteration 15217: 0.04209434711734913\n",
      "Train Loss at iteration 15218: 0.04209432179591098\n",
      "Train Loss at iteration 15219: 0.042094296478664237\n",
      "Train Loss at iteration 15220: 0.04209427116560823\n",
      "Train Loss at iteration 15221: 0.042094245856742256\n",
      "Train Loss at iteration 15222: 0.04209422055206561\n",
      "Train Loss at iteration 15223: 0.042094195251577614\n",
      "Train Loss at iteration 15224: 0.04209416995527755\n",
      "Train Loss at iteration 15225: 0.04209414466316475\n",
      "Train Loss at iteration 15226: 0.04209411937523851\n",
      "Train Loss at iteration 15227: 0.04209409409149814\n",
      "Train Loss at iteration 15228: 0.042094068811942935\n",
      "Train Loss at iteration 15229: 0.042094043536572215\n",
      "Train Loss at iteration 15230: 0.042094018265385266\n",
      "Train Loss at iteration 15231: 0.042093992998381416\n",
      "Train Loss at iteration 15232: 0.04209396773555996\n",
      "Train Loss at iteration 15233: 0.04209394247692022\n",
      "Train Loss at iteration 15234: 0.04209391722246148\n",
      "Train Loss at iteration 15235: 0.04209389197218306\n",
      "Train Loss at iteration 15236: 0.04209386672608428\n",
      "Train Loss at iteration 15237: 0.042093841484164424\n",
      "Train Loss at iteration 15238: 0.042093816246422805\n",
      "Train Loss at iteration 15239: 0.04209379101285874\n",
      "Train Loss at iteration 15240: 0.042093765783471536\n",
      "Train Loss at iteration 15241: 0.04209374055826049\n",
      "Train Loss at iteration 15242: 0.042093715337224925\n",
      "Train Loss at iteration 15243: 0.04209369012036414\n",
      "Train Loss at iteration 15244: 0.04209366490767743\n",
      "Train Loss at iteration 15245: 0.042093639699164125\n",
      "Train Loss at iteration 15246: 0.04209361449482354\n",
      "Train Loss at iteration 15247: 0.04209358929465495\n",
      "Train Loss at iteration 15248: 0.04209356409865769\n",
      "Train Loss at iteration 15249: 0.04209353890683106\n",
      "Train Loss at iteration 15250: 0.04209351371917438\n",
      "Train Loss at iteration 15251: 0.04209348853568694\n",
      "Train Loss at iteration 15252: 0.04209346335636806\n",
      "Train Loss at iteration 15253: 0.042093438181217045\n",
      "Train Loss at iteration 15254: 0.042093413010233206\n",
      "Train Loss at iteration 15255: 0.04209338784341586\n",
      "Train Loss at iteration 15256: 0.042093362680764315\n",
      "Train Loss at iteration 15257: 0.04209333752227787\n",
      "Train Loss at iteration 15258: 0.04209331236795584\n",
      "Train Loss at iteration 15259: 0.04209328721779753\n",
      "Train Loss at iteration 15260: 0.04209326207180226\n",
      "Train Loss at iteration 15261: 0.042093236929969344\n",
      "Train Loss at iteration 15262: 0.04209321179229808\n",
      "Train Loss at iteration 15263: 0.04209318665878778\n",
      "Train Loss at iteration 15264: 0.04209316152943776\n",
      "Train Loss at iteration 15265: 0.042093136404247326\n",
      "Train Loss at iteration 15266: 0.04209311128321579\n",
      "Train Loss at iteration 15267: 0.04209308616634246\n",
      "Train Loss at iteration 15268: 0.042093061053626646\n",
      "Train Loss at iteration 15269: 0.04209303594506767\n",
      "Train Loss at iteration 15270: 0.04209301084066486\n",
      "Train Loss at iteration 15271: 0.04209298574041747\n",
      "Train Loss at iteration 15272: 0.04209296064432485\n",
      "Train Loss at iteration 15273: 0.042092935552386315\n",
      "Train Loss at iteration 15274: 0.04209291046460116\n",
      "Train Loss at iteration 15275: 0.04209288538096871\n",
      "Train Loss at iteration 15276: 0.042092860301488275\n",
      "Train Loss at iteration 15277: 0.04209283522615917\n",
      "Train Loss at iteration 15278: 0.04209281015498068\n",
      "Train Loss at iteration 15279: 0.04209278508795216\n",
      "Train Loss at iteration 15280: 0.04209276002507288\n",
      "Train Loss at iteration 15281: 0.0420927349663422\n",
      "Train Loss at iteration 15282: 0.04209270991175938\n",
      "Train Loss at iteration 15283: 0.04209268486132377\n",
      "Train Loss at iteration 15284: 0.04209265981503467\n",
      "Train Loss at iteration 15285: 0.04209263477289138\n",
      "Train Loss at iteration 15286: 0.04209260973489325\n",
      "Train Loss at iteration 15287: 0.042092584701039555\n",
      "Train Loss at iteration 15288: 0.04209255967132963\n",
      "Train Loss at iteration 15289: 0.04209253464576278\n",
      "Train Loss at iteration 15290: 0.04209250962433831\n",
      "Train Loss at iteration 15291: 0.042092484607055565\n",
      "Train Loss at iteration 15292: 0.04209245959391382\n",
      "Train Loss at iteration 15293: 0.04209243458491241\n",
      "Train Loss at iteration 15294: 0.042092409580050644\n",
      "Train Loss at iteration 15295: 0.042092384579327846\n",
      "Train Loss at iteration 15296: 0.04209235958274331\n",
      "Train Loss at iteration 15297: 0.042092334590296376\n",
      "Train Loss at iteration 15298: 0.04209230960198633\n",
      "Train Loss at iteration 15299: 0.0420922846178125\n",
      "Train Loss at iteration 15300: 0.04209225963777421\n",
      "Train Loss at iteration 15301: 0.042092234661870766\n",
      "Train Loss at iteration 15302: 0.04209220969010148\n",
      "Train Loss at iteration 15303: 0.042092184722465655\n",
      "Train Loss at iteration 15304: 0.04209215975896264\n",
      "Train Loss at iteration 15305: 0.04209213479959173\n",
      "Train Loss at iteration 15306: 0.042092109844352237\n",
      "Train Loss at iteration 15307: 0.04209208489324348\n",
      "Train Loss at iteration 15308: 0.04209205994626477\n",
      "Train Loss at iteration 15309: 0.04209203500341544\n",
      "Train Loss at iteration 15310: 0.04209201006469478\n",
      "Train Loss at iteration 15311: 0.04209198513010213\n",
      "Train Loss at iteration 15312: 0.0420919601996368\n",
      "Train Loss at iteration 15313: 0.04209193527329808\n",
      "Train Loss at iteration 15314: 0.04209191035108532\n",
      "Train Loss at iteration 15315: 0.04209188543299782\n",
      "Train Loss at iteration 15316: 0.04209186051903491\n",
      "Train Loss at iteration 15317: 0.04209183560919589\n",
      "Train Loss at iteration 15318: 0.04209181070348008\n",
      "Train Loss at iteration 15319: 0.04209178580188681\n",
      "Train Loss at iteration 15320: 0.042091760904415375\n",
      "Train Loss at iteration 15321: 0.04209173601106511\n",
      "Train Loss at iteration 15322: 0.04209171112183533\n",
      "Train Loss at iteration 15323: 0.04209168623672535\n",
      "Train Loss at iteration 15324: 0.042091661355734486\n",
      "Train Loss at iteration 15325: 0.04209163647886205\n",
      "Train Loss at iteration 15326: 0.04209161160610736\n",
      "Train Loss at iteration 15327: 0.04209158673746975\n",
      "Train Loss at iteration 15328: 0.042091561872948526\n",
      "Train Loss at iteration 15329: 0.042091537012543\n",
      "Train Loss at iteration 15330: 0.042091512156252515\n",
      "Train Loss at iteration 15331: 0.04209148730407634\n",
      "Train Loss at iteration 15332: 0.042091462456013834\n",
      "Train Loss at iteration 15333: 0.04209143761206431\n",
      "Train Loss at iteration 15334: 0.04209141277222708\n",
      "Train Loss at iteration 15335: 0.042091387936501475\n",
      "Train Loss at iteration 15336: 0.04209136310488679\n",
      "Train Loss at iteration 15337: 0.04209133827738235\n",
      "Train Loss at iteration 15338: 0.042091313453987506\n",
      "Train Loss at iteration 15339: 0.04209128863470153\n",
      "Train Loss at iteration 15340: 0.04209126381952377\n",
      "Train Loss at iteration 15341: 0.04209123900845354\n",
      "Train Loss at iteration 15342: 0.04209121420149016\n",
      "Train Loss at iteration 15343: 0.04209118939863293\n",
      "Train Loss at iteration 15344: 0.04209116459988121\n",
      "Train Loss at iteration 15345: 0.04209113980523429\n",
      "Train Loss at iteration 15346: 0.04209111501469149\n",
      "Train Loss at iteration 15347: 0.04209109022825214\n",
      "Train Loss at iteration 15348: 0.04209106544591555\n",
      "Train Loss at iteration 15349: 0.04209104066768106\n",
      "Train Loss at iteration 15350: 0.04209101589354797\n",
      "Train Loss at iteration 15351: 0.0420909911235156\n",
      "Train Loss at iteration 15352: 0.04209096635758328\n",
      "Train Loss at iteration 15353: 0.04209094159575033\n",
      "Train Loss at iteration 15354: 0.04209091683801607\n",
      "Train Loss at iteration 15355: 0.04209089208437982\n",
      "Train Loss at iteration 15356: 0.042090867334840895\n",
      "Train Loss at iteration 15357: 0.04209084258939863\n",
      "Train Loss at iteration 15358: 0.042090817848052346\n",
      "Train Loss at iteration 15359: 0.04209079311080134\n",
      "Train Loss at iteration 15360: 0.04209076837764496\n",
      "Train Loss at iteration 15361: 0.04209074364858252\n",
      "Train Loss at iteration 15362: 0.042090718923613324\n",
      "Train Loss at iteration 15363: 0.042090694202736714\n",
      "Train Loss at iteration 15364: 0.04209066948595202\n",
      "Train Loss at iteration 15365: 0.042090644773258544\n",
      "Train Loss at iteration 15366: 0.04209062006465561\n",
      "Train Loss at iteration 15367: 0.04209059536014254\n",
      "Train Loss at iteration 15368: 0.04209057065971867\n",
      "Train Loss at iteration 15369: 0.04209054596338332\n",
      "Train Loss at iteration 15370: 0.04209052127113579\n",
      "Train Loss at iteration 15371: 0.04209049658297544\n",
      "Train Loss at iteration 15372: 0.04209047189890155\n",
      "Train Loss at iteration 15373: 0.042090447218913474\n",
      "Train Loss at iteration 15374: 0.042090422543010535\n",
      "Train Loss at iteration 15375: 0.04209039787119204\n",
      "Train Loss at iteration 15376: 0.042090373203457314\n",
      "Train Loss at iteration 15377: 0.04209034853980569\n",
      "Train Loss at iteration 15378: 0.04209032388023648\n",
      "Train Loss at iteration 15379: 0.042090299224749016\n",
      "Train Loss at iteration 15380: 0.04209027457334262\n",
      "Train Loss at iteration 15381: 0.04209024992601663\n",
      "Train Loss at iteration 15382: 0.04209022528277034\n",
      "Train Loss at iteration 15383: 0.04209020064360309\n",
      "Train Loss at iteration 15384: 0.04209017600851422\n",
      "Train Loss at iteration 15385: 0.04209015137750302\n",
      "Train Loss at iteration 15386: 0.04209012675056884\n",
      "Train Loss at iteration 15387: 0.042090102127711\n",
      "Train Loss at iteration 15388: 0.04209007750892881\n",
      "Train Loss at iteration 15389: 0.04209005289422162\n",
      "Train Loss at iteration 15390: 0.042090028283588744\n",
      "Train Loss at iteration 15391: 0.04209000367702949\n",
      "Train Loss at iteration 15392: 0.04208997907454319\n",
      "Train Loss at iteration 15393: 0.0420899544761292\n",
      "Train Loss at iteration 15394: 0.0420899298817868\n",
      "Train Loss at iteration 15395: 0.04208990529151536\n",
      "Train Loss at iteration 15396: 0.042089880705314164\n",
      "Train Loss at iteration 15397: 0.04208985612318255\n",
      "Train Loss at iteration 15398: 0.04208983154511987\n",
      "Train Loss at iteration 15399: 0.042089806971125424\n",
      "Train Loss at iteration 15400: 0.04208978240119854\n",
      "Train Loss at iteration 15401: 0.04208975783533854\n",
      "Train Loss at iteration 15402: 0.042089733273544754\n",
      "Train Loss at iteration 15403: 0.04208970871581652\n",
      "Train Loss at iteration 15404: 0.04208968416215316\n",
      "Train Loss at iteration 15405: 0.042089659612554\n",
      "Train Loss at iteration 15406: 0.042089635067018356\n",
      "Train Loss at iteration 15407: 0.04208961052554556\n",
      "Train Loss at iteration 15408: 0.04208958598813495\n",
      "Train Loss at iteration 15409: 0.04208956145478583\n",
      "Train Loss at iteration 15410: 0.042089536925497543\n",
      "Train Loss at iteration 15411: 0.042089512400269424\n",
      "Train Loss at iteration 15412: 0.04208948787910078\n",
      "Train Loss at iteration 15413: 0.04208946336199095\n",
      "Train Loss at iteration 15414: 0.04208943884893926\n",
      "Train Loss at iteration 15415: 0.04208941433994504\n",
      "Train Loss at iteration 15416: 0.042089389835007616\n",
      "Train Loss at iteration 15417: 0.04208936533412632\n",
      "Train Loss at iteration 15418: 0.04208934083730047\n",
      "Train Loss at iteration 15419: 0.0420893163445294\n",
      "Train Loss at iteration 15420: 0.04208929185581243\n",
      "Train Loss at iteration 15421: 0.0420892673711489\n",
      "Train Loss at iteration 15422: 0.042089242890538135\n",
      "Train Loss at iteration 15423: 0.04208921841397946\n",
      "Train Loss at iteration 15424: 0.04208919394147221\n",
      "Train Loss at iteration 15425: 0.042089169473015706\n",
      "Train Loss at iteration 15426: 0.04208914500860929\n",
      "Train Loss at iteration 15427: 0.04208912054825227\n",
      "Train Loss at iteration 15428: 0.04208909609194399\n",
      "Train Loss at iteration 15429: 0.042089071639683784\n",
      "Train Loss at iteration 15430: 0.04208904719147096\n",
      "Train Loss at iteration 15431: 0.04208902274730486\n",
      "Train Loss at iteration 15432: 0.04208899830718483\n",
      "Train Loss at iteration 15433: 0.04208897387111017\n",
      "Train Loss at iteration 15434: 0.04208894943908023\n",
      "Train Loss at iteration 15435: 0.042088925011094326\n",
      "Train Loss at iteration 15436: 0.04208890058715179\n",
      "Train Loss at iteration 15437: 0.04208887616725197\n",
      "Train Loss at iteration 15438: 0.04208885175139418\n",
      "Train Loss at iteration 15439: 0.04208882733957775\n",
      "Train Loss at iteration 15440: 0.04208880293180202\n",
      "Train Loss at iteration 15441: 0.0420887785280663\n",
      "Train Loss at iteration 15442: 0.04208875412836996\n",
      "Train Loss at iteration 15443: 0.0420887297327123\n",
      "Train Loss at iteration 15444: 0.042088705341092644\n",
      "Train Loss at iteration 15445: 0.04208868095351033\n",
      "Train Loss at iteration 15446: 0.0420886565699647\n",
      "Train Loss at iteration 15447: 0.04208863219045509\n",
      "Train Loss at iteration 15448: 0.04208860781498081\n",
      "Train Loss at iteration 15449: 0.04208858344354121\n",
      "Train Loss at iteration 15450: 0.042088559076135616\n",
      "Train Loss at iteration 15451: 0.04208853471276336\n",
      "Train Loss at iteration 15452: 0.04208851035342376\n",
      "Train Loss at iteration 15453: 0.042088485998116175\n",
      "Train Loss at iteration 15454: 0.0420884616468399\n",
      "Train Loss at iteration 15455: 0.0420884372995943\n",
      "Train Loss at iteration 15456: 0.04208841295637869\n",
      "Train Loss at iteration 15457: 0.04208838861719242\n",
      "Train Loss at iteration 15458: 0.0420883642820348\n",
      "Train Loss at iteration 15459: 0.042088339950905176\n",
      "Train Loss at iteration 15460: 0.04208831562380287\n",
      "Train Loss at iteration 15461: 0.04208829130072724\n",
      "Train Loss at iteration 15462: 0.042088266981677586\n",
      "Train Loss at iteration 15463: 0.04208824266665325\n",
      "Train Loss at iteration 15464: 0.04208821835565358\n",
      "Train Loss at iteration 15465: 0.04208819404867791\n",
      "Train Loss at iteration 15466: 0.04208816974572555\n",
      "Train Loss at iteration 15467: 0.04208814544679586\n",
      "Train Loss at iteration 15468: 0.04208812115188814\n",
      "Train Loss at iteration 15469: 0.042088096861001754\n",
      "Train Loss at iteration 15470: 0.04208807257413602\n",
      "Train Loss at iteration 15471: 0.042088048291290286\n",
      "Train Loss at iteration 15472: 0.04208802401246387\n",
      "Train Loss at iteration 15473: 0.04208799973765612\n",
      "Train Loss at iteration 15474: 0.042087975466866365\n",
      "Train Loss at iteration 15475: 0.04208795120009393\n",
      "Train Loss at iteration 15476: 0.04208792693733816\n",
      "Train Loss at iteration 15477: 0.04208790267859839\n",
      "Train Loss at iteration 15478: 0.04208787842387395\n",
      "Train Loss at iteration 15479: 0.04208785417316418\n",
      "Train Loss at iteration 15480: 0.04208782992646841\n",
      "Train Loss at iteration 15481: 0.04208780568378596\n",
      "Train Loss at iteration 15482: 0.0420877814451162\n",
      "Train Loss at iteration 15483: 0.042087757210458446\n",
      "Train Loss at iteration 15484: 0.04208773297981202\n",
      "Train Loss at iteration 15485: 0.04208770875317627\n",
      "Train Loss at iteration 15486: 0.04208768453055054\n",
      "Train Loss at iteration 15487: 0.042087660311934165\n",
      "Train Loss at iteration 15488: 0.04208763609732646\n",
      "Train Loss at iteration 15489: 0.042087611886726786\n",
      "Train Loss at iteration 15490: 0.04208758768013445\n",
      "Train Loss at iteration 15491: 0.04208756347754882\n",
      "Train Loss at iteration 15492: 0.04208753927896921\n",
      "Train Loss at iteration 15493: 0.04208751508439497\n",
      "Train Loss at iteration 15494: 0.042087490893825436\n",
      "Train Loss at iteration 15495: 0.04208746670725993\n",
      "Train Loss at iteration 15496: 0.04208744252469781\n",
      "Train Loss at iteration 15497: 0.042087418346138376\n",
      "Train Loss at iteration 15498: 0.042087394171581\n",
      "Train Loss at iteration 15499: 0.042087370001025026\n",
      "Train Loss at iteration 15500: 0.04208734583446974\n",
      "Train Loss at iteration 15501: 0.042087321671914535\n",
      "Train Loss at iteration 15502: 0.04208729751335872\n",
      "Train Loss at iteration 15503: 0.042087273358801636\n",
      "Train Loss at iteration 15504: 0.04208724920824263\n",
      "Train Loss at iteration 15505: 0.04208722506168102\n",
      "Train Loss at iteration 15506: 0.04208720091911616\n",
      "Train Loss at iteration 15507: 0.04208717678054738\n",
      "Train Loss at iteration 15508: 0.042087152645974034\n",
      "Train Loss at iteration 15509: 0.042087128515395436\n",
      "Train Loss at iteration 15510: 0.04208710438881094\n",
      "Train Loss at iteration 15511: 0.04208708026621987\n",
      "Train Loss at iteration 15512: 0.04208705614762158\n",
      "Train Loss at iteration 15513: 0.04208703203301541\n",
      "Train Loss at iteration 15514: 0.042087007922400686\n",
      "Train Loss at iteration 15515: 0.042086983815776746\n",
      "Train Loss at iteration 15516: 0.04208695971314294\n",
      "Train Loss at iteration 15517: 0.04208693561449859\n",
      "Train Loss at iteration 15518: 0.04208691151984306\n",
      "Train Loss at iteration 15519: 0.04208688742917568\n",
      "Train Loss at iteration 15520: 0.042086863342495784\n",
      "Train Loss at iteration 15521: 0.04208683925980271\n",
      "Train Loss at iteration 15522: 0.042086815181095784\n",
      "Train Loss at iteration 15523: 0.04208679110637438\n",
      "Train Loss at iteration 15524: 0.04208676703563781\n",
      "Train Loss at iteration 15525: 0.04208674296888543\n",
      "Train Loss at iteration 15526: 0.04208671890611656\n",
      "Train Loss at iteration 15527: 0.04208669484733056\n",
      "Train Loss at iteration 15528: 0.04208667079252676\n",
      "Train Loss at iteration 15529: 0.042086646741704516\n",
      "Train Loss at iteration 15530: 0.04208662269486315\n",
      "Train Loss at iteration 15531: 0.042086598652002\n",
      "Train Loss at iteration 15532: 0.04208657461312042\n",
      "Train Loss at iteration 15533: 0.042086550578217745\n",
      "Train Loss at iteration 15534: 0.042086526547293314\n",
      "Train Loss at iteration 15535: 0.042086502520346465\n",
      "Train Loss at iteration 15536: 0.042086478497376546\n",
      "Train Loss at iteration 15537: 0.0420864544783829\n",
      "Train Loss at iteration 15538: 0.04208643046336487\n",
      "Train Loss at iteration 15539: 0.04208640645232178\n",
      "Train Loss at iteration 15540: 0.042086382445252986\n",
      "Train Loss at iteration 15541: 0.042086358442157835\n",
      "Train Loss at iteration 15542: 0.042086334443035646\n",
      "Train Loss at iteration 15543: 0.04208631044788579\n",
      "Train Loss at iteration 15544: 0.042086286456707575\n",
      "Train Loss at iteration 15545: 0.04208626246950038\n",
      "Train Loss at iteration 15546: 0.04208623848626352\n",
      "Train Loss at iteration 15547: 0.04208621450699635\n",
      "Train Loss at iteration 15548: 0.0420861905316982\n",
      "Train Loss at iteration 15549: 0.04208616656036843\n",
      "Train Loss at iteration 15550: 0.04208614259300637\n",
      "Train Loss at iteration 15551: 0.04208611862961137\n",
      "Train Loss at iteration 15552: 0.04208609467018277\n",
      "Train Loss at iteration 15553: 0.0420860707147199\n",
      "Train Loss at iteration 15554: 0.04208604676322213\n",
      "Train Loss at iteration 15555: 0.042086022815688774\n",
      "Train Loss at iteration 15556: 0.0420859988721192\n",
      "Train Loss at iteration 15557: 0.04208597493251273\n",
      "Train Loss at iteration 15558: 0.04208595099686873\n",
      "Train Loss at iteration 15559: 0.042085927065186525\n",
      "Train Loss at iteration 15560: 0.04208590313746545\n",
      "Train Loss at iteration 15561: 0.04208587921370489\n",
      "Train Loss at iteration 15562: 0.04208585529390415\n",
      "Train Loss at iteration 15563: 0.042085831378062585\n",
      "Train Loss at iteration 15564: 0.04208580746617954\n",
      "Train Loss at iteration 15565: 0.04208578355825437\n",
      "Train Loss at iteration 15566: 0.04208575965428639\n",
      "Train Loss at iteration 15567: 0.042085735754274975\n",
      "Train Loss at iteration 15568: 0.04208571185821946\n",
      "Train Loss at iteration 15569: 0.042085687966119174\n",
      "Train Loss at iteration 15570: 0.042085664077973484\n",
      "Train Loss at iteration 15571: 0.04208564019378173\n",
      "Train Loss at iteration 15572: 0.042085616313543246\n",
      "Train Loss at iteration 15573: 0.04208559243725739\n",
      "Train Loss at iteration 15574: 0.0420855685649235\n",
      "Train Loss at iteration 15575: 0.04208554469654091\n",
      "Train Loss at iteration 15576: 0.042085520832108994\n",
      "Train Loss at iteration 15577: 0.042085496971627084\n",
      "Train Loss at iteration 15578: 0.0420854731150945\n",
      "Train Loss at iteration 15579: 0.04208544926251062\n",
      "Train Loss at iteration 15580: 0.0420854254138748\n",
      "Train Loss at iteration 15581: 0.04208540156918634\n",
      "Train Loss at iteration 15582: 0.042085377728444624\n",
      "Train Loss at iteration 15583: 0.04208535389164898\n",
      "Train Loss at iteration 15584: 0.04208533005879877\n",
      "Train Loss at iteration 15585: 0.04208530622989333\n",
      "Train Loss at iteration 15586: 0.042085282404932\n",
      "Train Loss at iteration 15587: 0.04208525858391414\n",
      "Train Loss at iteration 15588: 0.04208523476683909\n",
      "Train Loss at iteration 15589: 0.042085210953706195\n",
      "Train Loss at iteration 15590: 0.042085187144514806\n",
      "Train Loss at iteration 15591: 0.04208516333926427\n",
      "Train Loss at iteration 15592: 0.042085139537953925\n",
      "Train Loss at iteration 15593: 0.042085115740583136\n",
      "Train Loss at iteration 15594: 0.04208509194715124\n",
      "Train Loss at iteration 15595: 0.04208506815765758\n",
      "Train Loss at iteration 15596: 0.0420850443721015\n",
      "Train Loss at iteration 15597: 0.042085020590482365\n",
      "Train Loss at iteration 15598: 0.04208499681279952\n",
      "Train Loss at iteration 15599: 0.04208497303905229\n",
      "Train Loss at iteration 15600: 0.04208494926924005\n",
      "Train Loss at iteration 15601: 0.04208492550336213\n",
      "Train Loss at iteration 15602: 0.042084901741417886\n",
      "Train Loss at iteration 15603: 0.042084877983406675\n",
      "Train Loss at iteration 15604: 0.04208485422932783\n",
      "Train Loss at iteration 15605: 0.042084830479180706\n",
      "Train Loss at iteration 15606: 0.04208480673296466\n",
      "Train Loss at iteration 15607: 0.04208478299067902\n",
      "Train Loss at iteration 15608: 0.042084759252323155\n",
      "Train Loss at iteration 15609: 0.04208473551789641\n",
      "Train Loss at iteration 15610: 0.042084711787398116\n",
      "Train Loss at iteration 15611: 0.042084688060827644\n",
      "Train Loss at iteration 15612: 0.04208466433818433\n",
      "Train Loss at iteration 15613: 0.04208464061946754\n",
      "Train Loss at iteration 15614: 0.0420846169046766\n",
      "Train Loss at iteration 15615: 0.04208459319381088\n",
      "Train Loss at iteration 15616: 0.04208456948686972\n",
      "Train Loss at iteration 15617: 0.04208454578385247\n",
      "Train Loss at iteration 15618: 0.042084522084758484\n",
      "Train Loss at iteration 15619: 0.042084498389587104\n",
      "Train Loss at iteration 15620: 0.0420844746983377\n",
      "Train Loss at iteration 15621: 0.0420844510110096\n",
      "Train Loss at iteration 15622: 0.04208442732760215\n",
      "Train Loss at iteration 15623: 0.04208440364811473\n",
      "Train Loss at iteration 15624: 0.042084379972546666\n",
      "Train Loss at iteration 15625: 0.04208435630089731\n",
      "Train Loss at iteration 15626: 0.04208433263316602\n",
      "Train Loss at iteration 15627: 0.042084308969352155\n",
      "Train Loss at iteration 15628: 0.042084285309455044\n",
      "Train Loss at iteration 15629: 0.042084261653474064\n",
      "Train Loss at iteration 15630: 0.04208423800140855\n",
      "Train Loss at iteration 15631: 0.04208421435325786\n",
      "Train Loss at iteration 15632: 0.04208419070902134\n",
      "Train Loss at iteration 15633: 0.04208416706869833\n",
      "Train Loss at iteration 15634: 0.04208414343228822\n",
      "Train Loss at iteration 15635: 0.042084119799790314\n",
      "Train Loss at iteration 15636: 0.042084096171204\n",
      "Train Loss at iteration 15637: 0.0420840725465286\n",
      "Train Loss at iteration 15638: 0.0420840489257635\n",
      "Train Loss at iteration 15639: 0.04208402530890803\n",
      "Train Loss at iteration 15640: 0.04208400169596155\n",
      "Train Loss at iteration 15641: 0.04208397808692339\n",
      "Train Loss at iteration 15642: 0.04208395448179294\n",
      "Train Loss at iteration 15643: 0.04208393088056954\n",
      "Train Loss at iteration 15644: 0.04208390728325253\n",
      "Train Loss at iteration 15645: 0.04208388368984127\n",
      "Train Loss at iteration 15646: 0.042083860100335116\n",
      "Train Loss at iteration 15647: 0.04208383651473341\n",
      "Train Loss at iteration 15648: 0.04208381293303553\n",
      "Train Loss at iteration 15649: 0.0420837893552408\n",
      "Train Loss at iteration 15650: 0.042083765781348585\n",
      "Train Loss at iteration 15651: 0.04208374221135825\n",
      "Train Loss at iteration 15652: 0.04208371864526913\n",
      "Train Loss at iteration 15653: 0.04208369508308059\n",
      "Train Loss at iteration 15654: 0.04208367152479198\n",
      "Train Loss at iteration 15655: 0.042083647970402656\n",
      "Train Loss at iteration 15656: 0.042083624419911966\n",
      "Train Loss at iteration 15657: 0.04208360087331928\n",
      "Train Loss at iteration 15658: 0.04208357733062392\n",
      "Train Loss at iteration 15659: 0.04208355379182528\n",
      "Train Loss at iteration 15660: 0.0420835302569227\n",
      "Train Loss at iteration 15661: 0.04208350672591551\n",
      "Train Loss at iteration 15662: 0.0420834831988031\n",
      "Train Loss at iteration 15663: 0.0420834596755848\n",
      "Train Loss at iteration 15664: 0.042083436156259986\n",
      "Train Loss at iteration 15665: 0.04208341264082801\n",
      "Train Loss at iteration 15666: 0.0420833891292882\n",
      "Train Loss at iteration 15667: 0.042083365621639925\n",
      "Train Loss at iteration 15668: 0.04208334211788257\n",
      "Train Loss at iteration 15669: 0.04208331861801545\n",
      "Train Loss at iteration 15670: 0.04208329512203793\n",
      "Train Loss at iteration 15671: 0.04208327162994939\n",
      "Train Loss at iteration 15672: 0.04208324814174915\n",
      "Train Loss at iteration 15673: 0.04208322465743659\n",
      "Train Loss at iteration 15674: 0.04208320117701107\n",
      "Train Loss at iteration 15675: 0.04208317770047193\n",
      "Train Loss at iteration 15676: 0.042083154227818526\n",
      "Train Loss at iteration 15677: 0.04208313075905022\n",
      "Train Loss at iteration 15678: 0.04208310729416638\n",
      "Train Loss at iteration 15679: 0.04208308383316633\n",
      "Train Loss at iteration 15680: 0.04208306037604947\n",
      "Train Loss at iteration 15681: 0.04208303692281512\n",
      "Train Loss at iteration 15682: 0.04208301347346266\n",
      "Train Loss at iteration 15683: 0.04208299002799143\n",
      "Train Loss at iteration 15684: 0.042082966586400795\n",
      "Train Loss at iteration 15685: 0.04208294314869011\n",
      "Train Loss at iteration 15686: 0.04208291971485875\n",
      "Train Loss at iteration 15687: 0.042082896284906045\n",
      "Train Loss at iteration 15688: 0.04208287285883137\n",
      "Train Loss at iteration 15689: 0.04208284943663408\n",
      "Train Loss at iteration 15690: 0.042082826018313514\n",
      "Train Loss at iteration 15691: 0.04208280260386905\n",
      "Train Loss at iteration 15692: 0.04208277919330005\n",
      "Train Loss at iteration 15693: 0.04208275578660586\n",
      "Train Loss at iteration 15694: 0.042082732383785834\n",
      "Train Loss at iteration 15695: 0.04208270898483934\n",
      "Train Loss at iteration 15696: 0.04208268558976574\n",
      "Train Loss at iteration 15697: 0.04208266219856438\n",
      "Train Loss at iteration 15698: 0.042082638811234634\n",
      "Train Loss at iteration 15699: 0.04208261542777584\n",
      "Train Loss at iteration 15700: 0.04208259204818737\n",
      "Train Loss at iteration 15701: 0.04208256867246858\n",
      "Train Loss at iteration 15702: 0.042082545300618834\n",
      "Train Loss at iteration 15703: 0.04208252193263748\n",
      "Train Loss at iteration 15704: 0.04208249856852389\n",
      "Train Loss at iteration 15705: 0.04208247520827742\n",
      "Train Loss at iteration 15706: 0.04208245185189742\n",
      "Train Loss at iteration 15707: 0.04208242849938326\n",
      "Train Loss at iteration 15708: 0.042082405150734296\n",
      "Train Loss at iteration 15709: 0.042082381805949885\n",
      "Train Loss at iteration 15710: 0.04208235846502938\n",
      "Train Loss at iteration 15711: 0.04208233512797216\n",
      "Train Loss at iteration 15712: 0.04208231179477758\n",
      "Train Loss at iteration 15713: 0.04208228846544499\n",
      "Train Loss at iteration 15714: 0.04208226513997376\n",
      "Train Loss at iteration 15715: 0.04208224181836324\n",
      "Train Loss at iteration 15716: 0.04208221850061279\n",
      "Train Loss at iteration 15717: 0.042082195186721794\n",
      "Train Loss at iteration 15718: 0.042082171876689584\n",
      "Train Loss at iteration 15719: 0.04208214857051554\n",
      "Train Loss at iteration 15720: 0.042082125268198996\n",
      "Train Loss at iteration 15721: 0.042082101969739355\n",
      "Train Loss at iteration 15722: 0.04208207867513594\n",
      "Train Loss at iteration 15723: 0.04208205538438813\n",
      "Train Loss at iteration 15724: 0.04208203209749529\n",
      "Train Loss at iteration 15725: 0.04208200881445676\n",
      "Train Loss at iteration 15726: 0.042081985535271926\n",
      "Train Loss at iteration 15727: 0.04208196225994014\n",
      "Train Loss at iteration 15728: 0.04208193898846077\n",
      "Train Loss at iteration 15729: 0.04208191572083317\n",
      "Train Loss at iteration 15730: 0.04208189245705669\n",
      "Train Loss at iteration 15731: 0.04208186919713072\n",
      "Train Loss at iteration 15732: 0.0420818459410546\n",
      "Train Loss at iteration 15733: 0.04208182268882771\n",
      "Train Loss at iteration 15734: 0.04208179944044938\n",
      "Train Loss at iteration 15735: 0.04208177619591902\n",
      "Train Loss at iteration 15736: 0.042081752955235946\n",
      "Train Loss at iteration 15737: 0.04208172971839956\n",
      "Train Loss at iteration 15738: 0.042081706485409195\n",
      "Train Loss at iteration 15739: 0.042081683256264234\n",
      "Train Loss at iteration 15740: 0.04208166003096402\n",
      "Train Loss at iteration 15741: 0.04208163680950794\n",
      "Train Loss at iteration 15742: 0.04208161359189534\n",
      "Train Loss at iteration 15743: 0.04208159037812559\n",
      "Train Loss at iteration 15744: 0.042081567168198046\n",
      "Train Loss at iteration 15745: 0.04208154396211209\n",
      "Train Loss at iteration 15746: 0.042081520759867055\n",
      "Train Loss at iteration 15747: 0.04208149756146233\n",
      "Train Loss at iteration 15748: 0.04208147436689727\n",
      "Train Loss at iteration 15749: 0.04208145117617125\n",
      "Train Loss at iteration 15750: 0.04208142798928361\n",
      "Train Loss at iteration 15751: 0.042081404806233735\n",
      "Train Loss at iteration 15752: 0.042081381627020985\n",
      "Train Loss at iteration 15753: 0.042081358451644714\n",
      "Train Loss at iteration 15754: 0.04208133528010429\n",
      "Train Loss at iteration 15755: 0.04208131211239909\n",
      "Train Loss at iteration 15756: 0.04208128894852847\n",
      "Train Loss at iteration 15757: 0.04208126578849181\n",
      "Train Loss at iteration 15758: 0.04208124263228844\n",
      "Train Loss at iteration 15759: 0.04208121947991776\n",
      "Train Loss at iteration 15760: 0.042081196331379106\n",
      "Train Loss at iteration 15761: 0.042081173186671865\n",
      "Train Loss at iteration 15762: 0.0420811500457954\n",
      "Train Loss at iteration 15763: 0.042081126908749066\n",
      "Train Loss at iteration 15764: 0.04208110377553223\n",
      "Train Loss at iteration 15765: 0.04208108064614427\n",
      "Train Loss at iteration 15766: 0.04208105752058453\n",
      "Train Loss at iteration 15767: 0.0420810343988524\n",
      "Train Loss at iteration 15768: 0.04208101128094724\n",
      "Train Loss at iteration 15769: 0.0420809881668684\n",
      "Train Loss at iteration 15770: 0.042080965056615266\n",
      "Train Loss at iteration 15771: 0.04208094195018718\n",
      "Train Loss at iteration 15772: 0.04208091884758354\n",
      "Train Loss at iteration 15773: 0.04208089574880369\n",
      "Train Loss at iteration 15774: 0.042080872653847\n",
      "Train Loss at iteration 15775: 0.042080849562712845\n",
      "Train Loss at iteration 15776: 0.042080826475400596\n",
      "Train Loss at iteration 15777: 0.04208080339190959\n",
      "Train Loss at iteration 15778: 0.04208078031223922\n",
      "Train Loss at iteration 15779: 0.042080757236388856\n",
      "Train Loss at iteration 15780: 0.04208073416435785\n",
      "Train Loss at iteration 15781: 0.04208071109614557\n",
      "Train Loss at iteration 15782: 0.0420806880317514\n",
      "Train Loss at iteration 15783: 0.04208066497117469\n",
      "Train Loss at iteration 15784: 0.04208064191441482\n",
      "Train Loss at iteration 15785: 0.04208061886147115\n",
      "Train Loss at iteration 15786: 0.04208059581234305\n",
      "Train Loss at iteration 15787: 0.04208057276702989\n",
      "Train Loss at iteration 15788: 0.04208054972553103\n",
      "Train Loss at iteration 15789: 0.04208052668784585\n",
      "Train Loss at iteration 15790: 0.04208050365397372\n",
      "Train Loss at iteration 15791: 0.04208048062391398\n",
      "Train Loss at iteration 15792: 0.04208045759766603\n",
      "Train Loss at iteration 15793: 0.04208043457522923\n",
      "Train Loss at iteration 15794: 0.04208041155660295\n",
      "Train Loss at iteration 15795: 0.04208038854178655\n",
      "Train Loss at iteration 15796: 0.042080365530779394\n",
      "Train Loss at iteration 15797: 0.042080342523580866\n",
      "Train Loss at iteration 15798: 0.04208031952019034\n",
      "Train Loss at iteration 15799: 0.042080296520607165\n",
      "Train Loss at iteration 15800: 0.042080273524830736\n",
      "Train Loss at iteration 15801: 0.042080250532860394\n",
      "Train Loss at iteration 15802: 0.04208022754469553\n",
      "Train Loss at iteration 15803: 0.04208020456033549\n",
      "Train Loss at iteration 15804: 0.042080181579779684\n",
      "Train Loss at iteration 15805: 0.04208015860302743\n",
      "Train Loss at iteration 15806: 0.04208013563007813\n",
      "Train Loss at iteration 15807: 0.04208011266093117\n",
      "Train Loss at iteration 15808: 0.04208008969558588\n",
      "Train Loss at iteration 15809: 0.04208006673404165\n",
      "Train Loss at iteration 15810: 0.04208004377629786\n",
      "Train Loss at iteration 15811: 0.042080020822353854\n",
      "Train Loss at iteration 15812: 0.04207999787220903\n",
      "Train Loss at iteration 15813: 0.04207997492586276\n",
      "Train Loss at iteration 15814: 0.04207995198331438\n",
      "Train Loss at iteration 15815: 0.042079929044563286\n",
      "Train Loss at iteration 15816: 0.04207990610960885\n",
      "Train Loss at iteration 15817: 0.04207988317845044\n",
      "Train Loss at iteration 15818: 0.042079860251087425\n",
      "Train Loss at iteration 15819: 0.04207983732751918\n",
      "Train Loss at iteration 15820: 0.042079814407745074\n",
      "Train Loss at iteration 15821: 0.04207979149176447\n",
      "Train Loss at iteration 15822: 0.04207976857957675\n",
      "Train Loss at iteration 15823: 0.042079745671181286\n",
      "Train Loss at iteration 15824: 0.04207972276657745\n",
      "Train Loss at iteration 15825: 0.04207969986576462\n",
      "Train Loss at iteration 15826: 0.042079676968742145\n",
      "Train Loss at iteration 15827: 0.04207965407550941\n",
      "Train Loss at iteration 15828: 0.0420796311860658\n",
      "Train Loss at iteration 15829: 0.04207960830041067\n",
      "Train Loss at iteration 15830: 0.0420795854185434\n",
      "Train Loss at iteration 15831: 0.04207956254046336\n",
      "Train Loss at iteration 15832: 0.042079539666169924\n",
      "Train Loss at iteration 15833: 0.04207951679566246\n",
      "Train Loss at iteration 15834: 0.04207949392894036\n",
      "Train Loss at iteration 15835: 0.04207947106600297\n",
      "Train Loss at iteration 15836: 0.04207944820684968\n",
      "Train Loss at iteration 15837: 0.042079425351479856\n",
      "Train Loss at iteration 15838: 0.04207940249989287\n",
      "Train Loss at iteration 15839: 0.04207937965208811\n",
      "Train Loss at iteration 15840: 0.04207935680806493\n",
      "Train Loss at iteration 15841: 0.04207933396782272\n",
      "Train Loss at iteration 15842: 0.04207931113136084\n",
      "Train Loss at iteration 15843: 0.042079288298678665\n",
      "Train Loss at iteration 15844: 0.042079265469775584\n",
      "Train Loss at iteration 15845: 0.04207924264465096\n",
      "Train Loss at iteration 15846: 0.042079219823304155\n",
      "Train Loss at iteration 15847: 0.04207919700573457\n",
      "Train Loss at iteration 15848: 0.042079174191941564\n",
      "Train Loss at iteration 15849: 0.04207915138192451\n",
      "Train Loss at iteration 15850: 0.042079128575682787\n",
      "Train Loss at iteration 15851: 0.04207910577321576\n",
      "Train Loss at iteration 15852: 0.04207908297452282\n",
      "Train Loss at iteration 15853: 0.04207906017960333\n",
      "Train Loss at iteration 15854: 0.04207903738845667\n",
      "Train Loss at iteration 15855: 0.042079014601082224\n",
      "Train Loss at iteration 15856: 0.04207899181747934\n",
      "Train Loss at iteration 15857: 0.042078969037647414\n",
      "Train Loss at iteration 15858: 0.04207894626158583\n",
      "Train Loss at iteration 15859: 0.04207892348929392\n",
      "Train Loss at iteration 15860: 0.04207890072077111\n",
      "Train Loss at iteration 15861: 0.04207887795601676\n",
      "Train Loss at iteration 15862: 0.04207885519503024\n",
      "Train Loss at iteration 15863: 0.04207883243781093\n",
      "Train Loss at iteration 15864: 0.0420788096843582\n",
      "Train Loss at iteration 15865: 0.04207878693467143\n",
      "Train Loss at iteration 15866: 0.04207876418874999\n",
      "Train Loss at iteration 15867: 0.04207874144659327\n",
      "Train Loss at iteration 15868: 0.04207871870820063\n",
      "Train Loss at iteration 15869: 0.042078695973571466\n",
      "Train Loss at iteration 15870: 0.04207867324270513\n",
      "Train Loss at iteration 15871: 0.04207865051560103\n",
      "Train Loss at iteration 15872: 0.042078627792258516\n",
      "Train Loss at iteration 15873: 0.042078605072676974\n",
      "Train Loss at iteration 15874: 0.042078582356855784\n",
      "Train Loss at iteration 15875: 0.04207855964479431\n",
      "Train Loss at iteration 15876: 0.04207853693649195\n",
      "Train Loss at iteration 15877: 0.04207851423194807\n",
      "Train Loss at iteration 15878: 0.04207849153116205\n",
      "Train Loss at iteration 15879: 0.04207846883413326\n",
      "Train Loss at iteration 15880: 0.042078446140861085\n",
      "Train Loss at iteration 15881: 0.042078423451344896\n",
      "Train Loss at iteration 15882: 0.042078400765584084\n",
      "Train Loss at iteration 15883: 0.04207837808357802\n",
      "Train Loss at iteration 15884: 0.042078355405326076\n",
      "Train Loss at iteration 15885: 0.042078332730827624\n",
      "Train Loss at iteration 15886: 0.04207831006008207\n",
      "Train Loss at iteration 15887: 0.04207828739308878\n",
      "Train Loss at iteration 15888: 0.04207826472984711\n",
      "Train Loss at iteration 15889: 0.042078242070356466\n",
      "Train Loss at iteration 15890: 0.04207821941461623\n",
      "Train Loss at iteration 15891: 0.042078196762625746\n",
      "Train Loss at iteration 15892: 0.04207817411438443\n",
      "Train Loss at iteration 15893: 0.04207815146989164\n",
      "Train Loss at iteration 15894: 0.04207812882914675\n",
      "Train Loss at iteration 15895: 0.04207810619214917\n",
      "Train Loss at iteration 15896: 0.04207808355889825\n",
      "Train Loss at iteration 15897: 0.04207806092939338\n",
      "Train Loss at iteration 15898: 0.04207803830363395\n",
      "Train Loss at iteration 15899: 0.04207801568161931\n",
      "Train Loss at iteration 15900: 0.04207799306334887\n",
      "Train Loss at iteration 15901: 0.04207797044882199\n",
      "Train Loss at iteration 15902: 0.04207794783803807\n",
      "Train Loss at iteration 15903: 0.04207792523099647\n",
      "Train Loss at iteration 15904: 0.042077902627696576\n",
      "Train Loss at iteration 15905: 0.04207788002813777\n",
      "Train Loss at iteration 15906: 0.04207785743231943\n",
      "Train Loss at iteration 15907: 0.04207783484024093\n",
      "Train Loss at iteration 15908: 0.04207781225190167\n",
      "Train Loss at iteration 15909: 0.04207778966730102\n",
      "Train Loss at iteration 15910: 0.042077767086438365\n",
      "Train Loss at iteration 15911: 0.04207774450931307\n",
      "Train Loss at iteration 15912: 0.042077721935924545\n",
      "Train Loss at iteration 15913: 0.042077699366272134\n",
      "Train Loss at iteration 15914: 0.04207767680035525\n",
      "Train Loss at iteration 15915: 0.04207765423817325\n",
      "Train Loss at iteration 15916: 0.042077631679725544\n",
      "Train Loss at iteration 15917: 0.04207760912501148\n",
      "Train Loss at iteration 15918: 0.04207758657403046\n",
      "Train Loss at iteration 15919: 0.04207756402678187\n",
      "Train Loss at iteration 15920: 0.04207754148326507\n",
      "Train Loss at iteration 15921: 0.04207751894347947\n",
      "Train Loss at iteration 15922: 0.04207749640742443\n",
      "Train Loss at iteration 15923: 0.042077473875099344\n",
      "Train Loss at iteration 15924: 0.04207745134650358\n",
      "Train Loss at iteration 15925: 0.04207742882163654\n",
      "Train Loss at iteration 15926: 0.0420774063004976\n",
      "Train Loss at iteration 15927: 0.04207738378308612\n",
      "Train Loss at iteration 15928: 0.04207736126940151\n",
      "Train Loss at iteration 15929: 0.04207733875944315\n",
      "Train Loss at iteration 15930: 0.04207731625321041\n",
      "Train Loss at iteration 15931: 0.04207729375070268\n",
      "Train Loss at iteration 15932: 0.04207727125191934\n",
      "Train Loss at iteration 15933: 0.04207724875685979\n",
      "Train Loss at iteration 15934: 0.04207722626552338\n",
      "Train Loss at iteration 15935: 0.04207720377790953\n",
      "Train Loss at iteration 15936: 0.0420771812940176\n",
      "Train Loss at iteration 15937: 0.042077158813846965\n",
      "Train Loss at iteration 15938: 0.04207713633739704\n",
      "Train Loss at iteration 15939: 0.042077113864667184\n",
      "Train Loss at iteration 15940: 0.04207709139565679\n",
      "Train Loss at iteration 15941: 0.042077068930365245\n",
      "Train Loss at iteration 15942: 0.04207704646879193\n",
      "Train Loss at iteration 15943: 0.04207702401093621\n",
      "Train Loss at iteration 15944: 0.0420770015567975\n",
      "Train Loss at iteration 15945: 0.042076979106375166\n",
      "Train Loss at iteration 15946: 0.042076956659668596\n",
      "Train Loss at iteration 15947: 0.04207693421667719\n",
      "Train Loss at iteration 15948: 0.042076911777400305\n",
      "Train Loss at iteration 15949: 0.042076889341837335\n",
      "Train Loss at iteration 15950: 0.04207686690998767\n",
      "Train Loss at iteration 15951: 0.0420768444818507\n",
      "Train Loss at iteration 15952: 0.0420768220574258\n",
      "Train Loss at iteration 15953: 0.04207679963671237\n",
      "Train Loss at iteration 15954: 0.04207677721970978\n",
      "Train Loss at iteration 15955: 0.04207675480641741\n",
      "Train Loss at iteration 15956: 0.04207673239683466\n",
      "Train Loss at iteration 15957: 0.042076709990960914\n",
      "Train Loss at iteration 15958: 0.04207668758879556\n",
      "Train Loss at iteration 15959: 0.042076665190337974\n",
      "Train Loss at iteration 15960: 0.04207664279558754\n",
      "Train Loss at iteration 15961: 0.042076620404543644\n",
      "Train Loss at iteration 15962: 0.04207659801720569\n",
      "Train Loss at iteration 15963: 0.04207657563357306\n",
      "Train Loss at iteration 15964: 0.042076553253645115\n",
      "Train Loss at iteration 15965: 0.042076530877421255\n",
      "Train Loss at iteration 15966: 0.0420765085049009\n",
      "Train Loss at iteration 15967: 0.04207648613608337\n",
      "Train Loss at iteration 15968: 0.04207646377096811\n",
      "Train Loss at iteration 15969: 0.04207644140955449\n",
      "Train Loss at iteration 15970: 0.04207641905184188\n",
      "Train Loss at iteration 15971: 0.04207639669782967\n",
      "Train Loss at iteration 15972: 0.04207637434751728\n",
      "Train Loss at iteration 15973: 0.04207635200090406\n",
      "Train Loss at iteration 15974: 0.04207632965798941\n",
      "Train Loss at iteration 15975: 0.042076307318772724\n",
      "Train Loss at iteration 15976: 0.04207628498325338\n",
      "Train Loss at iteration 15977: 0.04207626265143076\n",
      "Train Loss at iteration 15978: 0.04207624032330427\n",
      "Train Loss at iteration 15979: 0.042076217998873296\n",
      "Train Loss at iteration 15980: 0.04207619567813721\n",
      "Train Loss at iteration 15981: 0.04207617336109541\n",
      "Train Loss at iteration 15982: 0.04207615104774728\n",
      "Train Loss at iteration 15983: 0.04207612873809221\n",
      "Train Loss at iteration 15984: 0.04207610643212959\n",
      "Train Loss at iteration 15985: 0.04207608412985882\n",
      "Train Loss at iteration 15986: 0.04207606183127925\n",
      "Train Loss at iteration 15987: 0.042076039536390324\n",
      "Train Loss at iteration 15988: 0.04207601724519138\n",
      "Train Loss at iteration 15989: 0.042075994957681835\n",
      "Train Loss at iteration 15990: 0.042075972673861074\n",
      "Train Loss at iteration 15991: 0.042075950393728474\n",
      "Train Loss at iteration 15992: 0.042075928117283445\n",
      "Train Loss at iteration 15993: 0.04207590584452535\n",
      "Train Loss at iteration 15994: 0.0420758835754536\n",
      "Train Loss at iteration 15995: 0.04207586131006758\n",
      "Train Loss at iteration 15996: 0.04207583904836668\n",
      "Train Loss at iteration 15997: 0.04207581679035029\n",
      "Train Loss at iteration 15998: 0.04207579453601778\n",
      "Train Loss at iteration 15999: 0.042075772285368566\n",
      "Train Loss at iteration 16000: 0.04207575003840203\n",
      "Train Loss at iteration 16001: 0.04207572779511755\n",
      "Train Loss at iteration 16002: 0.04207570555551453\n",
      "Train Loss at iteration 16003: 0.04207568331959236\n",
      "Train Loss at iteration 16004: 0.04207566108735041\n",
      "Train Loss at iteration 16005: 0.0420756388587881\n",
      "Train Loss at iteration 16006: 0.04207561663390482\n",
      "Train Loss at iteration 16007: 0.04207559441269993\n",
      "Train Loss at iteration 16008: 0.04207557219517285\n",
      "Train Loss at iteration 16009: 0.04207554998132295\n",
      "Train Loss at iteration 16010: 0.04207552777114963\n",
      "Train Loss at iteration 16011: 0.04207550556465229\n",
      "Train Loss at iteration 16012: 0.042075483361830306\n",
      "Train Loss at iteration 16013: 0.042075461162683084\n",
      "Train Loss at iteration 16014: 0.042075438967210006\n",
      "Train Loss at iteration 16015: 0.04207541677541045\n",
      "Train Loss at iteration 16016: 0.04207539458728384\n",
      "Train Loss at iteration 16017: 0.04207537240282953\n",
      "Train Loss at iteration 16018: 0.042075350222046955\n",
      "Train Loss at iteration 16019: 0.04207532804493546\n",
      "Train Loss at iteration 16020: 0.04207530587149449\n",
      "Train Loss at iteration 16021: 0.04207528370172338\n",
      "Train Loss at iteration 16022: 0.04207526153562156\n",
      "Train Loss at iteration 16023: 0.04207523937318841\n",
      "Train Loss at iteration 16024: 0.042075217214423324\n",
      "Train Loss at iteration 16025: 0.0420751950593257\n",
      "Train Loss at iteration 16026: 0.042075172907894934\n",
      "Train Loss at iteration 16027: 0.042075150760130395\n",
      "Train Loss at iteration 16028: 0.04207512861603149\n",
      "Train Loss at iteration 16029: 0.042075106475597625\n",
      "Train Loss at iteration 16030: 0.042075084338828166\n",
      "Train Loss at iteration 16031: 0.042075062205722526\n",
      "Train Loss at iteration 16032: 0.0420750400762801\n",
      "Train Loss at iteration 16033: 0.042075017950500254\n",
      "Train Loss at iteration 16034: 0.04207499582838243\n",
      "Train Loss at iteration 16035: 0.04207497370992597\n",
      "Train Loss at iteration 16036: 0.042074951595130305\n",
      "Train Loss at iteration 16037: 0.0420749294839948\n",
      "Train Loss at iteration 16038: 0.04207490737651887\n",
      "Train Loss at iteration 16039: 0.042074885272701895\n",
      "Train Loss at iteration 16040: 0.04207486317254329\n",
      "Train Loss at iteration 16041: 0.04207484107604242\n",
      "Train Loss at iteration 16042: 0.0420748189831987\n",
      "Train Loss at iteration 16043: 0.04207479689401153\n",
      "Train Loss at iteration 16044: 0.04207477480848028\n",
      "Train Loss at iteration 16045: 0.04207475272660435\n",
      "Train Loss at iteration 16046: 0.04207473064838315\n",
      "Train Loss at iteration 16047: 0.04207470857381607\n",
      "Train Loss at iteration 16048: 0.042074686502902504\n",
      "Train Loss at iteration 16049: 0.042074664435641826\n",
      "Train Loss at iteration 16050: 0.042074642372033455\n",
      "Train Loss at iteration 16051: 0.042074620312076795\n",
      "Train Loss at iteration 16052: 0.04207459825577121\n",
      "Train Loss at iteration 16053: 0.04207457620311611\n",
      "Train Loss at iteration 16054: 0.042074554154110905\n",
      "Train Loss at iteration 16055: 0.042074532108754975\n",
      "Train Loss at iteration 16056: 0.04207451006704771\n",
      "Train Loss at iteration 16057: 0.04207448802898851\n",
      "Train Loss at iteration 16058: 0.04207446599457678\n",
      "Train Loss at iteration 16059: 0.042074443963811915\n",
      "Train Loss at iteration 16060: 0.04207442193669331\n",
      "Train Loss at iteration 16061: 0.042074399913220345\n",
      "Train Loss at iteration 16062: 0.04207437789339243\n",
      "Train Loss at iteration 16063: 0.042074355877208966\n",
      "Train Loss at iteration 16064: 0.042074333864669335\n",
      "Train Loss at iteration 16065: 0.04207431185577295\n",
      "Train Loss at iteration 16066: 0.0420742898505192\n",
      "Train Loss at iteration 16067: 0.042074267848907464\n",
      "Train Loss at iteration 16068: 0.04207424585093717\n",
      "Train Loss at iteration 16069: 0.0420742238566077\n",
      "Train Loss at iteration 16070: 0.04207420186591844\n",
      "Train Loss at iteration 16071: 0.042074179878868814\n",
      "Train Loss at iteration 16072: 0.0420741578954582\n",
      "Train Loss at iteration 16073: 0.04207413591568599\n",
      "Train Loss at iteration 16074: 0.0420741139395516\n",
      "Train Loss at iteration 16075: 0.04207409196705441\n",
      "Train Loss at iteration 16076: 0.04207406999819383\n",
      "Train Loss at iteration 16077: 0.04207404803296925\n",
      "Train Loss at iteration 16078: 0.04207402607138007\n",
      "Train Loss at iteration 16079: 0.0420740041134257\n",
      "Train Loss at iteration 16080: 0.04207398215910551\n",
      "Train Loss at iteration 16081: 0.04207396020841893\n",
      "Train Loss at iteration 16082: 0.042073938261365317\n",
      "Train Loss at iteration 16083: 0.04207391631794412\n",
      "Train Loss at iteration 16084: 0.04207389437815471\n",
      "Train Loss at iteration 16085: 0.04207387244199649\n",
      "Train Loss at iteration 16086: 0.04207385050946884\n",
      "Train Loss at iteration 16087: 0.04207382858057119\n",
      "Train Loss at iteration 16088: 0.04207380665530293\n",
      "Train Loss at iteration 16089: 0.04207378473366343\n",
      "Train Loss at iteration 16090: 0.04207376281565212\n",
      "Train Loss at iteration 16091: 0.042073740901268405\n",
      "Train Loss at iteration 16092: 0.042073718990511655\n",
      "Train Loss at iteration 16093: 0.042073697083381295\n",
      "Train Loss at iteration 16094: 0.04207367517987671\n",
      "Train Loss at iteration 16095: 0.0420736532799973\n",
      "Train Loss at iteration 16096: 0.04207363138374247\n",
      "Train Loss at iteration 16097: 0.04207360949111162\n",
      "Train Loss at iteration 16098: 0.042073587602104154\n",
      "Train Loss at iteration 16099: 0.04207356571671944\n",
      "Train Loss at iteration 16100: 0.04207354383495692\n",
      "Train Loss at iteration 16101: 0.04207352195681598\n",
      "Train Loss at iteration 16102: 0.04207350008229601\n",
      "Train Loss at iteration 16103: 0.042073478211396426\n",
      "Train Loss at iteration 16104: 0.04207345634411661\n",
      "Train Loss at iteration 16105: 0.04207343448045599\n",
      "Train Loss at iteration 16106: 0.042073412620413926\n",
      "Train Loss at iteration 16107: 0.04207339076398986\n",
      "Train Loss at iteration 16108: 0.04207336891118318\n",
      "Train Loss at iteration 16109: 0.04207334706199326\n",
      "Train Loss at iteration 16110: 0.04207332521641952\n",
      "Train Loss at iteration 16111: 0.04207330337446139\n",
      "Train Loss at iteration 16112: 0.04207328153611822\n",
      "Train Loss at iteration 16113: 0.042073259701389455\n",
      "Train Loss at iteration 16114: 0.04207323787027446\n",
      "Train Loss at iteration 16115: 0.042073216042772664\n",
      "Train Loss at iteration 16116: 0.042073194218883454\n",
      "Train Loss at iteration 16117: 0.042073172398606244\n",
      "Train Loss at iteration 16118: 0.042073150581940416\n",
      "Train Loss at iteration 16119: 0.0420731287688854\n",
      "Train Loss at iteration 16120: 0.04207310695944057\n",
      "Train Loss at iteration 16121: 0.04207308515360533\n",
      "Train Loss at iteration 16122: 0.042073063351379114\n",
      "Train Loss at iteration 16123: 0.04207304155276129\n",
      "Train Loss at iteration 16124: 0.04207301975775127\n",
      "Train Loss at iteration 16125: 0.04207299796634846\n",
      "Train Loss at iteration 16126: 0.04207297617855227\n",
      "Train Loss at iteration 16127: 0.042072954394362094\n",
      "Train Loss at iteration 16128: 0.042072932613777314\n",
      "Train Loss at iteration 16129: 0.04207291083679737\n",
      "Train Loss at iteration 16130: 0.04207288906342165\n",
      "Train Loss at iteration 16131: 0.04207286729364955\n",
      "Train Loss at iteration 16132: 0.04207284552748049\n",
      "Train Loss at iteration 16133: 0.04207282376491386\n",
      "Train Loss at iteration 16134: 0.04207280200594906\n",
      "Train Loss at iteration 16135: 0.042072780250585494\n",
      "Train Loss at iteration 16136: 0.042072758498822575\n",
      "Train Loss at iteration 16137: 0.0420727367506597\n",
      "Train Loss at iteration 16138: 0.04207271500609628\n",
      "Train Loss at iteration 16139: 0.042072693265131716\n",
      "Train Loss at iteration 16140: 0.04207267152776541\n",
      "Train Loss at iteration 16141: 0.042072649793996754\n",
      "Train Loss at iteration 16142: 0.04207262806382518\n",
      "Train Loss at iteration 16143: 0.04207260633725006\n",
      "Train Loss at iteration 16144: 0.04207258461427083\n",
      "Train Loss at iteration 16145: 0.04207256289488687\n",
      "Train Loss at iteration 16146: 0.042072541179097596\n",
      "Train Loss at iteration 16147: 0.0420725194669024\n",
      "Train Loss at iteration 16148: 0.04207249775830071\n",
      "Train Loss at iteration 16149: 0.04207247605329192\n",
      "Train Loss at iteration 16150: 0.04207245435187543\n",
      "Train Loss at iteration 16151: 0.04207243265405063\n",
      "Train Loss at iteration 16152: 0.04207241095981696\n",
      "Train Loss at iteration 16153: 0.04207238926917382\n",
      "Train Loss at iteration 16154: 0.04207236758212057\n",
      "Train Loss at iteration 16155: 0.04207234589865667\n",
      "Train Loss at iteration 16156: 0.04207232421878151\n",
      "Train Loss at iteration 16157: 0.042072302542494486\n",
      "Train Loss at iteration 16158: 0.042072280869795\n",
      "Train Loss at iteration 16159: 0.042072259200682464\n",
      "Train Loss at iteration 16160: 0.04207223753515629\n",
      "Train Loss at iteration 16161: 0.042072215873215885\n",
      "Train Loss at iteration 16162: 0.04207219421486064\n",
      "Train Loss at iteration 16163: 0.042072172560089986\n",
      "Train Loss at iteration 16164: 0.0420721509089033\n",
      "Train Loss at iteration 16165: 0.04207212926130001\n",
      "Train Loss at iteration 16166: 0.04207210761727952\n",
      "Train Loss at iteration 16167: 0.04207208597684122\n",
      "Train Loss at iteration 16168: 0.042072064339984526\n",
      "Train Loss at iteration 16169: 0.042072042706708855\n",
      "Train Loss at iteration 16170: 0.04207202107701361\n",
      "Train Loss at iteration 16171: 0.042071999450898186\n",
      "Train Loss at iteration 16172: 0.042071977828361994\n",
      "Train Loss at iteration 16173: 0.04207195620940446\n",
      "Train Loss at iteration 16174: 0.04207193459402497\n",
      "Train Loss at iteration 16175: 0.042071912982222945\n",
      "Train Loss at iteration 16176: 0.04207189137399778\n",
      "Train Loss at iteration 16177: 0.04207186976934888\n",
      "Train Loss at iteration 16178: 0.04207184816827567\n",
      "Train Loss at iteration 16179: 0.04207182657077755\n",
      "Train Loss at iteration 16180: 0.04207180497685393\n",
      "Train Loss at iteration 16181: 0.04207178338650421\n",
      "Train Loss at iteration 16182: 0.0420717617997278\n",
      "Train Loss at iteration 16183: 0.04207174021652412\n",
      "Train Loss at iteration 16184: 0.04207171863689256\n",
      "Train Loss at iteration 16185: 0.042071697060832546\n",
      "Train Loss at iteration 16186: 0.04207167548834347\n",
      "Train Loss at iteration 16187: 0.04207165391942475\n",
      "Train Loss at iteration 16188: 0.04207163235407579\n",
      "Train Loss at iteration 16189: 0.04207161079229601\n",
      "Train Loss at iteration 16190: 0.042071589234084814\n",
      "Train Loss at iteration 16191: 0.0420715676794416\n",
      "Train Loss at iteration 16192: 0.04207154612836579\n",
      "Train Loss at iteration 16193: 0.042071524580856785\n",
      "Train Loss at iteration 16194: 0.042071503036913996\n",
      "Train Loss at iteration 16195: 0.04207148149653685\n",
      "Train Loss at iteration 16196: 0.04207145995972472\n",
      "Train Loss at iteration 16197: 0.042071438426477044\n",
      "Train Loss at iteration 16198: 0.04207141689679322\n",
      "Train Loss at iteration 16199: 0.04207139537067267\n",
      "Train Loss at iteration 16200: 0.04207137384811479\n",
      "Train Loss at iteration 16201: 0.042071352329119\n",
      "Train Loss at iteration 16202: 0.0420713308136847\n",
      "Train Loss at iteration 16203: 0.0420713093018113\n",
      "Train Loss at iteration 16204: 0.042071287793498224\n",
      "Train Loss at iteration 16205: 0.04207126628874488\n",
      "Train Loss at iteration 16206: 0.04207124478755066\n",
      "Train Loss at iteration 16207: 0.04207122328991499\n",
      "Train Loss at iteration 16208: 0.042071201795837274\n",
      "Train Loss at iteration 16209: 0.04207118030531694\n",
      "Train Loss at iteration 16210: 0.04207115881835337\n",
      "Train Loss at iteration 16211: 0.04207113733494601\n",
      "Train Loss at iteration 16212: 0.04207111585509424\n",
      "Train Loss at iteration 16213: 0.042071094378797476\n",
      "Train Loss at iteration 16214: 0.04207107290605515\n",
      "Train Loss at iteration 16215: 0.04207105143686664\n",
      "Train Loss at iteration 16216: 0.04207102997123139\n",
      "Train Loss at iteration 16217: 0.0420710085091488\n",
      "Train Loss at iteration 16218: 0.04207098705061828\n",
      "Train Loss at iteration 16219: 0.042070965595639234\n",
      "Train Loss at iteration 16220: 0.04207094414421107\n",
      "Train Loss at iteration 16221: 0.04207092269633323\n",
      "Train Loss at iteration 16222: 0.042070901252005105\n",
      "Train Loss at iteration 16223: 0.042070879811226106\n",
      "Train Loss at iteration 16224: 0.04207085837399564\n",
      "Train Loss at iteration 16225: 0.04207083694031315\n",
      "Train Loss at iteration 16226: 0.04207081551017801\n",
      "Train Loss at iteration 16227: 0.042070794083589644\n",
      "Train Loss at iteration 16228: 0.04207077266054748\n",
      "Train Loss at iteration 16229: 0.04207075124105091\n",
      "Train Loss at iteration 16230: 0.042070729825099365\n",
      "Train Loss at iteration 16231: 0.04207070841269224\n",
      "Train Loss at iteration 16232: 0.042070687003828974\n",
      "Train Loss at iteration 16233: 0.04207066559850895\n",
      "Train Loss at iteration 16234: 0.042070644196731594\n",
      "Train Loss at iteration 16235: 0.042070622798496334\n",
      "Train Loss at iteration 16236: 0.04207060140380256\n",
      "Train Loss at iteration 16237: 0.04207058001264969\n",
      "Train Loss at iteration 16238: 0.042070558625037155\n",
      "Train Loss at iteration 16239: 0.04207053724096434\n",
      "Train Loss at iteration 16240: 0.042070515860430695\n",
      "Train Loss at iteration 16241: 0.0420704944834356\n",
      "Train Loss at iteration 16242: 0.042070473109978475\n",
      "Train Loss at iteration 16243: 0.04207045174005876\n",
      "Train Loss at iteration 16244: 0.04207043037367584\n",
      "Train Loss at iteration 16245: 0.04207040901082914\n",
      "Train Loss at iteration 16246: 0.04207038765151808\n",
      "Train Loss at iteration 16247: 0.042070366295742075\n",
      "Train Loss at iteration 16248: 0.04207034494350053\n",
      "Train Loss at iteration 16249: 0.042070323594792854\n",
      "Train Loss at iteration 16250: 0.042070302249618476\n",
      "Train Loss at iteration 16251: 0.04207028090797681\n",
      "Train Loss at iteration 16252: 0.04207025956986726\n",
      "Train Loss at iteration 16253: 0.042070238235289256\n",
      "Train Loss at iteration 16254: 0.042070216904242196\n",
      "Train Loss at iteration 16255: 0.042070195576725505\n",
      "Train Loss at iteration 16256: 0.0420701742527386\n",
      "Train Loss at iteration 16257: 0.0420701529322809\n",
      "Train Loss at iteration 16258: 0.04207013161535181\n",
      "Train Loss at iteration 16259: 0.04207011030195074\n",
      "Train Loss at iteration 16260: 0.04207008899207713\n",
      "Train Loss at iteration 16261: 0.04207006768573037\n",
      "Train Loss at iteration 16262: 0.04207004638290989\n",
      "Train Loss at iteration 16263: 0.042070025083615126\n",
      "Train Loss at iteration 16264: 0.04207000378784545\n",
      "Train Loss at iteration 16265: 0.04206998249560029\n",
      "Train Loss at iteration 16266: 0.042069961206879086\n",
      "Train Loss at iteration 16267: 0.04206993992168124\n",
      "Train Loss at iteration 16268: 0.04206991864000617\n",
      "Train Loss at iteration 16269: 0.04206989736185329\n",
      "Train Loss at iteration 16270: 0.04206987608722202\n",
      "Train Loss at iteration 16271: 0.04206985481611177\n",
      "Train Loss at iteration 16272: 0.04206983354852197\n",
      "Train Loss at iteration 16273: 0.042069812284452016\n",
      "Train Loss at iteration 16274: 0.04206979102390135\n",
      "Train Loss at iteration 16275: 0.04206976976686938\n",
      "Train Loss at iteration 16276: 0.04206974851335551\n",
      "Train Loss at iteration 16277: 0.04206972726335917\n",
      "Train Loss at iteration 16278: 0.042069706016879774\n",
      "Train Loss at iteration 16279: 0.04206968477391674\n",
      "Train Loss at iteration 16280: 0.04206966353446949\n",
      "Train Loss at iteration 16281: 0.042069642298537435\n",
      "Train Loss at iteration 16282: 0.04206962106612\n",
      "Train Loss at iteration 16283: 0.0420695998372166\n",
      "Train Loss at iteration 16284: 0.04206957861182665\n",
      "Train Loss at iteration 16285: 0.04206955738994957\n",
      "Train Loss at iteration 16286: 0.042069536171584765\n",
      "Train Loss at iteration 16287: 0.04206951495673168\n",
      "Train Loss at iteration 16288: 0.04206949374538972\n",
      "Train Loss at iteration 16289: 0.04206947253755831\n",
      "Train Loss at iteration 16290: 0.04206945133323684\n",
      "Train Loss at iteration 16291: 0.04206943013242478\n",
      "Train Loss at iteration 16292: 0.0420694089351215\n",
      "Train Loss at iteration 16293: 0.04206938774132645\n",
      "Train Loss at iteration 16294: 0.042069366551039024\n",
      "Train Loss at iteration 16295: 0.042069345364258666\n",
      "Train Loss at iteration 16296: 0.04206932418098477\n",
      "Train Loss at iteration 16297: 0.04206930300121679\n",
      "Train Loss at iteration 16298: 0.04206928182495411\n",
      "Train Loss at iteration 16299: 0.04206926065219616\n",
      "Train Loss at iteration 16300: 0.042069239482942375\n",
      "Train Loss at iteration 16301: 0.04206921831719217\n",
      "Train Loss at iteration 16302: 0.04206919715494494\n",
      "Train Loss at iteration 16303: 0.04206917599620014\n",
      "Train Loss at iteration 16304: 0.042069154840957156\n",
      "Train Loss at iteration 16305: 0.04206913368921543\n",
      "Train Loss at iteration 16306: 0.04206911254097438\n",
      "Train Loss at iteration 16307: 0.042069091396233424\n",
      "Train Loss at iteration 16308: 0.04206907025499198\n",
      "Train Loss at iteration 16309: 0.04206904911724947\n",
      "Train Loss at iteration 16310: 0.04206902798300532\n",
      "Train Loss at iteration 16311: 0.042069006852258936\n",
      "Train Loss at iteration 16312: 0.04206898572500975\n",
      "Train Loss at iteration 16313: 0.04206896460125718\n",
      "Train Loss at iteration 16314: 0.042068943481000654\n",
      "Train Loss at iteration 16315: 0.04206892236423959\n",
      "Train Loss at iteration 16316: 0.042068901250973394\n",
      "Train Loss at iteration 16317: 0.04206888014120151\n",
      "Train Loss at iteration 16318: 0.04206885903492335\n",
      "Train Loss at iteration 16319: 0.04206883793213832\n",
      "Train Loss at iteration 16320: 0.042068816832845864\n",
      "Train Loss at iteration 16321: 0.0420687957370454\n",
      "Train Loss at iteration 16322: 0.04206877464473634\n",
      "Train Loss at iteration 16323: 0.042068753555918104\n",
      "Train Loss at iteration 16324: 0.04206873247059013\n",
      "Train Loss at iteration 16325: 0.042068711388751824\n",
      "Train Loss at iteration 16326: 0.042068690310402626\n",
      "Train Loss at iteration 16327: 0.042068669235541936\n",
      "Train Loss at iteration 16328: 0.042068648164169185\n",
      "Train Loss at iteration 16329: 0.042068627096283805\n",
      "Train Loss at iteration 16330: 0.04206860603188521\n",
      "Train Loss at iteration 16331: 0.04206858497097282\n",
      "Train Loss at iteration 16332: 0.042068563913546055\n",
      "Train Loss at iteration 16333: 0.04206854285960436\n",
      "Train Loss at iteration 16334: 0.04206852180914712\n",
      "Train Loss at iteration 16335: 0.042068500762173805\n",
      "Train Loss at iteration 16336: 0.0420684797186838\n",
      "Train Loss at iteration 16337: 0.04206845867867653\n",
      "Train Loss at iteration 16338: 0.04206843764215145\n",
      "Train Loss at iteration 16339: 0.04206841660910794\n",
      "Train Loss at iteration 16340: 0.042068395579545455\n",
      "Train Loss at iteration 16341: 0.04206837455346342\n",
      "Train Loss at iteration 16342: 0.04206835353086124\n",
      "Train Loss at iteration 16343: 0.04206833251173834\n",
      "Train Loss at iteration 16344: 0.042068311496094156\n",
      "Train Loss at iteration 16345: 0.04206829048392811\n",
      "Train Loss at iteration 16346: 0.04206826947523961\n",
      "Train Loss at iteration 16347: 0.04206824847002811\n",
      "Train Loss at iteration 16348: 0.04206822746829301\n",
      "Train Loss at iteration 16349: 0.042068206470033724\n",
      "Train Loss at iteration 16350: 0.042068185475249716\n",
      "Train Loss at iteration 16351: 0.04206816448394038\n",
      "Train Loss at iteration 16352: 0.042068143496105145\n",
      "Train Loss at iteration 16353: 0.04206812251174343\n",
      "Train Loss at iteration 16354: 0.0420681015308547\n",
      "Train Loss at iteration 16355: 0.04206808055343832\n",
      "Train Loss at iteration 16356: 0.04206805957949374\n",
      "Train Loss at iteration 16357: 0.0420680386090204\n",
      "Train Loss at iteration 16358: 0.04206801764201772\n",
      "Train Loss at iteration 16359: 0.04206799667848512\n",
      "Train Loss at iteration 16360: 0.04206797571842202\n",
      "Train Loss at iteration 16361: 0.04206795476182784\n",
      "Train Loss at iteration 16362: 0.042067933808702024\n",
      "Train Loss at iteration 16363: 0.04206791285904398\n",
      "Train Loss at iteration 16364: 0.04206789191285315\n",
      "Train Loss at iteration 16365: 0.04206787097012895\n",
      "Train Loss at iteration 16366: 0.04206785003087082\n",
      "Train Loss at iteration 16367: 0.04206782909507816\n",
      "Train Loss at iteration 16368: 0.042067808162750416\n",
      "Train Loss at iteration 16369: 0.042067787233887\n",
      "Train Loss at iteration 16370: 0.04206776630848735\n",
      "Train Loss at iteration 16371: 0.0420677453865509\n",
      "Train Loss at iteration 16372: 0.04206772446807706\n",
      "Train Loss at iteration 16373: 0.04206770355306526\n",
      "Train Loss at iteration 16374: 0.04206768264151493\n",
      "Train Loss at iteration 16375: 0.04206766173342549\n",
      "Train Loss at iteration 16376: 0.04206764082879637\n",
      "Train Loss at iteration 16377: 0.04206761992762702\n",
      "Train Loss at iteration 16378: 0.04206759902991683\n",
      "Train Loss at iteration 16379: 0.04206757813566525\n",
      "Train Loss at iteration 16380: 0.042067557244871705\n",
      "Train Loss at iteration 16381: 0.0420675363575356\n",
      "Train Loss at iteration 16382: 0.04206751547365639\n",
      "Train Loss at iteration 16383: 0.04206749459323351\n",
      "Train Loss at iteration 16384: 0.042067473716266346\n",
      "Train Loss at iteration 16385: 0.042067452842754365\n",
      "Train Loss at iteration 16386: 0.04206743197269697\n",
      "Train Loss at iteration 16387: 0.04206741110609361\n",
      "Train Loss at iteration 16388: 0.04206739024294369\n",
      "Train Loss at iteration 16389: 0.042067369383246664\n",
      "Train Loss at iteration 16390: 0.042067348527001946\n",
      "Train Loss at iteration 16391: 0.04206732767420897\n",
      "Train Loss at iteration 16392: 0.04206730682486714\n",
      "Train Loss at iteration 16393: 0.0420672859789759\n",
      "Train Loss at iteration 16394: 0.0420672651365347\n",
      "Train Loss at iteration 16395: 0.04206724429754295\n",
      "Train Loss at iteration 16396: 0.04206722346200008\n",
      "Train Loss at iteration 16397: 0.042067202629905515\n",
      "Train Loss at iteration 16398: 0.04206718180125868\n",
      "Train Loss at iteration 16399: 0.042067160976059025\n",
      "Train Loss at iteration 16400: 0.042067140154305946\n",
      "Train Loss at iteration 16401: 0.042067119335998916\n",
      "Train Loss at iteration 16402: 0.04206709852113733\n",
      "Train Loss at iteration 16403: 0.04206707770972062\n",
      "Train Loss at iteration 16404: 0.04206705690174824\n",
      "Train Loss at iteration 16405: 0.04206703609721959\n",
      "Train Loss at iteration 16406: 0.04206701529613411\n",
      "Train Loss at iteration 16407: 0.04206699449849124\n",
      "Train Loss at iteration 16408: 0.0420669737042904\n",
      "Train Loss at iteration 16409: 0.042066952913531026\n",
      "Train Loss at iteration 16410: 0.042066932126212535\n",
      "Train Loss at iteration 16411: 0.04206691134233437\n",
      "Train Loss at iteration 16412: 0.04206689056189597\n",
      "Train Loss at iteration 16413: 0.04206686978489674\n",
      "Train Loss at iteration 16414: 0.04206684901133612\n",
      "Train Loss at iteration 16415: 0.04206682824121355\n",
      "Train Loss at iteration 16416: 0.04206680747452846\n",
      "Train Loss at iteration 16417: 0.04206678671128027\n",
      "Train Loss at iteration 16418: 0.04206676595146841\n",
      "Train Loss at iteration 16419: 0.04206674519509233\n",
      "Train Loss at iteration 16420: 0.04206672444215144\n",
      "Train Loss at iteration 16421: 0.042066703692645174\n",
      "Train Loss at iteration 16422: 0.04206668294657298\n",
      "Train Loss at iteration 16423: 0.04206666220393428\n",
      "Train Loss at iteration 16424: 0.042066641464728494\n",
      "Train Loss at iteration 16425: 0.04206662072895507\n",
      "Train Loss at iteration 16426: 0.04206659999661342\n",
      "Train Loss at iteration 16427: 0.042066579267703\n",
      "Train Loss at iteration 16428: 0.04206655854222323\n",
      "Train Loss at iteration 16429: 0.04206653782017354\n",
      "Train Loss at iteration 16430: 0.04206651710155336\n",
      "Train Loss at iteration 16431: 0.042066496386362125\n",
      "Train Loss at iteration 16432: 0.042066475674599285\n",
      "Train Loss at iteration 16433: 0.04206645496626423\n",
      "Train Loss at iteration 16434: 0.04206643426135643\n",
      "Train Loss at iteration 16435: 0.04206641355987531\n",
      "Train Loss at iteration 16436: 0.04206639286182029\n",
      "Train Loss at iteration 16437: 0.042066372167190814\n",
      "Train Loss at iteration 16438: 0.04206635147598631\n",
      "Train Loss at iteration 16439: 0.042066330788206215\n",
      "Train Loss at iteration 16440: 0.04206631010384995\n",
      "Train Loss at iteration 16441: 0.04206628942291697\n",
      "Train Loss at iteration 16442: 0.04206626874540668\n",
      "Train Loss at iteration 16443: 0.042066248071318535\n",
      "Train Loss at iteration 16444: 0.04206622740065196\n",
      "Train Loss at iteration 16445: 0.0420662067334064\n",
      "Train Loss at iteration 16446: 0.04206618606958126\n",
      "Train Loss at iteration 16447: 0.042066165409176\n",
      "Train Loss at iteration 16448: 0.042066144752190046\n",
      "Train Loss at iteration 16449: 0.04206612409862283\n",
      "Train Loss at iteration 16450: 0.04206610344847379\n",
      "Train Loss at iteration 16451: 0.042066082801742365\n",
      "Train Loss at iteration 16452: 0.04206606215842797\n",
      "Train Loss at iteration 16453: 0.04206604151853005\n",
      "Train Loss at iteration 16454: 0.04206602088204805\n",
      "Train Loss at iteration 16455: 0.04206600024898138\n",
      "Train Loss at iteration 16456: 0.042065979619329494\n",
      "Train Loss at iteration 16457: 0.04206595899309182\n",
      "Train Loss at iteration 16458: 0.04206593837026779\n",
      "Train Loss at iteration 16459: 0.04206591775085684\n",
      "Train Loss at iteration 16460: 0.04206589713485843\n",
      "Train Loss at iteration 16461: 0.04206587652227195\n",
      "Train Loss at iteration 16462: 0.042065855913096854\n",
      "Train Loss at iteration 16463: 0.04206583530733259\n",
      "Train Loss at iteration 16464: 0.04206581470497858\n",
      "Train Loss at iteration 16465: 0.04206579410603427\n",
      "Train Loss at iteration 16466: 0.042065773510499074\n",
      "Train Loss at iteration 16467: 0.04206575291837244\n",
      "Train Loss at iteration 16468: 0.04206573232965382\n",
      "Train Loss at iteration 16469: 0.04206571174434262\n",
      "Train Loss at iteration 16470: 0.042065691162438294\n",
      "Train Loss at iteration 16471: 0.04206567058394028\n",
      "Train Loss at iteration 16472: 0.042065650008848\n",
      "Train Loss at iteration 16473: 0.042065629437160905\n",
      "Train Loss at iteration 16474: 0.04206560886887842\n",
      "Train Loss at iteration 16475: 0.04206558830399998\n",
      "Train Loss at iteration 16476: 0.04206556774252503\n",
      "Train Loss at iteration 16477: 0.04206554718445301\n",
      "Train Loss at iteration 16478: 0.042065526629783336\n",
      "Train Loss at iteration 16479: 0.04206550607851547\n",
      "Train Loss at iteration 16480: 0.04206548553064884\n",
      "Train Loss at iteration 16481: 0.04206546498618287\n",
      "Train Loss at iteration 16482: 0.042065444445117\n",
      "Train Loss at iteration 16483: 0.042065423907450675\n",
      "Train Loss at iteration 16484: 0.04206540337318334\n",
      "Train Loss at iteration 16485: 0.04206538284231441\n",
      "Train Loss at iteration 16486: 0.04206536231484334\n",
      "Train Loss at iteration 16487: 0.04206534179076957\n",
      "Train Loss at iteration 16488: 0.04206532127009251\n",
      "Train Loss at iteration 16489: 0.04206530075281163\n",
      "Train Loss at iteration 16490: 0.042065280238926354\n",
      "Train Loss at iteration 16491: 0.04206525972843612\n",
      "Train Loss at iteration 16492: 0.04206523922134036\n",
      "Train Loss at iteration 16493: 0.042065218717638526\n",
      "Train Loss at iteration 16494: 0.042065198217330055\n",
      "Train Loss at iteration 16495: 0.042065177720414355\n",
      "Train Loss at iteration 16496: 0.0420651572268909\n",
      "Train Loss at iteration 16497: 0.04206513673675912\n",
      "Train Loss at iteration 16498: 0.04206511625001844\n",
      "Train Loss at iteration 16499: 0.04206509576666831\n",
      "Train Loss at iteration 16500: 0.04206507528670816\n",
      "Train Loss at iteration 16501: 0.04206505481013744\n",
      "Train Loss at iteration 16502: 0.04206503433695559\n",
      "Train Loss at iteration 16503: 0.04206501386716202\n",
      "Train Loss at iteration 16504: 0.0420649934007562\n",
      "Train Loss at iteration 16505: 0.042064972937737574\n",
      "Train Loss at iteration 16506: 0.04206495247810555\n",
      "Train Loss at iteration 16507: 0.0420649320218596\n",
      "Train Loss at iteration 16508: 0.04206491156899913\n",
      "Train Loss at iteration 16509: 0.0420648911195236\n",
      "Train Loss at iteration 16510: 0.04206487067343245\n",
      "Train Loss at iteration 16511: 0.04206485023072513\n",
      "Train Loss at iteration 16512: 0.04206482979140103\n",
      "Train Loss at iteration 16513: 0.04206480935545965\n",
      "Train Loss at iteration 16514: 0.042064788922900394\n",
      "Train Loss at iteration 16515: 0.042064768493722725\n",
      "Train Loss at iteration 16516: 0.04206474806792606\n",
      "Train Loss at iteration 16517: 0.04206472764550986\n",
      "Train Loss at iteration 16518: 0.04206470722647354\n",
      "Train Loss at iteration 16519: 0.042064686810816564\n",
      "Train Loss at iteration 16520: 0.04206466639853837\n",
      "Train Loss at iteration 16521: 0.04206464598963838\n",
      "Train Loss at iteration 16522: 0.04206462558411606\n",
      "Train Loss at iteration 16523: 0.042064605181970825\n",
      "Train Loss at iteration 16524: 0.04206458478320214\n",
      "Train Loss at iteration 16525: 0.04206456438780943\n",
      "Train Loss at iteration 16526: 0.04206454399579213\n",
      "Train Loss at iteration 16527: 0.042064523607149705\n",
      "Train Loss at iteration 16528: 0.042064503221881565\n",
      "Train Loss at iteration 16529: 0.042064482839987184\n",
      "Train Loss at iteration 16530: 0.04206446246146599\n",
      "Train Loss at iteration 16531: 0.04206444208631741\n",
      "Train Loss at iteration 16532: 0.0420644217145409\n",
      "Train Loss at iteration 16533: 0.04206440134613591\n",
      "Train Loss at iteration 16534: 0.04206438098110187\n",
      "Train Loss at iteration 16535: 0.042064360619438204\n",
      "Train Loss at iteration 16536: 0.042064340261144396\n",
      "Train Loss at iteration 16537: 0.04206431990621985\n",
      "Train Loss at iteration 16538: 0.04206429955466403\n",
      "Train Loss at iteration 16539: 0.04206427920647637\n",
      "Train Loss at iteration 16540: 0.04206425886165631\n",
      "Train Loss at iteration 16541: 0.04206423852020329\n",
      "Train Loss at iteration 16542: 0.042064218182116773\n",
      "Train Loss at iteration 16543: 0.04206419784739618\n",
      "Train Loss at iteration 16544: 0.042064177516040954\n",
      "Train Loss at iteration 16545: 0.04206415718805055\n",
      "Train Loss at iteration 16546: 0.0420641368634244\n",
      "Train Loss at iteration 16547: 0.04206411654216196\n",
      "Train Loss at iteration 16548: 0.04206409622426266\n",
      "Train Loss at iteration 16549: 0.04206407590972594\n",
      "Train Loss at iteration 16550: 0.04206405559855125\n",
      "Train Loss at iteration 16551: 0.04206403529073806\n",
      "Train Loss at iteration 16552: 0.042064014986285765\n",
      "Train Loss at iteration 16553: 0.04206399468519383\n",
      "Train Loss at iteration 16554: 0.042063974387461706\n",
      "Train Loss at iteration 16555: 0.04206395409308882\n",
      "Train Loss at iteration 16556: 0.04206393380207465\n",
      "Train Loss at iteration 16557: 0.04206391351441859\n",
      "Train Loss at iteration 16558: 0.04206389323012013\n",
      "Train Loss at iteration 16559: 0.04206387294917867\n",
      "Train Loss at iteration 16560: 0.042063852671593696\n",
      "Train Loss at iteration 16561: 0.042063832397364644\n",
      "Train Loss at iteration 16562: 0.04206381212649092\n",
      "Train Loss at iteration 16563: 0.04206379185897201\n",
      "Train Loss at iteration 16564: 0.04206377159480735\n",
      "Train Loss at iteration 16565: 0.04206375133399637\n",
      "Train Loss at iteration 16566: 0.042063731076538526\n",
      "Train Loss at iteration 16567: 0.04206371082243327\n",
      "Train Loss at iteration 16568: 0.04206369057168003\n",
      "Train Loss at iteration 16569: 0.04206367032427825\n",
      "Train Loss at iteration 16570: 0.0420636500802274\n",
      "Train Loss at iteration 16571: 0.042063629839526905\n",
      "Train Loss at iteration 16572: 0.042063609602176213\n",
      "Train Loss at iteration 16573: 0.04206358936817477\n",
      "Train Loss at iteration 16574: 0.04206356913752202\n",
      "Train Loss at iteration 16575: 0.042063548910217415\n",
      "Train Loss at iteration 16576: 0.042063528686260385\n",
      "Train Loss at iteration 16577: 0.0420635084656504\n",
      "Train Loss at iteration 16578: 0.04206348824838688\n",
      "Train Loss at iteration 16579: 0.04206346803446929\n",
      "Train Loss at iteration 16580: 0.042063447823897065\n",
      "Train Loss at iteration 16581: 0.04206342761666965\n",
      "Train Loss at iteration 16582: 0.04206340741278652\n",
      "Train Loss at iteration 16583: 0.042063387212247075\n",
      "Train Loss at iteration 16584: 0.04206336701505079\n",
      "Train Loss at iteration 16585: 0.04206334682119711\n",
      "Train Loss at iteration 16586: 0.04206332663068547\n",
      "Train Loss at iteration 16587: 0.04206330644351532\n",
      "Train Loss at iteration 16588: 0.042063286259686125\n",
      "Train Loss at iteration 16589: 0.0420632660791973\n",
      "Train Loss at iteration 16590: 0.04206324590204831\n",
      "Train Loss at iteration 16591: 0.04206322572823862\n",
      "Train Loss at iteration 16592: 0.04206320555776764\n",
      "Train Loss at iteration 16593: 0.04206318539063483\n",
      "Train Loss at iteration 16594: 0.04206316522683966\n",
      "Train Loss at iteration 16595: 0.04206314506638155\n",
      "Train Loss at iteration 16596: 0.04206312490925995\n",
      "Train Loss at iteration 16597: 0.042063104755474326\n",
      "Train Loss at iteration 16598: 0.04206308460502411\n",
      "Train Loss at iteration 16599: 0.04206306445790875\n",
      "Train Loss at iteration 16600: 0.0420630443141277\n",
      "Train Loss at iteration 16601: 0.04206302417368041\n",
      "Train Loss at iteration 16602: 0.04206300403656631\n",
      "Train Loss at iteration 16603: 0.04206298390278487\n",
      "Train Loss at iteration 16604: 0.042062963772335524\n",
      "Train Loss at iteration 16605: 0.042062943645217725\n",
      "Train Loss at iteration 16606: 0.042062923521430935\n",
      "Train Loss at iteration 16607: 0.04206290340097458\n",
      "Train Loss at iteration 16608: 0.04206288328384811\n",
      "Train Loss at iteration 16609: 0.042062863170051\n",
      "Train Loss at iteration 16610: 0.04206284305958267\n",
      "Train Loss at iteration 16611: 0.04206282295244257\n",
      "Train Loss at iteration 16612: 0.04206280284863016\n",
      "Train Loss at iteration 16613: 0.0420627827481449\n",
      "Train Loss at iteration 16614: 0.04206276265098621\n",
      "Train Loss at iteration 16615: 0.04206274255715357\n",
      "Train Loss at iteration 16616: 0.042062722466646406\n",
      "Train Loss at iteration 16617: 0.04206270237946418\n",
      "Train Loss at iteration 16618: 0.042062682295606335\n",
      "Train Loss at iteration 16619: 0.042062662215072315\n",
      "Train Loss at iteration 16620: 0.04206264213786159\n",
      "Train Loss at iteration 16621: 0.04206262206397359\n",
      "Train Loss at iteration 16622: 0.04206260199340777\n",
      "Train Loss at iteration 16623: 0.042062581926163585\n",
      "Train Loss at iteration 16624: 0.04206256186224048\n",
      "Train Loss at iteration 16625: 0.0420625418016379\n",
      "Train Loss at iteration 16626: 0.04206252174435531\n",
      "Train Loss at iteration 16627: 0.04206250169039215\n",
      "Train Loss at iteration 16628: 0.04206248163974787\n",
      "Train Loss at iteration 16629: 0.042062461592421926\n",
      "Train Loss at iteration 16630: 0.04206244154841376\n",
      "Train Loss at iteration 16631: 0.04206242150772283\n",
      "Train Loss at iteration 16632: 0.04206240147034859\n",
      "Train Loss at iteration 16633: 0.042062381436290486\n",
      "Train Loss at iteration 16634: 0.04206236140554796\n",
      "Train Loss at iteration 16635: 0.04206234137812048\n",
      "Train Loss at iteration 16636: 0.04206232135400748\n",
      "Train Loss at iteration 16637: 0.04206230133320842\n",
      "Train Loss at iteration 16638: 0.04206228131572275\n",
      "Train Loss at iteration 16639: 0.042062261301549934\n",
      "Train Loss at iteration 16640: 0.0420622412906894\n",
      "Train Loss at iteration 16641: 0.04206222128314062\n",
      "Train Loss at iteration 16642: 0.04206220127890302\n",
      "Train Loss at iteration 16643: 0.04206218127797608\n",
      "Train Loss at iteration 16644: 0.042062161280359237\n",
      "Train Loss at iteration 16645: 0.04206214128605194\n",
      "Train Loss at iteration 16646: 0.042062121295053656\n",
      "Train Loss at iteration 16647: 0.04206210130736383\n",
      "Train Loss at iteration 16648: 0.042062081322981906\n",
      "Train Loss at iteration 16649: 0.04206206134190735\n",
      "Train Loss at iteration 16650: 0.042062041364139595\n",
      "Train Loss at iteration 16651: 0.042062021389678114\n",
      "Train Loss at iteration 16652: 0.04206200141852235\n",
      "Train Loss at iteration 16653: 0.04206198145067175\n",
      "Train Loss at iteration 16654: 0.04206196148612578\n",
      "Train Loss at iteration 16655: 0.042061941524883885\n",
      "Train Loss at iteration 16656: 0.042061921566945513\n",
      "Train Loss at iteration 16657: 0.042061901612310125\n",
      "Train Loss at iteration 16658: 0.042061881660977185\n",
      "Train Loss at iteration 16659: 0.04206186171294611\n",
      "Train Loss at iteration 16660: 0.04206184176821639\n",
      "Train Loss at iteration 16661: 0.04206182182678746\n",
      "Train Loss at iteration 16662: 0.04206180188865879\n",
      "Train Loss at iteration 16663: 0.042061781953829806\n",
      "Train Loss at iteration 16664: 0.04206176202229999\n",
      "Train Loss at iteration 16665: 0.042061742094068766\n",
      "Train Loss at iteration 16666: 0.04206172216913562\n",
      "Train Loss at iteration 16667: 0.042061702247499985\n",
      "Train Loss at iteration 16668: 0.04206168232916131\n",
      "Train Loss at iteration 16669: 0.04206166241411907\n",
      "Train Loss at iteration 16670: 0.042061642502372716\n",
      "Train Loss at iteration 16671: 0.04206162259392168\n",
      "Train Loss at iteration 16672: 0.042061602688765444\n",
      "Train Loss at iteration 16673: 0.042061582786903445\n",
      "Train Loss at iteration 16674: 0.042061562888335145\n",
      "Train Loss at iteration 16675: 0.04206154299305999\n",
      "Train Loss at iteration 16676: 0.04206152310107744\n",
      "Train Loss at iteration 16677: 0.042061503212386955\n",
      "Train Loss at iteration 16678: 0.04206148332698799\n",
      "Train Loss at iteration 16679: 0.04206146344487998\n",
      "Train Loss at iteration 16680: 0.04206144356606241\n",
      "Train Loss at iteration 16681: 0.04206142369053471\n",
      "Train Loss at iteration 16682: 0.04206140381829635\n",
      "Train Loss at iteration 16683: 0.042061383949346776\n",
      "Train Loss at iteration 16684: 0.04206136408368546\n",
      "Train Loss at iteration 16685: 0.04206134422131184\n",
      "Train Loss at iteration 16686: 0.042061324362225386\n",
      "Train Loss at iteration 16687: 0.04206130450642553\n",
      "Train Loss at iteration 16688: 0.04206128465391175\n",
      "Train Loss at iteration 16689: 0.042061264804683494\n",
      "Train Loss at iteration 16690: 0.042061244958740224\n",
      "Train Loss at iteration 16691: 0.04206122511608138\n",
      "Train Loss at iteration 16692: 0.04206120527670644\n",
      "Train Loss at iteration 16693: 0.04206118544061484\n",
      "Train Loss at iteration 16694: 0.04206116560780605\n",
      "Train Loss at iteration 16695: 0.04206114577827952\n",
      "Train Loss at iteration 16696: 0.04206112595203471\n",
      "Train Loss at iteration 16697: 0.04206110612907108\n",
      "Train Loss at iteration 16698: 0.042061086309388075\n",
      "Train Loss at iteration 16699: 0.04206106649298516\n",
      "Train Loss at iteration 16700: 0.042061046679861804\n",
      "Train Loss at iteration 16701: 0.04206102687001742\n",
      "Train Loss at iteration 16702: 0.042061007063451526\n",
      "Train Loss at iteration 16703: 0.04206098726016354\n",
      "Train Loss at iteration 16704: 0.04206096746015292\n",
      "Train Loss at iteration 16705: 0.04206094766341913\n",
      "Train Loss at iteration 16706: 0.04206092786996164\n",
      "Train Loss at iteration 16707: 0.042060908079779885\n",
      "Train Loss at iteration 16708: 0.04206088829287335\n",
      "Train Loss at iteration 16709: 0.042060868509241456\n",
      "Train Loss at iteration 16710: 0.042060848728883694\n",
      "Train Loss at iteration 16711: 0.04206082895179951\n",
      "Train Loss at iteration 16712: 0.04206080917798836\n",
      "Train Loss at iteration 16713: 0.042060789407449686\n",
      "Train Loss at iteration 16714: 0.04206076964018298\n",
      "Train Loss at iteration 16715: 0.042060749876187686\n",
      "Train Loss at iteration 16716: 0.042060730115463255\n",
      "Train Loss at iteration 16717: 0.04206071035800915\n",
      "Train Loss at iteration 16718: 0.04206069060382482\n",
      "Train Loss at iteration 16719: 0.042060670852909744\n",
      "Train Loss at iteration 16720: 0.04206065110526337\n",
      "Train Loss at iteration 16721: 0.04206063136088516\n",
      "Train Loss at iteration 16722: 0.042060611619774556\n",
      "Train Loss at iteration 16723: 0.04206059188193104\n",
      "Train Loss at iteration 16724: 0.04206057214735405\n",
      "Train Loss at iteration 16725: 0.04206055241604307\n",
      "Train Loss at iteration 16726: 0.04206053268799754\n",
      "Train Loss at iteration 16727: 0.04206051296321693\n",
      "Train Loss at iteration 16728: 0.0420604932417007\n",
      "Train Loss at iteration 16729: 0.04206047352344828\n",
      "Train Loss at iteration 16730: 0.04206045380845917\n",
      "Train Loss at iteration 16731: 0.04206043409673281\n",
      "Train Loss at iteration 16732: 0.042060414388268665\n",
      "Train Loss at iteration 16733: 0.042060394683066195\n",
      "Train Loss at iteration 16734: 0.04206037498112485\n",
      "Train Loss at iteration 16735: 0.0420603552824441\n",
      "Train Loss at iteration 16736: 0.042060335587023406\n",
      "Train Loss at iteration 16737: 0.04206031589486222\n",
      "Train Loss at iteration 16738: 0.04206029620596002\n",
      "Train Loss at iteration 16739: 0.04206027652031624\n",
      "Train Loss at iteration 16740: 0.04206025683793036\n",
      "Train Loss at iteration 16741: 0.04206023715880183\n",
      "Train Loss at iteration 16742: 0.042060217482930116\n",
      "Train Loss at iteration 16743: 0.042060197810314684\n",
      "Train Loss at iteration 16744: 0.04206017814095498\n",
      "Train Loss at iteration 16745: 0.04206015847485047\n",
      "Train Loss at iteration 16746: 0.04206013881200064\n",
      "Train Loss at iteration 16747: 0.042060119152404915\n",
      "Train Loss at iteration 16748: 0.042060099496062764\n",
      "Train Loss at iteration 16749: 0.04206007984297366\n",
      "Train Loss at iteration 16750: 0.042060060193137065\n",
      "Train Loss at iteration 16751: 0.04206004054655244\n",
      "Train Loss at iteration 16752: 0.04206002090321923\n",
      "Train Loss at iteration 16753: 0.042060001263136915\n",
      "Train Loss at iteration 16754: 0.04205998162630495\n",
      "Train Loss at iteration 16755: 0.04205996199272279\n",
      "Train Loss at iteration 16756: 0.04205994236238992\n",
      "Train Loss at iteration 16757: 0.04205992273530577\n",
      "Train Loss at iteration 16758: 0.042059903111469815\n",
      "Train Loss at iteration 16759: 0.04205988349088153\n",
      "Train Loss at iteration 16760: 0.04205986387354037\n",
      "Train Loss at iteration 16761: 0.042059844259445776\n",
      "Train Loss at iteration 16762: 0.04205982464859725\n",
      "Train Loss at iteration 16763: 0.042059805040994214\n",
      "Train Loss at iteration 16764: 0.04205978543663615\n",
      "Train Loss at iteration 16765: 0.04205976583552254\n",
      "Train Loss at iteration 16766: 0.04205974623765282\n",
      "Train Loss at iteration 16767: 0.04205972664302646\n",
      "Train Loss at iteration 16768: 0.04205970705164292\n",
      "Train Loss at iteration 16769: 0.042059687463501666\n",
      "Train Loss at iteration 16770: 0.042059667878602165\n",
      "Train Loss at iteration 16771: 0.04205964829694388\n",
      "Train Loss at iteration 16772: 0.04205962871852626\n",
      "Train Loss at iteration 16773: 0.04205960914334879\n",
      "Train Loss at iteration 16774: 0.04205958957141092\n",
      "Train Loss at iteration 16775: 0.042059570002712106\n",
      "Train Loss at iteration 16776: 0.04205955043725183\n",
      "Train Loss at iteration 16777: 0.04205953087502955\n",
      "Train Loss at iteration 16778: 0.042059511316044726\n",
      "Train Loss at iteration 16779: 0.04205949176029683\n",
      "Train Loss at iteration 16780: 0.04205947220778532\n",
      "Train Loss at iteration 16781: 0.04205945265850963\n",
      "Train Loss at iteration 16782: 0.0420594331124693\n",
      "Train Loss at iteration 16783: 0.04205941356966371\n",
      "Train Loss at iteration 16784: 0.042059394030092394\n",
      "Train Loss at iteration 16785: 0.04205937449375476\n",
      "Train Loss at iteration 16786: 0.04205935496065031\n",
      "Train Loss at iteration 16787: 0.0420593354307785\n",
      "Train Loss at iteration 16788: 0.042059315904138785\n",
      "Train Loss at iteration 16789: 0.04205929638073064\n",
      "Train Loss at iteration 16790: 0.04205927686055351\n",
      "Train Loss at iteration 16791: 0.042059257343606896\n",
      "Train Loss at iteration 16792: 0.04205923782989023\n",
      "Train Loss at iteration 16793: 0.042059218319403006\n",
      "Train Loss at iteration 16794: 0.042059198812144656\n",
      "Train Loss at iteration 16795: 0.04205917930811467\n",
      "Train Loss at iteration 16796: 0.04205915980731252\n",
      "Train Loss at iteration 16797: 0.04205914030973763\n",
      "Train Loss at iteration 16798: 0.04205912081538952\n",
      "Train Loss at iteration 16799: 0.042059101324267614\n",
      "Train Loss at iteration 16800: 0.042059081836371394\n",
      "Train Loss at iteration 16801: 0.04205906235170034\n",
      "Train Loss at iteration 16802: 0.04205904287025389\n",
      "Train Loss at iteration 16803: 0.04205902339203152\n",
      "Train Loss at iteration 16804: 0.0420590039170327\n",
      "Train Loss at iteration 16805: 0.04205898444525688\n",
      "Train Loss at iteration 16806: 0.04205896497670357\n",
      "Train Loss at iteration 16807: 0.04205894551137221\n",
      "Train Loss at iteration 16808: 0.04205892604926225\n",
      "Train Loss at iteration 16809: 0.042058906590373174\n",
      "Train Loss at iteration 16810: 0.042058887134704445\n",
      "Train Loss at iteration 16811: 0.04205886768225554\n",
      "Train Loss at iteration 16812: 0.0420588482330259\n",
      "Train Loss at iteration 16813: 0.042058828787015015\n",
      "Train Loss at iteration 16814: 0.04205880934422235\n",
      "Train Loss at iteration 16815: 0.04205878990464737\n",
      "Train Loss at iteration 16816: 0.042058770468289534\n",
      "Train Loss at iteration 16817: 0.04205875103514831\n",
      "Train Loss at iteration 16818: 0.04205873160522319\n",
      "Train Loss at iteration 16819: 0.042058712178513605\n",
      "Train Loss at iteration 16820: 0.04205869275501905\n",
      "Train Loss at iteration 16821: 0.04205867333473898\n",
      "Train Loss at iteration 16822: 0.04205865391767286\n",
      "Train Loss at iteration 16823: 0.042058634503820164\n",
      "Train Loss at iteration 16824: 0.04205861509318037\n",
      "Train Loss at iteration 16825: 0.042058595685752924\n",
      "Train Loss at iteration 16826: 0.0420585762815373\n",
      "Train Loss at iteration 16827: 0.04205855688053298\n",
      "Train Loss at iteration 16828: 0.042058537482739425\n",
      "Train Loss at iteration 16829: 0.04205851808815609\n",
      "Train Loss at iteration 16830: 0.042058498696782465\n",
      "Train Loss at iteration 16831: 0.04205847930861802\n",
      "Train Loss at iteration 16832: 0.042058459923662195\n",
      "Train Loss at iteration 16833: 0.04205844054191449\n",
      "Train Loss at iteration 16834: 0.04205842116337435\n",
      "Train Loss at iteration 16835: 0.04205840178804125\n",
      "Train Loss at iteration 16836: 0.04205838241591467\n",
      "Train Loss at iteration 16837: 0.04205836304699407\n",
      "Train Loss at iteration 16838: 0.04205834368127892\n",
      "Train Loss at iteration 16839: 0.04205832431876871\n",
      "Train Loss at iteration 16840: 0.04205830495946286\n",
      "Train Loss at iteration 16841: 0.04205828560336089\n",
      "Train Loss at iteration 16842: 0.04205826625046223\n",
      "Train Loss at iteration 16843: 0.042058246900766386\n",
      "Train Loss at iteration 16844: 0.042058227554272805\n",
      "Train Loss at iteration 16845: 0.04205820821098097\n",
      "Train Loss at iteration 16846: 0.04205818887089032\n",
      "Train Loss at iteration 16847: 0.04205816953400037\n",
      "Train Loss at iteration 16848: 0.04205815020031056\n",
      "Train Loss at iteration 16849: 0.04205813086982036\n",
      "Train Loss at iteration 16850: 0.04205811154252926\n",
      "Train Loss at iteration 16851: 0.04205809221843671\n",
      "Train Loss at iteration 16852: 0.042058072897542195\n",
      "Train Loss at iteration 16853: 0.04205805357984518\n",
      "Train Loss at iteration 16854: 0.04205803426534513\n",
      "Train Loss at iteration 16855: 0.04205801495404151\n",
      "Train Loss at iteration 16856: 0.04205799564593382\n",
      "Train Loss at iteration 16857: 0.04205797634102151\n",
      "Train Loss at iteration 16858: 0.04205795703930405\n",
      "Train Loss at iteration 16859: 0.04205793774078091\n",
      "Train Loss at iteration 16860: 0.04205791844545157\n",
      "Train Loss at iteration 16861: 0.042057899153315495\n",
      "Train Loss at iteration 16862: 0.04205787986437216\n",
      "Train Loss at iteration 16863: 0.042057860578621035\n",
      "Train Loss at iteration 16864: 0.042057841296061584\n",
      "Train Loss at iteration 16865: 0.04205782201669329\n",
      "Train Loss at iteration 16866: 0.042057802740515626\n",
      "Train Loss at iteration 16867: 0.04205778346752803\n",
      "Train Loss at iteration 16868: 0.042057764197730024\n",
      "Train Loss at iteration 16869: 0.042057744931121056\n",
      "Train Loss at iteration 16870: 0.0420577256677006\n",
      "Train Loss at iteration 16871: 0.04205770640746813\n",
      "Train Loss at iteration 16872: 0.042057687150423094\n",
      "Train Loss at iteration 16873: 0.042057667896565\n",
      "Train Loss at iteration 16874: 0.04205764864589331\n",
      "Train Loss at iteration 16875: 0.04205762939840749\n",
      "Train Loss at iteration 16876: 0.04205761015410701\n",
      "Train Loss at iteration 16877: 0.04205759091299135\n",
      "Train Loss at iteration 16878: 0.042057571675059975\n",
      "Train Loss at iteration 16879: 0.04205755244031237\n",
      "Train Loss at iteration 16880: 0.04205753320874799\n",
      "Train Loss at iteration 16881: 0.042057513980366334\n",
      "Train Loss at iteration 16882: 0.042057494755166853\n",
      "Train Loss at iteration 16883: 0.04205747553314903\n",
      "Train Loss at iteration 16884: 0.04205745631431232\n",
      "Train Loss at iteration 16885: 0.042057437098656225\n",
      "Train Loss at iteration 16886: 0.042057417886180205\n",
      "Train Loss at iteration 16887: 0.042057398676883725\n",
      "Train Loss at iteration 16888: 0.04205737947076628\n",
      "Train Loss at iteration 16889: 0.042057360267827304\n",
      "Train Loss at iteration 16890: 0.04205734106806632\n",
      "Train Loss at iteration 16891: 0.042057321871482777\n",
      "Train Loss at iteration 16892: 0.04205730267807615\n",
      "Train Loss at iteration 16893: 0.0420572834878459\n",
      "Train Loss at iteration 16894: 0.04205726430079153\n",
      "Train Loss at iteration 16895: 0.04205724511691249\n",
      "Train Loss at iteration 16896: 0.042057225936208274\n",
      "Train Loss at iteration 16897: 0.042057206758678335\n",
      "Train Loss at iteration 16898: 0.04205718758432216\n",
      "Train Loss at iteration 16899: 0.042057168413139216\n",
      "Train Loss at iteration 16900: 0.04205714924512899\n",
      "Train Loss at iteration 16901: 0.04205713008029095\n",
      "Train Loss at iteration 16902: 0.042057110918624574\n",
      "Train Loss at iteration 16903: 0.04205709176012932\n",
      "Train Loss at iteration 16904: 0.04205707260480468\n",
      "Train Loss at iteration 16905: 0.042057053452650144\n",
      "Train Loss at iteration 16906: 0.042057034303665156\n",
      "Train Loss at iteration 16907: 0.042057015157849195\n",
      "Train Loss at iteration 16908: 0.04205699601520175\n",
      "Train Loss at iteration 16909: 0.042056976875722296\n",
      "Train Loss at iteration 16910: 0.04205695773941029\n",
      "Train Loss at iteration 16911: 0.042056938606265235\n",
      "Train Loss at iteration 16912: 0.04205691947628658\n",
      "Train Loss at iteration 16913: 0.04205690034947383\n",
      "Train Loss at iteration 16914: 0.042056881225826445\n",
      "Train Loss at iteration 16915: 0.04205686210534389\n",
      "Train Loss at iteration 16916: 0.04205684298802565\n",
      "Train Loss at iteration 16917: 0.04205682387387122\n",
      "Train Loss at iteration 16918: 0.04205680476288004\n",
      "Train Loss at iteration 16919: 0.04205678565505161\n",
      "Train Loss at iteration 16920: 0.0420567665503854\n",
      "Train Loss at iteration 16921: 0.04205674744888089\n",
      "Train Loss at iteration 16922: 0.042056728350537545\n",
      "Train Loss at iteration 16923: 0.04205670925535486\n",
      "Train Loss at iteration 16924: 0.04205669016333231\n",
      "Train Loss at iteration 16925: 0.04205667107446935\n",
      "Train Loss at iteration 16926: 0.04205665198876548\n",
      "Train Loss at iteration 16927: 0.042056632906220165\n",
      "Train Loss at iteration 16928: 0.04205661382683288\n",
      "Train Loss at iteration 16929: 0.04205659475060311\n",
      "Train Loss at iteration 16930: 0.042056575677530336\n",
      "Train Loss at iteration 16931: 0.042056556607614023\n",
      "Train Loss at iteration 16932: 0.042056537540853645\n",
      "Train Loss at iteration 16933: 0.04205651847724871\n",
      "Train Loss at iteration 16934: 0.04205649941679866\n",
      "Train Loss at iteration 16935: 0.04205648035950299\n",
      "Train Loss at iteration 16936: 0.04205646130536117\n",
      "Train Loss at iteration 16937: 0.042056442254372696\n",
      "Train Loss at iteration 16938: 0.04205642320653702\n",
      "Train Loss at iteration 16939: 0.042056404161853626\n",
      "Train Loss at iteration 16940: 0.04205638512032201\n",
      "Train Loss at iteration 16941: 0.042056366081941625\n",
      "Train Loss at iteration 16942: 0.042056347046711975\n",
      "Train Loss at iteration 16943: 0.04205632801463253\n",
      "Train Loss at iteration 16944: 0.042056308985702756\n",
      "Train Loss at iteration 16945: 0.04205628995992213\n",
      "Train Loss at iteration 16946: 0.042056270937290155\n",
      "Train Loss at iteration 16947: 0.04205625191780629\n",
      "Train Loss at iteration 16948: 0.04205623290147002\n",
      "Train Loss at iteration 16949: 0.042056213888280826\n",
      "Train Loss at iteration 16950: 0.04205619487823818\n",
      "Train Loss at iteration 16951: 0.04205617587134157\n",
      "Train Loss at iteration 16952: 0.04205615686759047\n",
      "Train Loss at iteration 16953: 0.04205613786698435\n",
      "Train Loss at iteration 16954: 0.0420561188695227\n",
      "Train Loss at iteration 16955: 0.042056099875205005\n",
      "Train Loss at iteration 16956: 0.042056080884030726\n",
      "Train Loss at iteration 16957: 0.042056061895999364\n",
      "Train Loss at iteration 16958: 0.04205604291111039\n",
      "Train Loss at iteration 16959: 0.042056023929363265\n",
      "Train Loss at iteration 16960: 0.042056004950757495\n",
      "Train Loss at iteration 16961: 0.042055985975292566\n",
      "Train Loss at iteration 16962: 0.042055967002967924\n",
      "Train Loss at iteration 16963: 0.04205594803378306\n",
      "Train Loss at iteration 16964: 0.04205592906773748\n",
      "Train Loss at iteration 16965: 0.042055910104830646\n",
      "Train Loss at iteration 16966: 0.04205589114506203\n",
      "Train Loss at iteration 16967: 0.042055872188431126\n",
      "Train Loss at iteration 16968: 0.04205585323493741\n",
      "Train Loss at iteration 16969: 0.04205583428458036\n",
      "Train Loss at iteration 16970: 0.042055815337359456\n",
      "Train Loss at iteration 16971: 0.04205579639327418\n",
      "Train Loss at iteration 16972: 0.04205577745232402\n",
      "Train Loss at iteration 16973: 0.04205575851450845\n",
      "Train Loss at iteration 16974: 0.04205573957982694\n",
      "Train Loss at iteration 16975: 0.04205572064827899\n",
      "Train Loss at iteration 16976: 0.042055701719864086\n",
      "Train Loss at iteration 16977: 0.042055682794581696\n",
      "Train Loss at iteration 16978: 0.042055663872431294\n",
      "Train Loss at iteration 16979: 0.04205564495341237\n",
      "Train Loss at iteration 16980: 0.042055626037524406\n",
      "Train Loss at iteration 16981: 0.042055607124766886\n",
      "Train Loss at iteration 16982: 0.04205558821513929\n",
      "Train Loss at iteration 16983: 0.04205556930864111\n",
      "Train Loss at iteration 16984: 0.0420555504052718\n",
      "Train Loss at iteration 16985: 0.042055531505030865\n",
      "Train Loss at iteration 16986: 0.042055512607917775\n",
      "Train Loss at iteration 16987: 0.042055493713932024\n",
      "Train Loss at iteration 16988: 0.04205547482307309\n",
      "Train Loss at iteration 16989: 0.04205545593534045\n",
      "Train Loss at iteration 16990: 0.04205543705073359\n",
      "Train Loss at iteration 16991: 0.042055418169251985\n",
      "Train Loss at iteration 16992: 0.04205539929089514\n",
      "Train Loss at iteration 16993: 0.04205538041566252\n",
      "Train Loss at iteration 16994: 0.042055361543553604\n",
      "Train Loss at iteration 16995: 0.042055342674567876\n",
      "Train Loss at iteration 16996: 0.04205532380870483\n",
      "Train Loss at iteration 16997: 0.042055304945963955\n",
      "Train Loss at iteration 16998: 0.04205528608634472\n",
      "Train Loss at iteration 16999: 0.042055267229846593\n",
      "Train Loss at iteration 17000: 0.04205524837646908\n",
      "Train Loss at iteration 17001: 0.04205522952621166\n",
      "Train Loss at iteration 17002: 0.04205521067907381\n",
      "Train Loss at iteration 17003: 0.04205519183505504\n",
      "Train Loss at iteration 17004: 0.042055172994154795\n",
      "Train Loss at iteration 17005: 0.04205515415637258\n",
      "Train Loss at iteration 17006: 0.04205513532170787\n",
      "Train Loss at iteration 17007: 0.04205511649016016\n",
      "Train Loss at iteration 17008: 0.042055097661728914\n",
      "Train Loss at iteration 17009: 0.04205507883641364\n",
      "Train Loss at iteration 17010: 0.042055060014213816\n",
      "Train Loss at iteration 17011: 0.04205504119512892\n",
      "Train Loss at iteration 17012: 0.04205502237915843\n",
      "Train Loss at iteration 17013: 0.04205500356630185\n",
      "Train Loss at iteration 17014: 0.04205498475655865\n",
      "Train Loss at iteration 17015: 0.0420549659499283\n",
      "Train Loss at iteration 17016: 0.042054947146410324\n",
      "Train Loss at iteration 17017: 0.04205492834600418\n",
      "Train Loss at iteration 17018: 0.04205490954870936\n",
      "Train Loss at iteration 17019: 0.04205489075452533\n",
      "Train Loss at iteration 17020: 0.04205487196345161\n",
      "Train Loss at iteration 17021: 0.042054853175487655\n",
      "Train Loss at iteration 17022: 0.04205483439063297\n",
      "Train Loss at iteration 17023: 0.04205481560888701\n",
      "Train Loss at iteration 17024: 0.04205479683024931\n",
      "Train Loss at iteration 17025: 0.042054778054719306\n",
      "Train Loss at iteration 17026: 0.042054759282296524\n",
      "Train Loss at iteration 17027: 0.04205474051298042\n",
      "Train Loss at iteration 17028: 0.042054721746770496\n",
      "Train Loss at iteration 17029: 0.04205470298366622\n",
      "Train Loss at iteration 17030: 0.042054684223667094\n",
      "Train Loss at iteration 17031: 0.042054665466772605\n",
      "Train Loss at iteration 17032: 0.04205464671298223\n",
      "Train Loss at iteration 17033: 0.042054627962295456\n",
      "Train Loss at iteration 17034: 0.04205460921471178\n",
      "Train Loss at iteration 17035: 0.04205459047023068\n",
      "Train Loss at iteration 17036: 0.042054571728851624\n",
      "Train Loss at iteration 17037: 0.04205455299057413\n",
      "Train Loss at iteration 17038: 0.04205453425539767\n",
      "Train Loss at iteration 17039: 0.042054515523321734\n",
      "Train Loss at iteration 17040: 0.0420544967943458\n",
      "Train Loss at iteration 17041: 0.04205447806846936\n",
      "Train Loss at iteration 17042: 0.042054459345691915\n",
      "Train Loss at iteration 17043: 0.04205444062601292\n",
      "Train Loss at iteration 17044: 0.0420544219094319\n",
      "Train Loss at iteration 17045: 0.04205440319594832\n",
      "Train Loss at iteration 17046: 0.04205438448556165\n",
      "Train Loss at iteration 17047: 0.04205436577827141\n",
      "Train Loss at iteration 17048: 0.04205434707407708\n",
      "Train Loss at iteration 17049: 0.042054328372978136\n",
      "Train Loss at iteration 17050: 0.04205430967497408\n",
      "Train Loss at iteration 17051: 0.04205429098006437\n",
      "Train Loss at iteration 17052: 0.04205427228824853\n",
      "Train Loss at iteration 17053: 0.04205425359952603\n",
      "Train Loss at iteration 17054: 0.04205423491389636\n",
      "Train Loss at iteration 17055: 0.04205421623135902\n",
      "Train Loss at iteration 17056: 0.04205419755191348\n",
      "Train Loss at iteration 17057: 0.04205417887555923\n",
      "Train Loss at iteration 17058: 0.04205416020229576\n",
      "Train Loss at iteration 17059: 0.04205414153212257\n",
      "Train Loss at iteration 17060: 0.04205412286503914\n",
      "Train Loss at iteration 17061: 0.042054104201044946\n",
      "Train Loss at iteration 17062: 0.04205408554013949\n",
      "Train Loss at iteration 17063: 0.042054066882322265\n",
      "Train Loss at iteration 17064: 0.04205404822759275\n",
      "Train Loss at iteration 17065: 0.04205402957595044\n",
      "Train Loss at iteration 17066: 0.04205401092739482\n",
      "Train Loss at iteration 17067: 0.04205399228192538\n",
      "Train Loss at iteration 17068: 0.04205397363954161\n",
      "Train Loss at iteration 17069: 0.042053955000243004\n",
      "Train Loss at iteration 17070: 0.042053936364029036\n",
      "Train Loss at iteration 17071: 0.042053917730899205\n",
      "Train Loss at iteration 17072: 0.04205389910085301\n",
      "Train Loss at iteration 17073: 0.04205388047388993\n",
      "Train Loss at iteration 17074: 0.04205386185000945\n",
      "Train Loss at iteration 17075: 0.04205384322921105\n",
      "Train Loss at iteration 17076: 0.042053824611494255\n",
      "Train Loss at iteration 17077: 0.04205380599685853\n",
      "Train Loss at iteration 17078: 0.042053787385303375\n",
      "Train Loss at iteration 17079: 0.042053768776828274\n",
      "Train Loss at iteration 17080: 0.0420537501714327\n",
      "Train Loss at iteration 17081: 0.04205373156911617\n",
      "Train Loss at iteration 17082: 0.04205371296987818\n",
      "Train Loss at iteration 17083: 0.042053694373718184\n",
      "Train Loss at iteration 17084: 0.0420536757806357\n",
      "Train Loss at iteration 17085: 0.042053657190630214\n",
      "Train Loss at iteration 17086: 0.042053638603701204\n",
      "Train Loss at iteration 17087: 0.04205362001984818\n",
      "Train Loss at iteration 17088: 0.04205360143907062\n",
      "Train Loss at iteration 17089: 0.04205358286136803\n",
      "Train Loss at iteration 17090: 0.04205356428673988\n",
      "Train Loss at iteration 17091: 0.04205354571518568\n",
      "Train Loss at iteration 17092: 0.0420535271467049\n",
      "Train Loss at iteration 17093: 0.04205350858129705\n",
      "Train Loss at iteration 17094: 0.0420534900189616\n",
      "Train Loss at iteration 17095: 0.04205347145969807\n",
      "Train Loss at iteration 17096: 0.04205345290350594\n",
      "Train Loss at iteration 17097: 0.042053434350384694\n",
      "Train Loss at iteration 17098: 0.042053415800333824\n",
      "Train Loss at iteration 17099: 0.042053397253352824\n",
      "Train Loss at iteration 17100: 0.04205337870944119\n",
      "Train Loss at iteration 17101: 0.042053360168598404\n",
      "Train Loss at iteration 17102: 0.04205334163082398\n",
      "Train Loss at iteration 17103: 0.04205332309611738\n",
      "Train Loss at iteration 17104: 0.042053304564478124\n",
      "Train Loss at iteration 17105: 0.04205328603590568\n",
      "Train Loss at iteration 17106: 0.04205326751039955\n",
      "Train Loss at iteration 17107: 0.04205324898795924\n",
      "Train Loss at iteration 17108: 0.04205323046858422\n",
      "Train Loss at iteration 17109: 0.04205321195227399\n",
      "Train Loss at iteration 17110: 0.04205319343902805\n",
      "Train Loss at iteration 17111: 0.0420531749288459\n",
      "Train Loss at iteration 17112: 0.04205315642172701\n",
      "Train Loss at iteration 17113: 0.042053137917670874\n",
      "Train Loss at iteration 17114: 0.042053119416677005\n",
      "Train Loss at iteration 17115: 0.04205310091874489\n",
      "Train Loss at iteration 17116: 0.04205308242387402\n",
      "Train Loss at iteration 17117: 0.04205306393206387\n",
      "Train Loss at iteration 17118: 0.04205304544331395\n",
      "Train Loss at iteration 17119: 0.04205302695762376\n",
      "Train Loss at iteration 17120: 0.04205300847499278\n",
      "Train Loss at iteration 17121: 0.04205298999542051\n",
      "Train Loss at iteration 17122: 0.042052971518906454\n",
      "Train Loss at iteration 17123: 0.042052953045450085\n",
      "Train Loss at iteration 17124: 0.04205293457505091\n",
      "Train Loss at iteration 17125: 0.04205291610770841\n",
      "Train Loss at iteration 17126: 0.04205289764342209\n",
      "Train Loss at iteration 17127: 0.042052879182191465\n",
      "Train Loss at iteration 17128: 0.04205286072401598\n",
      "Train Loss at iteration 17129: 0.04205284226889516\n",
      "Train Loss at iteration 17130: 0.042052823816828504\n",
      "Train Loss at iteration 17131: 0.04205280536781548\n",
      "Train Loss at iteration 17132: 0.04205278692185562\n",
      "Train Loss at iteration 17133: 0.04205276847894839\n",
      "Train Loss at iteration 17134: 0.04205275003909329\n",
      "Train Loss at iteration 17135: 0.04205273160228981\n",
      "Train Loss at iteration 17136: 0.04205271316853747\n",
      "Train Loss at iteration 17137: 0.04205269473783573\n",
      "Train Loss at iteration 17138: 0.04205267631018412\n",
      "Train Loss at iteration 17139: 0.04205265788558209\n",
      "Train Loss at iteration 17140: 0.04205263946402918\n",
      "Train Loss at iteration 17141: 0.04205262104552486\n",
      "Train Loss at iteration 17142: 0.04205260263006864\n",
      "Train Loss at iteration 17143: 0.04205258421766\n",
      "Train Loss at iteration 17144: 0.04205256580829845\n",
      "Train Loss at iteration 17145: 0.04205254740198348\n",
      "Train Loss at iteration 17146: 0.04205252899871458\n",
      "Train Loss at iteration 17147: 0.04205251059849126\n",
      "Train Loss at iteration 17148: 0.042052492201312984\n",
      "Train Loss at iteration 17149: 0.042052473807179275\n",
      "Train Loss at iteration 17150: 0.04205245541608964\n",
      "Train Loss at iteration 17151: 0.04205243702804354\n",
      "Train Loss at iteration 17152: 0.0420524186430405\n",
      "Train Loss at iteration 17153: 0.04205240026108\n",
      "Train Loss at iteration 17154: 0.04205238188216155\n",
      "Train Loss at iteration 17155: 0.042052363506284635\n",
      "Train Loss at iteration 17156: 0.04205234513344875\n",
      "Train Loss at iteration 17157: 0.042052326763653405\n",
      "Train Loss at iteration 17158: 0.04205230839689808\n",
      "Train Loss at iteration 17159: 0.04205229003318228\n",
      "Train Loss at iteration 17160: 0.04205227167250551\n",
      "Train Loss at iteration 17161: 0.042052253314867255\n",
      "Train Loss at iteration 17162: 0.042052234960267006\n",
      "Train Loss at iteration 17163: 0.04205221660870428\n",
      "Train Loss at iteration 17164: 0.04205219826017856\n",
      "Train Loss at iteration 17165: 0.04205217991468935\n",
      "Train Loss at iteration 17166: 0.04205216157223614\n",
      "Train Loss at iteration 17167: 0.04205214323281843\n",
      "Train Loss at iteration 17168: 0.042052124896435714\n",
      "Train Loss at iteration 17169: 0.04205210656308751\n",
      "Train Loss at iteration 17170: 0.04205208823277329\n",
      "Train Loss at iteration 17171: 0.04205206990549256\n",
      "Train Loss at iteration 17172: 0.042052051581244816\n",
      "Train Loss at iteration 17173: 0.04205203326002958\n",
      "Train Loss at iteration 17174: 0.04205201494184631\n",
      "Train Loss at iteration 17175: 0.04205199662669452\n",
      "Train Loss at iteration 17176: 0.04205197831457372\n",
      "Train Loss at iteration 17177: 0.04205196000548339\n",
      "Train Loss at iteration 17178: 0.04205194169942305\n",
      "Train Loss at iteration 17179: 0.04205192339639219\n",
      "Train Loss at iteration 17180: 0.042051905096390285\n",
      "Train Loss at iteration 17181: 0.04205188679941686\n",
      "Train Loss at iteration 17182: 0.04205186850547142\n",
      "Train Loss at iteration 17183: 0.04205185021455344\n",
      "Train Loss at iteration 17184: 0.042051831926662425\n",
      "Train Loss at iteration 17185: 0.04205181364179788\n",
      "Train Loss at iteration 17186: 0.042051795359959315\n",
      "Train Loss at iteration 17187: 0.042051777081146206\n",
      "Train Loss at iteration 17188: 0.04205175880535806\n",
      "Train Loss at iteration 17189: 0.042051740532594375\n",
      "Train Loss at iteration 17190: 0.04205172226285466\n",
      "Train Loss at iteration 17191: 0.04205170399613841\n",
      "Train Loss at iteration 17192: 0.04205168573244512\n",
      "Train Loss at iteration 17193: 0.042051667471774294\n",
      "Train Loss at iteration 17194: 0.04205164921412542\n",
      "Train Loss at iteration 17195: 0.04205163095949802\n",
      "Train Loss at iteration 17196: 0.042051612707891574\n",
      "Train Loss at iteration 17197: 0.042051594459305584\n",
      "Train Loss at iteration 17198: 0.04205157621373956\n",
      "Train Loss at iteration 17199: 0.042051557971193004\n",
      "Train Loss at iteration 17200: 0.042051539731665394\n",
      "Train Loss at iteration 17201: 0.04205152149515625\n",
      "Train Loss at iteration 17202: 0.042051503261665076\n",
      "Train Loss at iteration 17203: 0.04205148503119135\n",
      "Train Loss at iteration 17204: 0.0420514668037346\n",
      "Train Loss at iteration 17205: 0.042051448579294305\n",
      "Train Loss at iteration 17206: 0.04205143035786998\n",
      "Train Loss at iteration 17207: 0.042051412139461106\n",
      "Train Loss at iteration 17208: 0.0420513939240672\n",
      "Train Loss at iteration 17209: 0.042051375711687766\n",
      "Train Loss at iteration 17210: 0.0420513575023223\n",
      "Train Loss at iteration 17211: 0.04205133929597029\n",
      "Train Loss at iteration 17212: 0.04205132109263126\n",
      "Train Loss at iteration 17213: 0.04205130289230469\n",
      "Train Loss at iteration 17214: 0.042051284694990095\n",
      "Train Loss at iteration 17215: 0.042051266500686975\n",
      "Train Loss at iteration 17216: 0.04205124830939483\n",
      "Train Loss at iteration 17217: 0.042051230121113156\n",
      "Train Loss at iteration 17218: 0.04205121193584146\n",
      "Train Loss at iteration 17219: 0.04205119375357924\n",
      "Train Loss at iteration 17220: 0.04205117557432601\n",
      "Train Loss at iteration 17221: 0.042051157398081254\n",
      "Train Loss at iteration 17222: 0.042051139224844485\n",
      "Train Loss at iteration 17223: 0.0420511210546152\n",
      "Train Loss at iteration 17224: 0.042051102887392895\n",
      "Train Loss at iteration 17225: 0.04205108472317709\n",
      "Train Loss at iteration 17226: 0.042051066561967276\n",
      "Train Loss at iteration 17227: 0.042051048403762954\n",
      "Train Loss at iteration 17228: 0.04205103024856364\n",
      "Train Loss at iteration 17229: 0.04205101209636882\n",
      "Train Loss at iteration 17230: 0.042050993947178004\n",
      "Train Loss at iteration 17231: 0.042050975800990674\n",
      "Train Loss at iteration 17232: 0.04205095765780637\n",
      "Train Loss at iteration 17233: 0.04205093951762458\n",
      "Train Loss at iteration 17234: 0.0420509213804448\n",
      "Train Loss at iteration 17235: 0.04205090324626653\n",
      "Train Loss at iteration 17236: 0.04205088511508928\n",
      "Train Loss at iteration 17237: 0.042050866986912565\n",
      "Train Loss at iteration 17238: 0.042050848861735865\n",
      "Train Loss at iteration 17239: 0.0420508307395587\n",
      "Train Loss at iteration 17240: 0.04205081262038056\n",
      "Train Loss at iteration 17241: 0.04205079450420097\n",
      "Train Loss at iteration 17242: 0.04205077639101941\n",
      "Train Loss at iteration 17243: 0.042050758280835404\n",
      "Train Loss at iteration 17244: 0.04205074017364842\n",
      "Train Loss at iteration 17245: 0.04205072206945801\n",
      "Train Loss at iteration 17246: 0.042050703968263645\n",
      "Train Loss at iteration 17247: 0.04205068587006484\n",
      "Train Loss at iteration 17248: 0.042050667774861106\n",
      "Train Loss at iteration 17249: 0.042050649682651924\n",
      "Train Loss at iteration 17250: 0.04205063159343683\n",
      "Train Loss at iteration 17251: 0.042050613507215305\n",
      "Train Loss at iteration 17252: 0.042050595423986856\n",
      "Train Loss at iteration 17253: 0.04205057734375099\n",
      "Train Loss at iteration 17254: 0.04205055926650722\n",
      "Train Loss at iteration 17255: 0.04205054119225504\n",
      "Train Loss at iteration 17256: 0.04205052312099396\n",
      "Train Loss at iteration 17257: 0.042050505052723494\n",
      "Train Loss at iteration 17258: 0.04205048698744312\n",
      "Train Loss at iteration 17259: 0.042050468925152365\n",
      "Train Loss at iteration 17260: 0.042050450865850734\n",
      "Train Loss at iteration 17261: 0.04205043280953771\n",
      "Train Loss at iteration 17262: 0.04205041475621284\n",
      "Train Loss at iteration 17263: 0.04205039670587558\n",
      "Train Loss at iteration 17264: 0.04205037865852547\n",
      "Train Loss at iteration 17265: 0.042050360614162015\n",
      "Train Loss at iteration 17266: 0.042050342572784685\n",
      "Train Loss at iteration 17267: 0.042050324534393016\n",
      "Train Loss at iteration 17268: 0.04205030649898652\n",
      "Train Loss at iteration 17269: 0.042050288466564695\n",
      "Train Loss at iteration 17270: 0.04205027043712704\n",
      "Train Loss at iteration 17271: 0.04205025241067306\n",
      "Train Loss at iteration 17272: 0.042050234387202255\n",
      "Train Loss at iteration 17273: 0.04205021636671416\n",
      "Train Loss at iteration 17274: 0.042050198349208256\n",
      "Train Loss at iteration 17275: 0.04205018033468407\n",
      "Train Loss at iteration 17276: 0.04205016232314106\n",
      "Train Loss at iteration 17277: 0.042050144314578784\n",
      "Train Loss at iteration 17278: 0.04205012630899673\n",
      "Train Loss at iteration 17279: 0.04205010830639442\n",
      "Train Loss at iteration 17280: 0.042050090306771336\n",
      "Train Loss at iteration 17281: 0.042050072310126994\n",
      "Train Loss at iteration 17282: 0.04205005431646091\n",
      "Train Loss at iteration 17283: 0.04205003632577257\n",
      "Train Loss at iteration 17284: 0.0420500183380615\n",
      "Train Loss at iteration 17285: 0.042050000353327205\n",
      "Train Loss at iteration 17286: 0.04204998237156917\n",
      "Train Loss at iteration 17287: 0.04204996439278695\n",
      "Train Loss at iteration 17288: 0.04204994641698\n",
      "Train Loss at iteration 17289: 0.04204992844414785\n",
      "Train Loss at iteration 17290: 0.04204991047429002\n",
      "Train Loss at iteration 17291: 0.042049892507406005\n",
      "Train Loss at iteration 17292: 0.042049874543495305\n",
      "Train Loss at iteration 17293: 0.04204985658255745\n",
      "Train Loss at iteration 17294: 0.042049838624591926\n",
      "Train Loss at iteration 17295: 0.04204982066959825\n",
      "Train Loss at iteration 17296: 0.04204980271757592\n",
      "Train Loss at iteration 17297: 0.04204978476852445\n",
      "Train Loss at iteration 17298: 0.042049766822443366\n",
      "Train Loss at iteration 17299: 0.04204974887933216\n",
      "Train Loss at iteration 17300: 0.042049730939190344\n",
      "Train Loss at iteration 17301: 0.0420497130020174\n",
      "Train Loss at iteration 17302: 0.04204969506781288\n",
      "Train Loss at iteration 17303: 0.04204967713657628\n",
      "Train Loss at iteration 17304: 0.0420496592083071\n",
      "Train Loss at iteration 17305: 0.04204964128300483\n",
      "Train Loss at iteration 17306: 0.04204962336066901\n",
      "Train Loss at iteration 17307: 0.04204960544129914\n",
      "Train Loss at iteration 17308: 0.04204958752489473\n",
      "Train Loss at iteration 17309: 0.04204956961145527\n",
      "Train Loss at iteration 17310: 0.042049551700980306\n",
      "Train Loss at iteration 17311: 0.04204953379346932\n",
      "Train Loss at iteration 17312: 0.04204951588892182\n",
      "Train Loss at iteration 17313: 0.042049497987337335\n",
      "Train Loss at iteration 17314: 0.042049480088715355\n",
      "Train Loss at iteration 17315: 0.042049462193055394\n",
      "Train Loss at iteration 17316: 0.042049444300356974\n",
      "Train Loss at iteration 17317: 0.042049426410619595\n",
      "Train Loss at iteration 17318: 0.04204940852384276\n",
      "Train Loss at iteration 17319: 0.042049390640025996\n",
      "Train Loss at iteration 17320: 0.042049372759168785\n",
      "Train Loss at iteration 17321: 0.042049354881270665\n",
      "Train Loss at iteration 17322: 0.04204933700633114\n",
      "Train Loss at iteration 17323: 0.04204931913434972\n",
      "Train Loss at iteration 17324: 0.042049301265325904\n",
      "Train Loss at iteration 17325: 0.04204928339925922\n",
      "Train Loss at iteration 17326: 0.04204926553614915\n",
      "Train Loss at iteration 17327: 0.04204924767599524\n",
      "Train Loss at iteration 17328: 0.04204922981879697\n",
      "Train Loss at iteration 17329: 0.04204921196455387\n",
      "Train Loss at iteration 17330: 0.04204919411326545\n",
      "Train Loss at iteration 17331: 0.04204917626493122\n",
      "Train Loss at iteration 17332: 0.042049158419550675\n",
      "Train Loss at iteration 17333: 0.04204914057712335\n",
      "Train Loss at iteration 17334: 0.04204912273764873\n",
      "Train Loss at iteration 17335: 0.04204910490112635\n",
      "Train Loss at iteration 17336: 0.0420490870675557\n",
      "Train Loss at iteration 17337: 0.04204906923693632\n",
      "Train Loss at iteration 17338: 0.04204905140926769\n",
      "Train Loss at iteration 17339: 0.042049033584549346\n",
      "Train Loss at iteration 17340: 0.042049015762780785\n",
      "Train Loss at iteration 17341: 0.04204899794396152\n",
      "Train Loss at iteration 17342: 0.04204898012809107\n",
      "Train Loss at iteration 17343: 0.04204896231516893\n",
      "Train Loss at iteration 17344: 0.042048944505194634\n",
      "Train Loss at iteration 17345: 0.04204892669816769\n",
      "Train Loss at iteration 17346: 0.04204890889408759\n",
      "Train Loss at iteration 17347: 0.04204889109295388\n",
      "Train Loss at iteration 17348: 0.04204887329476604\n",
      "Train Loss at iteration 17349: 0.0420488554995236\n",
      "Train Loss at iteration 17350: 0.04204883770722607\n",
      "Train Loss at iteration 17351: 0.04204881991787295\n",
      "Train Loss at iteration 17352: 0.042048802131463774\n",
      "Train Loss at iteration 17353: 0.04204878434799803\n",
      "Train Loss at iteration 17354: 0.04204876656747525\n",
      "Train Loss at iteration 17355: 0.04204874878989494\n",
      "Train Loss at iteration 17356: 0.042048731015256614\n",
      "Train Loss at iteration 17357: 0.04204871324355979\n",
      "Train Loss at iteration 17358: 0.042048695474803964\n",
      "Train Loss at iteration 17359: 0.042048677708988666\n",
      "Train Loss at iteration 17360: 0.0420486599461134\n",
      "Train Loss at iteration 17361: 0.04204864218617769\n",
      "Train Loss at iteration 17362: 0.04204862442918104\n",
      "Train Loss at iteration 17363: 0.04204860667512296\n",
      "Train Loss at iteration 17364: 0.042048588924002976\n",
      "Train Loss at iteration 17365: 0.042048571175820594\n",
      "Train Loss at iteration 17366: 0.04204855343057533\n",
      "Train Loss at iteration 17367: 0.04204853568826669\n",
      "Train Loss at iteration 17368: 0.04204851794889421\n",
      "Train Loss at iteration 17369: 0.04204850021245737\n",
      "Train Loss at iteration 17370: 0.0420484824789557\n",
      "Train Loss at iteration 17371: 0.04204846474838873\n",
      "Train Loss at iteration 17372: 0.042048447020755955\n",
      "Train Loss at iteration 17373: 0.04204842929605689\n",
      "Train Loss at iteration 17374: 0.04204841157429106\n",
      "Train Loss at iteration 17375: 0.04204839385545798\n",
      "Train Loss at iteration 17376: 0.04204837613955714\n",
      "Train Loss at iteration 17377: 0.042048358426588085\n",
      "Train Loss at iteration 17378: 0.042048340716550316\n",
      "Train Loss at iteration 17379: 0.04204832300944335\n",
      "Train Loss at iteration 17380: 0.042048305305266695\n",
      "Train Loss at iteration 17381: 0.042048287604019866\n",
      "Train Loss at iteration 17382: 0.042048269905702394\n",
      "Train Loss at iteration 17383: 0.04204825221031379\n",
      "Train Loss at iteration 17384: 0.04204823451785354\n",
      "Train Loss at iteration 17385: 0.0420482168283212\n",
      "Train Loss at iteration 17386: 0.042048199141716265\n",
      "Train Loss at iteration 17387: 0.04204818145803824\n",
      "Train Loss at iteration 17388: 0.042048163777286665\n",
      "Train Loss at iteration 17389: 0.04204814609946103\n",
      "Train Loss at iteration 17390: 0.04204812842456089\n",
      "Train Loss at iteration 17391: 0.042048110752585716\n",
      "Train Loss at iteration 17392: 0.04204809308353504\n",
      "Train Loss at iteration 17393: 0.04204807541740838\n",
      "Train Loss at iteration 17394: 0.04204805775420527\n",
      "Train Loss at iteration 17395: 0.042048040093925196\n",
      "Train Loss at iteration 17396: 0.04204802243656768\n",
      "Train Loss at iteration 17397: 0.042048004782132255\n",
      "Train Loss at iteration 17398: 0.04204798713061842\n",
      "Train Loss at iteration 17399: 0.04204796948202571\n",
      "Train Loss at iteration 17400: 0.04204795183635361\n",
      "Train Loss at iteration 17401: 0.042047934193601665\n",
      "Train Loss at iteration 17402: 0.04204791655376939\n",
      "Train Loss at iteration 17403: 0.04204789891685629\n",
      "Train Loss at iteration 17404: 0.042047881282861875\n",
      "Train Loss at iteration 17405: 0.04204786365178568\n",
      "Train Loss at iteration 17406: 0.04204784602362722\n",
      "Train Loss at iteration 17407: 0.04204782839838601\n",
      "Train Loss at iteration 17408: 0.04204781077606155\n",
      "Train Loss at iteration 17409: 0.04204779315665338\n",
      "Train Loss at iteration 17410: 0.042047775540161\n",
      "Train Loss at iteration 17411: 0.04204775792658394\n",
      "Train Loss at iteration 17412: 0.04204774031592171\n",
      "Train Loss at iteration 17413: 0.04204772270817384\n",
      "Train Loss at iteration 17414: 0.042047705103339834\n",
      "Train Loss at iteration 17415: 0.042047687501419205\n",
      "Train Loss at iteration 17416: 0.04204766990241148\n",
      "Train Loss at iteration 17417: 0.042047652306316186\n",
      "Train Loss at iteration 17418: 0.04204763471313283\n",
      "Train Loss at iteration 17419: 0.04204761712286092\n",
      "Train Loss at iteration 17420: 0.042047599535499976\n",
      "Train Loss at iteration 17421: 0.042047581951049545\n",
      "Train Loss at iteration 17422: 0.04204756436950912\n",
      "Train Loss at iteration 17423: 0.04204754679087823\n",
      "Train Loss at iteration 17424: 0.04204752921515637\n",
      "Train Loss at iteration 17425: 0.042047511642343084\n",
      "Train Loss at iteration 17426: 0.04204749407243789\n",
      "Train Loss at iteration 17427: 0.04204747650544029\n",
      "Train Loss at iteration 17428: 0.04204745894134981\n",
      "Train Loss at iteration 17429: 0.04204744138016598\n",
      "Train Loss at iteration 17430: 0.0420474238218883\n",
      "Train Loss at iteration 17431: 0.0420474062665163\n",
      "Train Loss at iteration 17432: 0.0420473887140495\n",
      "Train Loss at iteration 17433: 0.04204737116448741\n",
      "Train Loss at iteration 17434: 0.042047353617829554\n",
      "Train Loss at iteration 17435: 0.04204733607407546\n",
      "Train Loss at iteration 17436: 0.042047318533224645\n",
      "Train Loss at iteration 17437: 0.0420473009952766\n",
      "Train Loss at iteration 17438: 0.042047283460230886\n",
      "Train Loss at iteration 17439: 0.04204726592808701\n",
      "Train Loss at iteration 17440: 0.04204724839884447\n",
      "Train Loss at iteration 17441: 0.0420472308725028\n",
      "Train Loss at iteration 17442: 0.04204721334906152\n",
      "Train Loss at iteration 17443: 0.042047195828520165\n",
      "Train Loss at iteration 17444: 0.04204717831087824\n",
      "Train Loss at iteration 17445: 0.04204716079613526\n",
      "Train Loss at iteration 17446: 0.042047143284290746\n",
      "Train Loss at iteration 17447: 0.04204712577534423\n",
      "Train Loss at iteration 17448: 0.04204710826929522\n",
      "Train Loss at iteration 17449: 0.04204709076614325\n",
      "Train Loss at iteration 17450: 0.04204707326588782\n",
      "Train Loss at iteration 17451: 0.04204705576852847\n",
      "Train Loss at iteration 17452: 0.04204703827406472\n",
      "Train Loss at iteration 17453: 0.04204702078249607\n",
      "Train Loss at iteration 17454: 0.042047003293822056\n",
      "Train Loss at iteration 17455: 0.042046985808042205\n",
      "Train Loss at iteration 17456: 0.042046968325156024\n",
      "Train Loss at iteration 17457: 0.04204695084516305\n",
      "Train Loss at iteration 17458: 0.04204693336806279\n",
      "Train Loss at iteration 17459: 0.04204691589385476\n",
      "Train Loss at iteration 17460: 0.04204689842253849\n",
      "Train Loss at iteration 17461: 0.04204688095411352\n",
      "Train Loss at iteration 17462: 0.042046863488579334\n",
      "Train Loss at iteration 17463: 0.04204684602593547\n",
      "Train Loss at iteration 17464: 0.042046828566181464\n",
      "Train Loss at iteration 17465: 0.04204681110931682\n",
      "Train Loss at iteration 17466: 0.04204679365534107\n",
      "Train Loss at iteration 17467: 0.042046776204253726\n",
      "Train Loss at iteration 17468: 0.04204675875605432\n",
      "Train Loss at iteration 17469: 0.04204674131074236\n",
      "Train Loss at iteration 17470: 0.042046723868317384\n",
      "Train Loss at iteration 17471: 0.04204670642877891\n",
      "Train Loss at iteration 17472: 0.04204668899212646\n",
      "Train Loss at iteration 17473: 0.04204667155835953\n",
      "Train Loss at iteration 17474: 0.0420466541274777\n",
      "Train Loss at iteration 17475: 0.04204663669948043\n",
      "Train Loss at iteration 17476: 0.042046619274367286\n",
      "Train Loss at iteration 17477: 0.042046601852137766\n",
      "Train Loss at iteration 17478: 0.0420465844327914\n",
      "Train Loss at iteration 17479: 0.04204656701632771\n",
      "Train Loss at iteration 17480: 0.04204654960274623\n",
      "Train Loss at iteration 17481: 0.042046532192046474\n",
      "Train Loss at iteration 17482: 0.042046514784227954\n",
      "Train Loss at iteration 17483: 0.042046497379290206\n",
      "Train Loss at iteration 17484: 0.04204647997723277\n",
      "Train Loss at iteration 17485: 0.04204646257805514\n",
      "Train Loss at iteration 17486: 0.04204644518175685\n",
      "Train Loss at iteration 17487: 0.042046427788337414\n",
      "Train Loss at iteration 17488: 0.042046410397796365\n",
      "Train Loss at iteration 17489: 0.04204639301013323\n",
      "Train Loss at iteration 17490: 0.04204637562534753\n",
      "Train Loss at iteration 17491: 0.04204635824343878\n",
      "Train Loss at iteration 17492: 0.04204634086440651\n",
      "Train Loss at iteration 17493: 0.042046323488250256\n",
      "Train Loss at iteration 17494: 0.042046306114969526\n",
      "Train Loss at iteration 17495: 0.042046288744563835\n",
      "Train Loss at iteration 17496: 0.04204627137703274\n",
      "Train Loss at iteration 17497: 0.04204625401237573\n",
      "Train Loss at iteration 17498: 0.04204623665059236\n",
      "Train Loss at iteration 17499: 0.04204621929168213\n",
      "Train Loss at iteration 17500: 0.04204620193564456\n",
      "Train Loss at iteration 17501: 0.04204618458247921\n",
      "Train Loss at iteration 17502: 0.04204616723218557\n",
      "Train Loss at iteration 17503: 0.04204614988476319\n",
      "Train Loss at iteration 17504: 0.04204613254021156\n",
      "Train Loss at iteration 17505: 0.04204611519853024\n",
      "Train Loss at iteration 17506: 0.042046097859718745\n",
      "Train Loss at iteration 17507: 0.04204608052377658\n",
      "Train Loss at iteration 17508: 0.0420460631907033\n",
      "Train Loss at iteration 17509: 0.04204604586049842\n",
      "Train Loss at iteration 17510: 0.04204602853316145\n",
      "Train Loss at iteration 17511: 0.042046011208691926\n",
      "Train Loss at iteration 17512: 0.04204599388708938\n",
      "Train Loss at iteration 17513: 0.04204597656835332\n",
      "Train Loss at iteration 17514: 0.0420459592524833\n",
      "Train Loss at iteration 17515: 0.04204594193947881\n",
      "Train Loss at iteration 17516: 0.042045924629339405\n",
      "Train Loss at iteration 17517: 0.042045907322064605\n",
      "Train Loss at iteration 17518: 0.042045890017653915\n",
      "Train Loss at iteration 17519: 0.04204587271610688\n",
      "Train Loss at iteration 17520: 0.042045855417423034\n",
      "Train Loss at iteration 17521: 0.04204583812160188\n",
      "Train Loss at iteration 17522: 0.042045820828642964\n",
      "Train Loss at iteration 17523: 0.04204580353854579\n",
      "Train Loss at iteration 17524: 0.042045786251309905\n",
      "Train Loss at iteration 17525: 0.04204576896693483\n",
      "Train Loss at iteration 17526: 0.04204575168542009\n",
      "Train Loss at iteration 17527: 0.0420457344067652\n",
      "Train Loss at iteration 17528: 0.04204571713096972\n",
      "Train Loss at iteration 17529: 0.04204569985803313\n",
      "Train Loss at iteration 17530: 0.042045682587954984\n",
      "Train Loss at iteration 17531: 0.04204566532073482\n",
      "Train Loss at iteration 17532: 0.04204564805637215\n",
      "Train Loss at iteration 17533: 0.042045630794866486\n",
      "Train Loss at iteration 17534: 0.04204561353621738\n",
      "Train Loss at iteration 17535: 0.04204559628042435\n",
      "Train Loss at iteration 17536: 0.042045579027486915\n",
      "Train Loss at iteration 17537: 0.04204556177740462\n",
      "Train Loss at iteration 17538: 0.04204554453017698\n",
      "Train Loss at iteration 17539: 0.04204552728580352\n",
      "Train Loss at iteration 17540: 0.04204551004428377\n",
      "Train Loss at iteration 17541: 0.04204549280561726\n",
      "Train Loss at iteration 17542: 0.04204547556980352\n",
      "Train Loss at iteration 17543: 0.04204545833684209\n",
      "Train Loss at iteration 17544: 0.04204544110673247\n",
      "Train Loss at iteration 17545: 0.04204542387947421\n",
      "Train Loss at iteration 17546: 0.042045406655066814\n",
      "Train Loss at iteration 17547: 0.04204538943350983\n",
      "Train Loss at iteration 17548: 0.042045372214802784\n",
      "Train Loss at iteration 17549: 0.0420453549989452\n",
      "Train Loss at iteration 17550: 0.0420453377859366\n",
      "Train Loss at iteration 17551: 0.04204532057577653\n",
      "Train Loss at iteration 17552: 0.0420453033684645\n",
      "Train Loss at iteration 17553: 0.04204528616400006\n",
      "Train Loss at iteration 17554: 0.04204526896238272\n",
      "Train Loss at iteration 17555: 0.04204525176361201\n",
      "Train Loss at iteration 17556: 0.042045234567687474\n",
      "Train Loss at iteration 17557: 0.042045217374608625\n",
      "Train Loss at iteration 17558: 0.04204520018437499\n",
      "Train Loss at iteration 17559: 0.042045182996986105\n",
      "Train Loss at iteration 17560: 0.042045165812441505\n",
      "Train Loss at iteration 17561: 0.04204514863074071\n",
      "Train Loss at iteration 17562: 0.04204513145188325\n",
      "Train Loss at iteration 17563: 0.04204511427586866\n",
      "Train Loss at iteration 17564: 0.042045097102696466\n",
      "Train Loss at iteration 17565: 0.04204507993236619\n",
      "Train Loss at iteration 17566: 0.04204506276487738\n",
      "Train Loss at iteration 17567: 0.04204504560022955\n",
      "Train Loss at iteration 17568: 0.04204502843842224\n",
      "Train Loss at iteration 17569: 0.04204501127945496\n",
      "Train Loss at iteration 17570: 0.04204499412332727\n",
      "Train Loss at iteration 17571: 0.04204497697003867\n",
      "Train Loss at iteration 17572: 0.04204495981958871\n",
      "Train Loss at iteration 17573: 0.04204494267197691\n",
      "Train Loss at iteration 17574: 0.04204492552720282\n",
      "Train Loss at iteration 17575: 0.04204490838526592\n",
      "Train Loss at iteration 17576: 0.04204489124616581\n",
      "Train Loss at iteration 17577: 0.04204487410990197\n",
      "Train Loss at iteration 17578: 0.04204485697647395\n",
      "Train Loss at iteration 17579: 0.042044839845881275\n",
      "Train Loss at iteration 17580: 0.042044822718123466\n",
      "Train Loss at iteration 17581: 0.04204480559320008\n",
      "Train Loss at iteration 17582: 0.04204478847111063\n",
      "Train Loss at iteration 17583: 0.04204477135185464\n",
      "Train Loss at iteration 17584: 0.04204475423543165\n",
      "Train Loss at iteration 17585: 0.0420447371218412\n",
      "Train Loss at iteration 17586: 0.04204472001108281\n",
      "Train Loss at iteration 17587: 0.042044702903156\n",
      "Train Loss at iteration 17588: 0.04204468579806033\n",
      "Train Loss at iteration 17589: 0.04204466869579531\n",
      "Train Loss at iteration 17590: 0.04204465159636048\n",
      "Train Loss at iteration 17591: 0.042044634499755366\n",
      "Train Loss at iteration 17592: 0.042044617405979506\n",
      "Train Loss at iteration 17593: 0.04204460031503243\n",
      "Train Loss at iteration 17594: 0.04204458322691365\n",
      "Train Loss at iteration 17595: 0.042044566141622726\n",
      "Train Loss at iteration 17596: 0.04204454905915918\n",
      "Train Loss at iteration 17597: 0.042044531979522556\n",
      "Train Loss at iteration 17598: 0.04204451490271236\n",
      "Train Loss at iteration 17599: 0.042044497828728135\n",
      "Train Loss at iteration 17600: 0.04204448075756942\n",
      "Train Loss at iteration 17601: 0.042044463689235735\n",
      "Train Loss at iteration 17602: 0.04204444662372664\n",
      "Train Loss at iteration 17603: 0.04204442956104163\n",
      "Train Loss at iteration 17604: 0.04204441250118026\n",
      "Train Loss at iteration 17605: 0.04204439544414206\n",
      "Train Loss at iteration 17606: 0.04204437838992656\n",
      "Train Loss at iteration 17607: 0.042044361338533294\n",
      "Train Loss at iteration 17608: 0.04204434428996179\n",
      "Train Loss at iteration 17609: 0.04204432724421158\n",
      "Train Loss at iteration 17610: 0.04204431020128221\n",
      "Train Loss at iteration 17611: 0.0420442931611732\n",
      "Train Loss at iteration 17612: 0.042044276123884096\n",
      "Train Loss at iteration 17613: 0.04204425908941442\n",
      "Train Loss at iteration 17614: 0.04204424205776369\n",
      "Train Loss at iteration 17615: 0.04204422502893147\n",
      "Train Loss at iteration 17616: 0.0420442080029173\n",
      "Train Loss at iteration 17617: 0.042044190979720675\n",
      "Train Loss at iteration 17618: 0.04204417395934115\n",
      "Train Loss at iteration 17619: 0.042044156941778266\n",
      "Train Loss at iteration 17620: 0.042044139927031536\n",
      "Train Loss at iteration 17621: 0.04204412291510051\n",
      "Train Loss at iteration 17622: 0.04204410590598471\n",
      "Train Loss at iteration 17623: 0.04204408889968369\n",
      "Train Loss at iteration 17624: 0.04204407189619696\n",
      "Train Loss at iteration 17625: 0.042044054895524076\n",
      "Train Loss at iteration 17626: 0.04204403789766455\n",
      "Train Loss at iteration 17627: 0.04204402090261793\n",
      "Train Loss at iteration 17628: 0.04204400391038376\n",
      "Train Loss at iteration 17629: 0.042043986920961555\n",
      "Train Loss at iteration 17630: 0.042043969934350854\n",
      "Train Loss at iteration 17631: 0.042043952950551196\n",
      "Train Loss at iteration 17632: 0.04204393596956212\n",
      "Train Loss at iteration 17633: 0.04204391899138315\n",
      "Train Loss at iteration 17634: 0.04204390201601382\n",
      "Train Loss at iteration 17635: 0.042043885043453674\n",
      "Train Loss at iteration 17636: 0.04204386807370224\n",
      "Train Loss at iteration 17637: 0.042043851106759074\n",
      "Train Loss at iteration 17638: 0.042043834142623686\n",
      "Train Loss at iteration 17639: 0.042043817181295615\n",
      "Train Loss at iteration 17640: 0.042043800222774395\n",
      "Train Loss at iteration 17641: 0.04204378326705958\n",
      "Train Loss at iteration 17642: 0.04204376631415069\n",
      "Train Loss at iteration 17643: 0.042043749364047256\n",
      "Train Loss at iteration 17644: 0.042043732416748816\n",
      "Train Loss at iteration 17645: 0.04204371547225492\n",
      "Train Loss at iteration 17646: 0.04204369853056509\n",
      "Train Loss at iteration 17647: 0.042043681591678865\n",
      "Train Loss at iteration 17648: 0.04204366465559578\n",
      "Train Loss at iteration 17649: 0.042043647722315385\n",
      "Train Loss at iteration 17650: 0.042043630791837185\n",
      "Train Loss at iteration 17651: 0.042043613864160743\n",
      "Train Loss at iteration 17652: 0.04204359693928559\n",
      "Train Loss at iteration 17653: 0.04204358001721125\n",
      "Train Loss at iteration 17654: 0.04204356309793727\n",
      "Train Loss at iteration 17655: 0.042043546181463186\n",
      "Train Loss at iteration 17656: 0.04204352926778854\n",
      "Train Loss at iteration 17657: 0.04204351235691285\n",
      "Train Loss at iteration 17658: 0.042043495448835684\n",
      "Train Loss at iteration 17659: 0.04204347854355653\n",
      "Train Loss at iteration 17660: 0.042043461641074975\n",
      "Train Loss at iteration 17661: 0.04204344474139053\n",
      "Train Loss at iteration 17662: 0.04204342784450274\n",
      "Train Loss at iteration 17663: 0.042043410950411135\n",
      "Train Loss at iteration 17664: 0.04204339405911525\n",
      "Train Loss at iteration 17665: 0.04204337717061463\n",
      "Train Loss at iteration 17666: 0.04204336028490883\n",
      "Train Loss at iteration 17667: 0.04204334340199735\n",
      "Train Loss at iteration 17668: 0.04204332652187975\n",
      "Train Loss at iteration 17669: 0.04204330964455557\n",
      "Train Loss at iteration 17670: 0.042043292770024325\n",
      "Train Loss at iteration 17671: 0.04204327589828557\n",
      "Train Loss at iteration 17672: 0.04204325902933885\n",
      "Train Loss at iteration 17673: 0.04204324216318368\n",
      "Train Loss at iteration 17674: 0.04204322529981963\n",
      "Train Loss at iteration 17675: 0.042043208439246214\n",
      "Train Loss at iteration 17676: 0.042043191581462976\n",
      "Train Loss at iteration 17677: 0.04204317472646944\n",
      "Train Loss at iteration 17678: 0.042043157874265186\n",
      "Train Loss at iteration 17679: 0.04204314102484969\n",
      "Train Loss at iteration 17680: 0.04204312417822254\n",
      "Train Loss at iteration 17681: 0.04204310733438326\n",
      "Train Loss at iteration 17682: 0.04204309049333139\n",
      "Train Loss at iteration 17683: 0.04204307365506645\n",
      "Train Loss at iteration 17684: 0.042043056819588004\n",
      "Train Loss at iteration 17685: 0.0420430399868956\n",
      "Train Loss at iteration 17686: 0.04204302315698874\n",
      "Train Loss at iteration 17687: 0.04204300632986698\n",
      "Train Loss at iteration 17688: 0.04204298950552986\n",
      "Train Loss at iteration 17689: 0.04204297268397693\n",
      "Train Loss at iteration 17690: 0.042042955865207704\n",
      "Train Loss at iteration 17691: 0.04204293904922173\n",
      "Train Loss at iteration 17692: 0.04204292223601856\n",
      "Train Loss at iteration 17693: 0.042042905425597736\n",
      "Train Loss at iteration 17694: 0.04204288861795879\n",
      "Train Loss at iteration 17695: 0.04204287181310125\n",
      "Train Loss at iteration 17696: 0.04204285501102466\n",
      "Train Loss at iteration 17697: 0.04204283821172857\n",
      "Train Loss at iteration 17698: 0.04204282141521251\n",
      "Train Loss at iteration 17699: 0.04204280462147601\n",
      "Train Loss at iteration 17700: 0.04204278783051865\n",
      "Train Loss at iteration 17701: 0.04204277104233993\n",
      "Train Loss at iteration 17702: 0.04204275425693941\n",
      "Train Loss at iteration 17703: 0.04204273747431661\n",
      "Train Loss at iteration 17704: 0.0420427206944711\n",
      "Train Loss at iteration 17705: 0.0420427039174024\n",
      "Train Loss at iteration 17706: 0.04204268714311003\n",
      "Train Loss at iteration 17707: 0.042042670371593586\n",
      "Train Loss at iteration 17708: 0.04204265360285258\n",
      "Train Loss at iteration 17709: 0.04204263683688652\n",
      "Train Loss at iteration 17710: 0.04204262007369498\n",
      "Train Loss at iteration 17711: 0.04204260331327752\n",
      "Train Loss at iteration 17712: 0.04204258655563366\n",
      "Train Loss at iteration 17713: 0.042042569800762916\n",
      "Train Loss at iteration 17714: 0.04204255304866486\n",
      "Train Loss at iteration 17715: 0.042042536299339024\n",
      "Train Loss at iteration 17716: 0.04204251955278496\n",
      "Train Loss at iteration 17717: 0.04204250280900217\n",
      "Train Loss at iteration 17718: 0.04204248606799026\n",
      "Train Loss at iteration 17719: 0.04204246932974872\n",
      "Train Loss at iteration 17720: 0.04204245259427711\n",
      "Train Loss at iteration 17721: 0.04204243586157496\n",
      "Train Loss at iteration 17722: 0.04204241913164182\n",
      "Train Loss at iteration 17723: 0.04204240240447726\n",
      "Train Loss at iteration 17724: 0.04204238568008075\n",
      "Train Loss at iteration 17725: 0.0420423689584519\n",
      "Train Loss at iteration 17726: 0.04204235223959023\n",
      "Train Loss at iteration 17727: 0.04204233552349526\n",
      "Train Loss at iteration 17728: 0.04204231881016656\n",
      "Train Loss at iteration 17729: 0.04204230209960366\n",
      "Train Loss at iteration 17730: 0.042042285391806114\n",
      "Train Loss at iteration 17731: 0.04204226868677346\n",
      "Train Loss at iteration 17732: 0.04204225198450521\n",
      "Train Loss at iteration 17733: 0.042042235285000965\n",
      "Train Loss at iteration 17734: 0.04204221858826021\n",
      "Train Loss at iteration 17735: 0.04204220189428252\n",
      "Train Loss at iteration 17736: 0.042042185203067434\n",
      "Train Loss at iteration 17737: 0.04204216851461449\n",
      "Train Loss at iteration 17738: 0.04204215182892322\n",
      "Train Loss at iteration 17739: 0.042042135145993184\n",
      "Train Loss at iteration 17740: 0.04204211846582392\n",
      "Train Loss at iteration 17741: 0.042042101788414964\n",
      "Train Loss at iteration 17742: 0.04204208511376587\n",
      "Train Loss at iteration 17743: 0.042042068441876176\n",
      "Train Loss at iteration 17744: 0.042042051772745434\n",
      "Train Loss at iteration 17745: 0.04204203510637317\n",
      "Train Loss at iteration 17746: 0.04204201844275892\n",
      "Train Loss at iteration 17747: 0.04204200178190227\n",
      "Train Loss at iteration 17748: 0.04204198512380273\n",
      "Train Loss at iteration 17749: 0.04204196846845983\n",
      "Train Loss at iteration 17750: 0.042041951815873155\n",
      "Train Loss at iteration 17751: 0.04204193516604222\n",
      "Train Loss at iteration 17752: 0.04204191851896658\n",
      "Train Loss at iteration 17753: 0.04204190187464578\n",
      "Train Loss at iteration 17754: 0.042041885233079355\n",
      "Train Loss at iteration 17755: 0.04204186859426685\n",
      "Train Loss at iteration 17756: 0.04204185195820781\n",
      "Train Loss at iteration 17757: 0.04204183532490179\n",
      "Train Loss at iteration 17758: 0.042041818694348324\n",
      "Train Loss at iteration 17759: 0.042041802066546965\n",
      "Train Loss at iteration 17760: 0.04204178544149724\n",
      "Train Loss at iteration 17761: 0.042041768819198716\n",
      "Train Loss at iteration 17762: 0.04204175219965091\n",
      "Train Loss at iteration 17763: 0.042041735582853394\n",
      "Train Loss at iteration 17764: 0.042041718968805694\n",
      "Train Loss at iteration 17765: 0.042041702357507364\n",
      "Train Loss at iteration 17766: 0.04204168574895795\n",
      "Train Loss at iteration 17767: 0.042041669143157\n",
      "Train Loss at iteration 17768: 0.04204165254010406\n",
      "Train Loss at iteration 17769: 0.04204163593979864\n",
      "Train Loss at iteration 17770: 0.04204161934224033\n",
      "Train Loss at iteration 17771: 0.04204160274742866\n",
      "Train Loss at iteration 17772: 0.042041586155363186\n",
      "Train Loss at iteration 17773: 0.04204156956604342\n",
      "Train Loss at iteration 17774: 0.04204155297946892\n",
      "Train Loss at iteration 17775: 0.04204153639563927\n",
      "Train Loss at iteration 17776: 0.042041519814553965\n",
      "Train Loss at iteration 17777: 0.042041503236212585\n",
      "Train Loss at iteration 17778: 0.042041486660614656\n",
      "Train Loss at iteration 17779: 0.04204147008775974\n",
      "Train Loss at iteration 17780: 0.04204145351764737\n",
      "Train Loss at iteration 17781: 0.04204143695027708\n",
      "Train Loss at iteration 17782: 0.04204142038564844\n",
      "Train Loss at iteration 17783: 0.042041403823760995\n",
      "Train Loss at iteration 17784: 0.042041387264614274\n",
      "Train Loss at iteration 17785: 0.04204137070820784\n",
      "Train Loss at iteration 17786: 0.04204135415454123\n",
      "Train Loss at iteration 17787: 0.04204133760361399\n",
      "Train Loss at iteration 17788: 0.04204132105542568\n",
      "Train Loss at iteration 17789: 0.04204130450997582\n",
      "Train Loss at iteration 17790: 0.04204128796726398\n",
      "Train Loss at iteration 17791: 0.04204127142728971\n",
      "Train Loss at iteration 17792: 0.04204125489005254\n",
      "Train Loss at iteration 17793: 0.042041238355552006\n",
      "Train Loss at iteration 17794: 0.0420412218237877\n",
      "Train Loss at iteration 17795: 0.042041205294759125\n",
      "Train Loss at iteration 17796: 0.042041188768465854\n",
      "Train Loss at iteration 17797: 0.042041172244907415\n",
      "Train Loss at iteration 17798: 0.04204115572408338\n",
      "Train Loss at iteration 17799: 0.042041139205993255\n",
      "Train Loss at iteration 17800: 0.04204112269063663\n",
      "Train Loss at iteration 17801: 0.04204110617801304\n",
      "Train Loss at iteration 17802: 0.04204108966812203\n",
      "Train Loss at iteration 17803: 0.04204107316096314\n",
      "Train Loss at iteration 17804: 0.04204105665653593\n",
      "Train Loss at iteration 17805: 0.04204104015483994\n",
      "Train Loss at iteration 17806: 0.04204102365587471\n",
      "Train Loss at iteration 17807: 0.04204100715963981\n",
      "Train Loss at iteration 17808: 0.04204099066613478\n",
      "Train Loss at iteration 17809: 0.04204097417535915\n",
      "Train Loss at iteration 17810: 0.04204095768731249\n",
      "Train Loss at iteration 17811: 0.04204094120199434\n",
      "Train Loss at iteration 17812: 0.042040924719404245\n",
      "Train Loss at iteration 17813: 0.04204090823954176\n",
      "Train Loss at iteration 17814: 0.04204089176240644\n",
      "Train Loss at iteration 17815: 0.04204087528799782\n",
      "Train Loss at iteration 17816: 0.04204085881631545\n",
      "Train Loss at iteration 17817: 0.04204084234735887\n",
      "Train Loss at iteration 17818: 0.04204082588112766\n",
      "Train Loss at iteration 17819: 0.042040809417621346\n",
      "Train Loss at iteration 17820: 0.04204079295683947\n",
      "Train Loss at iteration 17821: 0.042040776498781594\n",
      "Train Loss at iteration 17822: 0.04204076004344728\n",
      "Train Loss at iteration 17823: 0.04204074359083604\n",
      "Train Loss at iteration 17824: 0.04204072714094746\n",
      "Train Loss at iteration 17825: 0.04204071069378106\n",
      "Train Loss at iteration 17826: 0.042040694249336416\n",
      "Train Loss at iteration 17827: 0.042040677807613065\n",
      "Train Loss at iteration 17828: 0.042040661368610566\n",
      "Train Loss at iteration 17829: 0.04204064493232843\n",
      "Train Loss at iteration 17830: 0.04204062849876625\n",
      "Train Loss at iteration 17831: 0.04204061206792358\n",
      "Train Loss at iteration 17832: 0.04204059563979992\n",
      "Train Loss at iteration 17833: 0.042040579214394865\n",
      "Train Loss at iteration 17834: 0.04204056279170797\n",
      "Train Loss at iteration 17835: 0.042040546371738735\n",
      "Train Loss at iteration 17836: 0.042040529954486754\n",
      "Train Loss at iteration 17837: 0.042040513539951564\n",
      "Train Loss at iteration 17838: 0.04204049712813272\n",
      "Train Loss at iteration 17839: 0.042040480719029764\n",
      "Train Loss at iteration 17840: 0.042040464312642266\n",
      "Train Loss at iteration 17841: 0.04204044790896974\n",
      "Train Loss at iteration 17842: 0.042040431508011775\n",
      "Train Loss at iteration 17843: 0.042040415109767894\n",
      "Train Loss at iteration 17844: 0.04204039871423766\n",
      "Train Loss at iteration 17845: 0.04204038232142063\n",
      "Train Loss at iteration 17846: 0.04204036593131633\n",
      "Train Loss at iteration 17847: 0.04204034954392435\n",
      "Train Loss at iteration 17848: 0.0420403331592442\n",
      "Train Loss at iteration 17849: 0.04204031677727547\n",
      "Train Loss at iteration 17850: 0.04204030039801766\n",
      "Train Loss at iteration 17851: 0.04204028402147037\n",
      "Train Loss at iteration 17852: 0.042040267647633134\n",
      "Train Loss at iteration 17853: 0.0420402512765055\n",
      "Train Loss at iteration 17854: 0.042040234908087046\n",
      "Train Loss at iteration 17855: 0.04204021854237727\n",
      "Train Loss at iteration 17856: 0.042040202179375756\n",
      "Train Loss at iteration 17857: 0.04204018581908207\n",
      "Train Loss at iteration 17858: 0.04204016946149574\n",
      "Train Loss at iteration 17859: 0.042040153106616324\n",
      "Train Loss at iteration 17860: 0.042040136754443375\n",
      "Train Loss at iteration 17861: 0.04204012040497644\n",
      "Train Loss at iteration 17862: 0.04204010405821507\n",
      "Train Loss at iteration 17863: 0.04204008771415885\n",
      "Train Loss at iteration 17864: 0.04204007137280728\n",
      "Train Loss at iteration 17865: 0.04204005503415996\n",
      "Train Loss at iteration 17866: 0.0420400386982164\n",
      "Train Loss at iteration 17867: 0.04204002236497618\n",
      "Train Loss at iteration 17868: 0.04204000603443884\n",
      "Train Loss at iteration 17869: 0.04203998970660395\n",
      "Train Loss at iteration 17870: 0.04203997338147104\n",
      "Train Loss at iteration 17871: 0.042039957059039686\n",
      "Train Loss at iteration 17872: 0.04203994073930941\n",
      "Train Loss at iteration 17873: 0.04203992442227979\n",
      "Train Loss at iteration 17874: 0.04203990810795037\n",
      "Train Loss at iteration 17875: 0.042039891796320714\n",
      "Train Loss at iteration 17876: 0.042039875487390355\n",
      "Train Loss at iteration 17877: 0.04203985918115886\n",
      "Train Loss at iteration 17878: 0.04203984287762578\n",
      "Train Loss at iteration 17879: 0.04203982657679066\n",
      "Train Loss at iteration 17880: 0.04203981027865308\n",
      "Train Loss at iteration 17881: 0.04203979398321256\n",
      "Train Loss at iteration 17882: 0.04203977769046866\n",
      "Train Loss at iteration 17883: 0.042039761400420955\n",
      "Train Loss at iteration 17884: 0.04203974511306899\n",
      "Train Loss at iteration 17885: 0.042039728828412296\n",
      "Train Loss at iteration 17886: 0.04203971254645046\n",
      "Train Loss at iteration 17887: 0.04203969626718301\n",
      "Train Loss at iteration 17888: 0.04203967999060953\n",
      "Train Loss at iteration 17889: 0.042039663716729535\n",
      "Train Loss at iteration 17890: 0.0420396474455426\n",
      "Train Loss at iteration 17891: 0.04203963117704829\n",
      "Train Loss at iteration 17892: 0.042039614911246144\n",
      "Train Loss at iteration 17893: 0.042039598648135716\n",
      "Train Loss at iteration 17894: 0.042039582387716566\n",
      "Train Loss at iteration 17895: 0.04203956612998824\n",
      "Train Loss at iteration 17896: 0.04203954987495031\n",
      "Train Loss at iteration 17897: 0.04203953362260232\n",
      "Train Loss at iteration 17898: 0.04203951737294383\n",
      "Train Loss at iteration 17899: 0.04203950112597437\n",
      "Train Loss at iteration 17900: 0.042039484881693526\n",
      "Train Loss at iteration 17901: 0.042039468640100844\n",
      "Train Loss at iteration 17902: 0.04203945240119587\n",
      "Train Loss at iteration 17903: 0.04203943616497817\n",
      "Train Loss at iteration 17904: 0.04203941993144729\n",
      "Train Loss at iteration 17905: 0.042039403700602794\n",
      "Train Loss at iteration 17906: 0.04203938747244423\n",
      "Train Loss at iteration 17907: 0.04203937124697116\n",
      "Train Loss at iteration 17908: 0.042039355024183135\n",
      "Train Loss at iteration 17909: 0.042039338804079715\n",
      "Train Loss at iteration 17910: 0.042039322586660446\n",
      "Train Loss at iteration 17911: 0.0420393063719249\n",
      "Train Loss at iteration 17912: 0.04203929015987261\n",
      "Train Loss at iteration 17913: 0.04203927395050316\n",
      "Train Loss at iteration 17914: 0.042039257743816075\n",
      "Train Loss at iteration 17915: 0.04203924153981092\n",
      "Train Loss at iteration 17916: 0.04203922533848728\n",
      "Train Loss at iteration 17917: 0.042039209139844685\n",
      "Train Loss at iteration 17918: 0.042039192943882676\n",
      "Train Loss at iteration 17919: 0.04203917675060084\n",
      "Train Loss at iteration 17920: 0.04203916055999873\n",
      "Train Loss at iteration 17921: 0.042039144372075885\n",
      "Train Loss at iteration 17922: 0.04203912818683187\n",
      "Train Loss at iteration 17923: 0.042039112004266244\n",
      "Train Loss at iteration 17924: 0.04203909582437856\n",
      "Train Loss at iteration 17925: 0.042039079647168374\n",
      "Train Loss at iteration 17926: 0.04203906347263525\n",
      "Train Loss at iteration 17927: 0.04203904730077874\n",
      "Train Loss at iteration 17928: 0.0420390311315984\n",
      "Train Loss at iteration 17929: 0.04203901496509379\n",
      "Train Loss at iteration 17930: 0.04203899880126446\n",
      "Train Loss at iteration 17931: 0.042038982640109974\n",
      "Train Loss at iteration 17932: 0.04203896648162989\n",
      "Train Loss at iteration 17933: 0.042038950325823755\n",
      "Train Loss at iteration 17934: 0.042038934172691146\n",
      "Train Loss at iteration 17935: 0.04203891802223161\n",
      "Train Loss at iteration 17936: 0.042038901874444694\n",
      "Train Loss at iteration 17937: 0.04203888572932996\n",
      "Train Loss at iteration 17938: 0.042038869586886976\n",
      "Train Loss at iteration 17939: 0.0420388534471153\n",
      "Train Loss at iteration 17940: 0.042038837310014475\n",
      "Train Loss at iteration 17941: 0.04203882117558408\n",
      "Train Loss at iteration 17942: 0.04203880504382365\n",
      "Train Loss at iteration 17943: 0.04203878891473276\n",
      "Train Loss at iteration 17944: 0.04203877278831095\n",
      "Train Loss at iteration 17945: 0.0420387566645578\n",
      "Train Loss at iteration 17946: 0.04203874054347286\n",
      "Train Loss at iteration 17947: 0.042038724425055685\n",
      "Train Loss at iteration 17948: 0.04203870830930583\n",
      "Train Loss at iteration 17949: 0.04203869219622287\n",
      "Train Loss at iteration 17950: 0.04203867608580634\n",
      "Train Loss at iteration 17951: 0.042038659978055816\n",
      "Train Loss at iteration 17952: 0.042038643872970856\n",
      "Train Loss at iteration 17953: 0.04203862777055101\n",
      "Train Loss at iteration 17954: 0.04203861167079584\n",
      "Train Loss at iteration 17955: 0.0420385955737049\n",
      "Train Loss at iteration 17956: 0.04203857947927777\n",
      "Train Loss at iteration 17957: 0.04203856338751397\n",
      "Train Loss at iteration 17958: 0.0420385472984131\n",
      "Train Loss at iteration 17959: 0.04203853121197471\n",
      "Train Loss at iteration 17960: 0.042038515128198346\n",
      "Train Loss at iteration 17961: 0.04203849904708356\n",
      "Train Loss at iteration 17962: 0.04203848296862994\n",
      "Train Loss at iteration 17963: 0.04203846689283702\n",
      "Train Loss at iteration 17964: 0.04203845081970438\n",
      "Train Loss at iteration 17965: 0.042038434749231565\n",
      "Train Loss at iteration 17966: 0.042038418681418124\n",
      "Train Loss at iteration 17967: 0.04203840261626365\n",
      "Train Loss at iteration 17968: 0.042038386553767675\n",
      "Train Loss at iteration 17969: 0.042038370493929784\n",
      "Train Loss at iteration 17970: 0.042038354436749505\n",
      "Train Loss at iteration 17971: 0.042038338382226426\n",
      "Train Loss at iteration 17972: 0.04203832233036009\n",
      "Train Loss at iteration 17973: 0.042038306281150066\n",
      "Train Loss at iteration 17974: 0.0420382902345959\n",
      "Train Loss at iteration 17975: 0.042038274190697174\n",
      "Train Loss at iteration 17976: 0.04203825814945343\n",
      "Train Loss at iteration 17977: 0.042038242110864245\n",
      "Train Loss at iteration 17978: 0.04203822607492917\n",
      "Train Loss at iteration 17979: 0.042038210041647775\n",
      "Train Loss at iteration 17980: 0.042038194011019596\n",
      "Train Loss at iteration 17981: 0.04203817798304422\n",
      "Train Loss at iteration 17982: 0.0420381619577212\n",
      "Train Loss at iteration 17983: 0.042038145935050085\n",
      "Train Loss at iteration 17984: 0.042038129915030455\n",
      "Train Loss at iteration 17985: 0.04203811389766186\n",
      "Train Loss at iteration 17986: 0.04203809788294386\n",
      "Train Loss at iteration 17987: 0.04203808187087603\n",
      "Train Loss at iteration 17988: 0.04203806586145791\n",
      "Train Loss at iteration 17989: 0.042038049854689084\n",
      "Train Loss at iteration 17990: 0.04203803385056909\n",
      "Train Loss at iteration 17991: 0.04203801784909751\n",
      "Train Loss at iteration 17992: 0.0420380018502739\n",
      "Train Loss at iteration 17993: 0.04203798585409781\n",
      "Train Loss at iteration 17994: 0.04203796986056881\n",
      "Train Loss at iteration 17995: 0.04203795386968647\n",
      "Train Loss at iteration 17996: 0.042037937881450334\n",
      "Train Loss at iteration 17997: 0.04203792189585999\n",
      "Train Loss at iteration 17998: 0.04203790591291498\n",
      "Train Loss at iteration 17999: 0.04203788993261486\n",
      "Train Loss at iteration 18000: 0.042037873954959205\n",
      "Train Loss at iteration 18001: 0.042037857979947586\n",
      "Train Loss at iteration 18002: 0.04203784200757955\n",
      "Train Loss at iteration 18003: 0.04203782603785465\n",
      "Train Loss at iteration 18004: 0.04203781007077249\n",
      "Train Loss at iteration 18005: 0.042037794106332585\n",
      "Train Loss at iteration 18006: 0.04203777814453452\n",
      "Train Loss at iteration 18007: 0.04203776218537785\n",
      "Train Loss at iteration 18008: 0.04203774622886215\n",
      "Train Loss at iteration 18009: 0.04203773027498697\n",
      "Train Loss at iteration 18010: 0.04203771432375189\n",
      "Train Loss at iteration 18011: 0.04203769837515645\n",
      "Train Loss at iteration 18012: 0.04203768242920024\n",
      "Train Loss at iteration 18013: 0.04203766648588279\n",
      "Train Loss at iteration 18014: 0.04203765054520369\n",
      "Train Loss at iteration 18015: 0.04203763460716249\n",
      "Train Loss at iteration 18016: 0.04203761867175877\n",
      "Train Loss at iteration 18017: 0.04203760273899206\n",
      "Train Loss at iteration 18018: 0.04203758680886197\n",
      "Train Loss at iteration 18019: 0.04203757088136802\n",
      "Train Loss at iteration 18020: 0.0420375549565098\n",
      "Train Loss at iteration 18021: 0.04203753903428686\n",
      "Train Loss at iteration 18022: 0.04203752311469877\n",
      "Train Loss at iteration 18023: 0.04203750719774509\n",
      "Train Loss at iteration 18024: 0.0420374912834254\n",
      "Train Loss at iteration 18025: 0.04203747537173925\n",
      "Train Loss at iteration 18026: 0.04203745946268618\n",
      "Train Loss at iteration 18027: 0.0420374435562658\n",
      "Train Loss at iteration 18028: 0.042037427652477644\n",
      "Train Loss at iteration 18029: 0.0420374117513213\n",
      "Train Loss at iteration 18030: 0.04203739585279631\n",
      "Train Loss at iteration 18031: 0.04203737995690226\n",
      "Train Loss at iteration 18032: 0.042037364063638685\n",
      "Train Loss at iteration 18033: 0.04203734817300517\n",
      "Train Loss at iteration 18034: 0.042037332285001276\n",
      "Train Loss at iteration 18035: 0.042037316399626566\n",
      "Train Loss at iteration 18036: 0.0420373005168806\n",
      "Train Loss at iteration 18037: 0.04203728463676296\n",
      "Train Loss at iteration 18038: 0.042037268759273196\n",
      "Train Loss at iteration 18039: 0.04203725288441088\n",
      "Train Loss at iteration 18040: 0.042037237012175564\n",
      "Train Loss at iteration 18041: 0.04203722114256683\n",
      "Train Loss at iteration 18042: 0.042037205275584244\n",
      "Train Loss at iteration 18043: 0.042037189411227346\n",
      "Train Loss at iteration 18044: 0.04203717354949573\n",
      "Train Loss at iteration 18045: 0.04203715769038895\n",
      "Train Loss at iteration 18046: 0.042037141833906576\n",
      "Train Loss at iteration 18047: 0.04203712598004817\n",
      "Train Loss at iteration 18048: 0.04203711012881329\n",
      "Train Loss at iteration 18049: 0.0420370942802015\n",
      "Train Loss at iteration 18050: 0.04203707843421239\n",
      "Train Loss at iteration 18051: 0.04203706259084549\n",
      "Train Loss at iteration 18052: 0.042037046750100414\n",
      "Train Loss at iteration 18053: 0.04203703091197669\n",
      "Train Loss at iteration 18054: 0.04203701507647389\n",
      "Train Loss at iteration 18055: 0.04203699924359158\n",
      "Train Loss at iteration 18056: 0.04203698341332935\n",
      "Train Loss at iteration 18057: 0.04203696758568674\n",
      "Train Loss at iteration 18058: 0.04203695176066331\n",
      "Train Loss at iteration 18059: 0.04203693593825865\n",
      "Train Loss at iteration 18060: 0.04203692011847232\n",
      "Train Loss at iteration 18061: 0.04203690430130387\n",
      "Train Loss at iteration 18062: 0.0420368884867529\n",
      "Train Loss at iteration 18063: 0.04203687267481895\n",
      "Train Loss at iteration 18064: 0.042036856865501585\n",
      "Train Loss at iteration 18065: 0.04203684105880038\n",
      "Train Loss at iteration 18066: 0.042036825254714916\n",
      "Train Loss at iteration 18067: 0.04203680945324474\n",
      "Train Loss at iteration 18068: 0.04203679365438942\n",
      "Train Loss at iteration 18069: 0.04203677785814853\n",
      "Train Loss at iteration 18070: 0.042036762064521636\n",
      "Train Loss at iteration 18071: 0.04203674627350831\n",
      "Train Loss at iteration 18072: 0.042036730485108115\n",
      "Train Loss at iteration 18073: 0.04203671469932062\n",
      "Train Loss at iteration 18074: 0.04203669891614539\n",
      "Train Loss at iteration 18075: 0.04203668313558199\n",
      "Train Loss at iteration 18076: 0.04203666735762998\n",
      "Train Loss at iteration 18077: 0.042036651582288954\n",
      "Train Loss at iteration 18078: 0.04203663580955846\n",
      "Train Loss at iteration 18079: 0.04203662003943807\n",
      "Train Loss at iteration 18080: 0.042036604271927354\n",
      "Train Loss at iteration 18081: 0.04203658850702587\n",
      "Train Loss at iteration 18082: 0.04203657274473321\n",
      "Train Loss at iteration 18083: 0.04203655698504891\n",
      "Train Loss at iteration 18084: 0.04203654122797257\n",
      "Train Loss at iteration 18085: 0.042036525473503734\n",
      "Train Loss at iteration 18086: 0.04203650972164197\n",
      "Train Loss at iteration 18087: 0.04203649397238689\n",
      "Train Loss at iteration 18088: 0.04203647822573799\n",
      "Train Loss at iteration 18089: 0.0420364624816949\n",
      "Train Loss at iteration 18090: 0.04203644674025715\n",
      "Train Loss at iteration 18091: 0.04203643100142433\n",
      "Train Loss at iteration 18092: 0.04203641526519602\n",
      "Train Loss at iteration 18093: 0.042036399531571754\n",
      "Train Loss at iteration 18094: 0.04203638380055113\n",
      "Train Loss at iteration 18095: 0.04203636807213369\n",
      "Train Loss at iteration 18096: 0.04203635234631904\n",
      "Train Loss at iteration 18097: 0.04203633662310672\n",
      "Train Loss at iteration 18098: 0.04203632090249631\n",
      "Train Loss at iteration 18099: 0.042036305184487366\n",
      "Train Loss at iteration 18100: 0.04203628946907948\n",
      "Train Loss at iteration 18101: 0.042036273756272204\n",
      "Train Loss at iteration 18102: 0.042036258046065124\n",
      "Train Loss at iteration 18103: 0.042036242338457784\n",
      "Train Loss at iteration 18104: 0.04203622663344978\n",
      "Train Loss at iteration 18105: 0.04203621093104066\n",
      "Train Loss at iteration 18106: 0.04203619523123001\n",
      "Train Loss at iteration 18107: 0.0420361795340174\n",
      "Train Loss at iteration 18108: 0.042036163839402396\n",
      "Train Loss at iteration 18109: 0.04203614814738455\n",
      "Train Loss at iteration 18110: 0.042036132457963456\n",
      "Train Loss at iteration 18111: 0.042036116771138686\n",
      "Train Loss at iteration 18112: 0.04203610108690979\n",
      "Train Loss at iteration 18113: 0.04203608540527635\n",
      "Train Loss at iteration 18114: 0.04203606972623793\n",
      "Train Loss at iteration 18115: 0.04203605404979412\n",
      "Train Loss at iteration 18116: 0.04203603837594448\n",
      "Train Loss at iteration 18117: 0.042036022704688564\n",
      "Train Loss at iteration 18118: 0.04203600703602596\n",
      "Train Loss at iteration 18119: 0.04203599136995623\n",
      "Train Loss at iteration 18120: 0.04203597570647896\n",
      "Train Loss at iteration 18121: 0.0420359600455937\n",
      "Train Loss at iteration 18122: 0.04203594438730004\n",
      "Train Loss at iteration 18123: 0.04203592873159754\n",
      "Train Loss at iteration 18124: 0.04203591307848576\n",
      "Train Loss at iteration 18125: 0.0420358974279643\n",
      "Train Loss at iteration 18126: 0.04203588178003271\n",
      "Train Loss at iteration 18127: 0.04203586613469056\n",
      "Train Loss at iteration 18128: 0.042035850491937435\n",
      "Train Loss at iteration 18129: 0.0420358348517729\n",
      "Train Loss at iteration 18130: 0.04203581921419654\n",
      "Train Loss at iteration 18131: 0.04203580357920789\n",
      "Train Loss at iteration 18132: 0.04203578794680656\n",
      "Train Loss at iteration 18133: 0.0420357723169921\n",
      "Train Loss at iteration 18134: 0.04203575668976409\n",
      "Train Loss at iteration 18135: 0.0420357410651221\n",
      "Train Loss at iteration 18136: 0.0420357254430657\n",
      "Train Loss at iteration 18137: 0.042035709823594454\n",
      "Train Loss at iteration 18138: 0.04203569420670796\n",
      "Train Loss at iteration 18139: 0.04203567859240577\n",
      "Train Loss at iteration 18140: 0.042035662980687456\n",
      "Train Loss at iteration 18141: 0.0420356473715526\n",
      "Train Loss at iteration 18142: 0.04203563176500077\n",
      "Train Loss at iteration 18143: 0.04203561616103153\n",
      "Train Loss at iteration 18144: 0.042035600559644476\n",
      "Train Loss at iteration 18145: 0.042035584960839145\n",
      "Train Loss at iteration 18146: 0.042035569364615145\n",
      "Train Loss at iteration 18147: 0.042035553770972026\n",
      "Train Loss at iteration 18148: 0.04203553817990936\n",
      "Train Loss at iteration 18149: 0.04203552259142673\n",
      "Train Loss at iteration 18150: 0.04203550700552372\n",
      "Train Loss at iteration 18151: 0.04203549142219989\n",
      "Train Loss at iteration 18152: 0.0420354758414548\n",
      "Train Loss at iteration 18153: 0.04203546026328805\n",
      "Train Loss at iteration 18154: 0.04203544468769918\n",
      "Train Loss at iteration 18155: 0.0420354291146878\n",
      "Train Loss at iteration 18156: 0.04203541354425346\n",
      "Train Loss at iteration 18157: 0.04203539797639574\n",
      "Train Loss at iteration 18158: 0.042035382411114215\n",
      "Train Loss at iteration 18159: 0.04203536684840845\n",
      "Train Loss at iteration 18160: 0.04203535128827802\n",
      "Train Loss at iteration 18161: 0.04203533573072251\n",
      "Train Loss at iteration 18162: 0.042035320175741506\n",
      "Train Loss at iteration 18163: 0.042035304623334546\n",
      "Train Loss at iteration 18164: 0.04203528907350123\n",
      "Train Loss at iteration 18165: 0.0420352735262411\n",
      "Train Loss at iteration 18166: 0.04203525798155377\n",
      "Train Loss at iteration 18167: 0.042035242439438805\n",
      "Train Loss at iteration 18168: 0.04203522689989575\n",
      "Train Loss at iteration 18169: 0.04203521136292422\n",
      "Train Loss at iteration 18170: 0.04203519582852376\n",
      "Train Loss at iteration 18171: 0.042035180296693966\n",
      "Train Loss at iteration 18172: 0.042035164767434394\n",
      "Train Loss at iteration 18173: 0.042035149240744626\n",
      "Train Loss at iteration 18174: 0.04203513371662424\n",
      "Train Loss at iteration 18175: 0.0420351181950728\n",
      "Train Loss at iteration 18176: 0.0420351026760899\n",
      "Train Loss at iteration 18177: 0.0420350871596751\n",
      "Train Loss at iteration 18178: 0.04203507164582797\n",
      "Train Loss at iteration 18179: 0.042035056134548096\n",
      "Train Loss at iteration 18180: 0.04203504062583505\n",
      "Train Loss at iteration 18181: 0.042035025119688416\n",
      "Train Loss at iteration 18182: 0.042035009616107746\n",
      "Train Loss at iteration 18183: 0.04203499411509263\n",
      "Train Loss at iteration 18184: 0.042034978616642654\n",
      "Train Loss at iteration 18185: 0.04203496312075737\n",
      "Train Loss at iteration 18186: 0.04203494762743638\n",
      "Train Loss at iteration 18187: 0.04203493213667924\n",
      "Train Loss at iteration 18188: 0.04203491664848552\n",
      "Train Loss at iteration 18189: 0.04203490116285481\n",
      "Train Loss at iteration 18190: 0.04203488567978669\n",
      "Train Loss at iteration 18191: 0.04203487019928073\n",
      "Train Loss at iteration 18192: 0.0420348547213365\n",
      "Train Loss at iteration 18193: 0.04203483924595359\n",
      "Train Loss at iteration 18194: 0.04203482377313155\n",
      "Train Loss at iteration 18195: 0.04203480830286997\n",
      "Train Loss at iteration 18196: 0.04203479283516844\n",
      "Train Loss at iteration 18197: 0.04203477737002653\n",
      "Train Loss at iteration 18198: 0.0420347619074438\n",
      "Train Loss at iteration 18199: 0.04203474644741984\n",
      "Train Loss at iteration 18200: 0.042034730989954215\n",
      "Train Loss at iteration 18201: 0.04203471553504653\n",
      "Train Loss at iteration 18202: 0.04203470008269633\n",
      "Train Loss at iteration 18203: 0.0420346846329032\n",
      "Train Loss at iteration 18204: 0.04203466918566672\n",
      "Train Loss at iteration 18205: 0.04203465374098647\n",
      "Train Loss at iteration 18206: 0.04203463829886203\n",
      "Train Loss at iteration 18207: 0.04203462285929297\n",
      "Train Loss at iteration 18208: 0.042034607422278866\n",
      "Train Loss at iteration 18209: 0.0420345919878193\n",
      "Train Loss at iteration 18210: 0.04203457655591385\n",
      "Train Loss at iteration 18211: 0.042034561126562085\n",
      "Train Loss at iteration 18212: 0.04203454569976357\n",
      "Train Loss at iteration 18213: 0.04203453027551792\n",
      "Train Loss at iteration 18214: 0.04203451485382467\n",
      "Train Loss at iteration 18215: 0.04203449943468345\n",
      "Train Loss at iteration 18216: 0.0420344840180938\n",
      "Train Loss at iteration 18217: 0.04203446860405528\n",
      "Train Loss at iteration 18218: 0.042034453192567524\n",
      "Train Loss at iteration 18219: 0.04203443778363005\n",
      "Train Loss at iteration 18220: 0.04203442237724248\n",
      "Train Loss at iteration 18221: 0.04203440697340437\n",
      "Train Loss at iteration 18222: 0.04203439157211532\n",
      "Train Loss at iteration 18223: 0.042034376173374875\n",
      "Train Loss at iteration 18224: 0.04203436077718263\n",
      "Train Loss at iteration 18225: 0.04203434538353817\n",
      "Train Loss at iteration 18226: 0.04203432999244106\n",
      "Train Loss at iteration 18227: 0.0420343146038909\n",
      "Train Loss at iteration 18228: 0.04203429921788723\n",
      "Train Loss at iteration 18229: 0.04203428383442967\n",
      "Train Loss at iteration 18230: 0.042034268453517765\n",
      "Train Loss at iteration 18231: 0.04203425307515113\n",
      "Train Loss at iteration 18232: 0.04203423769932931\n",
      "Train Loss at iteration 18233: 0.0420342223260519\n",
      "Train Loss at iteration 18234: 0.04203420695531847\n",
      "Train Loss at iteration 18235: 0.04203419158712861\n",
      "Train Loss at iteration 18236: 0.042034176221481895\n",
      "Train Loss at iteration 18237: 0.0420341608583779\n",
      "Train Loss at iteration 18238: 0.04203414549781619\n",
      "Train Loss at iteration 18239: 0.04203413013979638\n",
      "Train Loss at iteration 18240: 0.042034114784318025\n",
      "Train Loss at iteration 18241: 0.04203409943138071\n",
      "Train Loss at iteration 18242: 0.04203408408098402\n",
      "Train Loss at iteration 18243: 0.04203406873312751\n",
      "Train Loss at iteration 18244: 0.0420340533878108\n",
      "Train Loss at iteration 18245: 0.04203403804503343\n",
      "Train Loss at iteration 18246: 0.042034022704795004\n",
      "Train Loss at iteration 18247: 0.04203400736709509\n",
      "Train Loss at iteration 18248: 0.042033992031933275\n",
      "Train Loss at iteration 18249: 0.042033976699309146\n",
      "Train Loss at iteration 18250: 0.04203396136922226\n",
      "Train Loss at iteration 18251: 0.04203394604167223\n",
      "Train Loss at iteration 18252: 0.04203393071665859\n",
      "Train Loss at iteration 18253: 0.042033915394180964\n",
      "Train Loss at iteration 18254: 0.042033900074238906\n",
      "Train Loss at iteration 18255: 0.042033884756832014\n",
      "Train Loss at iteration 18256: 0.04203386944195985\n",
      "Train Loss at iteration 18257: 0.04203385412962202\n",
      "Train Loss at iteration 18258: 0.04203383881981807\n",
      "Train Loss at iteration 18259: 0.042033823512547606\n",
      "Train Loss at iteration 18260: 0.0420338082078102\n",
      "Train Loss at iteration 18261: 0.042033792905605445\n",
      "Train Loss at iteration 18262: 0.0420337776059329\n",
      "Train Loss at iteration 18263: 0.04203376230879217\n",
      "Train Loss at iteration 18264: 0.04203374701418281\n",
      "Train Loss at iteration 18265: 0.04203373172210442\n",
      "Train Loss at iteration 18266: 0.04203371643255657\n",
      "Train Loss at iteration 18267: 0.04203370114553886\n",
      "Train Loss at iteration 18268: 0.04203368586105084\n",
      "Train Loss at iteration 18269: 0.04203367057909212\n",
      "Train Loss at iteration 18270: 0.04203365529966226\n",
      "Train Loss at iteration 18271: 0.04203364002276087\n",
      "Train Loss at iteration 18272: 0.042033624748387496\n",
      "Train Loss at iteration 18273: 0.04203360947654173\n",
      "Train Loss at iteration 18274: 0.042033594207223184\n",
      "Train Loss at iteration 18275: 0.0420335789404314\n",
      "Train Loss at iteration 18276: 0.04203356367616599\n",
      "Train Loss at iteration 18277: 0.042033548414426504\n",
      "Train Loss at iteration 18278: 0.042033533155212556\n",
      "Train Loss at iteration 18279: 0.04203351789852371\n",
      "Train Loss at iteration 18280: 0.04203350264435954\n",
      "Train Loss at iteration 18281: 0.04203348739271965\n",
      "Train Loss at iteration 18282: 0.042033472143603604\n",
      "Train Loss at iteration 18283: 0.04203345689701101\n",
      "Train Loss at iteration 18284: 0.04203344165294142\n",
      "Train Loss at iteration 18285: 0.042033426411394424\n",
      "Train Loss at iteration 18286: 0.042033411172369625\n",
      "Train Loss at iteration 18287: 0.04203339593586659\n",
      "Train Loss at iteration 18288: 0.04203338070188488\n",
      "Train Loss at iteration 18289: 0.04203336547042411\n",
      "Train Loss at iteration 18290: 0.04203335024148386\n",
      "Train Loss at iteration 18291: 0.04203333501506369\n",
      "Train Loss at iteration 18292: 0.042033319791163204\n",
      "Train Loss at iteration 18293: 0.04203330456978197\n",
      "Train Loss at iteration 18294: 0.0420332893509196\n",
      "Train Loss at iteration 18295: 0.042033274134575645\n",
      "Train Loss at iteration 18296: 0.042033258920749696\n",
      "Train Loss at iteration 18297: 0.042033243709441344\n",
      "Train Loss at iteration 18298: 0.04203322850065017\n",
      "Train Loss at iteration 18299: 0.04203321329437575\n",
      "Train Loss at iteration 18300: 0.04203319809061768\n",
      "Train Loss at iteration 18301: 0.04203318288937553\n",
      "Train Loss at iteration 18302: 0.04203316769064888\n",
      "Train Loss at iteration 18303: 0.04203315249443735\n",
      "Train Loss at iteration 18304: 0.04203313730074048\n",
      "Train Loss at iteration 18305: 0.042033122109557876\n",
      "Train Loss at iteration 18306: 0.042033106920889116\n",
      "Train Loss at iteration 18307: 0.042033091734733775\n",
      "Train Loss at iteration 18308: 0.04203307655109146\n",
      "Train Loss at iteration 18309: 0.04203306136996174\n",
      "Train Loss at iteration 18310: 0.0420330461913442\n",
      "Train Loss at iteration 18311: 0.04203303101523843\n",
      "Train Loss at iteration 18312: 0.042033015841644\n",
      "Train Loss at iteration 18313: 0.04203300067056051\n",
      "Train Loss at iteration 18314: 0.04203298550198755\n",
      "Train Loss at iteration 18315: 0.042032970335924666\n",
      "Train Loss at iteration 18316: 0.04203295517237148\n",
      "Train Loss at iteration 18317: 0.04203294001132757\n",
      "Train Loss at iteration 18318: 0.042032924852792516\n",
      "Train Loss at iteration 18319: 0.042032909696765895\n",
      "Train Loss at iteration 18320: 0.042032894543247316\n",
      "Train Loss at iteration 18321: 0.04203287939223633\n",
      "Train Loss at iteration 18322: 0.042032864243732544\n",
      "Train Loss at iteration 18323: 0.042032849097735546\n",
      "Train Loss at iteration 18324: 0.04203283395424492\n",
      "Train Loss at iteration 18325: 0.04203281881326022\n",
      "Train Loss at iteration 18326: 0.04203280367478107\n",
      "Train Loss at iteration 18327: 0.04203278853880705\n",
      "Train Loss at iteration 18328: 0.04203277340533774\n",
      "Train Loss at iteration 18329: 0.0420327582743727\n",
      "Train Loss at iteration 18330: 0.042032743145911565\n",
      "Train Loss at iteration 18331: 0.04203272801995386\n",
      "Train Loss at iteration 18332: 0.04203271289649922\n",
      "Train Loss at iteration 18333: 0.04203269777554723\n",
      "Train Loss at iteration 18334: 0.04203268265709744\n",
      "Train Loss at iteration 18335: 0.042032667541149475\n",
      "Train Loss at iteration 18336: 0.04203265242770289\n",
      "Train Loss at iteration 18337: 0.04203263731675728\n",
      "Train Loss at iteration 18338: 0.04203262220831224\n",
      "Train Loss at iteration 18339: 0.04203260710236734\n",
      "Train Loss at iteration 18340: 0.042032591998922196\n",
      "Train Loss at iteration 18341: 0.042032576897976355\n",
      "Train Loss at iteration 18342: 0.04203256179952943\n",
      "Train Loss at iteration 18343: 0.04203254670358101\n",
      "Train Loss at iteration 18344: 0.04203253161013066\n",
      "Train Loss at iteration 18345: 0.04203251651917798\n",
      "Train Loss at iteration 18346: 0.04203250143072257\n",
      "Train Loss at iteration 18347: 0.04203248634476399\n",
      "Train Loss at iteration 18348: 0.04203247126130183\n",
      "Train Loss at iteration 18349: 0.0420324561803357\n",
      "Train Loss at iteration 18350: 0.04203244110186515\n",
      "Train Loss at iteration 18351: 0.04203242602588982\n",
      "Train Loss at iteration 18352: 0.04203241095240924\n",
      "Train Loss at iteration 18353: 0.04203239588142304\n",
      "Train Loss at iteration 18354: 0.04203238081293078\n",
      "Train Loss at iteration 18355: 0.042032365746932074\n",
      "Train Loss at iteration 18356: 0.04203235068342647\n",
      "Train Loss at iteration 18357: 0.04203233562241359\n",
      "Train Loss at iteration 18358: 0.04203232056389301\n",
      "Train Loss at iteration 18359: 0.042032305507864305\n",
      "Train Loss at iteration 18360: 0.04203229045432708\n",
      "Train Loss at iteration 18361: 0.04203227540328092\n",
      "Train Loss at iteration 18362: 0.042032260354725424\n",
      "Train Loss at iteration 18363: 0.04203224530866014\n",
      "Train Loss at iteration 18364: 0.042032230265084694\n",
      "Train Loss at iteration 18365: 0.04203221522399866\n",
      "Train Loss at iteration 18366: 0.04203220018540162\n",
      "Train Loss at iteration 18367: 0.04203218514929319\n",
      "Train Loss at iteration 18368: 0.04203217011567292\n",
      "Train Loss at iteration 18369: 0.042032155084540425\n",
      "Train Loss at iteration 18370: 0.04203214005589526\n",
      "Train Loss at iteration 18371: 0.04203212502973707\n",
      "Train Loss at iteration 18372: 0.04203211000606539\n",
      "Train Loss at iteration 18373: 0.04203209498487983\n",
      "Train Loss at iteration 18374: 0.04203207996617998\n",
      "Train Loss at iteration 18375: 0.04203206494996543\n",
      "Train Loss at iteration 18376: 0.042032049936235764\n",
      "Train Loss at iteration 18377: 0.042032034924990566\n",
      "Train Loss at iteration 18378: 0.04203201991622944\n",
      "Train Loss at iteration 18379: 0.04203200490995196\n",
      "Train Loss at iteration 18380: 0.042031989906157724\n",
      "Train Loss at iteration 18381: 0.0420319749048463\n",
      "Train Loss at iteration 18382: 0.0420319599060173\n",
      "Train Loss at iteration 18383: 0.042031944909670306\n",
      "Train Loss at iteration 18384: 0.04203192991580492\n",
      "Train Loss at iteration 18385: 0.04203191492442072\n",
      "Train Loss at iteration 18386: 0.04203189993551727\n",
      "Train Loss at iteration 18387: 0.04203188494909421\n",
      "Train Loss at iteration 18388: 0.04203186996515109\n",
      "Train Loss at iteration 18389: 0.04203185498368751\n",
      "Train Loss at iteration 18390: 0.04203184000470308\n",
      "Train Loss at iteration 18391: 0.042031825028197356\n",
      "Train Loss at iteration 18392: 0.04203181005416995\n",
      "Train Loss at iteration 18393: 0.04203179508262045\n",
      "Train Loss at iteration 18394: 0.04203178011354843\n",
      "Train Loss at iteration 18395: 0.042031765146953504\n",
      "Train Loss at iteration 18396: 0.04203175018283524\n",
      "Train Loss at iteration 18397: 0.04203173522119324\n",
      "Train Loss at iteration 18398: 0.042031720262027096\n",
      "Train Loss at iteration 18399: 0.04203170530533639\n",
      "Train Loss at iteration 18400: 0.042031690351120726\n",
      "Train Loss at iteration 18401: 0.04203167539937968\n",
      "Train Loss at iteration 18402: 0.042031660450112834\n",
      "Train Loss at iteration 18403: 0.0420316455033198\n",
      "Train Loss at iteration 18404: 0.04203163055900017\n",
      "Train Loss at iteration 18405: 0.04203161561715352\n",
      "Train Loss at iteration 18406: 0.042031600677779425\n",
      "Train Loss at iteration 18407: 0.042031585740877515\n",
      "Train Loss at iteration 18408: 0.04203157080644736\n",
      "Train Loss at iteration 18409: 0.042031555874488556\n",
      "Train Loss at iteration 18410: 0.04203154094500068\n",
      "Train Loss at iteration 18411: 0.04203152601798334\n",
      "Train Loss at iteration 18412: 0.04203151109343613\n",
      "Train Loss at iteration 18413: 0.042031496171358616\n",
      "Train Loss at iteration 18414: 0.042031481251750415\n",
      "Train Loss at iteration 18415: 0.0420314663346111\n",
      "Train Loss at iteration 18416: 0.04203145141994029\n",
      "Train Loss at iteration 18417: 0.04203143650773753\n",
      "Train Loss at iteration 18418: 0.04203142159800245\n",
      "Train Loss at iteration 18419: 0.042031406690734636\n",
      "Train Loss at iteration 18420: 0.042031391785933674\n",
      "Train Loss at iteration 18421: 0.04203137688359915\n",
      "Train Loss at iteration 18422: 0.04203136198373066\n",
      "Train Loss at iteration 18423: 0.042031347086327796\n",
      "Train Loss at iteration 18424: 0.04203133219139016\n",
      "Train Loss at iteration 18425: 0.042031317298917334\n",
      "Train Loss at iteration 18426: 0.042031302408908906\n",
      "Train Loss at iteration 18427: 0.042031287521364466\n",
      "Train Loss at iteration 18428: 0.042031272636283624\n",
      "Train Loss at iteration 18429: 0.042031257753665965\n",
      "Train Loss at iteration 18430: 0.04203124287351107\n",
      "Train Loss at iteration 18431: 0.04203122799581854\n",
      "Train Loss at iteration 18432: 0.04203121312058798\n",
      "Train Loss at iteration 18433: 0.04203119824781895\n",
      "Train Loss at iteration 18434: 0.04203118337751107\n",
      "Train Loss at iteration 18435: 0.04203116850966392\n",
      "Train Loss at iteration 18436: 0.042031153644277106\n",
      "Train Loss at iteration 18437: 0.04203113878135021\n",
      "Train Loss at iteration 18438: 0.04203112392088282\n",
      "Train Loss at iteration 18439: 0.04203110906287453\n",
      "Train Loss at iteration 18440: 0.042031094207324954\n",
      "Train Loss at iteration 18441: 0.04203107935423367\n",
      "Train Loss at iteration 18442: 0.04203106450360027\n",
      "Train Loss at iteration 18443: 0.04203104965542434\n",
      "Train Loss at iteration 18444: 0.04203103480970548\n",
      "Train Loss at iteration 18445: 0.042031019966443296\n",
      "Train Loss at iteration 18446: 0.04203100512563736\n",
      "Train Loss at iteration 18447: 0.042030990287287275\n",
      "Train Loss at iteration 18448: 0.04203097545139263\n",
      "Train Loss at iteration 18449: 0.04203096061795304\n",
      "Train Loss at iteration 18450: 0.042030945786968074\n",
      "Train Loss at iteration 18451: 0.04203093095843733\n",
      "Train Loss at iteration 18452: 0.04203091613236041\n",
      "Train Loss at iteration 18453: 0.0420309013087369\n",
      "Train Loss at iteration 18454: 0.0420308864875664\n",
      "Train Loss at iteration 18455: 0.04203087166884851\n",
      "Train Loss at iteration 18456: 0.0420308568525828\n",
      "Train Loss at iteration 18457: 0.04203084203876889\n",
      "Train Loss at iteration 18458: 0.04203082722740636\n",
      "Train Loss at iteration 18459: 0.04203081241849481\n",
      "Train Loss at iteration 18460: 0.04203079761203382\n",
      "Train Loss at iteration 18461: 0.04203078280802301\n",
      "Train Loss at iteration 18462: 0.04203076800646196\n",
      "Train Loss at iteration 18463: 0.04203075320735026\n",
      "Train Loss at iteration 18464: 0.042030738410687524\n",
      "Train Loss at iteration 18465: 0.04203072361647332\n",
      "Train Loss at iteration 18466: 0.04203070882470726\n",
      "Train Loss at iteration 18467: 0.042030694035388935\n",
      "Train Loss at iteration 18468: 0.04203067924851793\n",
      "Train Loss at iteration 18469: 0.04203066446409386\n",
      "Train Loss at iteration 18470: 0.04203064968211631\n",
      "Train Loss at iteration 18471: 0.042030634902584865\n",
      "Train Loss at iteration 18472: 0.042030620125499144\n",
      "Train Loss at iteration 18473: 0.042030605350858716\n",
      "Train Loss at iteration 18474: 0.042030590578663186\n",
      "Train Loss at iteration 18475: 0.04203057580891216\n",
      "Train Loss at iteration 18476: 0.04203056104160522\n",
      "Train Loss at iteration 18477: 0.042030546276741965\n",
      "Train Loss at iteration 18478: 0.04203053151432199\n",
      "Train Loss at iteration 18479: 0.0420305167543449\n",
      "Train Loss at iteration 18480: 0.04203050199681028\n",
      "Train Loss at iteration 18481: 0.04203048724171773\n",
      "Train Loss at iteration 18482: 0.04203047248906684\n",
      "Train Loss at iteration 18483: 0.04203045773885722\n",
      "Train Loss at iteration 18484: 0.04203044299108845\n",
      "Train Loss at iteration 18485: 0.04203042824576012\n",
      "Train Loss at iteration 18486: 0.04203041350287186\n",
      "Train Loss at iteration 18487: 0.04203039876242323\n",
      "Train Loss at iteration 18488: 0.04203038402441384\n",
      "Train Loss at iteration 18489: 0.0420303692888433\n",
      "Train Loss at iteration 18490: 0.042030354555711184\n",
      "Train Loss at iteration 18491: 0.042030339825017096\n",
      "Train Loss at iteration 18492: 0.04203032509676064\n",
      "Train Loss at iteration 18493: 0.0420303103709414\n",
      "Train Loss at iteration 18494: 0.04203029564755899\n",
      "Train Loss at iteration 18495: 0.04203028092661298\n",
      "Train Loss at iteration 18496: 0.042030266208103\n",
      "Train Loss at iteration 18497: 0.04203025149202862\n",
      "Train Loss at iteration 18498: 0.04203023677838944\n",
      "Train Loss at iteration 18499: 0.04203022206718507\n",
      "Train Loss at iteration 18500: 0.042030207358415096\n",
      "Train Loss at iteration 18501: 0.042030192652079135\n",
      "Train Loss at iteration 18502: 0.042030177948176754\n",
      "Train Loss at iteration 18503: 0.04203016324670757\n",
      "Train Loss at iteration 18504: 0.042030148547671176\n",
      "Train Loss at iteration 18505: 0.042030133851067165\n",
      "Train Loss at iteration 18506: 0.04203011915689514\n",
      "Train Loss at iteration 18507: 0.0420301044651547\n",
      "Train Loss at iteration 18508: 0.04203008977584543\n",
      "Train Loss at iteration 18509: 0.042030075088966945\n",
      "Train Loss at iteration 18510: 0.04203006040451881\n",
      "Train Loss at iteration 18511: 0.042030045722500686\n",
      "Train Loss at iteration 18512: 0.04203003104291209\n",
      "Train Loss at iteration 18513: 0.042030016365752695\n",
      "Train Loss at iteration 18514: 0.042030001691022056\n",
      "Train Loss at iteration 18515: 0.042029987018719774\n",
      "Train Loss at iteration 18516: 0.04202997234884546\n",
      "Train Loss at iteration 18517: 0.0420299576813987\n",
      "Train Loss at iteration 18518: 0.04202994301637911\n",
      "Train Loss at iteration 18519: 0.04202992835378626\n",
      "Train Loss at iteration 18520: 0.042029913693619775\n",
      "Train Loss at iteration 18521: 0.04202989903587923\n",
      "Train Loss at iteration 18522: 0.04202988438056425\n",
      "Train Loss at iteration 18523: 0.04202986972767442\n",
      "Train Loss at iteration 18524: 0.04202985507720933\n",
      "Train Loss at iteration 18525: 0.04202984042916858\n",
      "Train Loss at iteration 18526: 0.04202982578355179\n",
      "Train Loss at iteration 18527: 0.042029811140358544\n",
      "Train Loss at iteration 18528: 0.04202979649958843\n",
      "Train Loss at iteration 18529: 0.042029781861241074\n",
      "Train Loss at iteration 18530: 0.04202976722531605\n",
      "Train Loss at iteration 18531: 0.04202975259181298\n",
      "Train Loss at iteration 18532: 0.04202973796073143\n",
      "Train Loss at iteration 18533: 0.04202972333207103\n",
      "Train Loss at iteration 18534: 0.04202970870583137\n",
      "Train Loss at iteration 18535: 0.042029694082012035\n",
      "Train Loss at iteration 18536: 0.04202967946061265\n",
      "Train Loss at iteration 18537: 0.04202966484163279\n",
      "Train Loss at iteration 18538: 0.04202965022507208\n",
      "Train Loss at iteration 18539: 0.042029635610930094\n",
      "Train Loss at iteration 18540: 0.04202962099920644\n",
      "Train Loss at iteration 18541: 0.042029606389900734\n",
      "Train Loss at iteration 18542: 0.04202959178301255\n",
      "Train Loss at iteration 18543: 0.04202957717854152\n",
      "Train Loss at iteration 18544: 0.042029562576487205\n",
      "Train Loss at iteration 18545: 0.04202954797684923\n",
      "Train Loss at iteration 18546: 0.04202953337962719\n",
      "Train Loss at iteration 18547: 0.042029518784820676\n",
      "Train Loss at iteration 18548: 0.04202950419242932\n",
      "Train Loss at iteration 18549: 0.042029489602452674\n",
      "Train Loss at iteration 18550: 0.04202947501489037\n",
      "Train Loss at iteration 18551: 0.04202946042974201\n",
      "Train Loss at iteration 18552: 0.04202944584700718\n",
      "Train Loss at iteration 18553: 0.0420294312666855\n",
      "Train Loss at iteration 18554: 0.04202941668877653\n",
      "Train Loss at iteration 18555: 0.042029402113279914\n",
      "Train Loss at iteration 18556: 0.04202938754019525\n",
      "Train Loss at iteration 18557: 0.042029372969522104\n",
      "Train Loss at iteration 18558: 0.04202935840126012\n",
      "Train Loss at iteration 18559: 0.04202934383540885\n",
      "Train Loss at iteration 18560: 0.042029329271967945\n",
      "Train Loss at iteration 18561: 0.04202931471093697\n",
      "Train Loss at iteration 18562: 0.04202930015231555\n",
      "Train Loss at iteration 18563: 0.042029285596103264\n",
      "Train Loss at iteration 18564: 0.04202927104229973\n",
      "Train Loss at iteration 18565: 0.04202925649090455\n",
      "Train Loss at iteration 18566: 0.04202924194191731\n",
      "Train Loss at iteration 18567: 0.042029227395337626\n",
      "Train Loss at iteration 18568: 0.04202921285116509\n",
      "Train Loss at iteration 18569: 0.04202919830939932\n",
      "Train Loss at iteration 18570: 0.0420291837700399\n",
      "Train Loss at iteration 18571: 0.04202916923308644\n",
      "Train Loss at iteration 18572: 0.042029154698538525\n",
      "Train Loss at iteration 18573: 0.042029140166395786\n",
      "Train Loss at iteration 18574: 0.0420291256366578\n",
      "Train Loss at iteration 18575: 0.042029111109324196\n",
      "Train Loss at iteration 18576: 0.04202909658439454\n",
      "Train Loss at iteration 18577: 0.042029082061868464\n",
      "Train Loss at iteration 18578: 0.04202906754174556\n",
      "Train Loss at iteration 18579: 0.042029053024025424\n",
      "Train Loss at iteration 18580: 0.04202903850870768\n",
      "Train Loss at iteration 18581: 0.0420290239957919\n",
      "Train Loss at iteration 18582: 0.0420290094852777\n",
      "Train Loss at iteration 18583: 0.04202899497716468\n",
      "Train Loss at iteration 18584: 0.04202898047145245\n",
      "Train Loss at iteration 18585: 0.04202896596814061\n",
      "Train Loss at iteration 18586: 0.04202895146722876\n",
      "Train Loss at iteration 18587: 0.042028936968716506\n",
      "Train Loss at iteration 18588: 0.042028922472603446\n",
      "Train Loss at iteration 18589: 0.04202890797888919\n",
      "Train Loss at iteration 18590: 0.042028893487573334\n",
      "Train Loss at iteration 18591: 0.04202887899865548\n",
      "Train Loss at iteration 18592: 0.042028864512135236\n",
      "Train Loss at iteration 18593: 0.04202885002801221\n",
      "Train Loss at iteration 18594: 0.042028835546286\n",
      "Train Loss at iteration 18595: 0.042028821066956205\n",
      "Train Loss at iteration 18596: 0.04202880659002242\n",
      "Train Loss at iteration 18597: 0.042028792115484284\n",
      "Train Loss at iteration 18598: 0.04202877764334136\n",
      "Train Loss at iteration 18599: 0.04202876317359327\n",
      "Train Loss at iteration 18600: 0.04202874870623962\n",
      "Train Loss at iteration 18601: 0.042028734241280015\n",
      "Train Loss at iteration 18602: 0.04202871977871406\n",
      "Train Loss at iteration 18603: 0.04202870531854132\n",
      "Train Loss at iteration 18604: 0.04202869086076146\n",
      "Train Loss at iteration 18605: 0.042028676405374034\n",
      "Train Loss at iteration 18606: 0.042028661952378685\n",
      "Train Loss at iteration 18607: 0.042028647501775\n",
      "Train Loss at iteration 18608: 0.04202863305356257\n",
      "Train Loss at iteration 18609: 0.042028618607741014\n",
      "Train Loss at iteration 18610: 0.04202860416430995\n",
      "Train Loss at iteration 18611: 0.04202858972326894\n",
      "Train Loss at iteration 18612: 0.042028575284617634\n",
      "Train Loss at iteration 18613: 0.042028560848355614\n",
      "Train Loss at iteration 18614: 0.04202854641448248\n",
      "Train Loss at iteration 18615: 0.04202853198299785\n",
      "Train Loss at iteration 18616: 0.04202851755390133\n",
      "Train Loss at iteration 18617: 0.04202850312719251\n",
      "Train Loss at iteration 18618: 0.042028488702871\n",
      "Train Loss at iteration 18619: 0.042028474280936395\n",
      "Train Loss at iteration 18620: 0.04202845986138834\n",
      "Train Loss at iteration 18621: 0.04202844544422639\n",
      "Train Loss at iteration 18622: 0.04202843102945019\n",
      "Train Loss at iteration 18623: 0.04202841661705931\n",
      "Train Loss at iteration 18624: 0.04202840220705338\n",
      "Train Loss at iteration 18625: 0.04202838779943201\n",
      "Train Loss at iteration 18626: 0.04202837339419478\n",
      "Train Loss at iteration 18627: 0.042028358991341316\n",
      "Train Loss at iteration 18628: 0.04202834459087121\n",
      "Train Loss at iteration 18629: 0.042028330192784076\n",
      "Train Loss at iteration 18630: 0.04202831579707951\n",
      "Train Loss at iteration 18631: 0.042028301403757136\n",
      "Train Loss at iteration 18632: 0.04202828701281654\n",
      "Train Loss at iteration 18633: 0.04202827262425734\n",
      "Train Loss at iteration 18634: 0.04202825823807914\n",
      "Train Loss at iteration 18635: 0.04202824385428155\n",
      "Train Loss at iteration 18636: 0.04202822947286415\n",
      "Train Loss at iteration 18637: 0.04202821509382658\n",
      "Train Loss at iteration 18638: 0.042028200717168444\n",
      "Train Loss at iteration 18639: 0.04202818634288932\n",
      "Train Loss at iteration 18640: 0.042028171970988834\n",
      "Train Loss at iteration 18641: 0.04202815760146659\n",
      "Train Loss at iteration 18642: 0.0420281432343222\n",
      "Train Loss at iteration 18643: 0.04202812886955525\n",
      "Train Loss at iteration 18644: 0.04202811450716537\n",
      "Train Loss at iteration 18645: 0.04202810014715215\n",
      "Train Loss at iteration 18646: 0.04202808578951521\n",
      "Train Loss at iteration 18647: 0.04202807143425415\n",
      "Train Loss at iteration 18648: 0.04202805708136858\n",
      "Train Loss at iteration 18649: 0.04202804273085809\n",
      "Train Loss at iteration 18650: 0.04202802838272232\n",
      "Train Loss at iteration 18651: 0.04202801403696085\n",
      "Train Loss at iteration 18652: 0.04202799969357329\n",
      "Train Loss at iteration 18653: 0.04202798535255925\n",
      "Train Loss at iteration 18654: 0.04202797101391834\n",
      "Train Loss at iteration 18655: 0.04202795667765018\n",
      "Train Loss at iteration 18656: 0.04202794234375435\n",
      "Train Loss at iteration 18657: 0.04202792801223047\n",
      "Train Loss at iteration 18658: 0.04202791368307816\n",
      "Train Loss at iteration 18659: 0.042027899356297006\n",
      "Train Loss at iteration 18660: 0.04202788503188662\n",
      "Train Loss at iteration 18661: 0.04202787070984663\n",
      "Train Loss at iteration 18662: 0.04202785639017662\n",
      "Train Loss at iteration 18663: 0.0420278420728762\n",
      "Train Loss at iteration 18664: 0.04202782775794501\n",
      "Train Loss at iteration 18665: 0.042027813445382596\n",
      "Train Loss at iteration 18666: 0.04202779913518862\n",
      "Train Loss at iteration 18667: 0.04202778482736267\n",
      "Train Loss at iteration 18668: 0.042027770521904366\n",
      "Train Loss at iteration 18669: 0.04202775621881329\n",
      "Train Loss at iteration 18670: 0.04202774191808908\n",
      "Train Loss at iteration 18671: 0.04202772761973132\n",
      "Train Loss at iteration 18672: 0.04202771332373964\n",
      "Train Loss at iteration 18673: 0.04202769903011363\n",
      "Train Loss at iteration 18674: 0.04202768473885289\n",
      "Train Loss at iteration 18675: 0.042027670449957075\n",
      "Train Loss at iteration 18676: 0.04202765616342574\n",
      "Train Loss at iteration 18677: 0.04202764187925853\n",
      "Train Loss at iteration 18678: 0.042027627597455035\n",
      "Train Loss at iteration 18679: 0.04202761331801488\n",
      "Train Loss at iteration 18680: 0.04202759904093764\n",
      "Train Loss at iteration 18681: 0.04202758476622296\n",
      "Train Loss at iteration 18682: 0.042027570493870445\n",
      "Train Loss at iteration 18683: 0.04202755622387968\n",
      "Train Loss at iteration 18684: 0.042027541956250296\n",
      "Train Loss at iteration 18685: 0.042027527690981885\n",
      "Train Loss at iteration 18686: 0.042027513428074065\n",
      "Train Loss at iteration 18687: 0.042027499167526454\n",
      "Train Loss at iteration 18688: 0.04202748490933867\n",
      "Train Loss at iteration 18689: 0.04202747065351029\n",
      "Train Loss at iteration 18690: 0.04202745640004094\n",
      "Train Loss at iteration 18691: 0.04202744214893023\n",
      "Train Loss at iteration 18692: 0.042027427900177765\n",
      "Train Loss at iteration 18693: 0.04202741365378316\n",
      "Train Loss at iteration 18694: 0.04202739940974602\n",
      "Train Loss at iteration 18695: 0.04202738516806598\n",
      "Train Loss at iteration 18696: 0.04202737092874261\n",
      "Train Loss at iteration 18697: 0.042027356691775535\n",
      "Train Loss at iteration 18698: 0.04202734245716438\n",
      "Train Loss at iteration 18699: 0.04202732822490873\n",
      "Train Loss at iteration 18700: 0.04202731399500822\n",
      "Train Loss at iteration 18701: 0.042027299767462445\n",
      "Train Loss at iteration 18702: 0.042027285542271024\n",
      "Train Loss at iteration 18703: 0.042027271319433554\n",
      "Train Loss at iteration 18704: 0.04202725709894965\n",
      "Train Loss at iteration 18705: 0.04202724288081893\n",
      "Train Loss at iteration 18706: 0.042027228665041015\n",
      "Train Loss at iteration 18707: 0.04202721445161548\n",
      "Train Loss at iteration 18708: 0.042027200240541955\n",
      "Train Loss at iteration 18709: 0.04202718603182007\n",
      "Train Loss at iteration 18710: 0.04202717182544941\n",
      "Train Loss at iteration 18711: 0.04202715762142959\n",
      "Train Loss at iteration 18712: 0.042027143419760236\n",
      "Train Loss at iteration 18713: 0.04202712922044094\n",
      "Train Loss at iteration 18714: 0.042027115023471326\n",
      "Train Loss at iteration 18715: 0.042027100828851\n",
      "Train Loss at iteration 18716: 0.042027086636579565\n",
      "Train Loss at iteration 18717: 0.04202707244665664\n",
      "Train Loss at iteration 18718: 0.04202705825908186\n",
      "Train Loss at iteration 18719: 0.04202704407385479\n",
      "Train Loss at iteration 18720: 0.04202702989097506\n",
      "Train Loss at iteration 18721: 0.04202701571044231\n",
      "Train Loss at iteration 18722: 0.042027001532256095\n",
      "Train Loss at iteration 18723: 0.04202698735641609\n",
      "Train Loss at iteration 18724: 0.04202697318292186\n",
      "Train Loss at iteration 18725: 0.04202695901177304\n",
      "Train Loss at iteration 18726: 0.042026944842969226\n",
      "Train Loss at iteration 18727: 0.04202693067651005\n",
      "Train Loss at iteration 18728: 0.0420269165123951\n",
      "Train Loss at iteration 18729: 0.042026902350624014\n",
      "Train Loss at iteration 18730: 0.042026888191196383\n",
      "Train Loss at iteration 18731: 0.04202687403411182\n",
      "Train Loss at iteration 18732: 0.04202685987936995\n",
      "Train Loss at iteration 18733: 0.04202684572697038\n",
      "Train Loss at iteration 18734: 0.042026831576912727\n",
      "Train Loss at iteration 18735: 0.04202681742919659\n",
      "Train Loss at iteration 18736: 0.042026803283821586\n",
      "Train Loss at iteration 18737: 0.04202678914078734\n",
      "Train Loss at iteration 18738: 0.04202677500009345\n",
      "Train Loss at iteration 18739: 0.04202676086173953\n",
      "Train Loss at iteration 18740: 0.0420267467257252\n",
      "Train Loss at iteration 18741: 0.04202673259205008\n",
      "Train Loss at iteration 18742: 0.04202671846071376\n",
      "Train Loss at iteration 18743: 0.042026704331715854\n",
      "Train Loss at iteration 18744: 0.04202669020505601\n",
      "Train Loss at iteration 18745: 0.04202667608073381\n",
      "Train Loss at iteration 18746: 0.04202666195874887\n",
      "Train Loss at iteration 18747: 0.0420266478391008\n",
      "Train Loss at iteration 18748: 0.04202663372178923\n",
      "Train Loss at iteration 18749: 0.042026619606813774\n",
      "Train Loss at iteration 18750: 0.042026605494174016\n",
      "Train Loss at iteration 18751: 0.0420265913838696\n",
      "Train Loss at iteration 18752: 0.042026577275900125\n",
      "Train Loss at iteration 18753: 0.04202656317026521\n",
      "Train Loss at iteration 18754: 0.042026549066964465\n",
      "Train Loss at iteration 18755: 0.0420265349659975\n",
      "Train Loss at iteration 18756: 0.042026520867363935\n",
      "Train Loss at iteration 18757: 0.04202650677106339\n",
      "Train Loss at iteration 18758: 0.04202649267709547\n",
      "Train Loss at iteration 18759: 0.042026478585459784\n",
      "Train Loss at iteration 18760: 0.04202646449615596\n",
      "Train Loss at iteration 18761: 0.0420264504091836\n",
      "Train Loss at iteration 18762: 0.042026436324542324\n",
      "Train Loss at iteration 18763: 0.042026422242231756\n",
      "Train Loss at iteration 18764: 0.04202640816225148\n",
      "Train Loss at iteration 18765: 0.04202639408460113\n",
      "Train Loss at iteration 18766: 0.04202638000928032\n",
      "Train Loss at iteration 18767: 0.042026365936288675\n",
      "Train Loss at iteration 18768: 0.04202635186562579\n",
      "Train Loss at iteration 18769: 0.042026337797291306\n",
      "Train Loss at iteration 18770: 0.042026323731284794\n",
      "Train Loss at iteration 18771: 0.0420263096676059\n",
      "Train Loss at iteration 18772: 0.04202629560625424\n",
      "Train Loss at iteration 18773: 0.042026281547229434\n",
      "Train Loss at iteration 18774: 0.04202626749053107\n",
      "Train Loss at iteration 18775: 0.042026253436158775\n",
      "Train Loss at iteration 18776: 0.042026239384112166\n",
      "Train Loss at iteration 18777: 0.04202622533439087\n",
      "Train Loss at iteration 18778: 0.042026211286994485\n",
      "Train Loss at iteration 18779: 0.04202619724192263\n",
      "Train Loss at iteration 18780: 0.042026183199174935\n",
      "Train Loss at iteration 18781: 0.04202616915875099\n",
      "Train Loss at iteration 18782: 0.04202615512065043\n",
      "Train Loss at iteration 18783: 0.042026141084872855\n",
      "Train Loss at iteration 18784: 0.042026127051417894\n",
      "Train Loss at iteration 18785: 0.04202611302028516\n",
      "Train Loss at iteration 18786: 0.04202609899147426\n",
      "Train Loss at iteration 18787: 0.04202608496498483\n",
      "Train Loss at iteration 18788: 0.04202607094081647\n",
      "Train Loss at iteration 18789: 0.04202605691896878\n",
      "Train Loss at iteration 18790: 0.04202604289944141\n",
      "Train Loss at iteration 18791: 0.042026028882233966\n",
      "Train Loss at iteration 18792: 0.04202601486734604\n",
      "Train Loss at iteration 18793: 0.04202600085477728\n",
      "Train Loss at iteration 18794: 0.042025986844527276\n",
      "Train Loss at iteration 18795: 0.04202597283659566\n",
      "Train Loss at iteration 18796: 0.04202595883098205\n",
      "Train Loss at iteration 18797: 0.042025944827686054\n",
      "Train Loss at iteration 18798: 0.042025930826707296\n",
      "Train Loss at iteration 18799: 0.04202591682804538\n",
      "Train Loss at iteration 18800: 0.04202590283169993\n",
      "Train Loss at iteration 18801: 0.04202588883767056\n",
      "Train Loss at iteration 18802: 0.0420258748459569\n",
      "Train Loss at iteration 18803: 0.042025860856558565\n",
      "Train Loss at iteration 18804: 0.042025846869475136\n",
      "Train Loss at iteration 18805: 0.04202583288470628\n",
      "Train Loss at iteration 18806: 0.04202581890225158\n",
      "Train Loss at iteration 18807: 0.042025804922110666\n",
      "Train Loss at iteration 18808: 0.04202579094428315\n",
      "Train Loss at iteration 18809: 0.042025776968768656\n",
      "Train Loss at iteration 18810: 0.042025762995566804\n",
      "Train Loss at iteration 18811: 0.04202574902467719\n",
      "Train Loss at iteration 18812: 0.04202573505609946\n",
      "Train Loss at iteration 18813: 0.042025721089833205\n",
      "Train Loss at iteration 18814: 0.04202570712587805\n",
      "Train Loss at iteration 18815: 0.042025693164233636\n",
      "Train Loss at iteration 18816: 0.042025679204899545\n",
      "Train Loss at iteration 18817: 0.04202566524787543\n",
      "Train Loss at iteration 18818: 0.042025651293160875\n",
      "Train Loss at iteration 18819: 0.04202563734075552\n",
      "Train Loss at iteration 18820: 0.04202562339065897\n",
      "Train Loss at iteration 18821: 0.04202560944287085\n",
      "Train Loss at iteration 18822: 0.04202559549739078\n",
      "Train Loss at iteration 18823: 0.04202558155421837\n",
      "Train Loss at iteration 18824: 0.04202556761335324\n",
      "Train Loss at iteration 18825: 0.04202555367479502\n",
      "Train Loss at iteration 18826: 0.04202553973854331\n",
      "Train Loss at iteration 18827: 0.042025525804597744\n",
      "Train Loss at iteration 18828: 0.04202551187295792\n",
      "Train Loss at iteration 18829: 0.04202549794362348\n",
      "Train Loss at iteration 18830: 0.042025484016594034\n",
      "Train Loss at iteration 18831: 0.0420254700918692\n",
      "Train Loss at iteration 18832: 0.04202545616944858\n",
      "Train Loss at iteration 18833: 0.04202544224933183\n",
      "Train Loss at iteration 18834: 0.04202542833151854\n",
      "Train Loss at iteration 18835: 0.042025414416008336\n",
      "Train Loss at iteration 18836: 0.04202540050280083\n",
      "Train Loss at iteration 18837: 0.04202538659189564\n",
      "Train Loss at iteration 18838: 0.0420253726832924\n",
      "Train Loss at iteration 18839: 0.042025358776990714\n",
      "Train Loss at iteration 18840: 0.04202534487299022\n",
      "Train Loss at iteration 18841: 0.04202533097129053\n",
      "Train Loss at iteration 18842: 0.042025317071891236\n",
      "Train Loss at iteration 18843: 0.042025303174792\n",
      "Train Loss at iteration 18844: 0.04202528927999241\n",
      "Train Loss at iteration 18845: 0.042025275387492095\n",
      "Train Loss at iteration 18846: 0.04202526149729068\n",
      "Train Loss at iteration 18847: 0.042025247609387785\n",
      "Train Loss at iteration 18848: 0.04202523372378303\n",
      "Train Loss at iteration 18849: 0.04202521984047601\n",
      "Train Loss at iteration 18850: 0.04202520595946638\n",
      "Train Loss at iteration 18851: 0.04202519208075374\n",
      "Train Loss at iteration 18852: 0.04202517820433772\n",
      "Train Loss at iteration 18853: 0.04202516433021792\n",
      "Train Loss at iteration 18854: 0.04202515045839398\n",
      "Train Loss at iteration 18855: 0.04202513658886551\n",
      "Train Loss at iteration 18856: 0.04202512272163214\n",
      "Train Loss at iteration 18857: 0.04202510885669348\n",
      "Train Loss at iteration 18858: 0.042025094994049154\n",
      "Train Loss at iteration 18859: 0.04202508113369879\n",
      "Train Loss at iteration 18860: 0.042025067275641995\n",
      "Train Loss at iteration 18861: 0.04202505341987839\n",
      "Train Loss at iteration 18862: 0.0420250395664076\n",
      "Train Loss at iteration 18863: 0.04202502571522925\n",
      "Train Loss at iteration 18864: 0.04202501186634295\n",
      "Train Loss at iteration 18865: 0.04202499801974834\n",
      "Train Loss at iteration 18866: 0.04202498417544503\n",
      "Train Loss at iteration 18867: 0.04202497033343264\n",
      "Train Loss at iteration 18868: 0.04202495649371077\n",
      "Train Loss at iteration 18869: 0.04202494265627908\n",
      "Train Loss at iteration 18870: 0.04202492882113717\n",
      "Train Loss at iteration 18871: 0.04202491498828465\n",
      "Train Loss at iteration 18872: 0.042024901157721166\n",
      "Train Loss at iteration 18873: 0.04202488732944632\n",
      "Train Loss at iteration 18874: 0.042024873503459736\n",
      "Train Loss at iteration 18875: 0.04202485967976105\n",
      "Train Loss at iteration 18876: 0.04202484585834987\n",
      "Train Loss at iteration 18877: 0.04202483203922582\n",
      "Train Loss at iteration 18878: 0.042024818222388516\n",
      "Train Loss at iteration 18879: 0.04202480440783759\n",
      "Train Loss at iteration 18880: 0.04202479059557266\n",
      "Train Loss at iteration 18881: 0.04202477678559334\n",
      "Train Loss at iteration 18882: 0.04202476297789926\n",
      "Train Loss at iteration 18883: 0.042024749172490036\n",
      "Train Loss at iteration 18884: 0.042024735369365296\n",
      "Train Loss at iteration 18885: 0.04202472156852467\n",
      "Train Loss at iteration 18886: 0.042024707769967765\n",
      "Train Loss at iteration 18887: 0.042024693973694204\n",
      "Train Loss at iteration 18888: 0.042024680179703615\n",
      "Train Loss at iteration 18889: 0.04202466638799562\n",
      "Train Loss at iteration 18890: 0.04202465259856982\n",
      "Train Loss at iteration 18891: 0.04202463881142587\n",
      "Train Loss at iteration 18892: 0.042024625026563386\n",
      "Train Loss at iteration 18893: 0.04202461124398198\n",
      "Train Loss at iteration 18894: 0.04202459746368127\n",
      "Train Loss at iteration 18895: 0.04202458368566089\n",
      "Train Loss at iteration 18896: 0.04202456990992047\n",
      "Train Loss at iteration 18897: 0.0420245561364596\n",
      "Train Loss at iteration 18898: 0.04202454236527794\n",
      "Train Loss at iteration 18899: 0.04202452859637508\n",
      "Train Loss at iteration 18900: 0.04202451482975067\n",
      "Train Loss at iteration 18901: 0.04202450106540433\n",
      "Train Loss at iteration 18902: 0.042024487303335675\n",
      "Train Loss at iteration 18903: 0.04202447354354431\n",
      "Train Loss at iteration 18904: 0.04202445978602989\n",
      "Train Loss at iteration 18905: 0.04202444603079203\n",
      "Train Loss at iteration 18906: 0.04202443227783033\n",
      "Train Loss at iteration 18907: 0.042024418527144454\n",
      "Train Loss at iteration 18908: 0.042024404778733995\n",
      "Train Loss at iteration 18909: 0.04202439103259857\n",
      "Train Loss at iteration 18910: 0.04202437728873783\n",
      "Train Loss at iteration 18911: 0.042024363547151375\n",
      "Train Loss at iteration 18912: 0.042024349807838854\n",
      "Train Loss at iteration 18913: 0.04202433607079987\n",
      "Train Loss at iteration 18914: 0.042024322336034044\n",
      "Train Loss at iteration 18915: 0.04202430860354101\n",
      "Train Loss at iteration 18916: 0.0420242948733204\n",
      "Train Loss at iteration 18917: 0.04202428114537181\n",
      "Train Loss at iteration 18918: 0.04202426741969489\n",
      "Train Loss at iteration 18919: 0.04202425369628925\n",
      "Train Loss at iteration 18920: 0.042024239975154525\n",
      "Train Loss at iteration 18921: 0.04202422625629034\n",
      "Train Loss at iteration 18922: 0.04202421253969631\n",
      "Train Loss at iteration 18923: 0.042024198825372054\n",
      "Train Loss at iteration 18924: 0.042024185113317215\n",
      "Train Loss at iteration 18925: 0.042024171403531395\n",
      "Train Loss at iteration 18926: 0.042024157696014246\n",
      "Train Loss at iteration 18927: 0.04202414399076536\n",
      "Train Loss at iteration 18928: 0.04202413028778439\n",
      "Train Loss at iteration 18929: 0.04202411658707095\n",
      "Train Loss at iteration 18930: 0.04202410288862466\n",
      "Train Loss at iteration 18931: 0.04202408919244514\n",
      "Train Loss at iteration 18932: 0.042024075498532036\n",
      "Train Loss at iteration 18933: 0.04202406180688496\n",
      "Train Loss at iteration 18934: 0.04202404811750353\n",
      "Train Loss at iteration 18935: 0.04202403443038738\n",
      "Train Loss at iteration 18936: 0.04202402074553614\n",
      "Train Loss at iteration 18937: 0.04202400706294942\n",
      "Train Loss at iteration 18938: 0.042023993382626856\n",
      "Train Loss at iteration 18939: 0.042023979704568074\n",
      "Train Loss at iteration 18940: 0.04202396602877269\n",
      "Train Loss at iteration 18941: 0.04202395235524034\n",
      "Train Loss at iteration 18942: 0.04202393868397064\n",
      "Train Loss at iteration 18943: 0.042023925014963236\n",
      "Train Loss at iteration 18944: 0.04202391134821772\n",
      "Train Loss at iteration 18945: 0.042023897683733755\n",
      "Train Loss at iteration 18946: 0.042023884021510936\n",
      "Train Loss at iteration 18947: 0.0420238703615489\n",
      "Train Loss at iteration 18948: 0.04202385670384727\n",
      "Train Loss at iteration 18949: 0.04202384304840568\n",
      "Train Loss at iteration 18950: 0.04202382939522375\n",
      "Train Loss at iteration 18951: 0.04202381574430111\n",
      "Train Loss at iteration 18952: 0.042023802095637386\n",
      "Train Loss at iteration 18953: 0.04202378844923219\n",
      "Train Loss at iteration 18954: 0.04202377480508517\n",
      "Train Loss at iteration 18955: 0.04202376116319594\n",
      "Train Loss at iteration 18956: 0.04202374752356412\n",
      "Train Loss at iteration 18957: 0.04202373388618936\n",
      "Train Loss at iteration 18958: 0.04202372025107126\n",
      "Train Loss at iteration 18959: 0.04202370661820946\n",
      "Train Loss at iteration 18960: 0.042023692987603584\n",
      "Train Loss at iteration 18961: 0.04202367935925325\n",
      "Train Loss at iteration 18962: 0.0420236657331581\n",
      "Train Loss at iteration 18963: 0.042023652109317765\n",
      "Train Loss at iteration 18964: 0.04202363848773184\n",
      "Train Loss at iteration 18965: 0.04202362486839999\n",
      "Train Loss at iteration 18966: 0.04202361125132182\n",
      "Train Loss at iteration 18967: 0.042023597636496955\n",
      "Train Loss at iteration 18968: 0.04202358402392503\n",
      "Train Loss at iteration 18969: 0.04202357041360568\n",
      "Train Loss at iteration 18970: 0.04202355680553853\n",
      "Train Loss at iteration 18971: 0.04202354319972318\n",
      "Train Loss at iteration 18972: 0.042023529596159286\n",
      "Train Loss at iteration 18973: 0.04202351599484647\n",
      "Train Loss at iteration 18974: 0.04202350239578436\n",
      "Train Loss at iteration 18975: 0.04202348879897257\n",
      "Train Loss at iteration 18976: 0.04202347520441074\n",
      "Train Loss at iteration 18977: 0.042023461612098495\n",
      "Train Loss at iteration 18978: 0.04202344802203546\n",
      "Train Loss at iteration 18979: 0.042023434434221275\n",
      "Train Loss at iteration 18980: 0.042023420848655554\n",
      "Train Loss at iteration 18981: 0.04202340726533794\n",
      "Train Loss at iteration 18982: 0.04202339368426804\n",
      "Train Loss at iteration 18983: 0.04202338010544548\n",
      "Train Loss at iteration 18984: 0.04202336652886992\n",
      "Train Loss at iteration 18985: 0.04202335295454097\n",
      "Train Loss at iteration 18986: 0.04202333938245824\n",
      "Train Loss at iteration 18987: 0.04202332581262138\n",
      "Train Loss at iteration 18988: 0.04202331224503001\n",
      "Train Loss at iteration 18989: 0.04202329867968377\n",
      "Train Loss at iteration 18990: 0.04202328511658228\n",
      "Train Loss at iteration 18991: 0.04202327155572516\n",
      "Train Loss at iteration 18992: 0.04202325799711205\n",
      "Train Loss at iteration 18993: 0.042023244440742576\n",
      "Train Loss at iteration 18994: 0.04202323088661637\n",
      "Train Loss at iteration 18995: 0.042023217334733055\n",
      "Train Loss at iteration 18996: 0.04202320378509226\n",
      "Train Loss at iteration 18997: 0.042023190237693624\n",
      "Train Loss at iteration 18998: 0.04202317669253675\n",
      "Train Loss at iteration 18999: 0.0420231631496213\n",
      "Train Loss at iteration 19000: 0.04202314960894689\n",
      "Train Loss at iteration 19001: 0.042023136070513126\n",
      "Train Loss at iteration 19002: 0.04202312253431967\n",
      "Train Loss at iteration 19003: 0.04202310900036615\n",
      "Train Loss at iteration 19004: 0.04202309546865217\n",
      "Train Loss at iteration 19005: 0.04202308193917737\n",
      "Train Loss at iteration 19006: 0.042023068411941394\n",
      "Train Loss at iteration 19007: 0.042023054886943856\n",
      "Train Loss at iteration 19008: 0.042023041364184395\n",
      "Train Loss at iteration 19009: 0.04202302784366262\n",
      "Train Loss at iteration 19010: 0.0420230143253782\n",
      "Train Loss at iteration 19011: 0.04202300080933072\n",
      "Train Loss at iteration 19012: 0.04202298729551984\n",
      "Train Loss at iteration 19013: 0.042022973783945185\n",
      "Train Loss at iteration 19014: 0.04202296027460637\n",
      "Train Loss at iteration 19015: 0.04202294676750305\n",
      "Train Loss at iteration 19016: 0.04202293326263482\n",
      "Train Loss at iteration 19017: 0.042022919760001345\n",
      "Train Loss at iteration 19018: 0.04202290625960224\n",
      "Train Loss at iteration 19019: 0.042022892761437135\n",
      "Train Loss at iteration 19020: 0.04202287926550566\n",
      "Train Loss at iteration 19021: 0.042022865771807444\n",
      "Train Loss at iteration 19022: 0.042022852280342124\n",
      "Train Loss at iteration 19023: 0.04202283879110932\n",
      "Train Loss at iteration 19024: 0.04202282530410866\n",
      "Train Loss at iteration 19025: 0.042022811819339805\n",
      "Train Loss at iteration 19026: 0.042022798336802364\n",
      "Train Loss at iteration 19027: 0.04202278485649595\n",
      "Train Loss at iteration 19028: 0.042022771378420226\n",
      "Train Loss at iteration 19029: 0.0420227579025748\n",
      "Train Loss at iteration 19030: 0.04202274442895931\n",
      "Train Loss at iteration 19031: 0.04202273095757339\n",
      "Train Loss at iteration 19032: 0.04202271748841667\n",
      "Train Loss at iteration 19033: 0.042022704021488784\n",
      "Train Loss at iteration 19034: 0.04202269055678935\n",
      "Train Loss at iteration 19035: 0.04202267709431803\n",
      "Train Loss at iteration 19036: 0.042022663634074406\n",
      "Train Loss at iteration 19037: 0.04202265017605815\n",
      "Train Loss at iteration 19038: 0.04202263672026888\n",
      "Train Loss at iteration 19039: 0.04202262326670622\n",
      "Train Loss at iteration 19040: 0.04202260981536983\n",
      "Train Loss at iteration 19041: 0.0420225963662593\n",
      "Train Loss at iteration 19042: 0.04202258291937429\n",
      "Train Loss at iteration 19043: 0.04202256947471442\n",
      "Train Loss at iteration 19044: 0.042022556032279335\n",
      "Train Loss at iteration 19045: 0.04202254259206865\n",
      "Train Loss at iteration 19046: 0.04202252915408201\n",
      "Train Loss at iteration 19047: 0.04202251571831904\n",
      "Train Loss at iteration 19048: 0.04202250228477936\n",
      "Train Loss at iteration 19049: 0.04202248885346263\n",
      "Train Loss at iteration 19050: 0.04202247542436847\n",
      "Train Loss at iteration 19051: 0.0420224619974965\n",
      "Train Loss at iteration 19052: 0.04202244857284637\n",
      "Train Loss at iteration 19053: 0.042022435150417696\n",
      "Train Loss at iteration 19054: 0.04202242173021012\n",
      "Train Loss at iteration 19055: 0.04202240831222327\n",
      "Train Loss at iteration 19056: 0.042022394896456795\n",
      "Train Loss at iteration 19057: 0.0420223814829103\n",
      "Train Loss at iteration 19058: 0.04202236807158344\n",
      "Train Loss at iteration 19059: 0.04202235466247584\n",
      "Train Loss at iteration 19060: 0.04202234125558712\n",
      "Train Loss at iteration 19061: 0.042022327850916925\n",
      "Train Loss at iteration 19062: 0.0420223144484649\n",
      "Train Loss at iteration 19063: 0.04202230104823066\n",
      "Train Loss at iteration 19064: 0.04202228765021384\n",
      "Train Loss at iteration 19065: 0.042022274254414085\n",
      "Train Loss at iteration 19066: 0.042022260860831016\n",
      "Train Loss at iteration 19067: 0.042022247469464265\n",
      "Train Loss at iteration 19068: 0.04202223408031347\n",
      "Train Loss at iteration 19069: 0.042022220693378264\n",
      "Train Loss at iteration 19070: 0.04202220730865828\n",
      "Train Loss at iteration 19071: 0.04202219392615315\n",
      "Train Loss at iteration 19072: 0.042022180545862514\n",
      "Train Loss at iteration 19073: 0.04202216716778599\n",
      "Train Loss at iteration 19074: 0.042022153791923235\n",
      "Train Loss at iteration 19075: 0.042022140418273866\n",
      "Train Loss at iteration 19076: 0.042022127046837526\n",
      "Train Loss at iteration 19077: 0.04202211367761383\n",
      "Train Loss at iteration 19078: 0.04202210031060243\n",
      "Train Loss at iteration 19079: 0.042022086945802964\n",
      "Train Loss at iteration 19080: 0.04202207358321505\n",
      "Train Loss at iteration 19081: 0.042022060222838324\n",
      "Train Loss at iteration 19082: 0.042022046864672424\n",
      "Train Loss at iteration 19083: 0.042022033508716986\n",
      "Train Loss at iteration 19084: 0.04202202015497165\n",
      "Train Loss at iteration 19085: 0.04202200680343604\n",
      "Train Loss at iteration 19086: 0.042021993454109785\n",
      "Train Loss at iteration 19087: 0.04202198010699254\n",
      "Train Loss at iteration 19088: 0.04202196676208393\n",
      "Train Loss at iteration 19089: 0.042021953419383584\n",
      "Train Loss at iteration 19090: 0.042021940078891123\n",
      "Train Loss at iteration 19091: 0.04202192674060621\n",
      "Train Loss at iteration 19092: 0.04202191340452846\n",
      "Train Loss at iteration 19093: 0.04202190007065752\n",
      "Train Loss at iteration 19094: 0.04202188673899303\n",
      "Train Loss at iteration 19095: 0.0420218734095346\n",
      "Train Loss at iteration 19096: 0.04202186008228189\n",
      "Train Loss at iteration 19097: 0.04202184675723453\n",
      "Train Loss at iteration 19098: 0.04202183343439213\n",
      "Train Loss at iteration 19099: 0.042021820113754355\n",
      "Train Loss at iteration 19100: 0.04202180679532082\n",
      "Train Loss at iteration 19101: 0.042021793479091185\n",
      "Train Loss at iteration 19102: 0.04202178016506507\n",
      "Train Loss at iteration 19103: 0.042021766853242104\n",
      "Train Loss at iteration 19104: 0.042021753543621934\n",
      "Train Loss at iteration 19105: 0.042021740236204176\n",
      "Train Loss at iteration 19106: 0.0420217269309885\n",
      "Train Loss at iteration 19107: 0.0420217136279745\n",
      "Train Loss at iteration 19108: 0.042021700327161855\n",
      "Train Loss at iteration 19109: 0.04202168702855016\n",
      "Train Loss at iteration 19110: 0.04202167373213908\n",
      "Train Loss at iteration 19111: 0.04202166043792824\n",
      "Train Loss at iteration 19112: 0.042021647145917274\n",
      "Train Loss at iteration 19113: 0.042021633856105814\n",
      "Train Loss at iteration 19114: 0.042021620568493505\n",
      "Train Loss at iteration 19115: 0.042021607283079986\n",
      "Train Loss at iteration 19116: 0.04202159399986489\n",
      "Train Loss at iteration 19117: 0.042021580718847845\n",
      "Train Loss at iteration 19118: 0.04202156744002848\n",
      "Train Loss at iteration 19119: 0.04202155416340646\n",
      "Train Loss at iteration 19120: 0.0420215408889814\n",
      "Train Loss at iteration 19121: 0.04202152761675294\n",
      "Train Loss at iteration 19122: 0.042021514346720724\n",
      "Train Loss at iteration 19123: 0.04202150107888437\n",
      "Train Loss at iteration 19124: 0.042021487813243545\n",
      "Train Loss at iteration 19125: 0.04202147454979785\n",
      "Train Loss at iteration 19126: 0.04202146128854694\n",
      "Train Loss at iteration 19127: 0.04202144802949046\n",
      "Train Loss at iteration 19128: 0.04202143477262804\n",
      "Train Loss at iteration 19129: 0.04202142151795932\n",
      "Train Loss at iteration 19130: 0.042021408265483916\n",
      "Train Loss at iteration 19131: 0.04202139501520148\n",
      "Train Loss at iteration 19132: 0.04202138176711166\n",
      "Train Loss at iteration 19133: 0.04202136852121408\n",
      "Train Loss at iteration 19134: 0.04202135527750838\n",
      "Train Loss at iteration 19135: 0.042021342035994204\n",
      "Train Loss at iteration 19136: 0.04202132879667116\n",
      "Train Loss at iteration 19137: 0.04202131555953893\n",
      "Train Loss at iteration 19138: 0.04202130232459713\n",
      "Train Loss at iteration 19139: 0.04202128909184538\n",
      "Train Loss at iteration 19140: 0.042021275861283354\n",
      "Train Loss at iteration 19141: 0.04202126263291066\n",
      "Train Loss at iteration 19142: 0.042021249406726954\n",
      "Train Loss at iteration 19143: 0.04202123618273185\n",
      "Train Loss at iteration 19144: 0.042021222960924996\n",
      "Train Loss at iteration 19145: 0.04202120974130606\n",
      "Train Loss at iteration 19146: 0.04202119652387465\n",
      "Train Loss at iteration 19147: 0.0420211833086304\n",
      "Train Loss at iteration 19148: 0.04202117009557295\n",
      "Train Loss at iteration 19149: 0.042021156884701955\n",
      "Train Loss at iteration 19150: 0.04202114367601704\n",
      "Train Loss at iteration 19151: 0.04202113046951785\n",
      "Train Loss at iteration 19152: 0.04202111726520402\n",
      "Train Loss at iteration 19153: 0.04202110406307518\n",
      "Train Loss at iteration 19154: 0.04202109086313099\n",
      "Train Loss at iteration 19155: 0.04202107766537107\n",
      "Train Loss at iteration 19156: 0.042021064469795054\n",
      "Train Loss at iteration 19157: 0.0420210512764026\n",
      "Train Loss at iteration 19158: 0.04202103808519332\n",
      "Train Loss at iteration 19159: 0.04202102489616689\n",
      "Train Loss at iteration 19160: 0.042021011709322924\n",
      "Train Loss at iteration 19161: 0.042020998524661055\n",
      "Train Loss at iteration 19162: 0.04202098534218092\n",
      "Train Loss at iteration 19163: 0.042020972161882206\n",
      "Train Loss at iteration 19164: 0.042020958983764484\n",
      "Train Loss at iteration 19165: 0.04202094580782744\n",
      "Train Loss at iteration 19166: 0.04202093263407069\n",
      "Train Loss at iteration 19167: 0.04202091946249389\n",
      "Train Loss at iteration 19168: 0.042020906293096655\n",
      "Train Loss at iteration 19169: 0.04202089312587866\n",
      "Train Loss at iteration 19170: 0.0420208799608395\n",
      "Train Loss at iteration 19171: 0.04202086679797885\n",
      "Train Loss at iteration 19172: 0.04202085363729633\n",
      "Train Loss at iteration 19173: 0.0420208404787916\n",
      "Train Loss at iteration 19174: 0.04202082732246428\n",
      "Train Loss at iteration 19175: 0.04202081416831402\n",
      "Train Loss at iteration 19176: 0.04202080101634045\n",
      "Train Loss at iteration 19177: 0.042020787866543216\n",
      "Train Loss at iteration 19178: 0.042020774718921954\n",
      "Train Loss at iteration 19179: 0.04202076157347631\n",
      "Train Loss at iteration 19180: 0.04202074843020592\n",
      "Train Loss at iteration 19181: 0.042020735289110434\n",
      "Train Loss at iteration 19182: 0.04202072215018948\n",
      "Train Loss at iteration 19183: 0.04202070901344269\n",
      "Train Loss at iteration 19184: 0.04202069587886973\n",
      "Train Loss at iteration 19185: 0.042020682746470216\n",
      "Train Loss at iteration 19186: 0.04202066961624381\n",
      "Train Loss at iteration 19187: 0.04202065648819013\n",
      "Train Loss at iteration 19188: 0.042020643362308825\n",
      "Train Loss at iteration 19189: 0.042020630238599546\n",
      "Train Loss at iteration 19190: 0.04202061711706192\n",
      "Train Loss at iteration 19191: 0.04202060399769559\n",
      "Train Loss at iteration 19192: 0.0420205908805002\n",
      "Train Loss at iteration 19193: 0.04202057776547538\n",
      "Train Loss at iteration 19194: 0.042020564652620794\n",
      "Train Loss at iteration 19195: 0.04202055154193607\n",
      "Train Loss at iteration 19196: 0.042020538433420844\n",
      "Train Loss at iteration 19197: 0.042020525327074766\n",
      "Train Loss at iteration 19198: 0.04202051222289747\n",
      "Train Loss at iteration 19199: 0.042020499120888594\n",
      "Train Loss at iteration 19200: 0.04202048602104779\n",
      "Train Loss at iteration 19201: 0.042020472923374685\n",
      "Train Loss at iteration 19202: 0.042020459827868936\n",
      "Train Loss at iteration 19203: 0.042020446734530176\n",
      "Train Loss at iteration 19204: 0.04202043364335804\n",
      "Train Loss at iteration 19205: 0.042020420554352185\n",
      "Train Loss at iteration 19206: 0.04202040746751224\n",
      "Train Loss at iteration 19207: 0.04202039438283786\n",
      "Train Loss at iteration 19208: 0.042020381300328674\n",
      "Train Loss at iteration 19209: 0.04202036821998432\n",
      "Train Loss at iteration 19210: 0.042020355141804466\n",
      "Train Loss at iteration 19211: 0.04202034206578871\n",
      "Train Loss at iteration 19212: 0.04202032899193673\n",
      "Train Loss at iteration 19213: 0.04202031592024815\n",
      "Train Loss at iteration 19214: 0.04202030285072262\n",
      "Train Loss at iteration 19215: 0.04202028978335979\n",
      "Train Loss at iteration 19216: 0.042020276718159284\n",
      "Train Loss at iteration 19217: 0.04202026365512075\n",
      "Train Loss at iteration 19218: 0.04202025059424383\n",
      "Train Loss at iteration 19219: 0.042020237535528186\n",
      "Train Loss at iteration 19220: 0.042020224478973425\n",
      "Train Loss at iteration 19221: 0.04202021142457923\n",
      "Train Loss at iteration 19222: 0.0420201983723452\n",
      "Train Loss at iteration 19223: 0.042020185322270996\n",
      "Train Loss at iteration 19224: 0.04202017227435628\n",
      "Train Loss at iteration 19225: 0.04202015922860066\n",
      "Train Loss at iteration 19226: 0.04202014618500381\n",
      "Train Loss at iteration 19227: 0.04202013314356536\n",
      "Train Loss at iteration 19228: 0.04202012010428494\n",
      "Train Loss at iteration 19229: 0.04202010706716222\n",
      "Train Loss at iteration 19230: 0.042020094032196816\n",
      "Train Loss at iteration 19231: 0.0420200809993884\n",
      "Train Loss at iteration 19232: 0.04202006796873658\n",
      "Train Loss at iteration 19233: 0.04202005494024101\n",
      "Train Loss at iteration 19234: 0.04202004191390136\n",
      "Train Loss at iteration 19235: 0.042020028889717224\n",
      "Train Loss at iteration 19236: 0.042020015867688304\n",
      "Train Loss at iteration 19237: 0.04202000284781421\n",
      "Train Loss at iteration 19238: 0.04201998983009458\n",
      "Train Loss at iteration 19239: 0.04201997681452907\n",
      "Train Loss at iteration 19240: 0.04201996380111731\n",
      "Train Loss at iteration 19241: 0.04201995078985896\n",
      "Train Loss at iteration 19242: 0.04201993778075368\n",
      "Train Loss at iteration 19243: 0.04201992477380105\n",
      "Train Loss at iteration 19244: 0.04201991176900077\n",
      "Train Loss at iteration 19245: 0.04201989876635248\n",
      "Train Loss at iteration 19246: 0.0420198857658558\n",
      "Train Loss at iteration 19247: 0.042019872767510395\n",
      "Train Loss at iteration 19248: 0.042019859771315886\n",
      "Train Loss at iteration 19249: 0.04201984677727195\n",
      "Train Loss at iteration 19250: 0.0420198337853782\n",
      "Train Loss at iteration 19251: 0.0420198207956343\n",
      "Train Loss at iteration 19252: 0.04201980780803987\n",
      "Train Loss at iteration 19253: 0.042019794822594596\n",
      "Train Loss at iteration 19254: 0.042019781839298075\n",
      "Train Loss at iteration 19255: 0.04201976885814998\n",
      "Train Loss at iteration 19256: 0.04201975587914995\n",
      "Train Loss at iteration 19257: 0.042019742902297624\n",
      "Train Loss at iteration 19258: 0.04201972992759266\n",
      "Train Loss at iteration 19259: 0.042019716955034696\n",
      "Train Loss at iteration 19260: 0.042019703984623354\n",
      "Train Loss at iteration 19261: 0.042019691016358315\n",
      "Train Loss at iteration 19262: 0.042019678050239206\n",
      "Train Loss at iteration 19263: 0.042019665086265666\n",
      "Train Loss at iteration 19264: 0.042019652124437354\n",
      "Train Loss at iteration 19265: 0.042019639164753916\n",
      "Train Loss at iteration 19266: 0.04201962620721498\n",
      "Train Loss at iteration 19267: 0.042019613251820194\n",
      "Train Loss at iteration 19268: 0.04201960029856922\n",
      "Train Loss at iteration 19269: 0.04201958734746169\n",
      "Train Loss at iteration 19270: 0.04201957439849725\n",
      "Train Loss at iteration 19271: 0.042019561451675555\n",
      "Train Loss at iteration 19272: 0.042019548506996234\n",
      "Train Loss at iteration 19273: 0.04201953556445895\n",
      "Train Loss at iteration 19274: 0.042019522624063325\n",
      "Train Loss at iteration 19275: 0.04201950968580904\n",
      "Train Loss at iteration 19276: 0.042019496749695705\n",
      "Train Loss at iteration 19277: 0.042019483815722986\n",
      "Train Loss at iteration 19278: 0.04201947088389051\n",
      "Train Loss at iteration 19279: 0.04201945795419795\n",
      "Train Loss at iteration 19280: 0.04201944502664493\n",
      "Train Loss at iteration 19281: 0.04201943210123112\n",
      "Train Loss at iteration 19282: 0.04201941917795614\n",
      "Train Loss at iteration 19283: 0.04201940625681964\n",
      "Train Loss at iteration 19284: 0.04201939333782127\n",
      "Train Loss at iteration 19285: 0.042019380420960685\n",
      "Train Loss at iteration 19286: 0.042019367506237526\n",
      "Train Loss at iteration 19287: 0.04201935459365144\n",
      "Train Loss at iteration 19288: 0.042019341683202066\n",
      "Train Loss at iteration 19289: 0.04201932877488906\n",
      "Train Loss at iteration 19290: 0.04201931586871205\n",
      "Train Loss at iteration 19291: 0.04201930296467071\n",
      "Train Loss at iteration 19292: 0.04201929006276466\n",
      "Train Loss at iteration 19293: 0.04201927716299357\n",
      "Train Loss at iteration 19294: 0.04201926426535707\n",
      "Train Loss at iteration 19295: 0.04201925136985483\n",
      "Train Loss at iteration 19296: 0.04201923847648646\n",
      "Train Loss at iteration 19297: 0.04201922558525163\n",
      "Train Loss at iteration 19298: 0.042019212696149985\n",
      "Train Loss at iteration 19299: 0.04201919980918117\n",
      "Train Loss at iteration 19300: 0.04201918692434482\n",
      "Train Loss at iteration 19301: 0.04201917404164062\n",
      "Train Loss at iteration 19302: 0.04201916116106817\n",
      "Train Loss at iteration 19303: 0.04201914828262715\n",
      "Train Loss at iteration 19304: 0.04201913540631719\n",
      "Train Loss at iteration 19305: 0.04201912253213795\n",
      "Train Loss at iteration 19306: 0.04201910966008906\n",
      "Train Loss at iteration 19307: 0.04201909679017019\n",
      "Train Loss at iteration 19308: 0.04201908392238096\n",
      "Train Loss at iteration 19309: 0.04201907105672105\n",
      "Train Loss at iteration 19310: 0.04201905819319008\n",
      "Train Loss at iteration 19311: 0.04201904533178772\n",
      "Train Loss at iteration 19312: 0.04201903247251359\n",
      "Train Loss at iteration 19313: 0.04201901961536736\n",
      "Train Loss at iteration 19314: 0.042019006760348664\n",
      "Train Loss at iteration 19315: 0.042018993907457176\n",
      "Train Loss at iteration 19316: 0.042018981056692516\n",
      "Train Loss at iteration 19317: 0.04201896820805433\n",
      "Train Loss at iteration 19318: 0.04201895536154229\n",
      "Train Loss at iteration 19319: 0.04201894251715603\n",
      "Train Loss at iteration 19320: 0.0420189296748952\n",
      "Train Loss at iteration 19321: 0.042018916834759445\n",
      "Train Loss at iteration 19322: 0.04201890399674842\n",
      "Train Loss at iteration 19323: 0.04201889116086177\n",
      "Train Loss at iteration 19324: 0.04201887832709914\n",
      "Train Loss at iteration 19325: 0.04201886549546018\n",
      "Train Loss at iteration 19326: 0.042018852665944545\n",
      "Train Loss at iteration 19327: 0.04201883983855188\n",
      "Train Loss at iteration 19328: 0.04201882701328183\n",
      "Train Loss at iteration 19329: 0.04201881419013404\n",
      "Train Loss at iteration 19330: 0.042018801369108176\n",
      "Train Loss at iteration 19331: 0.042018788550203856\n",
      "Train Loss at iteration 19332: 0.04201877573342078\n",
      "Train Loss at iteration 19333: 0.04201876291875854\n",
      "Train Loss at iteration 19334: 0.04201875010621682\n",
      "Train Loss at iteration 19335: 0.04201873729579526\n",
      "Train Loss at iteration 19336: 0.042018724487493504\n",
      "Train Loss at iteration 19337: 0.0420187116813112\n",
      "Train Loss at iteration 19338: 0.042018698877248024\n",
      "Train Loss at iteration 19339: 0.04201868607530359\n",
      "Train Loss at iteration 19340: 0.042018673275477564\n",
      "Train Loss at iteration 19341: 0.04201866047776959\n",
      "Train Loss at iteration 19342: 0.042018647682179326\n",
      "Train Loss at iteration 19343: 0.04201863488870642\n",
      "Train Loss at iteration 19344: 0.042018622097350494\n",
      "Train Loss at iteration 19345: 0.04201860930811125\n",
      "Train Loss at iteration 19346: 0.04201859652098829\n",
      "Train Loss at iteration 19347: 0.042018583735981295\n",
      "Train Loss at iteration 19348: 0.042018570953089904\n",
      "Train Loss at iteration 19349: 0.04201855817231376\n",
      "Train Loss at iteration 19350: 0.04201854539365252\n",
      "Train Loss at iteration 19351: 0.04201853261710583\n",
      "Train Loss at iteration 19352: 0.04201851984267334\n",
      "Train Loss at iteration 19353: 0.04201850707035471\n",
      "Train Loss at iteration 19354: 0.04201849430014957\n",
      "Train Loss at iteration 19355: 0.04201848153205759\n",
      "Train Loss at iteration 19356: 0.04201846876607842\n",
      "Train Loss at iteration 19357: 0.04201845600221169\n",
      "Train Loss at iteration 19358: 0.04201844324045708\n",
      "Train Loss at iteration 19359: 0.04201843048081421\n",
      "Train Loss at iteration 19360: 0.04201841772328276\n",
      "Train Loss at iteration 19361: 0.04201840496786235\n",
      "Train Loss at iteration 19362: 0.042018392214552655\n",
      "Train Loss at iteration 19363: 0.04201837946335331\n",
      "Train Loss at iteration 19364: 0.042018366714263976\n",
      "Train Loss at iteration 19365: 0.04201835396728431\n",
      "Train Loss at iteration 19366: 0.04201834122241394\n",
      "Train Loss at iteration 19367: 0.04201832847965254\n",
      "Train Loss at iteration 19368: 0.042018315738999744\n",
      "Train Loss at iteration 19369: 0.042018303000455205\n",
      "Train Loss at iteration 19370: 0.04201829026401859\n",
      "Train Loss at iteration 19371: 0.042018277529689535\n",
      "Train Loss at iteration 19372: 0.0420182647974677\n",
      "Train Loss at iteration 19373: 0.04201825206735272\n",
      "Train Loss at iteration 19374: 0.04201823933934427\n",
      "Train Loss at iteration 19375: 0.04201822661344199\n",
      "Train Loss at iteration 19376: 0.042018213889645516\n",
      "Train Loss at iteration 19377: 0.04201820116795453\n",
      "Train Loss at iteration 19378: 0.04201818844836865\n",
      "Train Loss at iteration 19379: 0.04201817573088757\n",
      "Train Loss at iteration 19380: 0.0420181630155109\n",
      "Train Loss at iteration 19381: 0.042018150302238315\n",
      "Train Loss at iteration 19382: 0.042018137591069454\n",
      "Train Loss at iteration 19383: 0.04201812488200398\n",
      "Train Loss at iteration 19384: 0.04201811217504155\n",
      "Train Loss at iteration 19385: 0.0420180994701818\n",
      "Train Loss at iteration 19386: 0.042018086767424384\n",
      "Train Loss at iteration 19387: 0.042018074066768954\n",
      "Train Loss at iteration 19388: 0.042018061368215176\n",
      "Train Loss at iteration 19389: 0.04201804867176269\n",
      "Train Loss at iteration 19390: 0.04201803597741114\n",
      "Train Loss at iteration 19391: 0.0420180232851602\n",
      "Train Loss at iteration 19392: 0.04201801059500951\n",
      "Train Loss at iteration 19393: 0.042017997906958725\n",
      "Train Loss at iteration 19394: 0.042017985221007505\n",
      "Train Loss at iteration 19395: 0.04201797253715549\n",
      "Train Loss at iteration 19396: 0.04201795985540233\n",
      "Train Loss at iteration 19397: 0.042017947175747694\n",
      "Train Loss at iteration 19398: 0.042017934498191224\n",
      "Train Loss at iteration 19399: 0.04201792182273256\n",
      "Train Loss at iteration 19400: 0.04201790914937138\n",
      "Train Loss at iteration 19401: 0.04201789647810732\n",
      "Train Loss at iteration 19402: 0.042017883808940046\n",
      "Train Loss at iteration 19403: 0.04201787114186919\n",
      "Train Loss at iteration 19404: 0.042017858476894425\n",
      "Train Loss at iteration 19405: 0.0420178458140154\n",
      "Train Loss at iteration 19406: 0.04201783315323176\n",
      "Train Loss at iteration 19407: 0.042017820494543175\n",
      "Train Loss at iteration 19408: 0.04201780783794929\n",
      "Train Loss at iteration 19409: 0.04201779518344973\n",
      "Train Loss at iteration 19410: 0.04201778253104419\n",
      "Train Loss at iteration 19411: 0.04201776988073231\n",
      "Train Loss at iteration 19412: 0.04201775723251374\n",
      "Train Loss at iteration 19413: 0.04201774458638813\n",
      "Train Loss at iteration 19414: 0.04201773194235515\n",
      "Train Loss at iteration 19415: 0.042017719300414424\n",
      "Train Loss at iteration 19416: 0.042017706660565625\n",
      "Train Loss at iteration 19417: 0.04201769402280842\n",
      "Train Loss at iteration 19418: 0.042017681387142444\n",
      "Train Loss at iteration 19419: 0.04201766875356735\n",
      "Train Loss at iteration 19420: 0.04201765612208279\n",
      "Train Loss at iteration 19421: 0.042017643492688425\n",
      "Train Loss at iteration 19422: 0.04201763086538393\n",
      "Train Loss at iteration 19423: 0.042017618240168926\n",
      "Train Loss at iteration 19424: 0.042017605617043076\n",
      "Train Loss at iteration 19425: 0.04201759299600604\n",
      "Train Loss at iteration 19426: 0.04201758037705747\n",
      "Train Loss at iteration 19427: 0.042017567760197015\n",
      "Train Loss at iteration 19428: 0.042017555145424344\n",
      "Train Loss at iteration 19429: 0.0420175425327391\n",
      "Train Loss at iteration 19430: 0.04201752992214094\n",
      "Train Loss at iteration 19431: 0.04201751731362951\n",
      "Train Loss at iteration 19432: 0.04201750470720448\n",
      "Train Loss at iteration 19433: 0.0420174921028655\n",
      "Train Loss at iteration 19434: 0.042017479500612215\n",
      "Train Loss at iteration 19435: 0.042017466900444306\n",
      "Train Loss at iteration 19436: 0.04201745430236138\n",
      "Train Loss at iteration 19437: 0.04201744170636314\n",
      "Train Loss at iteration 19438: 0.042017429112449226\n",
      "Train Loss at iteration 19439: 0.04201741652061927\n",
      "Train Loss at iteration 19440: 0.042017403930872964\n",
      "Train Loss at iteration 19441: 0.04201739134320994\n",
      "Train Loss at iteration 19442: 0.04201737875762985\n",
      "Train Loss at iteration 19443: 0.04201736617413237\n",
      "Train Loss at iteration 19444: 0.04201735359271714\n",
      "Train Loss at iteration 19445: 0.042017341013383815\n",
      "Train Loss at iteration 19446: 0.042017328436132054\n",
      "Train Loss at iteration 19447: 0.04201731586096152\n",
      "Train Loss at iteration 19448: 0.04201730328787186\n",
      "Train Loss at iteration 19449: 0.042017290716862714\n",
      "Train Loss at iteration 19450: 0.042017278147933775\n",
      "Train Loss at iteration 19451: 0.04201726558108467\n",
      "Train Loss at iteration 19452: 0.04201725301631506\n",
      "Train Loss at iteration 19453: 0.04201724045362461\n",
      "Train Loss at iteration 19454: 0.04201722789301296\n",
      "Train Loss at iteration 19455: 0.04201721533447979\n",
      "Train Loss at iteration 19456: 0.042017202778024725\n",
      "Train Loss at iteration 19457: 0.04201719022364745\n",
      "Train Loss at iteration 19458: 0.042017177671347605\n",
      "Train Loss at iteration 19459: 0.04201716512112485\n",
      "Train Loss at iteration 19460: 0.042017152572978854\n",
      "Train Loss at iteration 19461: 0.04201714002690925\n",
      "Train Loss at iteration 19462: 0.042017127482915695\n",
      "Train Loss at iteration 19463: 0.042017114940997864\n",
      "Train Loss at iteration 19464: 0.042017102401155415\n",
      "Train Loss at iteration 19465: 0.04201708986338798\n",
      "Train Loss at iteration 19466: 0.04201707732769523\n",
      "Train Loss at iteration 19467: 0.04201706479407684\n",
      "Train Loss at iteration 19468: 0.04201705226253243\n",
      "Train Loss at iteration 19469: 0.04201703973306167\n",
      "Train Loss at iteration 19470: 0.042017027205664234\n",
      "Train Loss at iteration 19471: 0.04201701468033977\n",
      "Train Loss at iteration 19472: 0.04201700215708792\n",
      "Train Loss at iteration 19473: 0.042016989635908365\n",
      "Train Loss at iteration 19474: 0.04201697711680075\n",
      "Train Loss at iteration 19475: 0.042016964599764735\n",
      "Train Loss at iteration 19476: 0.04201695208479995\n",
      "Train Loss at iteration 19477: 0.042016939571906095\n",
      "Train Loss at iteration 19478: 0.04201692706108281\n",
      "Train Loss at iteration 19479: 0.04201691455232975\n",
      "Train Loss at iteration 19480: 0.04201690204564657\n",
      "Train Loss at iteration 19481: 0.04201688954103293\n",
      "Train Loss at iteration 19482: 0.042016877038488484\n",
      "Train Loss at iteration 19483: 0.0420168645380129\n",
      "Train Loss at iteration 19484: 0.042016852039605826\n",
      "Train Loss at iteration 19485: 0.042016839543266925\n",
      "Train Loss at iteration 19486: 0.042016827048995856\n",
      "Train Loss at iteration 19487: 0.04201681455679227\n",
      "Train Loss at iteration 19488: 0.042016802066655826\n",
      "Train Loss at iteration 19489: 0.042016789578586185\n",
      "Train Loss at iteration 19490: 0.04201677709258301\n",
      "Train Loss at iteration 19491: 0.04201676460864595\n",
      "Train Loss at iteration 19492: 0.04201675212677466\n",
      "Train Loss at iteration 19493: 0.042016739646968816\n",
      "Train Loss at iteration 19494: 0.04201672716922805\n",
      "Train Loss at iteration 19495: 0.042016714693552046\n",
      "Train Loss at iteration 19496: 0.04201670221994046\n",
      "Train Loss at iteration 19497: 0.04201668974839293\n",
      "Train Loss at iteration 19498: 0.04201667727890912\n",
      "Train Loss at iteration 19499: 0.0420166648114887\n",
      "Train Loss at iteration 19500: 0.04201665234613133\n",
      "Train Loss at iteration 19501: 0.04201663988283665\n",
      "Train Loss at iteration 19502: 0.04201662742160433\n",
      "Train Loss at iteration 19503: 0.042016614962434036\n",
      "Train Loss at iteration 19504: 0.04201660250532543\n",
      "Train Loss at iteration 19505: 0.042016590050278135\n",
      "Train Loss at iteration 19506: 0.04201657759729186\n",
      "Train Loss at iteration 19507: 0.04201656514636622\n",
      "Train Loss at iteration 19508: 0.0420165526975009\n",
      "Train Loss at iteration 19509: 0.042016540250695546\n",
      "Train Loss at iteration 19510: 0.042016527805949826\n",
      "Train Loss at iteration 19511: 0.04201651536326342\n",
      "Train Loss at iteration 19512: 0.04201650292263592\n",
      "Train Loss at iteration 19513: 0.042016490484067054\n",
      "Train Loss at iteration 19514: 0.04201647804755646\n",
      "Train Loss at iteration 19515: 0.04201646561310378\n",
      "Train Loss at iteration 19516: 0.04201645318070869\n",
      "Train Loss at iteration 19517: 0.042016440750370865\n",
      "Train Loss at iteration 19518: 0.04201642832208993\n",
      "Train Loss at iteration 19519: 0.04201641589586556\n",
      "Train Loss at iteration 19520: 0.04201640347169741\n",
      "Train Loss at iteration 19521: 0.042016391049585156\n",
      "Train Loss at iteration 19522: 0.04201637862952844\n",
      "Train Loss at iteration 19523: 0.04201636621152692\n",
      "Train Loss at iteration 19524: 0.04201635379558029\n",
      "Train Loss at iteration 19525: 0.04201634138168816\n",
      "Train Loss at iteration 19526: 0.04201632896985023\n",
      "Train Loss at iteration 19527: 0.04201631656006613\n",
      "Train Loss at iteration 19528: 0.042016304152335544\n",
      "Train Loss at iteration 19529: 0.04201629174665812\n",
      "Train Loss at iteration 19530: 0.04201627934303353\n",
      "Train Loss at iteration 19531: 0.04201626694146141\n",
      "Train Loss at iteration 19532: 0.04201625454194144\n",
      "Train Loss at iteration 19533: 0.04201624214447328\n",
      "Train Loss at iteration 19534: 0.042016229749056576\n",
      "Train Loss at iteration 19535: 0.042016217355691006\n",
      "Train Loss at iteration 19536: 0.042016204964376226\n",
      "Train Loss at iteration 19537: 0.04201619257511188\n",
      "Train Loss at iteration 19538: 0.04201618018789766\n",
      "Train Loss at iteration 19539: 0.0420161678027332\n",
      "Train Loss at iteration 19540: 0.04201615541961818\n",
      "Train Loss at iteration 19541: 0.04201614303855223\n",
      "Train Loss at iteration 19542: 0.04201613065953505\n",
      "Train Loss at iteration 19543: 0.042016118282566274\n",
      "Train Loss at iteration 19544: 0.04201610590764557\n",
      "Train Loss at iteration 19545: 0.042016093534772606\n",
      "Train Loss at iteration 19546: 0.04201608116394704\n",
      "Train Loss at iteration 19547: 0.04201606879516853\n",
      "Train Loss at iteration 19548: 0.04201605642843674\n",
      "Train Loss at iteration 19549: 0.04201604406375132\n",
      "Train Loss at iteration 19550: 0.04201603170111194\n",
      "Train Loss at iteration 19551: 0.04201601934051827\n",
      "Train Loss at iteration 19552: 0.04201600698196995\n",
      "Train Loss at iteration 19553: 0.04201599462546667\n",
      "Train Loss at iteration 19554: 0.04201598227100807\n",
      "Train Loss at iteration 19555: 0.042015969918593826\n",
      "Train Loss at iteration 19556: 0.04201595756822358\n",
      "Train Loss at iteration 19557: 0.04201594521989701\n",
      "Train Loss at iteration 19558: 0.04201593287361377\n",
      "Train Loss at iteration 19559: 0.042015920529373536\n",
      "Train Loss at iteration 19560: 0.04201590818717595\n",
      "Train Loss at iteration 19561: 0.04201589584702069\n",
      "Train Loss at iteration 19562: 0.04201588350890741\n",
      "Train Loss at iteration 19563: 0.04201587117283576\n",
      "Train Loss at iteration 19564: 0.042015858838805424\n",
      "Train Loss at iteration 19565: 0.042015846506816064\n",
      "Train Loss at iteration 19566: 0.04201583417686733\n",
      "Train Loss at iteration 19567: 0.04201582184895888\n",
      "Train Loss at iteration 19568: 0.04201580952309039\n",
      "Train Loss at iteration 19569: 0.04201579719926152\n",
      "Train Loss at iteration 19570: 0.04201578487747191\n",
      "Train Loss at iteration 19571: 0.04201577255772126\n",
      "Train Loss at iteration 19572: 0.04201576024000921\n",
      "Train Loss at iteration 19573: 0.04201574792433543\n",
      "Train Loss at iteration 19574: 0.04201573561069958\n",
      "Train Loss at iteration 19575: 0.04201572329910132\n",
      "Train Loss at iteration 19576: 0.04201571098954031\n",
      "Train Loss at iteration 19577: 0.04201569868201621\n",
      "Train Loss at iteration 19578: 0.0420156863765287\n",
      "Train Loss at iteration 19579: 0.04201567407307746\n",
      "Train Loss at iteration 19580: 0.04201566177166209\n",
      "Train Loss at iteration 19581: 0.0420156494722823\n",
      "Train Loss at iteration 19582: 0.04201563717493774\n",
      "Train Loss at iteration 19583: 0.04201562487962808\n",
      "Train Loss at iteration 19584: 0.04201561258635298\n",
      "Train Loss at iteration 19585: 0.04201560029511211\n",
      "Train Loss at iteration 19586: 0.04201558800590511\n",
      "Train Loss at iteration 19587: 0.04201557571873167\n",
      "Train Loss at iteration 19588: 0.042015563433591446\n",
      "Train Loss at iteration 19589: 0.04201555115048409\n",
      "Train Loss at iteration 19590: 0.04201553886940928\n",
      "Train Loss at iteration 19591: 0.04201552659036667\n",
      "Train Loss at iteration 19592: 0.04201551431335593\n",
      "Train Loss at iteration 19593: 0.04201550203837672\n",
      "Train Loss at iteration 19594: 0.042015489765428696\n",
      "Train Loss at iteration 19595: 0.042015477494511545\n",
      "Train Loss at iteration 19596: 0.0420154652256249\n",
      "Train Loss at iteration 19597: 0.04201545295876846\n",
      "Train Loss at iteration 19598: 0.04201544069394185\n",
      "Train Loss at iteration 19599: 0.04201542843114477\n",
      "Train Loss at iteration 19600: 0.04201541617037686\n",
      "Train Loss at iteration 19601: 0.042015403911637814\n",
      "Train Loss at iteration 19602: 0.04201539165492725\n",
      "Train Loss at iteration 19603: 0.04201537940024486\n",
      "Train Loss at iteration 19604: 0.04201536714759031\n",
      "Train Loss at iteration 19605: 0.04201535489696326\n",
      "Train Loss at iteration 19606: 0.04201534264836338\n",
      "Train Loss at iteration 19607: 0.04201533040179033\n",
      "Train Loss at iteration 19608: 0.04201531815724376\n",
      "Train Loss at iteration 19609: 0.04201530591472336\n",
      "Train Loss at iteration 19610: 0.04201529367422878\n",
      "Train Loss at iteration 19611: 0.04201528143575969\n",
      "Train Loss at iteration 19612: 0.042015269199315745\n",
      "Train Loss at iteration 19613: 0.04201525696489663\n",
      "Train Loss at iteration 19614: 0.04201524473250198\n",
      "Train Loss at iteration 19615: 0.04201523250213149\n",
      "Train Loss at iteration 19616: 0.04201522027378481\n",
      "Train Loss at iteration 19617: 0.04201520804746161\n",
      "Train Loss at iteration 19618: 0.04201519582316155\n",
      "Train Loss at iteration 19619: 0.0420151836008843\n",
      "Train Loss at iteration 19620: 0.04201517138062951\n",
      "Train Loss at iteration 19621: 0.04201515916239687\n",
      "Train Loss at iteration 19622: 0.04201514694618603\n",
      "Train Loss at iteration 19623: 0.04201513473199666\n",
      "Train Loss at iteration 19624: 0.042015122519828425\n",
      "Train Loss at iteration 19625: 0.04201511030968098\n",
      "Train Loss at iteration 19626: 0.042015098101554016\n",
      "Train Loss at iteration 19627: 0.042015085895447175\n",
      "Train Loss at iteration 19628: 0.042015073691360136\n",
      "Train Loss at iteration 19629: 0.04201506148929255\n",
      "Train Loss at iteration 19630: 0.0420150492892441\n",
      "Train Loss at iteration 19631: 0.04201503709121443\n",
      "Train Loss at iteration 19632: 0.04201502489520324\n",
      "Train Loss at iteration 19633: 0.04201501270121017\n",
      "Train Loss at iteration 19634: 0.04201500050923489\n",
      "Train Loss at iteration 19635: 0.042014988319277066\n",
      "Train Loss at iteration 19636: 0.04201497613133637\n",
      "Train Loss at iteration 19637: 0.04201496394541246\n",
      "Train Loss at iteration 19638: 0.04201495176150501\n",
      "Train Loss at iteration 19639: 0.04201493957961367\n",
      "Train Loss at iteration 19640: 0.042014927399738146\n",
      "Train Loss at iteration 19641: 0.04201491522187806\n",
      "Train Loss at iteration 19642: 0.0420149030460331\n",
      "Train Loss at iteration 19643: 0.04201489087220291\n",
      "Train Loss at iteration 19644: 0.042014878700387205\n",
      "Train Loss at iteration 19645: 0.0420148665305856\n",
      "Train Loss at iteration 19646: 0.042014854362797796\n",
      "Train Loss at iteration 19647: 0.042014842197023436\n",
      "Train Loss at iteration 19648: 0.0420148300332622\n",
      "Train Loss at iteration 19649: 0.04201481787151377\n",
      "Train Loss at iteration 19650: 0.04201480571177778\n",
      "Train Loss at iteration 19651: 0.04201479355405391\n",
      "Train Loss at iteration 19652: 0.04201478139834183\n",
      "Train Loss at iteration 19653: 0.042014769244641224\n",
      "Train Loss at iteration 19654: 0.04201475709295173\n",
      "Train Loss at iteration 19655: 0.04201474494327302\n",
      "Train Loss at iteration 19656: 0.042014732795604776\n",
      "Train Loss at iteration 19657: 0.04201472064994666\n",
      "Train Loss at iteration 19658: 0.04201470850629833\n",
      "Train Loss at iteration 19659: 0.04201469636465946\n",
      "Train Loss at iteration 19660: 0.042014684225029734\n",
      "Train Loss at iteration 19661: 0.04201467208740878\n",
      "Train Loss at iteration 19662: 0.0420146599517963\n",
      "Train Loss at iteration 19663: 0.04201464781819194\n",
      "Train Loss at iteration 19664: 0.042014635686595386\n",
      "Train Loss at iteration 19665: 0.0420146235570063\n",
      "Train Loss at iteration 19666: 0.04201461142942436\n",
      "Train Loss at iteration 19667: 0.04201459930384919\n",
      "Train Loss at iteration 19668: 0.04201458718028052\n",
      "Train Loss at iteration 19669: 0.042014575058717966\n",
      "Train Loss at iteration 19670: 0.042014562939161226\n",
      "Train Loss at iteration 19671: 0.042014550821609965\n",
      "Train Loss at iteration 19672: 0.042014538706063836\n",
      "Train Loss at iteration 19673: 0.04201452659252251\n",
      "Train Loss at iteration 19674: 0.04201451448098568\n",
      "Train Loss at iteration 19675: 0.04201450237145298\n",
      "Train Loss at iteration 19676: 0.0420144902639241\n",
      "Train Loss at iteration 19677: 0.042014478158398705\n",
      "Train Loss at iteration 19678: 0.042014466054876465\n",
      "Train Loss at iteration 19679: 0.04201445395335703\n",
      "Train Loss at iteration 19680: 0.04201444185384009\n",
      "Train Loss at iteration 19681: 0.042014429756325314\n",
      "Train Loss at iteration 19682: 0.042014417660812355\n",
      "Train Loss at iteration 19683: 0.04201440556730089\n",
      "Train Loss at iteration 19684: 0.04201439347579059\n",
      "Train Loss at iteration 19685: 0.04201438138628113\n",
      "Train Loss at iteration 19686: 0.04201436929877217\n",
      "Train Loss at iteration 19687: 0.04201435721326337\n",
      "Train Loss at iteration 19688: 0.0420143451297544\n",
      "Train Loss at iteration 19689: 0.042014333048244944\n",
      "Train Loss at iteration 19690: 0.04201432096873468\n",
      "Train Loss at iteration 19691: 0.04201430889122325\n",
      "Train Loss at iteration 19692: 0.042014296815710325\n",
      "Train Loss at iteration 19693: 0.042014284742195596\n",
      "Train Loss at iteration 19694: 0.04201427267067872\n",
      "Train Loss at iteration 19695: 0.04201426060115936\n",
      "Train Loss at iteration 19696: 0.04201424853363721\n",
      "Train Loss at iteration 19697: 0.042014236468111914\n",
      "Train Loss at iteration 19698: 0.04201422440458314\n",
      "Train Loss at iteration 19699: 0.04201421234305057\n",
      "Train Loss at iteration 19700: 0.042014200283513874\n",
      "Train Loss at iteration 19701: 0.042014188225972714\n",
      "Train Loss at iteration 19702: 0.04201417617042677\n",
      "Train Loss at iteration 19703: 0.042014164116875696\n",
      "Train Loss at iteration 19704: 0.04201415206531919\n",
      "Train Loss at iteration 19705: 0.04201414001575689\n",
      "Train Loss at iteration 19706: 0.04201412796818848\n",
      "Train Loss at iteration 19707: 0.04201411592261363\n",
      "Train Loss at iteration 19708: 0.04201410387903202\n",
      "Train Loss at iteration 19709: 0.042014091837443285\n",
      "Train Loss at iteration 19710: 0.04201407979784714\n",
      "Train Loss at iteration 19711: 0.04201406776024324\n",
      "Train Loss at iteration 19712: 0.042014055724631225\n",
      "Train Loss at iteration 19713: 0.04201404369101081\n",
      "Train Loss at iteration 19714: 0.042014031659381654\n",
      "Train Loss at iteration 19715: 0.0420140196297434\n",
      "Train Loss at iteration 19716: 0.042014007602095754\n",
      "Train Loss at iteration 19717: 0.042013995576438354\n",
      "Train Loss at iteration 19718: 0.0420139835527709\n",
      "Train Loss at iteration 19719: 0.04201397153109304\n",
      "Train Loss at iteration 19720: 0.04201395951140448\n",
      "Train Loss at iteration 19721: 0.04201394749370484\n",
      "Train Loss at iteration 19722: 0.04201393547799383\n",
      "Train Loss at iteration 19723: 0.042013923464271095\n",
      "Train Loss at iteration 19724: 0.04201391145253632\n",
      "Train Loss at iteration 19725: 0.04201389944278918\n",
      "Train Loss at iteration 19726: 0.042013887435029346\n",
      "Train Loss at iteration 19727: 0.04201387542925648\n",
      "Train Loss at iteration 19728: 0.042013863425470255\n",
      "Train Loss at iteration 19729: 0.04201385142367034\n",
      "Train Loss at iteration 19730: 0.04201383942385641\n",
      "Train Loss at iteration 19731: 0.04201382742602814\n",
      "Train Loss at iteration 19732: 0.0420138154301852\n",
      "Train Loss at iteration 19733: 0.042013803436327254\n",
      "Train Loss at iteration 19734: 0.04201379144445399\n",
      "Train Loss at iteration 19735: 0.04201377945456505\n",
      "Train Loss at iteration 19736: 0.042013767466660135\n",
      "Train Loss at iteration 19737: 0.04201375548073891\n",
      "Train Loss at iteration 19738: 0.04201374349680103\n",
      "Train Loss at iteration 19739: 0.04201373151484619\n",
      "Train Loss at iteration 19740: 0.04201371953487405\n",
      "Train Loss at iteration 19741: 0.04201370755688427\n",
      "Train Loss at iteration 19742: 0.04201369558087655\n",
      "Train Loss at iteration 19743: 0.042013683606850545\n",
      "Train Loss at iteration 19744: 0.042013671634805926\n",
      "Train Loss at iteration 19745: 0.04201365966474237\n",
      "Train Loss at iteration 19746: 0.042013647696659544\n",
      "Train Loss at iteration 19747: 0.042013635730557115\n",
      "Train Loss at iteration 19748: 0.04201362376643478\n",
      "Train Loss at iteration 19749: 0.04201361180429219\n",
      "Train Loss at iteration 19750: 0.042013599844129\n",
      "Train Loss at iteration 19751: 0.04201358788594494\n",
      "Train Loss at iteration 19752: 0.04201357592973963\n",
      "Train Loss at iteration 19753: 0.04201356397551275\n",
      "Train Loss at iteration 19754: 0.042013552023264\n",
      "Train Loss at iteration 19755: 0.04201354007299302\n",
      "Train Loss at iteration 19756: 0.0420135281246995\n",
      "Train Loss at iteration 19757: 0.042013516178383115\n",
      "Train Loss at iteration 19758: 0.04201350423404354\n",
      "Train Loss at iteration 19759: 0.042013492291680436\n",
      "Train Loss at iteration 19760: 0.04201348035129348\n",
      "Train Loss at iteration 19761: 0.04201346841288233\n",
      "Train Loss at iteration 19762: 0.0420134564764467\n",
      "Train Loss at iteration 19763: 0.042013444541986224\n",
      "Train Loss at iteration 19764: 0.04201343260950059\n",
      "Train Loss at iteration 19765: 0.04201342067898948\n",
      "Train Loss at iteration 19766: 0.04201340875045254\n",
      "Train Loss at iteration 19767: 0.04201339682388948\n",
      "Train Loss at iteration 19768: 0.04201338489929995\n",
      "Train Loss at iteration 19769: 0.042013372976683616\n",
      "Train Loss at iteration 19770: 0.042013361056040165\n",
      "Train Loss at iteration 19771: 0.042013349137369274\n",
      "Train Loss at iteration 19772: 0.04201333722067061\n",
      "Train Loss at iteration 19773: 0.042013325305943844\n",
      "Train Loss at iteration 19774: 0.04201331339318865\n",
      "Train Loss at iteration 19775: 0.04201330148240472\n",
      "Train Loss at iteration 19776: 0.0420132895735917\n",
      "Train Loss at iteration 19777: 0.04201327766674928\n",
      "Train Loss at iteration 19778: 0.042013265761877125\n",
      "Train Loss at iteration 19779: 0.04201325385897492\n",
      "Train Loss at iteration 19780: 0.04201324195804233\n",
      "Train Loss at iteration 19781: 0.04201323005907903\n",
      "Train Loss at iteration 19782: 0.04201321816208471\n",
      "Train Loss at iteration 19783: 0.04201320626705902\n",
      "Train Loss at iteration 19784: 0.04201319437400165\n",
      "Train Loss at iteration 19785: 0.04201318248291227\n",
      "Train Loss at iteration 19786: 0.042013170593790554\n",
      "Train Loss at iteration 19787: 0.04201315870663616\n",
      "Train Loss at iteration 19788: 0.04201314682144879\n",
      "Train Loss at iteration 19789: 0.042013134938228114\n",
      "Train Loss at iteration 19790: 0.042013123056973786\n",
      "Train Loss at iteration 19791: 0.0420131111776855\n",
      "Train Loss at iteration 19792: 0.04201309930036291\n",
      "Train Loss at iteration 19793: 0.042013087425005737\n",
      "Train Loss at iteration 19794: 0.04201307555161359\n",
      "Train Loss at iteration 19795: 0.042013063680186195\n",
      "Train Loss at iteration 19796: 0.042013051810723206\n",
      "Train Loss at iteration 19797: 0.04201303994322431\n",
      "Train Loss at iteration 19798: 0.04201302807768916\n",
      "Train Loss at iteration 19799: 0.04201301621411745\n",
      "Train Loss at iteration 19800: 0.04201300435250885\n",
      "Train Loss at iteration 19801: 0.04201299249286304\n",
      "Train Loss at iteration 19802: 0.042012980635179685\n",
      "Train Loss at iteration 19803: 0.04201296877945846\n",
      "Train Loss at iteration 19804: 0.04201295692569904\n",
      "Train Loss at iteration 19805: 0.042012945073901124\n",
      "Train Loss at iteration 19806: 0.04201293322406436\n",
      "Train Loss at iteration 19807: 0.042012921376188436\n",
      "Train Loss at iteration 19808: 0.04201290953027301\n",
      "Train Loss at iteration 19809: 0.04201289768631778\n",
      "Train Loss at iteration 19810: 0.04201288584432242\n",
      "Train Loss at iteration 19811: 0.04201287400428659\n",
      "Train Loss at iteration 19812: 0.04201286216620999\n",
      "Train Loss at iteration 19813: 0.042012850330092257\n",
      "Train Loss at iteration 19814: 0.042012838495933104\n",
      "Train Loss at iteration 19815: 0.04201282666373219\n",
      "Train Loss at iteration 19816: 0.042012814833489204\n",
      "Train Loss at iteration 19817: 0.04201280300520379\n",
      "Train Loss at iteration 19818: 0.04201279117887566\n",
      "Train Loss at iteration 19819: 0.042012779354504474\n",
      "Train Loss at iteration 19820: 0.04201276753208991\n",
      "Train Loss at iteration 19821: 0.04201275571163165\n",
      "Train Loss at iteration 19822: 0.04201274389312935\n",
      "Train Loss at iteration 19823: 0.0420127320765827\n",
      "Train Loss at iteration 19824: 0.04201272026199138\n",
      "Train Loss at iteration 19825: 0.04201270844935508\n",
      "Train Loss at iteration 19826: 0.04201269663867344\n",
      "Train Loss at iteration 19827: 0.04201268482994618\n",
      "Train Loss at iteration 19828: 0.042012673023172936\n",
      "Train Loss at iteration 19829: 0.04201266121835341\n",
      "Train Loss at iteration 19830: 0.042012649415487255\n",
      "Train Loss at iteration 19831: 0.042012637614574175\n",
      "Train Loss at iteration 19832: 0.04201262581561383\n",
      "Train Loss at iteration 19833: 0.042012614018605905\n",
      "Train Loss at iteration 19834: 0.042012602223550084\n",
      "Train Loss at iteration 19835: 0.042012590430446015\n",
      "Train Loss at iteration 19836: 0.04201257863929341\n",
      "Train Loss at iteration 19837: 0.04201256685009192\n",
      "Train Loss at iteration 19838: 0.04201255506284123\n",
      "Train Loss at iteration 19839: 0.042012543277541024\n",
      "Train Loss at iteration 19840: 0.04201253149419096\n",
      "Train Loss at iteration 19841: 0.042012519712790755\n",
      "Train Loss at iteration 19842: 0.04201250793334004\n",
      "Train Loss at iteration 19843: 0.04201249615583852\n",
      "Train Loss at iteration 19844: 0.04201248438028586\n",
      "Train Loss at iteration 19845: 0.042012472606681756\n",
      "Train Loss at iteration 19846: 0.04201246083502586\n",
      "Train Loss at iteration 19847: 0.04201244906531787\n",
      "Train Loss at iteration 19848: 0.04201243729755745\n",
      "Train Loss at iteration 19849: 0.042012425531744294\n",
      "Train Loss at iteration 19850: 0.04201241376787806\n",
      "Train Loss at iteration 19851: 0.04201240200595844\n",
      "Train Loss at iteration 19852: 0.0420123902459851\n",
      "Train Loss at iteration 19853: 0.042012378487957726\n",
      "Train Loss at iteration 19854: 0.04201236673187599\n",
      "Train Loss at iteration 19855: 0.042012354977739585\n",
      "Train Loss at iteration 19856: 0.04201234322554818\n",
      "Train Loss at iteration 19857: 0.042012331475301444\n",
      "Train Loss at iteration 19858: 0.042012319726999064\n",
      "Train Loss at iteration 19859: 0.04201230798064073\n",
      "Train Loss at iteration 19860: 0.04201229623622609\n",
      "Train Loss at iteration 19861: 0.04201228449375485\n",
      "Train Loss at iteration 19862: 0.04201227275322667\n",
      "Train Loss at iteration 19863: 0.04201226101464125\n",
      "Train Loss at iteration 19864: 0.04201224927799824\n",
      "Train Loss at iteration 19865: 0.04201223754329734\n",
      "Train Loss at iteration 19866: 0.042012225810538234\n",
      "Train Loss at iteration 19867: 0.04201221407972057\n",
      "Train Loss at iteration 19868: 0.042012202350844054\n",
      "Train Loss at iteration 19869: 0.04201219062390836\n",
      "Train Loss at iteration 19870: 0.042012178898913156\n",
      "Train Loss at iteration 19871: 0.04201216717585813\n",
      "Train Loss at iteration 19872: 0.04201215545474296\n",
      "Train Loss at iteration 19873: 0.042012143735567324\n",
      "Train Loss at iteration 19874: 0.042012132018330914\n",
      "Train Loss at iteration 19875: 0.04201212030303338\n",
      "Train Loss at iteration 19876: 0.04201210858967441\n",
      "Train Loss at iteration 19877: 0.042012096878253705\n",
      "Train Loss at iteration 19878: 0.04201208516877093\n",
      "Train Loss at iteration 19879: 0.042012073461225756\n",
      "Train Loss at iteration 19880: 0.042012061755617884\n",
      "Train Loss at iteration 19881: 0.04201205005194696\n",
      "Train Loss at iteration 19882: 0.0420120383502127\n",
      "Train Loss at iteration 19883: 0.04201202665041476\n",
      "Train Loss at iteration 19884: 0.04201201495255283\n",
      "Train Loss at iteration 19885: 0.04201200325662658\n",
      "Train Loss at iteration 19886: 0.0420119915626357\n",
      "Train Loss at iteration 19887: 0.042011979870579856\n",
      "Train Loss at iteration 19888: 0.042011968180458756\n",
      "Train Loss at iteration 19889: 0.04201195649227204\n",
      "Train Loss at iteration 19890: 0.04201194480601942\n",
      "Train Loss at iteration 19891: 0.04201193312170056\n",
      "Train Loss at iteration 19892: 0.04201192143931515\n",
      "Train Loss at iteration 19893: 0.042011909758862855\n",
      "Train Loss at iteration 19894: 0.04201189808034339\n",
      "Train Loss at iteration 19895: 0.042011886403756374\n",
      "Train Loss at iteration 19896: 0.04201187472910154\n",
      "Train Loss at iteration 19897: 0.042011863056378565\n",
      "Train Loss at iteration 19898: 0.0420118513855871\n",
      "Train Loss at iteration 19899: 0.042011839716726826\n",
      "Train Loss at iteration 19900: 0.04201182804979746\n",
      "Train Loss at iteration 19901: 0.04201181638479865\n",
      "Train Loss at iteration 19902: 0.0420118047217301\n",
      "Train Loss at iteration 19903: 0.042011793060591475\n",
      "Train Loss at iteration 19904: 0.042011781401382446\n",
      "Train Loss at iteration 19905: 0.04201176974410272\n",
      "Train Loss at iteration 19906: 0.042011758088751947\n",
      "Train Loss at iteration 19907: 0.042011746435329844\n",
      "Train Loss at iteration 19908: 0.04201173478383606\n",
      "Train Loss at iteration 19909: 0.0420117231342703\n",
      "Train Loss at iteration 19910: 0.04201171148663222\n",
      "Train Loss at iteration 19911: 0.04201169984092153\n",
      "Train Loss at iteration 19912: 0.04201168819713788\n",
      "Train Loss at iteration 19913: 0.04201167655528097\n",
      "Train Loss at iteration 19914: 0.04201166491535047\n",
      "Train Loss at iteration 19915: 0.04201165327734609\n",
      "Train Loss at iteration 19916: 0.04201164164126747\n",
      "Train Loss at iteration 19917: 0.042011630007114324\n",
      "Train Loss at iteration 19918: 0.04201161837488631\n",
      "Train Loss at iteration 19919: 0.042011606744583124\n",
      "Train Loss at iteration 19920: 0.04201159511620445\n",
      "Train Loss at iteration 19921: 0.04201158348974997\n",
      "Train Loss at iteration 19922: 0.04201157186521933\n",
      "Train Loss at iteration 19923: 0.04201156024261227\n",
      "Train Loss at iteration 19924: 0.04201154862192842\n",
      "Train Loss at iteration 19925: 0.04201153700316749\n",
      "Train Loss at iteration 19926: 0.04201152538632916\n",
      "Train Loss at iteration 19927: 0.0420115137714131\n",
      "Train Loss at iteration 19928: 0.042011502158419006\n",
      "Train Loss at iteration 19929: 0.04201149054734655\n",
      "Train Loss at iteration 19930: 0.04201147893819543\n",
      "Train Loss at iteration 19931: 0.042011467330965294\n",
      "Train Loss at iteration 19932: 0.042011455725655844\n",
      "Train Loss at iteration 19933: 0.04201144412226678\n",
      "Train Loss at iteration 19934: 0.042011432520797753\n",
      "Train Loss at iteration 19935: 0.042011420921248474\n",
      "Train Loss at iteration 19936: 0.0420114093236186\n",
      "Train Loss at iteration 19937: 0.04201139772790783\n",
      "Train Loss at iteration 19938: 0.042011386134115834\n",
      "Train Loss at iteration 19939: 0.04201137454224232\n",
      "Train Loss at iteration 19940: 0.042011362952286926\n",
      "Train Loss at iteration 19941: 0.042011351364249376\n",
      "Train Loss at iteration 19942: 0.04201133977812933\n",
      "Train Loss at iteration 19943: 0.04201132819392648\n",
      "Train Loss at iteration 19944: 0.042011316611640516\n",
      "Train Loss at iteration 19945: 0.042011305031271096\n",
      "Train Loss at iteration 19946: 0.04201129345281792\n",
      "Train Loss at iteration 19947: 0.04201128187628067\n",
      "Train Loss at iteration 19948: 0.042011270301659034\n",
      "Train Loss at iteration 19949: 0.042011258728952686\n",
      "Train Loss at iteration 19950: 0.042011247158161305\n",
      "Train Loss at iteration 19951: 0.04201123558928459\n",
      "Train Loss at iteration 19952: 0.04201122402232222\n",
      "Train Loss at iteration 19953: 0.04201121245727386\n",
      "Train Loss at iteration 19954: 0.042011200894139206\n",
      "Train Loss at iteration 19955: 0.04201118933291795\n",
      "Train Loss at iteration 19956: 0.04201117777360977\n",
      "Train Loss at iteration 19957: 0.042011166216214334\n",
      "Train Loss at iteration 19958: 0.04201115466073134\n",
      "Train Loss at iteration 19959: 0.042011143107160476\n",
      "Train Loss at iteration 19960: 0.042011131555501416\n",
      "Train Loss at iteration 19961: 0.04201112000575385\n",
      "Train Loss at iteration 19962: 0.04201110845791746\n",
      "Train Loss at iteration 19963: 0.04201109691199191\n",
      "Train Loss at iteration 19964: 0.04201108536797692\n",
      "Train Loss at iteration 19965: 0.04201107382587216\n",
      "Train Loss at iteration 19966: 0.04201106228567729\n",
      "Train Loss at iteration 19967: 0.04201105074739203\n",
      "Train Loss at iteration 19968: 0.04201103921101604\n",
      "Train Loss at iteration 19969: 0.042011027676549004\n",
      "Train Loss at iteration 19970: 0.04201101614399064\n",
      "Train Loss at iteration 19971: 0.04201100461334058\n",
      "Train Loss at iteration 19972: 0.04201099308459854\n",
      "Train Loss at iteration 19973: 0.04201098155776419\n",
      "Train Loss at iteration 19974: 0.042010970032837235\n",
      "Train Loss at iteration 19975: 0.04201095850981734\n",
      "Train Loss at iteration 19976: 0.0420109469887042\n",
      "Train Loss at iteration 19977: 0.042010935469497496\n",
      "Train Loss at iteration 19978: 0.04201092395219692\n",
      "Train Loss at iteration 19979: 0.04201091243680213\n",
      "Train Loss at iteration 19980: 0.04201090092331285\n",
      "Train Loss at iteration 19981: 0.04201088941172873\n",
      "Train Loss at iteration 19982: 0.042010877902049465\n",
      "Train Loss at iteration 19983: 0.042010866394274746\n",
      "Train Loss at iteration 19984: 0.04201085488840426\n",
      "Train Loss at iteration 19985: 0.04201084338443769\n",
      "Train Loss at iteration 19986: 0.042010831882374707\n",
      "Train Loss at iteration 19987: 0.042010820382215014\n",
      "Train Loss at iteration 19988: 0.042010808883958285\n",
      "Train Loss at iteration 19989: 0.04201079738760422\n",
      "Train Loss at iteration 19990: 0.04201078589315249\n",
      "Train Loss at iteration 19991: 0.04201077440060279\n",
      "Train Loss at iteration 19992: 0.04201076290995478\n",
      "Train Loss at iteration 19993: 0.04201075142120816\n",
      "Train Loss at iteration 19994: 0.04201073993436263\n",
      "Train Loss at iteration 19995: 0.04201072844941788\n",
      "Train Loss at iteration 19996: 0.04201071696637356\n",
      "Train Loss at iteration 19997: 0.04201070548522939\n",
      "Train Loss at iteration 19998: 0.04201069400598502\n",
      "Train Loss at iteration 19999: 0.042010682528640174\n",
      "Final Train Loss: 0.042010682528640174\n",
      "Time elapsed: 36.88\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "regressor = LinearRegression(lr = 1e-1, n_iters= 20000)\n",
    "start = time.time()\n",
    "regressor.fit(trainPredictor, trainResponse)\n",
    "end = time.time()\n",
    "print(f'Time elapsed: {end - start:.4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "regressor.plot_loss_history()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = regressor.inference(testPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Performance: 0.041648782192216044\n"
     ]
    }
   ],
   "source": [
    "def mse(testResponse, predictions):\n",
    "    error = testResponse - predictions\n",
    "    squaredError = np.dot(error.T, error)\n",
    "    meanSquaredError = 1/(testResponse.size) * squaredError\n",
    "    return meanSquaredError\n",
    "\n",
    "predictions = regressor.inference(valPredictor)\n",
    "MSE = mse(valResponse, predictions)\n",
    "print(f'Validation Performance: {MSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Performance: 0.0420862047791961\n"
     ]
    }
   ],
   "source": [
    "predictions = regressor.inference(testPredictor)\n",
    "MSE = mse(testResponse, predictions)\n",
    "print(f'Test Performance: {MSE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.13296271] Response: 0.0388\n"
     ]
    }
   ],
   "source": [
    "# Demo prediction\n",
    "demoInstanceLoc = 11401\n",
    "demoPredictor = predictorData[demoInstanceLoc]\n",
    "demoResponse = responseData[demoInstanceLoc]\n",
    "demoPredictor = np.expand_dims(demoPredictor, axis=0)\n",
    "demoPrediction = regressor.inference(demoPredictor)\n",
    "print('Prediction:', demoPrediction, 'Response:', demoResponse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
